                                                Understanding the Linux Kernel (Third Edition)
Linux kernel version : 2.6.34.1

Chapter 1 : Introducation
    Linux joins System V Release 4 (SVR4),developed by AT&T.
    other UNIX OS or UNIX-like OS :
      4.4 BSD
      Digital UNIX
      AIX
      HP-UX
      Solaris
      Mac OS X
      ...

      (opensource)
      FreeBSD
      NetBSD
      OpenBSD
      Linux
      ...

    The common attributes between Linux and well-known commercial Unix kernels :
      monolithic kernel
      compiled and statically linked traditional Unix kernels
      (of course support module for dynamically load and unload)
      kernel threading
      multithreaded application support(lightweight process LWP supporting)
      preemptive kernel
      multiprocessor support(SMP)
      filesystem

      streams(but Linux has no analogue to the STREAMS I/O subsystem introduced in SVR4)  

    The advantages of Linux :
      Linux is cost-free
      Linux is fully customizable in all its components
      Linux runs on low-end,inexpensive hardware platforms
      Linux is powerful
      Linux developers are excellent programmers
      The linux kernel can be very small and compact
      Linux is highly compatible with many common operating systems
      Linux is well supported

    Basic Operating System Concepts :
      the operating system must fulfill two main objectives :
        >  interact with the hardware components,servicing all low-level programmable elements
	   included in the hardware platform.
	>  provide an execution environment to the applications that run on the computer system.
	
      modern OS does not allow user program interact with hardware directly and forbid them to
      access arbitrary memory locations.
      in particular,the hardware introduces at least two different execution modes for the CPU :
        a nonprivileged mode for user programs
	a privileged mode for the kernel
	/*  UNIX calls these
	 *    User Mode and Kernel Mode
	 */

    Multiuser Systems :
      A multiuser system is a computer that is able to concurrently and independently execute several
      applications belonging to two or more users.

      Concurrently :
        applications can be active at the same time and contend for various resources such as CPU,memory,
	hard disks,and so on.
      Independently :
        each application can perform its task with no concern for what the applications of the other uses are
	doing.

      Multiuser operating systems must include several features :
        >  An authentication mechanism for verifying the user's identify
	>  A protection mechanism against buggy user programs that could block other applications running 
	   in the system
	>  A protection mechanism against malicious user programs that could interfere with or spy on the 
	   activity of other users
	>  An accounting mechanism that limits the amount of resource units assigned to each user

      To ensure safe protection mechanisms,operating systems must use the hardware protection associated 
      with the CPU privileged mode.
      Unix is a multiuser system that enforces the hardware protection of system resources.

    Users and Groups :
      in a multiuser system,each user has a private space on the machine,the operating system must ensure that
      the private portion of a user space is visible only to its owner.Unix-like system use UserID or UID as
      user identifier,it is a unique number.
      to selectively share material with other users,each user is a member of one or more user groups,which 
      are identified by a unique number called a user group ID.each file is associated with exactly one group.
      any Unix-like operating system has a special user called root or superuser,the root user can do almost
      everything,because the operating system does not apply the usual protection mechanism to it.

    Processes :
      A process can be defined either as "an instance of a program in execution" or  as the "execution context"
      of a running program.
      in traditional operating systems,a process executes a single sequence of instructions in an address space;
      the address space is the set of memory addresses that the process is allowed to reference.

      multiple processes environment :
        in modern operating system allow processes with multiple execution flows,that is,multiple sequences
	of instructions executed in the same address space.
	system that allow concurent active processes are said to be multiprogramming or multiprocessing.

	multiuser systems must enforce an multiple processes environment,but some multiprocessing operating
	systems are not multiuser.

	it is important to distinguish programs from processes;several processes can execute the same program
	concurently,while the same process can execute several programs sequentially.

	scheduler is an important system compoent in multiprocessing system,which process should be hold the 
	CPU to execute is determined by scheduler.
	
	preemptable :
	  some operating systems allow only nonpreemptable processes,which means that the scheduler is invoked
	  only when a process voluntarily relinquishes the CPU.but process of a multiuser system must be
	  preemptable.

	  UNIX is a multiuer and multiprocessing operating system with  preemptable processes.

	UNIX-like operating systems adopt a process/kernel model.each process has the illusion that it is the
	only process on the machine,and it has exclusive access to the operating system services.

    Kernel Architecture :
      monolithic kernel,each kernel layer is integrated into the whole kernel program and runs in Kernel Mode
      on behalf of the current process.

      microkernel operating systems demand a very small set of functions from the kernel,generally including a 
      few synchronization primitives,a simple scheduler,and an interprocess communication mechanism.
      several system processes that run on top of the microkernel implement other operating system layer functions,
      like memory allocators,device drivers,and system call handlers.

      comparison between monolithic kernel and microkernel :
        1>  microkernel is slower than monolithic kernel,because kernel layer communication has a cost.
	2>  microkernel force the system programmers to adopt a modularized approach,that means microkernel
	    is very modularized.every relativly kernel layer is independent program that must interact with
	    the other layers through well-defined and clean software interfaces.
	3>  microkernel can be easily ported to other architectures fairly easily.all hardware-dependent components
	    are generally encapsulated in the microkernel code.
        4>  microkernel tend to make better use of random access memory than monolithic kernel,system processes
	    that are not implementing needed functionalities might be swapped out or destroyed.

      UNIX and UNIX-like operating system almost satisfy all advantages of microkernel.

      the main advantages of using modules include :
        >  a modularized approach.
	>  platform independence
	>  frugal main memory usage
	>  no performance penalty

    An Overview of Unix Filesystem :
      the UNIX operating system design is centered on its filesystem,which has several interesting characteristics.

        Files - A Unix file is an information container structured as a sequence of bytes;the kernel does not
	      	interpret the contents of a file.

        Hard and Soft Links - A filename included in a directory is called a file hard link,or more simply,a link.
	     	      	      Soft links also called symbolic links,symbolic links are short files that contain an
			      arbitrary pathname of another file.

			      Hard links limitations :
			        it is not possible to create hard links for directories.
				links can be created only among files included in the same filesystem.

	File Types - File type represent what the file is.
	     	     Unix files may have one of the following types :
		       Regular file
		       Directory
		       Symbolic link
		       Block-oriented device file
		       Character-oriented device file
		       Pipe and named pipe
		       Socket

	File Descriptor and Inode - A file descriptor is a number greater than or equal to zero,the number
	     		    	    associared with a file data structure which represent the file opened by
				    process.
				    All information needed by the filesystem to handle a file is included in a 
				    data structure called an inode.each file has its own inode,which the filesystem
				    uses to identify the file.

        Access Rights and File Mode - there are three types of access rights :
	       	      	       	      read
				      write
				      execute
				      /*  access rights occupied nine bits  */
				      
				      Three additonal flags :
				      suid(Set User ID)
				      sgid(Set Group ID)
				      sticky - an executable file with the sticky flag set corresponds to request to
				      	       the kernel to keep the program in memory after its execution terminates.

				      File Mode is a mode mixed Access Rights and Additonal flags.

         File-Handling System Calls -  kernel provide some primitives to user mode to help user interact with actual
	 	       	      	       file stored in block device.
				       opening a file - open
				       accessing an opened file - read write lseek ...
				       closing a file - close
				       renaming and deleting a file - rename unlink link ...
				       
    An Overview of Unix Kernels :
      Unix kernels provide an execution environment in which applications may run.
      
      The Process/Kernel Model - Actually,some CPUs can have more than two execution states,but all standard Unix
      	  		       	 kernels use only Kernel Mode and User Mode.
				 A program usually executes in User Mode and switches to Kernel Mode only when requesting
				 a service provided by the kernel.when the kernel has satisfied the program's request,it
				 puts the program back in User Mode.the way is system calls.
				 after sets up parameters of syscall,then executes the hardware-dependent CPU instruction to
				 switch from User Mode to Kernel Mode.
				 Unix systems include a few privileged processes called kernel threads with the following
				 characteristics :
				   >  they run in Kernel Mode in the kernel address space
				   >  they do not interact with users,and thus do not require terminal devices
				   >  they are usually created during system startup and remain alive until the system
				      is shut down.
				 Unix kernels do much more than handle system calls;in fact,kernel routines can be actived
				 in several ways :
				   >  A process invokes a system call
				   >  the CPU executing the process signals an exception,which is an unusual condition such as
				      an invalid instruction.the kernel handles the exception on behalf of the process that 
				      caused it.
				   >  A peripheral device issues an interrupt signal to the CPU to notify it of an event such
				      as a request for attention,a status change,of the completion of an I/O operation.
				      each interrupt signal is dealt by a kernel program called an interrupt handler.
				   >  A kernel thread is executed.because it runs in Kernel Mode,the corresponding program
				      must be considered part of the kernel.

      Process Implementation - To let the kernel manage processes,each process is represented by a process descriptor that
      	      		       includes information about the current state of the process.
			       that include :
			         >  the program counter(PC) and stack pointer(SP) registers
				 >  the general purpose registers
				 >  the floating point registers
				 >  the processor control registers(Processor Status Word) containing information about
				    the CPU state
				 >  the memory management registers used to keep track of the RAM accessed by the process

			       When a process is scheduled,then kernel use the former stored information to recover process status,
			       and set IP to the next instruction.(it also known as PC,process counter,but IP is a instruction
			       register)

      Reentrant Kernels - All Unix kernels are reentrant.this means that several processes may be executing in Kernel Mode at
      			  the same time.of course,on uniprocessor systems,only one process can progress,but many can be blocked
			  in Kernel Mode when waiting for the CPU or the completion of some I/O operation.
			  reentrant functions : the functions they modify only local variables and do not alter global data 
			  	    	      	structures.
			  (reentrant functions,that is how some real-time kernels are implemented)
			  but kernel can include nonreentrant functions and use locking mechanisms to ensure that only one process
			  can execute a nonreentrant function at a time.

			  generial kernel control path :
			    Run process -> Timer interrupt -> Deal with interrupt -> Schedule tasks -> Run process -> ...

			  when one of the following events occurs,the CPU will enter a new path to do something :
			    >  a process executing in User Mode invokdes a system call.if the service would not be satisfied,
			       then scheduler pick next process to run,the former process enter sleeping until condition is
			       satisifed or wake up by a signal.
			       (system calls is triggered via soft interrupt,on x86,it is "int 0x80")
			    >  the CPU detects an exception.CPU must starts the execution of a suitable procedure.
			       after the procedure terminates,CPU return to the path before the exception is detected.
			    >  a hardware interrupt occurs,and the interrupts enabled.
			       CPU must starts processing another kernel control path to handle the interrupt.
			    >  an interrupt occurs while the CPU is running with kernel preemption enabled,and a higher priority
			       process is runnable.(lower intterupt could be preempted by higher interrupt)

			  three states of CPU current be :
			    >  running a process in User Mode			(process context)
			    >  running an exception or a system call handler	(process context)
			    >  running an interrupt handler	     		(interrupt context)

      Process Address Space - Each process runs in its private address space.A process running in User Mode refers to private stack,
      	      	      	      data,and code areas.when running in Kernel Mode,the process addresses the kernel data and code areas
			      and uses another private stack.
			      reentrant kernel,each kernel control path refers to its own private kernel stack.
			      sometimes,kernel shares process address space to another same program,certainly that is code area.
			      but process also can initiativly shares its address space to another different program.(IPC)

      Synchronization and Critical Regions - reentrant kernel requires the use of synchronization.
      		      	  	   	     if a kernel control path is suspended while acting on a kernel data structure,no other
					     kernel control path should be allowed to act on the same data structure unless it has
					     been reset to a consistent state.
					     critical region :
					       any section of code that should be finished by each process that begins it before
					       another process can enter it.

      Kernel preemption disabling - A synchronization mechanism applicable to preemptive kernel consists of disabling kernel 
      	     			    preemption before entering a critical region and reenabling it right after levaing the region.
				    nonpreemptability is not enough for multiprocessor systems,because two kernel control paths running
				    on different CPUs can concurrently access the same data structure.

      Interrupt disabling - Another synchronization mechanism for uniprocessor systems consists of disabling all hardware interrupts
      			    before entering a critical region and reenabling them right after leaving it.
			    if critical region is too large,this will down system efficiency;moreover,on a multiprocessor system,
			    disabling interrupts on the local CPU is not sufficient.

      Semaphores - A semaphore is simply a counter associated with a data structure;it is checked by all kernel threads before they
      		   try to access the data structure.
		   each semaphore may be viewed as an object composed of :
		     an integer variable
		     a list of waiting processes
		     two atomic method : down() and up()

		   down() decrease semaphore value,up() increase semaphore value.if its value is less than 0,process have to wait on
		   it until semaphore is available.

      Spin locks - some kernel data structures should be protected from being concurrently accessed by kernel control paths that run
      	   	   on different CPUs.in this case,if the time required to update the data structure is short,a semaphore could be very
		   inefficient.
		   a spink lock is very similar to semaphore,but it has no process list;when a process finds the lock closed by another
		   process,it "spins" around repeatedly,executing a tight instruction loop until the lock becomes open.
		   of course,spin locks are useless in a uniprocessor environment.

      Avoiding deadlocks - the simplest case of deadlock occurs when process p1 gains access to data structure a and process p2 gains
      	       		   access to b,but p1 then waits for b and p2 waits for a.
			   Several operating systems,including Linux,avoid this problem by requesting locks in a predefined order.

    Signals and Interprocess Communication :
      Unix signals provide a mechanism for notifying processes of system events.
      there are two kinds of system events :
        Asynchronous notifications
	Synchronous notifications

      POSIX standard defines about 20 different signals,2 of which are user-definable and may be used as a primitive mechanism for
      communication and synchronization among processes in UserMode.

      process may ignore the signal or asynchronously execute a specified procedure(signal handler),kernel provides the default
      signal handler for every signal.
      the default actions are :
        terminate the process
	write the execution context and the contents of the address space in a file(coredump) and terminate the process
	ignore the signal
	suspend the process
	resume the process's execution,if it was stopped

      SystemV IPC(AT&T's Unix System V) - semaphores, message queues, shared memory
      POSIX standard also defined some IPCs - posix semaphores, posix message queues, posix shared memory

    Process Management : 
      Unix makes a neat distinction between the process and the program it is executing.
      process is the program which is loadded into memory,it contains the resources all the program needs.
      the program it is executing,that means it had been loadded and the process is TASK_RUNNING.
      a parent process is such process it has been called fork() syscall,fork() would create a new process and its resources
      is duplicated to parent process,the process is child process.

      Copy-On-Write - this approach defers page duplication until the last moment(i.e., until the parent or the child is
      		      required to write into a page).
		      the naive implementation of fork() was quite time consuming.

      Zombie process - a process would exited if it called _exit(),and kernel send SIGCHLD to its parent.
      	     	       the zombie process is such process it had been exited but its status had not been checked,generally,
		       this work should be completed by its parent.
		       when the process exited,some resources are still effect and saved in its process address space,until
		       parent calls wait() systemcall to wait the process,that would destroy all resources.
		       if parent exited before wait its child,then kernel process init will adopts childs,that procedure calls
		       wait() periodically on its child processes.

      Process groups and login sessions - modern Unix operating systems introduce the notion of process groups to represent a 
      	      	     	       		  "job" abstraction.
					  a job,it might be combined by several processes,and they are in the same process group,
					  the group leader is the process whose PID is equal to the group ID.
					  a new process would be inserted into its parent's group initially.
					  normally,a job is treated as a single entity.
					  moder Unix kernels also introduce login sessions.
					  informally,a login session contains all processes that are descendants of the process that
					  has started a working session on a specific terminal--usually,the first command shell process
					  created for the user.
					  all processes in a process group must be in the same login session,a login session may have
					  several process groups active simultaneously;one of these process groups is always in the
					  foreground,which means that it has access to the terminal.others are in the background.

    Memory Management :
      Virtual memory -  all recent Unix systems provide a useful abstraction called virtual memory.
      	      	     	virtual memory acts as a logical layer between the application memory requests and the hardware
			Memory Management Unit.(MMU)
			its puposes and advantages :
			  several processes can be executed concurrently.
			  it is possible to run applications whose memory needs are larget than the available physical memory.
			  processes can execute a program whose code is only partially loaded in memory.
			  each process is allowed to access a subset of the available physical memory.
			  processes can share a single memory image of a library or program.
			  programs can be relocatable--that is,they can be placed anywhere in physical memory.
			  programmers can write machine-independent code,because they do not need to be concerned about physical
			  memory organization.
			the main ingredient of a virtual memory subsystem is the notion of virtual address space.

      Random access memory usage - all Unix operating systems clearly distinguish between two portions of the
      	     	    	   	   random access memory(RAM).a few megabytes are dedicated to storing the kernel
				   image.the remaining portion of RAM is usually handled by the virtual memory
				   system and is used in three possible ways :
				     to satisfy kernel requests for buffers,descriptors,and other dynamic kernel data structures.
				     to satisfy process requests for generic memory areas and for memory mapping of files.
				     to get better performance from disks and other buffered devices by means of caches.

      Kernel Memory Allocator - the kernel Memory Allocator(KMA) is a subsystem that tries to satisfy the requests fof memory areas
      	     	    	      	from all parts of the system.
				a good KMA should have the following features :
				  it must be fast.actually,this is the most crucial attribute,because it is invoked by all kernel
				  subsystems.
				  it should minimize the amount of wasted memory.
				  it should try to reduce the memory fragmentation problem.
				  it should be able to cooperate with the other memory management subsystems to borrow and release
				  page frames from them.
				all recent Unix operating systems adopt a memory allocation strategy called demand pagine.
				with demand paging,a process can start program execution with none of its pages in physical memory.

      Caching - a good part of the available physical memory is used as cache for hard disks and other block devices.this is because
      	      	hard drives are very slow.
		data read previously from disk and no longer used by any process continue to stay in RAM,and defer writing to disk as
		long as possible.
		the sync() system call forces disk synchronization by writing all of the "dirty" buffers into disk.to avoid data loss,
		all operating systems take care to periodically write dirty buffers back to disk.

    Device Drivers :
      the kernel interacts with I/O devices by means of device drivers.device drivers are included in the kernel and consist of
      data structures and functions that control one or more devices.
      each driver interacts with the remaining part of the kernel through a specific interface,this approach has the following
      advantages :
        >  device-specific code can be encapsulated in a specific module.
	>  vendors can add new devices without knowing ther kernel source code,only the interface specifications must be known.
	>  the kernel deals with all devices in a uniform way and accesses them through the same interface.
	>  it is possible to write a device driver as a module that can be dynamically loaded in the kernel without requiring
	   the system to be rebooted.it is also possible to dynamically unload a module that is no longer needed,therefore
	   minimizing the size of the kernel image strored in RAM.


Chapter 2 : Memory Addressing
    Memory Address :
      the three kinds of addresses on 80x86 microprocessors >
        1>  logical address
	    included in the machine language instructions to specify the address of an operand or of an instruction.
	    this type of address embodies the well-known 80x86 segmented architecture.
	    each logical address consists of a segment and an offset that denotes the distance from the start of
	    the segment to the actual address.

	2>  linear address(also known as virtual address)
	    a signle 32-bit unsigned integer that can be used to address up to 4GB.
	    linear addresses are usually represented in hexadecimal notation,their values range from
	    0x00000000 -- 0xffffffff

	3>  physical address
	    used to address memory cells in memory chips.
	    they correspond to the electrical signals sent along the address pins of the microprocessor to the memory bus.
	    they are represented as 32-bit or 36-bit unsigned integers.

      MMU Memory Management Unit,it transforms a logical address into a linear address by means of a hardware circuit called
      a segmentation unit.
      Page Unit,it transforms the linear address into a physical address.

      In multiprocessor systems,RAM chips may be accessed concurrently(CPUs share same memory).
      Memory Arbiter is inserted between the bus and every RAM chip,it is used to ensure serially operations perform.
      (if the RAM chip is free or it is busy servicing a request by another CPU(in this case,it delay the CPU's request))
      even uniprocessor systems use memory arbiters,because they include specialized processors called DMA(Direct Memory Access)
      controllers that operate concurrently with the CPU.
      (GPU might use DMA zone)

      /*  Memory Arbiter is unvisible to program  */

    Segmentation in Hardware :
      (80386 model)
      Intel microprocessors perform address translation in two different ways called "real mode" and "protected mode".

      Segment selectors and Segmentation registers :
        a logical address consists of two parts : a segment identifier and an offset that specifies the relative address
	within the segment.

	Logical Address : (Segment Identifier , Offset)
	segment identifier : a 16-bit field called the Segment Selector.
	offset : a 32-bit field associated the segment identifier(Segment Selector).

	Segment Selector { index (15-3), TI (2), RPL (1-0) }
	/*  TI : Table Indicator
	 *  RPL : Requestor Privilege Level
	 */

	processor provides six segmentation registers for retrive segment selector quickly :
	  cs, ss, ds, es, fs, gs
	  /*  the only purpose is to hold Segment Selectors  */
	program could reuse some segmentation registers,but have to store its contents in memory,and then
	restore it later.

	cs : code segment register,which points to a segment containing program instructions.
	ss : stack segment register,which points to a segment containing the current program stack.
	ds : data segment register,which points to a segment containing global and static data.
	es,fs,gs : these are extra segment registers available for far pointer addressing like video
		   memory and such.
        
	the remaining three segmentation registers are general purpose and may refer to arbitrary data segments.

	cs register has another important function :
	  it includes a 2-bit field that specifies the Current Privilege Level(CPL) of the CPU.
	  value 0 denotes the highest privilege level,value 3 denotes the lowest one.
	  (Linux only use 0(kernel mode) and 3(user mode))

      Segment Descriptors : 
        Global Descriptor Table(GDT)
	Local Descriptor Table(LDT)

	each segment is represented by an 8-byte segment descriptor,they are stored either in GDT or LDT.
	program is allowed to have its own LDT,if it have to stores some segments besides those stored in GDT.

	gdtr : gdtr control register,it contains the address and size of the GDT in main memory.
	ldtr : ldtr control register,it contains the address and size of the current LDT in main memory.

	Format of Segment Descriptor : (every 8-byte 64-bit)
	  Field :
	    Base - contains the linear address of the first byte of the segment.
	    G - granularity flags,if it is cleared(0),the segment size is expressed in bytes;otherwise,it is
	      	expressed in multiples of 4096 bytes.
	    Limit - holds the offset of the last memory cell in the segment,thus binding the segment length.
	    S - system flag,if it is cleared(0),the segment is a system segment that stores critical data 
	      	structures such as the Local Descriptor Table;otherwise it is a normal code or data segment.
            Type - Characterizes the segment type and its access rights.
	    DPL - Descriptor Privilege Level,used to restrict accesses to the segment.it represents the minimal
	    	  CPU privilege level requested for accessing the segment.
            P - Segment-Present flag,it is equal to 0 if the segment is not stored currently in main memory.
	      	Linux always sets this flag (bit 47) to 1,because it never swaps out whole segments to disk.
            D or B - called D or B depending on whether the segment contains code or data.
	      	     its meaning is slightly different in the two cases,but it is basically set(equal to 1) if the
		     addresses used as segment offsets are 32 bits long,and it is cleared if they are 16 bits long.
	    AVL - may be used by the operating system,but it is ignored by Linux.

	  Composing :
	    0-15 : LIMIT (0-15)
	    16-31 : BASE (0-15)
	    32-39 : BASE (16-23)
	    40-43 : TYPE
	    44 : S
	    45-46 : DPL
	    47 : P
	    48-51 : LIMIT (16-19)
	    52 : AVL
	    53 : none
	    54 : D or B
	    55 : G
	    56-63 : BASE (24-31)

	The Segment Descriptors widely used in Linux :
	  Code Segment Descriptor -
	    it may be included either GDT or LDT,the descriptor has the S flag set.
	  Data Segment Descriptor - 
	    data segment.included either GDT or LDT,has S flag set.
	    stack segments are implemented by means of generic data segments.
	  Task State Segment Descriptor(TSSD) -
	    task state segment,it is a segment used to save the contents of the processor registers;it can appear
	    only in the GDT.The corresponding Type field has the value 11 or 9,depending on whether the corresponding
	    process is currently executing on a CPU.has no S flag.
	  Local Descriptor Table Descriptor(LDTD) -
	    a segment containing an LDT;it can appear only in the GDT.
	    the corresponding Type field has the value 2.has no S flag.

	Fast Access to Segment Descriptors :
	  80x86 processor provides an additional nonprogrammable register for each of the six programmable segmentation registers.
	  each nonprogrammable register contains the 8-byte segment descriptor specified by the segment selector contained in the
	  corresponding segmentation register.
	  every time a segment selector is loaded in a segmentation register,the corresponding segment descriptor is loaded from
	  memory into the matching nonprogrammable CPU register.

	    segs1 --> cs  &&  segd1 --> unprogrammable(cs)

	  CPU only need to access GDT or LDT is the time to change the contents of the segmentation registers!

	  segment selector fields :
	    index - identifies the segment descriptor entry contained in the GDT or in the LDT.
	    TI    - table indicator,specifies whether the segment descriptor is inclued in the GDT(TI = 0) or
	    	    in the LDT(TI = 1).
	    RPL   - requestor privilege level,specifies the current privilege level of the CPU when the corresponding
	    	    segment selector is loaded in to the cs register;it also may be used to selectively weaken the
		    processor privilege level when accessing data segments.

          segment descriptor index = GDT + segment selector index field * 8 (every segment descriptor occupy 8-byte)

	  ! the first entry in GDT always set to 0,this ensures that logical address with a null segment selector will be considered
	    invalid,thus causing a processor exception.
	  ! maximum of number of segment descriptors in GDT is 8191(2^13 - 1).

	Segmentation Unit :
	  segmentation unit handle address translation.
	  translate logical address to linear address : (in : read, out : write)
	  /*  logical addres : (composed by) segment identifer, offset  */

	    if register changed,then in segment register
	    else in nonprogrammable register
	    ->
	    if TI == 1,  in LDT(ldtr)
	    else TI == 0,  in GDT(gdtr)
	    ->
	    in index
	    ->
	    segment descriptor = index * 8 + GDT
	    ->
	    in segment descriptor
	    ->
	    in BASE field
	    ->
	    linear address = BASE + offset (LIMIT determine the length of the segment)

	Segmentation in Linux :
	  in fact,segmentation and paging do same work,linear address to physical space.
	  linux prefers paging to segmentation for these reasons :
	    >  memory management is simpler when all process use the same segment register values,
	       that is,when they share the same set of linear addresses.
	    >  one of the design objectives of Linux is portability to a wide range of architectures;
	       RISC architectures in particular have limited support for segmentation.
	    /*  linux 2.6 use segmentation only when required by the 80x86 architecture.  */

	  Linux processes running in User Mode use the same pair of segments to address instructions and data.
	    user code segment AND user data segment (Segment Selector : __USER_CS, __USER_DS /* macros */)
	  Linux processes running in Kernel Mode use the same pair of segments to address instructions and data.
	    kernel code segment AND kernel data segment (Segment Selector : __KERNEL_CS, __KERNEL_DS /* macros */)

	  CS means cs register, DS means ds register.

	  all processes either in User Mode or in Kernel Mode,may use the same logical address.
	  (linear address associated such segment start at 0,and reach the addressing limit of (2^32 - 1))

	  Linux,logical address coincide with linear address.(so all segments start at 0x00000000)

	  about RPL :
	    when CPL of cs is changed,ds and ss have to correspondingly updated.
	    e.g.
	      CPL = 3, ds -> user data segment, ss -> user stack inside user data segment
	      CPL = 0, ds -> kernel data segment, ss -> kernel stack inside kernel data segment

	    ! When switching from User Mode to Kernel Mode,Linux always makes sure that the 
	      ss register contains the Segment Selector of the kernel data segment.
          
	  when saving a pointer to an instruction or to a data structure,the kernel does not need to store the
	  Segment Selector component of the logical address,because the ss register contains the current 
	  Segment Selector.
	  /*  a function,it has a stack frame.and ss contains the Segment Selector used to get Segment Descriptor
	   *  of the stack.so a pointer just contains the offset for the Segment Descriptor(in unprogrammable register).
	   *  in the case for instruction is same,but use the cs register.
	   */

	The Linux GDT :
	  uniprocessor : one GDT
	  multiprocessor : every processor one GDT

	  all GDTs are stored in the "cpu_gdt_table" array,while the addresses and sizes of the GDTs are stored in the 
	  "cpu_gdt_descr" array.(2.6.34.1,use struct "desc_struct" to represent segment descriptor)

	  each GDT includes 18 segment descriptors and 14 null,unused,or reserved entries.unused entries are inserted
	  on purpose so that Segment Descriptors usually accessed together are kept in the same 32-byte line of hardware
	  cache.
	  the 18 segment descriptors included in each GDT point to the following segments :
	    >  four user and kernel code and data segments
	    >  a task state segment(TSS),different for each processor in the system.
	       the linear address space corresponding to a TSS is a small subset of the linear address space corresponding
	       to the kernel data segment.
	       TSS stored in the "init_tss" array.
	       in particualr :
	         BASE -> index in init_tss
		 G = 0 ; if LIMIT = 0xeb (because TSS is 236 byte)
		 TYPE = 9 || 11
		 DPL = 0
	    >  a segment including the default Local Descriptor Table(LDT),usually shared by all processes.
	    >  three Thread-Local Storage(TLS) segments :
	         multithreaded applications could make used of up to TLSs containing data local to each thread.
		 syscall set_thread_area() and get_thread_area() create and release a TLS segment for the
		 executing process.
            >  three segments related to Advanced Power Management(APM) :
	         the BIOS code makes use of segments,so when the Linux APM driver invokes BIOS functions to get
		 or set the status of APM devices,it may use custom code and data segments.
            >  five segments related to Plug and Play(PnP) BIOS services.
	         Linux PnP driver invokes BIOS functions to detect the resources used by PnP devices.
		 (custom data and data segments).
	    >  a special TSS used by the kernel to handle "Double fault" exceptions.

	  each processor has its own TSS.
	  a few entries in the GDT may depend on the process that the CPU is executing(i.e. TLS).
	  in some cases a processor may temporarily modify an entry in its copy of the GDT(i.e. invoke APM's BIOS procedure).

	The Linux LDTs :
	  most Linux User Mode applications do not make use of a Local Descriptor Table.
	  thus,kernel defines a default LDT to be shared by most processes,it is stored in the "default_ldt" array.
	  default_ldt includes five entries,and two of them are used by kernel effectively :
	    a call gate for iBCS executables;
	    a call gate for Solaris/x86 executables;

	  /*  Call gate is a mechanism provided by 80x86 to change CPU privilege while invoking a predefined function */

	  process may require to set up their own LDT,via syscall modify_ldt().(i.e. Wine)
	  any custom LDT created by modify_ldt() also requires its own segment.
	  if the CPU is executing a process which has a custom LDT,then the LDT entry in CPU GDT also be changed accordingly.

	  /*  User Mode applications also may allocate new segments by means of modify_ldt()  */
	  /*  kernel never use these segments and do not keep track of the corresponding Segment Descriptor  */

    Paging in Hardware :
      the paging unit translates linear address into physical ones,it must to check the requested access type against the
      access rights of the linear address.if the memory access is not valid,it generates a Page Fault Exception.

      linear addresses are grouped in fixed-length intervals called page :
        contiguous linear addresses within a page are mapped into contiguous physical addresses.
	in this way,kernel can specify the physical address and the access rights of a page instead
	of those of all the linear addresses included in it.

      page unit thinks of all RAM as partitioned into fixed-length page frames(or physical pages),each page frame contains a page.
      a page frame is a constituent of main memory,and hence it is a storage area.
      
      ! the data structures that map linear to physical addresses are called page tables,they are stored in memory and kernel will
      	initializes them before enabling page unit.	    

      ! start with 80386,80x86 processors :
      	  if cr0.PG = 1,enable page;
	  if cr0.PG = 0,linear addresses are interpreted as physical addresses;

      regular paging :
        start with 80386,a page handles 4kB memory.
	32bit linear address : Directory(10bit), Table(10bit), Offset(12bit)
	      	     	       (most significant) (middle)     (least significant)
	translation of linear address(based on type of translation table) :
	  the physical address of the Page Directory in use is stored in a control register named cr3.
	  get Page Directory from cr3 ->
	  use Directory field to determine the Page Table associated with the linear address in Page Directory ->
	  use Table field to determine the Page Frame in the Directory ->
	  use Offset field to determine the relative position within the Page Frame

	  cr3 { Page Directory address }
	  Page Directory {
	    ...
	    Page Table
	    Page Table {	/*  10bit Directory as index  */
	      ...
	      Page Frame
	      Page Frame {      /*  10bit Table as index  */
	        address
		address		/*  12bit Offset as index for addresses in a page frame  */
		...
	      }
	    }
	  }

	  physical address = retrivePageFrame(retrivePageTable(cr3, linear.Directory), linear.Table) + Offset

	  /*  12bit Offset,each page consists of 4kB of data.  */
	  /*  10bit Directory,so Page Directory include 1024 entries;10bit Table,so Page Table include 1024 entries,
	   *  so a Page Directory can address up to 1024x1024x4096 = 2^32 memory cells.
	   */

	the entries of Page Directories and Page Tables have the same structure,each entry includes the following fields :
	  Present flag :
	    it = 1,the referred-to page(or Page Table) is contained in main memory;
	    it = 0,the referred-to page(or Page Table) is not contained in main memory,the remainder bits may be used by OS for
	    its own purpose;
	    it = 0 and access physical address via page unit,page unit will stores the linear address in cr2,and generates 
	    exception 14 : Page Fault

          Field containing the 20 most significant bits of a page frame physical address :
  	    a page frame has a 4-kB capacity,so its physical address must be multiple of 4096,so the 12 least significant
	    bits are always equal to 0.
	    it -> Page Directory,the page frame contains a Page Table;
	    it -> Page Table,the page frame contains a page of data;

	  Accessed flag :
	    page unit set each time accesses the corresponding page frame.(page unit never reset it,OS have to do this)
	    OS may be use it when selecting pages to be swapped out.
	    
	  Dirty flag :
	    Page Table entry only,it is set each time a write operation is performed on the page frame.
	    page unit never reset it,OS have to do this.

	  Read/Write flag :
	    contains the access right of the page or of the Page Table.
	    
	  User/Supervisor flag :
	    contains the privilege level required to access the page or Page Table.

	  PCD and PWT flags :
	    controls the way the page or Page Table is handled by the hardware cache.

	  Page Size flag :
	    Page Directory entry only,if it = 1,then entry refers to a 2MB- or 4MB-long page frame.

	  Global flag :
	    Page Table entry only,this flag was introduced in the Pentium Pro to prevent frequently used pages from being
	    flushed from the TLB cache.it works only if the Page Global Enable(PGE) flag of register cr4 is set.
	    
      extended paging :
        starting with the pentium model,80x86 microprocessors introduce extended paging.
	extended paging allows page frames to be 4MB.in this case,Page Table is no longer need,thus save memory and 
	preserve TLB entries.

	extended paging : Directory(10bit), Offset(22bit)

	Page Directory entries for extended paging are the same as for normal paging,except that :
	  >  the Page Size flag must be set.
	  >  Only the 10 most significant bits of the 20-bit physical address field are significant.

	extended paging coexists with regular paging;it is enabled by setting the PSE flag of the cr4 processor register.

      Hardware Protection Scheme :
        only two privilege levels are associated with pages and Page Tables,it is indicated by User/Supervisor field.
	when User/Supervisor == 0,the page can be addressed only when the CPL is less than 3.
	when User/Supervisor == 1,the page can always be addressed.

	segmentation has three types of access rights,but only two types of access rights are associated with pages.
	if Read/Write == 0,the corresponding Page Table or page can only be read;otherwise it can be read and written.
	(Read Page Directory to get Page Table,read Page Table to get page frame)

      An Example of Regular Paging :
        a process is allowed to access linear addresses from 0x20000000 - 0x2003ffff.
	Directory field is 0010000000 for all addresses,
	Table field contain values in 0000000000 - 0000111111 (0 - 63)
	Offset field contain values in 000000000000 - 111111111111

	Directory is 0x80 or 128 decimal,so the 129th entry in Page Directory will be selected,
	the 129th entry contains the physical address of the Page Table of current process.
	Table is 0 - 63,so all the remaining 1024 - 64(960) entries are filled with zeros,so
	only the first Page to 63th Page in Page Table is valid.
	Finally,use Offset to access Page Frame.

	Suppose that the process needs to read the byte at linear address 0x20021406,this address is handled by the paging
	unit as follows :
	  1>  use 0x80 to select the 129th entry in Page Directory.
	  2>  the Table field 0x21 is used to select entry 0x21 of the Page Table.
	  3>  Finally,the Offset field 0x406 is used to select the byte at offset 0x406 in the desired page frame.

	if process accessed a linear address which is outside to its linear address space,because the address is not in its
	address space,thus the address's Present flag is 0,cleared,Page Unit will issues an exception.
	all addresses are not valid for process in its linear address space,Present flag of each address will be cleared.

      The Physical Address Externsion (PAE) Paging Mechanism :
        normal CPU only supports 4GB RAM (from 80386 to the Pentium),which is used 32-bit physical addresses.
	in pratice,due to the linear address space requirements of User Mode processes,the kernel cannot directly address
	more than 1GB of RAM.
	and server needs more than 4GB RAM.
	Intel introduced a mechanism called Physical Address Extension(PAE)<36-bit physical address> on Pentium Pro processor.

	PAE is actived by setting the Physical Address Externsion flag in the cr4 control register.
	if PAE is open,then the Page Size flag in page directory entry will set to 1.

	PAE paging mechanism :
	  >:
	  the 64 GB of RAM are split into 2^24 distinct page frames,and the physical address field of Page Table entries has
	  been expanded from 20 to 24 bits.
	  the Page Table entry size has been doubled from 32bits to 64 bits,as a result,a 4-kB PAE Page Table includes 512
	  entries instead of 1024.

	  >:
	  a new level of Page Table called the Page Directory Pointer Table(PDPT) consisting of four 64-bit entries has
	  been introduced.

	  >:
	  the cr3 control register contains a 27-bit Page Directory Pointer Table base address field.because PDPTs are stored
	  in the first 4GB of RAM and aligned to a multiple of 32 bytes,27 bits are sufficient to represent the base address
	  of such tables.

	  >:
	  when mapping linear addresses to 4kB pages,the 32 bits of a linear address are interpreted in the following way :
	    cr3 Points to a PDPT
	    
	    bits 31-30
	      point to 1 of 4 possible entries in PDPT

	    bits 29-21
	      point to 1 of 512 possible entries in Page Directory

	    bits 20-12
	      point to 1 of 512 possible entries in Page Table

	    bits 11-0
	      Offset of 4-kB page

	    linear-address : { PDPT Index(31-30), Page Directory Index(29-21), Page Table Index(20-12), Offset(11-0) }

	  >:
	  when mapping linear addresses to 2-MB pages(PS flag open),the 32 bits of a linear address are interpreted in
	  the following way :
	    cr3 Points to a PDPT

	    bits 31-30
	      point to 1 of 4 possible entries in PDPT

	    bits 29-21
	      point to 1 of 512 possible entries in Page Directory

	    bits 20-0
	      Offset of 2-MB page

	    linear-address : { PDPT Index(31-30), Page Directory Index(29-21), Offset(20-0) }

        once cr3 is set,it is possible to address up to 4 GB of RAM,if we want to address more RAM,we will have to put 
	a new value in cr3 or change the content of the PDPT.
	PAE only extend physical address,User Mode processes are still address 4 GB linear address space(still 32 bits).
	    
      Paging for 64-bit Architectures : 
        Paging levels in some 64-bit architectures >
	  Platform name	      Page size		   address bits		paging levels		linear address splitting
	  alpha	   	      8kB  		   43	   		3      			10 + 10 + 10 + 13
	  ia64		      4kB		   39			3			9 + 9 + 9 + 12
	  ppc64		      4kB		   41			3			10 + 10 + 9 + 12
	  sh64		      4kB		   41			3			10 + 10 + 9 + 12
	  x86_64	      4kB		   48			4			9 + 9 + 9 + 9 + 12

	two levels paging :
	  Page Directory  <level 1>
	  Page Table	  <level 2>

	  cr3 get Page Directory address.
	  PD[Directory] -> Page Table
	  PT[Table] -> Page Frame
	  PF[Offset] -> page of data

      Hardware Cache : 
        Hardware cache memories were introduced to reduce the speed mismatch between CPU and RAM.
	they are based on the well-known locality principle,which holds both for programs and data structures.
	it makes sense to introduce a smaller and faster memory that contains the most recently used code and data.
	for this purpose,a new unit called the "line" was introduced into the 80x86 architecture.

	DRAM : dynamic RAM
	SRAM : static RAM,it is more faster than DRAM and it is on-chip

	the cache is subdivided into subsets of lines.
	there are some different strategy to determine how to store cache :
	  1>  the cache can be direct mapped,in which case a line in main memory is always stored at the exact
	      same location in the cache.
	  2>  the cache is fully associative,meaning that any line in memory can be stored at any location in the cache.
	  3>  degree N-way set associative,where any line of main memory can be stored in any one of N lines of the cache.

	      	     	   DRAM Main Memory
	      	     	       |
	      	     	       v
	CPU { SRAM cache -> Cache controller <- Paging unit }

	SRAM stores the actual lines of memory.(it is the cache memory)
	Cache controller stores an array of entries,every entry is the line of the cache memory.
	Each entry includes a tag and a few flags that describe the status of the cache line,the tag consists of some
	bits that allow the cache controller to recognize the memory location currently mapped by the line.

	The bits of the memory's physical address : { TAG, SUBSET INDEX, OFFSET }

	when accessing a RAM memory cell,the CPU extracts the subset index from the physical address and compares the tags
	of all lines in the subset with the high-order bits of the physical address.
	if a line with the same tag as the high-order bits of the address is found,the CPU has a cache hit;otherwise,it
	has a cache miss.

	when a cache hit occurs,the cache controller behaves differently,depending on the access type :
	  READ >
	    controller selects the data from the cache line and transfers it into a CPU register.

	  WRITE >
	    write-through :
	      write data into both cache line and mapped RAM.

	    write-back :
	      just write data into cache line,the controller updates RAM only when the CPU executes an instruction
	      requiring a flush of cache entries or then a FLUSH hardware signal occurs.

	when a cache miss occurs,the cache line is written to memory,if necessary,and the correct line is fetched from
	RAM into the cache entry.

	Multiprocessor system :
	  there must have an additional hardware circuitry to synchronize the cache contents.
	  whenever a CPU modifies its hardware cache,it must check whether the same data is contained in the other
	  hardware cache;if so,it must notify the other CPU to update it with the proper value.(cache snooping)

	new model have more cache,L1-cache,L2-cache,L3-cache,etc.
	linux ignore hardware details,and assumes there is a single cache.

	the CD flag of the cr0 control register is used to enable or diable the cache circuitry.
	the NW flag,in the same register,specifies whether the write-through or the write-back strategy is used for
	the caches.

	some processors allow OS associate a different cache management policy with each page frame,that is PCD flag
	in Page Directory and Page Table;and PWT(Page Write-Through),which specifies whether the write-back or the 
	write-through strategy must be applied while writing data into the page frame.
	(Linux default clear these flags)

      Translation Lookaside Buffers(TLB) :
        80x86 processors include another cache called Translation Lookaside Buffers(TLB) to speed up linear address
	translation.
	when a linear address is used for the first time,the corresponding physical address is computed through slow
	accesses to the Page Tables in RAM.the physical address is then stored in a TLB entry for further accessing.
	
	in a multiprocessor system,each CPU has its own TLB,called the local TLB of the CPU.contrary to the hardware
	cache,these local TLB need not to be synchronized,because processes running on the existing CPUs may associate
	the same linear address with different physical ones.

	when the cr3 control register of a CPU is modified,the hardware automatically invalidates all entries of the 
	local TLB,because a new set of page tables is in use and the TLBs are pointing to old data.

    Paging in Linux :
      before 2.6.11,linux paging model has three level,
      starting with 2.6.11,linux paging model has four level.
      (linux adopts a common paging model that fits both 32-bit and 64-bit architectures.)

      paging level :
        Page Global Directory
	Page Upper Directory
	Page Middle Directory
	Page Table

	PGD { PUDs }
	PUD { PMDs }
	PMD { PTs }
	PT -> Page Frame (a page frame)
	
	cr3 -> Address of PGD

      on 32-bit architecture(no PAE),PUD and PMD fields will be zero.(code still same,so it is work on 64-bit)
      but kernel keeps a position for the PUD and PMD by setting the number of entries in them to 1 and mapping
      these two entries into the proper entry of the PGD.

      on 32-bit architecture(PAE),PUD is eliminated.
      PGD -> 80x86's Page Directory Pointer Table
      PMD -> 80x86's Page Directory
      PT  -> 80x86's Page Table

      on 64-bit architecture,three or four levels of paging are used depending on the linear address bit splitting
      performed by the hardware.

      Linux's handling of processes relies heavily on paging :
        linear address to physical address,design >
	  < assign a different physical address space to each process,ensuring an efficient protection against
	    addressing errors.
	  < distinguish pages(groups of data) from page frames(physical ddress in main memory),this allows the same
	    page to be stored in a page frame,then saved to disk and later reloaded in a different page frame,
	    this is "the basic ingredient" of the virtual memory mechanism.

      The Linear Address Fields :
        the macros simplify Page Table handling >
	  PAGE_SHIFT
	    specifies the length in bits of the Offset field;
	    this macro is used by PAGE_SIZE to return the size of the page,finally,
	    the PAGE_MASK macro yields the value 0xfffff000 and is used to mask all the bits of the Offset field.

	  PMD_SHIFT
	    the total length in bits of the Offset and Table fields of a linear address;
	    the logarithm of the size of the area a Page Middle Directory entry can map.
	    the PMD_MASK macro is used to mask all the bits of the Offset and Table fields.
	    PAE -> off
	      PMD_SHIFT = 22 (12 Offset + 10 Table)
	      PMD_SIZE = 2^22 (4MB)
	      PMD_MASK = 0xffc00000

	    PAE -> on
	      PMD_SHIFT = 21 (12 Offset + 9 Table)
	      PMD_SIZE = 2^21 (2MB)
	      PMD_MASK = 0xffe00000

	    !  large pages do not make use of the last level of page tables,thus LARGE_PAGE_SIZE which yields
	       the size of a large page,is equal to PMD_SIZE(2PMD_SHIFT)
	       LARGE_PAGE_MASK is used to mask all the bits of the Offset and Table fields in a large page address,
	       is equal to PMD_MASK.

	  PUD_SHIFT
	    determines the logarithm of the size of the area a Page Upper Directory entry can map.
	    PUD_SIZE macro computes the size of the area mapped by a single entry of the Page Global Directory,
	    PUD_MASK macro is used to mask all the bits of the Offset,Table,Middle Air,and Upper Air fields.

	  PGDIR_SHIFT
	    determines the logarithm of the size of the area that a Page Global Directory entry can map.
      	    PGDIR_SIZE macro computes the size of the area mapped by a single entry of the Page Global Directory,
	    the PGDIR_MASK macro is used to mask all the bits of the Offset,Table,Middle Air,and Upper Air fields.
	    PAE -> off
	      PGDIR_SHIFT = 22 (the same value yielded by PMD_SHIFT and by PUD_SHIFT)
	      PGDIR_SIZE = 2^22 (4MB)
	      PGDIR_MASK = 0xffc00000

	    PAE -> on
	      PGDIR_SHIFT = 30 (12 Offset + 9 Table + 9 Middle Air)
	      PGDIR_SIZE = 2^30 (1GB)
	      PGDIR_MASK = 0xc0000000

	  PTRS_PER_PTE PTRS_PER_PMD PTRS_PER_PUD PTRS_PER_PGD
	    compute the number of entries in the Page Table,Page Middle Directory,Page Upper Directory,Page Global Directory.
	    they yield the values 1024, 1, 1, 1024, respectively; (PAE disabled)
	    they yield the values 512, 512, 1, 4, respectively; (PAE enabled)

      Page Table Handling :
        pte_t -> Page Table entry
	pmd_t -> Page Middle Directory entry
	pud_t -> Page Upper Directory entry
	pgd_t -> Page Global Directory entry
	pgprot_t -> the protection flags associated with a single entry        
	(64-bit PAE on,32-bit PAE off)

	conversion macros :
	  pte_t __pte(unsigned int)
	  pmd_t __pmd(unsigned int)
	  pud_t __pud(unsigned int)
	  pgd_t __pgd(unsigned int)
	  pgprot_t __pgprot(unsigned int)

	  unsigned int pte_val(pte_t)
	  unsigned int pmd_val(pmd_t)
	  unsigned int pud_val(pud_t)
	  unsigned int pgd_val(pgd_t)
	  /*  reverse casting  */

	macros for RW a page table entry operations :
	  unsigned pte_none(pte_t)
	  unsigned pmd_none(pmd_t)
	  unsigned pud_none(pud_t)
	  unsigned pgd_none(pgd_t)
	  /*  return 0 if @arg == 1,return 1 if @arg == 0  */

	  pte_clear(mm, addr, ptep)
	  pmd_clear(pmd)
	  pud_clear(pud)
	  pgd_clear(pgd)
	  /*  clear an entry of the corresponding page table  */
	  /*  forbidding a process to use the linear addresses mapped by the page table entry  */

	  static inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr, pte_t *ptep);
	  /*  clears a Page Table entry and returns the previous value  */

	  set_pte(ptep, pte)
	  set_pmd(pmdp, pmd)
	  set_pud(pudp, pud)
	  set_pgd(pgdp, pgd)
	  /*  write a given value into a page table entry  */

	  set_pte_atomic(ptep, pte)
	  /*  atomic operation  */
	  /*  if PAE on,it also ensures that the 64-bit value is written atomically  */

	  static inline int pte_same(pte_t, pte_t);
	  /*  returns 1 if @arg1 == @arg2,specify @arg1.privileges == @arg2.privileges,otherwise retuns 0  */

	  static inline int pmd_large(pmd_t pte);
	  /*  returns 1 if the Page Middle Directory entry refers to a large page(2MB or 4MB),0 otherwise  */

	  static inline int pmd_bad(pmd_t pmd);
	  /*  it is used by functions to check Page Middle Directory entries passed as input parameters.
	   *  returns 1 if the entry points to a bad Page Table :
	   *    Present flag cleared
	   *    Read/Write flag cleared
	   *    Accessed or Dirty cleared
	   *  0 otherwise.
	   */

	  static inline int pud_bad(pud_t pud);
	  static inline int pgd_bad(pgd_t pgd);
	  /*  these macros always yield 0.  */

	  no pte_bad() macro is defined,because no Present,no Read/Write,no Accessed or no Dirty is legal for
	  Page Table entry.

	  pte_present(pte_t)
	  /*  returns 1 if Present == 1 or Page Size == 1,0 otherwise  */

	  Page Size flag in Page Table entries has no meaning for the paging unit of the microprocessor,
	  but kernel marks Present = 0 and Page Size = 1 for the pages present in main memory but without read,
	  write,or execute privileges.
	  any access to such pages will triggers a Page Fault exception because Present is cleared,but kernel
	  is able to detect that the Page Fault exception is not due to a missing page by checking the value of
	  Page Size!

	  pmd_present(pmd_t pmd)
	  /*  returns 1 if the Present == 1,0 otherwise  */

	  pud_present(pud_t pud)
	  pgd_present(pgd_t pgd)
	  /*  same as above  */

        query or setting functions for Page Table entry's value of any of the flags :
	  Page flag reading :
	    pte_user()	    /*  User/Supervisor  */
	    pte_read()	    /*  User/Supervisor,pages on the 80x86 processor cannot be protected against reading  */
	    pte_write()	    /*  Read/Write  */
	    pte_exec()	    /*  User/Supervisor,pages on the 80x86 processor cannot be protected against code execution  */
	    pte_dirty()     /*  Dirty  */
	    pte_young()	    /*  Accessed  */
	    pte_file()	    /*  Dirty,when the Present is cleared and the Dirty flag is set,the page belongs to a
	    		     *  non-linear disk file mapping.
			     */

	  Page flag setting :
	    mk_pte_huge()   /*  Page Size and Present  */
	    pte_wrprotect() /*  Read/Write */
	    pte_rdprotect() /*  User/Supervisor  */
	    pte_exprotect() /*  User/Supervisor  */
	    pte_mkwrite()   /*  Read/Write  */
	    pte_mkread()    /*  User/Supervisor  */
	    pte_mkexec()    /*  User/Supervisor  */
	    pte_mkclean()   /*  Dirty  */
	    pte_mkdirty()   /*  Dirty  */
	    pte_mkold()	    /*  Accessed  */
	    pte_mkyoung()   /*  Accessed  */
	    pte_modify(p, v)	        /*  Sets all access rights in a Page Table entry @p to a specified value @v  */
	    ptep_set_wrprotect()    	/*  pointer version  */
	    ptep_set_access_flags() 	/*  if the Dirty == 1,sets the page's access rights to a specified value and
	    			     	 *  invokes flush_tlb_page()
				     	 */
	   ptep_mkdirty()	    	/*  pointer version  */
	   ptep_test_and_clear_dirty()	/*  like pte_mkclen() but acts on a pointer to a Page Table entry and returns
	   				 *  the old value of the flag
					 */
	   ptep_test_and_clear_young()	/*  like pte_mkold() but acts on a pointer to a Page Table entry and returns 
	   				 *  the old value of the flag
					 */

        macros acting on Page Table entries :
	  pgd_index(addr)     
	  /*  yields the index of the entry in Page Global Directory that maps the linear address @addr  */

	  pgd_offset(mm, addr)
	  /*  yields the linear address of the entry in a Page Global Directory that corresponds to the 
	   *  address @addr,the Page Global Directory is found through a pointer within the memory descriptor
	   */

	  pgd_offset_k(addr)
	  /*  yields the linear address of the entry in the master Kernel Page Global Directory that corresponds
	   *  to the address @addr
	   */

	  pgd_page(pgd)
	  /*  yields the page descriptor address of the page frame containing the Page Upper Directory referred to
	   *  by the Page Global Directory entry @pgd,in a two or three-level paging system,this macro is
	   *  equivalent to pud_page() applied to the folded Page Upper Directory entry
	   */

	  pud_offset(pgd, addr)
	  /*  yields the linear address of the entry in a Page Upper Directory that corresponds to @addr,in a 
	   *  two- or three-level paging system,this macro yields @pgd,the address of a Page Global Directory entry
	   */

	  pud_page(pud)
	  /*  yields the linear address of the Page Middle Directory referred to by the Page Upper Directory entry @pud,
	   *  in a two- or three-level paging system,this macro is equivalent to pmd_page() applied to the foled
	   *  Page Middle Directory entry
	   */

	  pmd_index(addr)
	  /*  yields the index of the entry in the Page Middle Directory that maps the linear address @addr  */
	  
	  pmd_offset(pud, addr)
	  /*  yields the address of the entry in a Page Middle Directory that corresponds to @addr,
	   *  in a two- or three-level paging system,it yields @pud,the address of a Page Global Directory entry
	   */

	  pmd_page(pmd)
	  /*  yields the page descriptor address of the Page Table referred to by the Page Middle Directory entry @pmd,
	   *  in a two- or three-level paging system,@pmd is actually an entry of a Page Global Directory
	   */

	  mk_pte(p, prot)
	  /*  use a page descriptor @p and a group of access rights @prot to builds the corresponding Page Table entry  */

	  pte_index(@addr)
	  /*  yields the index of the entry in the Page Table that maps the linear address @addr  */

	  pte_offset_kernel(dir, addr)
	  /*  yields the linear address of the Page Table that corresponds to the linear address @addr mapped by the 
	   *  Page Middle Directory @dir,used only on the master kernel page tables
	   */

	  pte_offset_map(dir, addr)
	  /*  yields the linear address of the entry in the Page Table that corresponds to the linear address @addr,
	   *  if the Page Table is kept in high memory,the kernel establishes a temporary kernel mapping,to be released
	   *  by means of pte_unmap.
	   *  the macros pte_offset_map_nested() and pte_unmap_nested() are identical,but they use a different temporary
	   *  kernel mapping.
           */

	  pte_page(x)
	  /*  returns the page descriptor address of the page referenced by the Page Table entry @x  */

	  pte_to_pgoff(pte)
	  /*  extracts from the content @pte of a Page Table entry the file offset corresponding to a page belonging to
	   *  a non-linear file memory mapping
	   */

	  pgoff_to_pte(offset)
	  /*  sets up the content of a Page Table entry for a page belonging to a non-linear file memory mapping  */

        page allocation functions :
	  pgd_alloc(mm)
	  /*  allocates a new Page Global Directory;if PAE is on,it also allocates the three children Page Middle Directories
	   *  that map the User Mode linear addresses.the argument @mm is ignored on the 80x86 architecture.
	   */

	  pgd_free(pgd)
	  /*  releases the Page Global Directory at address @pgd;if PAE is on,it also releases the three Page Middle
           *  Directories that map the User Mode linear addresses
	   */

	  pud_alloc(mm, pgd, addr)
	  /*  in a two- or three-level paging system,this function does nothing;it simply returns the linear address of
	   *  the Page Global Directory entry pgd
	   */

	  pud_free(x)
	  /*  in a two- or three-level paging system,this macro does nothing  */

	  pmd_alloc(mm, pud, addr)
	  /*  defined so generic three-level paging systems can allocate a new Page Middle Directory for the linear address
	   *  @addr;if PAE is off,the function simply returns the @pud -- that is,the address of the entry in the
	   *  Page Global Directory;if PAE is on,the function returns the linear address of the Page Middle Directory entry
	   *  that maps the linear address @addr,the argument cw is ignored
	   */

	  pmd_free(x)
	  /*  does nothing,because Page Middle Directories are allocated and deallocated together with their parent
	   *  Page Global Directory.
	   */

	  pte_alloc_map(mm, pmd, addr)
	  /*  returns the address of the Page Table entry corresponding to @addr,if the Page Middle Directory entry is null,
	   *  the function allocates a new Page Table by invoking pte_alloc_one().
	   *  if a new Page Table is allocated,the entry corresponding to @addr is initialized and the User/Supervisor is set,
	   *  if the Page Table is kept in high memory,the kernel establishes a temporary kernel mapping,to be released by
	   *  pte_unmap().
	   */

	  pte_alloc_kernel(mm, pmd, addr)
	  /*  if @pmd associated with the address @addr is null,the function allocates a new Page Table,it then returns the 
	   *  linear address of the Page Table entry associated with @addr,used only for master kernel page tables
	   */

	  pte_free(pte)
	  /*  releases the Page Table associated with the @pte page descriptor pointer  */
	  
	  pte_free_kernel(pte)
	  /*  equivalent to pte_free(),but used for master kernel page tables  */

	  clear_page_range(mmu, start, end)
	  /*  clears the contents of the page tables of a process from linear address @start to @end by iteratively
	   *  releasing its Page Tables and clearing the Page Middle Directory entries
	   */

	  because the Page Table that is supposed to contain it might not exist,in such cases,it is necessary to allocate
	  a new page frame,fill it with zeros,and add the entry.
	  if PAE on,the kernel uses three-level paging,when the kernel creates a new Page Global Directory,it also allocates
	  the four corresponding Page Middle Directories;these are freed only when the parent Page Global Directory is released.
	  when two or three-level paging is used,the Page Upper Directory entry is always mapped as a single entry within the
	  Page Global Directory.
	
	  
      Physical Memory Layout : 
        kernel must build a physical addresses map that specifies which physical address ranges are usable by the kernel and 
	which are unavailable(hardware device I/O shared memory or BIOS data).

	reserved page frames :
	  those falling in the unavailable physical address ranges
	  those containing the kernel's code and initialized data structures
	(such page frames can never be dynamically assigned or swapped to disk)

	/*  general rule,linux kernel is installed in RAM starting from the physical address 0x00100000,
	 *  from the second megabyte.
	 *  the total number of page frames required depends on how the kernel is configured.
	 */	

        why kernel loaded starting with the second megabyte?
	:  the PC architecture has several peculiarities that must be taken into account.
	     PF0 (BIOS data,system hardware configuration detected during POST(Power-On Self Test)).
	     0x000a0000 -- 0x000fffff are usually reserved to BIOS rountines and to map the internal memory of ISA graphics
	     cards.
	     additional page frames within the first megabyte may be reserved by specific computer models.

	in the early stage of the boot sequence,the kernel queries the BIOS and learns the size of the physical memory,
	later,kernel executes the machine_specific_memory_setup() rountine,which builds the physical address map.
	(such functions is named default_machine_specific_memory_setup() in arch/x86/kernel/e820.c)

	/*  kernel builds this table on the basis of the BIOS list,if this is available,otherwise the kernel builds the table
	 *  following the conservative default setup:
	 *    all page frames with numbers from 0x9f(LOWMEMSIZE()) to 0x100(HIGH_MEMORY) are marked as reserved.
	 */

	/*  POST stage,BIOS writes information about the system hardware devices into the proper page frames,
	 *  and initialization stage,kernel will copies such data into the suitable kernel data structures,
	 *  then consider these page frames usable.
	 *  BIOS may not provide information for some physical address ranges,in such case,linux kernel assumes
	 *  such ranges are not usable.
	 */

	 after machine_specific_memory_setup(),the function setup_memory() will be invoked,it analyzes the table
	 of physical memory regions and initializes a few variables that describe the kernel's physical memory
	 layout.
	   the vairables :  (thest variables are declared in *.c files under the directory mm/)
	     num_physpages      --  page frame number of the highest usable page frame
	     totalram_pages 	--  total number of usable page frames
	     min_low_pfn    	--  page frame number of the first usable page frame after the kernel image in RAM
	     max_pfn	    	--  page frame number of the last usable page frame
	     max_low_pfn    	--  page frame number of the last page frame directly mapped by the kernel(low memory)
	     totalhigh_pages	--  total number of page frames not directly mapped by the kernel(high memory)
	     highstart_pfn	--  page frame number of the first page frame not directly mapped by the kernel
	     highend_pfn	--  page frame number of the last page frame not directly mapped by the kernel

	 /*  under x86,function setup_arch()<kernel/setup.c> invokes setup_memory_map()<kernel/e820.c>,this function
	  *  will copies the e820 data structure object from &e820 to &e820_saved,and setup_arch() will updates 
	  *  some of the variables,another will be updated by the functions in the files under mm/ .
	  */
	     
	 /*  linux kernel prefers to skip the first megabyte of RAM to ensure it can never be loaded into groups of
	  *  noncontiguous page frames.
	  *  generally,kernel keeps initialized data structures right after kernel code,and the uninitialized data
	  *  structures follows it.
	  */

      Process Page Tables :
        two parts of the linear address space of a process :
	  1>  linear address 0x00000000 -- 0xbfffffff can be addressed either User or Kernel Mode.
	  2>  linear address 0xc0000000 -- 0xffffffff can only be addressed in Kernel Mode.

	macro PAGE_OFFSET yields value 0xc0000000,this is the offset in the linear address space of a process
	where the kernel lives.
	(in some cases,the process is running in Kernel Mode maybe need to access the linear address space of 
	 User Mode for retrieve or store data.)

	!  the content of the first entries of the Page Global Directory that map linear address lower than
	   0xc0000000(768 entries with PAE disabled,3 entries with PAE enabled),depends on the specific process.

      Kernel Page Tables :
        the kernel maintains a set of page tables for its own use,rooted at a so-called master kernel Page Global
	Directory.
	after these page tables were initialized,they will never be directly accessed by any process or kernel thread.
	(the highest entries of the master kernel Page Global Directory are the reference model for the corresponding
	 entries of the Page Global Directories of every regular process in the system.)

	kernel initializes these page tables with two-phase activity :
	  1>  the kernel creates a limited address space including the kernel's code and data segments,the initial
	      Page Tables,and 128KB for some dynamic data structures.
	      the minimal address space is just large enough to install the kernel in RAM and to initialize its core
	      data structures.
	  2>  the kernel takes advantage of all of the existing RAM and sets up the page tables properly.

	(right after kernel image loaded,CPU is stil running in real mode,thus,paging is not enabled)

	Provisional kernel Page Tables :
	  A provisional Page Global Directory is initialized statically during kernel compilation,while the provisonal
	  Page Tables are initialized by startup_32() assembly function defined in <arch/x86/kernel/head_32.S>.

	  the Provisional Page Global Directory is contained in swapper_pg_dir variable,the Provisional Page Tables
	  are stored starting from pg0.pg0 is the Page Frame which number is 0,it is the first Page Frame,right after
	  the end of the kernel's uninitialized data segments(symbol _end).

	  suppose all the limited address space fit in the first 8MB of RAM.there,kernel just required two Page Tables,
	  but each Page Table is points to a Page Frame,and Page Frame holds one Page,each Page Frame consisting with
	  a Page size is 4KB(1024(entries) * 2(Page Tables) * 4KB(Page Size) = 8MB).(at this time,PAE is off)

	  for easily addressed both in real mode and protected mode to the 8MB,kernel must create a mapping from
	  both the linera addresses [0x00000000 -- 0x007fffff] and the linera addresses [0xc0000000 -- 0xc07fffff]
	  into the physical address [0x00000000 -- 0x007fffff].
	    linear [0x00000000 -- 0x007fffff] -> [0x00000000 -- 0x007fffff] physical
	    linear [0xc0000000 -- 0xc07fffff] -> [0x00000000 -- 0x007fffff] physical

	  kernel create the desired mapping by filling all the swapper_pag_dir entries(1024) with zeroes,except for
	  entries 0, 1, 0x300(768), 0x301(769) . 
	  entries 0x300 and 0x301 will span all linear addresses between [0xc000000 -- 0xc07fffff].
	  /*  head_32.S defined swapper_pg_dir :
	   *    ENTRY(swapper_pg_dir)
	   *		.fill 1024,4,0	#  .fill repear,size(byte),value
	   */

	  initialization :
	    > the address field of entries 0 and 0x300 is set to the physical address of pg0,while the address field
	      of entries 1 and 0x301 is set to the physical address of the page frame following pg0(it is pg1).
	    > the Present, Read/Write, User/Supervisor flags are set in all four entries(on -> 1).
	    > the Accessed, Dirty, PCD, PWD, and Page Size flags are cleared in all four entries(off -> 0).

	  start_32() copies the address of swapper_pg_dir to cr3,and enables paging unit(PG flag of the cr0).
	  /*  A part of code  */
	    movl     $swapper_pg_dir-0xc0000000,%eax
	    movl     $eax,%cr3			#  set he page table pointer
	    movl     %cr0,%eax			#  get value of cr0
	    orl	     $0x80000000,%eax		#  open PG
	    movl     %eax,%cr0			#  write back

	    #  0xc0000000 is the __PAGE_OFFSET
	    #  addresses in [0x00000000, 0xc00000000) is used for User Mode.

        Final Kernel Page Table :
	  Final Kernel Page Table when RAM size is less than 896MB >
	    the final mapping provided by the kernel page tables must transform linear addresses starting
	    from 0xc0000000 into physical addresses starting from 0.

	    <arch/x86/include/asm/page.h>
	    #define __pa(x)	__phys_addr((unsigned long)(x))
	    /*  convert a linear address starting from PAGE_OFFSET to the corresponding physical address  */

	    #define __va(x)    ((void *)((unsigned long)(x) + PAGE_OFFSET))
	    /*  convert a physical address to the corresponding linear address starting from PAGE_OFFSET  */

	    the master kernel Page Global Directory is still stored in swapper_pg_dir,it is initialized by
	    the paging_init() which is declared in <arch/x86/include/asm/pgtable_32.h>,and defined in
	    <arch/x86/mm/init_32.c> with the prototype "void __init paging_init(void);" .
	    it executes these works :
	      invoke pagetable_init() to set up the Page Table entries properly;
	      writes the physical address of swapper_pg_dir in the cr3;
	      if the CPU supports PAE and if the kernel is compiled with PAE support,sets the PAE flag in
	      the cr4;
	      invokes __flush_tlb_all() to invalidate all TLB entries;

	    /*  body of it  */
	    void __init paging_init(void)
	    {
	            pagetable_init();
		    __flush_tlb_all();
		    kmap_init();
		    sparse_init();
		    zone_sizes_init();
	    }

	    /*  this routines also unmaps the page at virtual kernel address 0,so
	     *  that we can trap those pesky NULL-reference erros in the kernel.
	     */

	    the actions performed by pagetable_init() depend on both the amount of RAM  present and on the CPU
	    model.(suppose less than 896MB)
	    /*  the highest 128MB of linear address are left available for several kinds of mapping,the kernel
	     *  address space left for mapping the RAM is thus 1GB - 128MB = 896MB
	     */

	    the identity mapping of the first megabytes of physical memory(8MB,the supposed size) built by startup_32()
	    is required to complete the initialization phase of the kernel,when this mapping is no longer necessary,
	    the kernel clears the corresponding page table entries by invoking the zap_low_mappings() .

          Final Kernel Page Table when RAM size is between 896MB and 4096MB >
	    in this case,the RAM can not be mapped entirely into the kernel linear address space.
	    Linux does a mapping to map a RAM window of 896MB into the kernel linear address space,if a program
	    need to address other parts of the existing RAM,some other linear address interval must be mapped to
	    the required RAM,this implies changing the value of some page table entries.
	    (that is the remaining RAM(4096-896) is left unmapped status and handled by dynamic remapping)

	  Final Kernel Page Table when RAM size is more than 4096MB >
	    in this case,CPU supports PAE,and kernel is compiled with PAE support.
	    even PAE is on,linear address is till 32-bit,but physical address is 36-bit.
	    the main difference with the previous case is that,at this time,three-level paging model is used.
	    (pgd -> pmd -> pte)
	    /*  but kernel still directly maps 896MB RAM window  */

	    the setups :
	      > kernel initializes the first three entries(pgd_index(PAGE_OFFSET) = 3) in the Page Global Directory
	        corresponding to the user linear address space with the address of an empty page(empty_zero_page).
	      > kernel sets the fourth entry with the address of a Page Middle Directory(pmd) allocated by
	        invoking alloc_bootmem_low_pages().
	      > kernel sets the first 448 entries(896MB / 2MB(PAGE_SIZE)) in the Page Middle Directory(PAE on,512 entries)
	        are filled with the physical address of the first 896MB of RAM.
		/*  phys_addr = 0x00000000;
		 *  for(j = 0; j < PTRS_PER_PMD && phys_addr < max_low_pfn * PAGE_SIZE; ++j) {
		 *          ...
		 *	    phys_addr += PTRS_PER_PTE * PAGE_SIZE;  /*  PAE on,512  */
		 *  }
		 */
	      > kernel copies the fourth Page Global Directory entry into the first entry,so as to mirror the mapping
	      	of the low physical memory in the first 896MB of the linear address space.

		/*  this mapping is required in order to complete the initialization of SMP  */
		(if these page table entries is no longer neccesary,kernel will call zap_low_mappings() to clears them.)

      
      Fix-Mapped Linear Address :
        the initial part of the fourth gigabyte of kernel linear address maps the physical memory of the system,
	however,at least 128MB of linear addresses are always left available because the kernel uses them to implement
	noncontiguous memory allocation and fix-mapped linear addresses.
	(noncontiguous memory allocation is just a special way to dynamically allocate and release pages of memory)

	basically,a fix-mapped linear address is a constant linear address like 0xffffc000 whose corresponding physical
	address does not have to be the linear address minus 0xc0000000,but rather a physical address set in an arbitrary
	way.thus,each fix-mapped linear address maps one page frame of the physical memory.
	fix-mapped linear address is similiar to the linear address that map the first 896MB conceptually,but it can map
	any physical address,while the mapping established by the linear address in the initial portion of the fourth
	gigabyte is linear.

	each fix-mapped linear address is represented by a small integer index defined in the enum fixed_addresses 
	data structure :
	  <arch/x86/include/asm/fixmap.h>
	  enum fixed_addresses {
	          FIX_HOLE,
		  FIX_VSYSCALL,
		  FIX_APIC_BASE,
		  FIX_IO_APIC_BASE_0,
		  [...]
		  __end_of_fixed_addresses
          };

	  fix-mapped linear addresses are placed at the end of the fourth gigabyte of linear addresses,
	  fix_to_virt() function computes the constant linear address starting from the index.
	  <arch/x86/include/asm/fixmap.h>
	    /*  fix_to_virt - computes the constant linear address starting from the index in fixed_addresses.
	     *  @idx : the index.
	     *  return - the constant linear address.
	     *  #  if idx is not in fixed_addresses,then an error "undefined symbol __this_fixmap_does_not_exist"
	     *	   will be reported by compiler.
	     */
	    static __always_inline unsigned long fix_to_virt(const unsigned int idx);

	    /*  set_fixmap - associates a fix-mapped address with a physical address.
	     *  @idx : the index in fixed_address.
	     *  @phys : the physical address have to be associated.
	     */
	    #define set_fixmap(idx, phys)  __set_fixmap(idx, phys, PAGE_KERNEL)

	    /*  set_fixmap_nocache - nocache version.
	     *    this function will sets PCD flag of the Page Table entry,thus disabling the hardware cache when
	     *    accessing the data in the page frame.
	     */
	    #define set_fixmap_nocache(idx, phys)  __set_fixmap(idx, phys, PAGE_KERNEL_NOCACHE)

	    /*  clear_fixmap - clear fix-mapped,removes the linking between a fix-mapped linear address
	     *		       and the physical address.
	     *  @idx : the index of member in fixed_addresses.
	     */
	    #define clear_fixmap(idx)  __set_fixmap(idx, 0, __pgprot(0))

	
      Handling the Hardware Cache and the TLB :
        Handling the hardware cache >
	  macro L1_CACHE_BYTES yields the size of a cache line in bytes.
	  <arch/x86/include/asm/cache.h>
	    #define L1_CACHE_BYTES  (1 << L1_CACHE_SHIFT)

	  to optimize the cahce hit rate,the kernel considers the architecture in making the following decisions :
	    1>  the most frequently used fields of a data structure are placed at the low offset within the 
	    	data structure,so they can be cached in the same line.
            2>  when allocating a large set of data structures,the kernel tries to store each of them in memory
	    	in such a way that all cache lines are used uniformly.

          #  80x86 microprocessors does cache synchronization automatically,linux need not to care about anymore.
	     the kernel does provide,however,cache flushing interfaces for processors that does not synchronize caches.

	Handling the TLB >
	  TLB is used keep records about mapping between linear addresses and physical addresses,
	  so processors can not synchronize their own TLB cache automatically.(kernel determines if the mapping is invalid)

	  the methods linux provides for TLB synchronization :
	    <arch/x86/include/asm/tlbflush.h>
              /*  flush_tlb_all - flushes all TLB entries even refer to global pages.
	       *  #  typically used when changing the kernel page table entries.
	       */
	      #define flush_tlb_all()  __flush_tlb_all()

	      /*  flush_tlb_kernel_range - flushes all TLB entries in a given range of linear addresses,
	       *  			   even refer to global pages.
	       *  @start : range start.
	       *  @end   : range end.
	       */
	      static inline void flush_tlb_kernel_range(unsigned long start, unsigned long end);

	      /*  flush_tlb - flushes all TLB entries of the non-global pages owned by the current process.
	       *  	      (current mm struct TLBs)
	       */
	      #define flush_tlb()  __flush_tlb()

	      /*  flush_tlb_mm - flushes all TLB entries of the non-global pages owned by a given process.
	       *  @mm : a pointer points to a mm_struct object.
	       *  #  typically used when forking a new process.
	       */
	      static inline void flush_tlb_mm(struct mm_struct *mm);

	      /*  flush_tlb_range - flushes the TLB entries corresponding to a linear address interval of a given process.
	       *  @vma : the virtual memory area of current process.
	       *  @start : range start.
	       *  @end : range end.
	       *  #  typically releasing a linear address interval of a process.
	       */
	      static inline void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);

	      /*  flush_tlb_pgtables - flushes the TLBs of a given contiguous subset of page tables of a given process.  */
	      flush_tlb_pgtables
	      !  some architecure does not offer such function,i.e. x86.
	      #  80x86 architecure nothing has to be done when a page table is unlinked from its parent table.

	      /*  flush_tlb_page - flushes the TLB of a single Page Table entry of a given process.
	       *  @vma : the virtual memory area of the process.
	       *  @addr : specified address.
	       *  #  typically used processing a Page Fault.
	       */
	      static inline void flush_tlb_page(struct vm_area_struct *vma, unsigned long addr);

          every microprocessor usually offers a far more restricted set of TLB-invalidating assembly language instructions,
	  Intel microprocessors offers only two TLB-invalidating techniques :
	    >  all Pentium models automatically flush the TLBs relative to non-global pages when a value is loaded into cr3.
	    >  in Pentium Pro and later models,the "invlpg" assembly language instruction invalidates a single TLB mapping a
	       given linear address.

	  Linux macros that exploit such hardware technique :
	    <arch/x86/include/asm/tlbflush.h>
	      /*  __flush_tlb - rewrites cr3 register back into itself.  */
	      #define __flush_tlb()  __native_flush_tlb()

	      /*  __flush_tlb_global - disables global pages by clearing the PGE flag of cr4,
	       *                       rewrites cr3 register back into itself,and sets again the PGE flag.
	       */
	      #define __flush_tlb_global()  __native_flush_tlb_global()

	      /*  __flush_tlb_single - executes "invlpg" assembly language instruction with parameter @addr.  */
	      #define __flush_tlb_single(addr)  __native_flush_tlb_single(addr)

	    #  for SMP : a processor sends a interprocessor interrupt to other to forces synchronizing.

	  kernel avoids TLB flushes :
	    >  when performing a process switch between two regular processes that use the same set of page tables.
	    >  when performing a process switch between a regular process and a kernel thread.

	    #  when the kernel assigns a page frame to a User Mode process and stores its physical address into Page Table
	       entry,it must flush any local TLB entry that refers to the corresponding linear address,on SMP,synchronization
	       have to be done between CPUs.

	  Lazy TLB mode : (kernel use this strategy to avoid useless TLB flushing in SMP)
	    the basica idea is :
	      if several CPUs are using same page tables and a TLB entry must be flushed on all of them,then TLB flushing
	      may,in some cases,be delayed on CPUs running kernel threads.

	    when some CPUs start running a kernel thread,the kernel sets it into lazy TLB mode.
	    each CPU in lazy TLB mode does not flush the corresponding entries;however,the CPU remembers that its current
	    process is running on a set of page tables whose TLBs for the User Mode addresses are invalid.
	    as soon as the CPU in lazy TLB mode switches to a regular process with a different set of page tables,the
	    hardware automatically flushes the TLBs,and the kernel sets the CPU back in non-lazy TLB mode.
	    but,if a CPU in lazy TLB mode switches to a regular process that owns the same set of page tables used by the
	    previously running kernel thread,then any deferred TLB invalidation must be effectively applied by the kernel.

	    data structures associated to lazy TLB mode :
	      @cpu_tlbstate variable is a static array of @NR_CPUS structures consisting of an @active_mm filed pointing to the
	      memory descriptor of the current process,and a @state flag that can assume only two values : TLBSTATE_OK | TLBSTATE_LAZY
	      each memory descriptor includes a @cpu_vm_mask field that stores the indices of the CPUs that should receive
	      Interprocessor Interrupts related to TLB flushing.(it is meaningful just for current process is executing)
	      (the CPU has relative to the active memory,in default,indices of all CPUs of the system will be stored into
	       @cpu_vm_mask,including the CPU is lazy TLB mode)

	    if a CPU recevied a Interprocess interrupt related to TLB flushing,kernel have to checks @state field of its
	    @cpu_tlbstate element is equal to TLBSTATE_LAZY,in this case,the kernel refuses to invalidate the TLBs and
	    removes the CPU index from the @cpu_vm_mask filed of the memory descriptor.
	    two consequences :
	      >  as long as the CPU remains in lazy TLB mode,it will note receive other Interprocessor Interrupts related to
	         TLB flushing.

	      >  if the CPU switches to another process that is using the same set of page tables as the kernel thread that
	         is being replaced,the kernel invokes __flush_tlb() to invalidate all non-global TLBs of the CPU.


Chapter 3 : Processes
    processes are often called "tasks" or "threads" in the Linux source code.

    Processes,Lightweight Processes,and Threads :
      a process is an intance of a program in execution!
      kernel's point of view :
        the purpose of a process is to act as an entity to which system resources are allocated.

      multithreaded applications :
        user programs having many relatively independent execution flows sharing a large portion of the application data
	structures.
      kernel's point of view :
        a multithreaded application was just a normal process.

      "Linux uses lightweight processes to offer better support for multithreaded applications."
      Basically,two lightweight processes may share some resources,if they were associated.

      NPTL - Native POSIX Thread Library
      NGPT - Next Generation POSIX Threading Package

      thread groups :
        in Linux,a thread group is basically a set of lightweight processes that implement a multithreaded application
	and act as a whole with regards to some system calls such as getpid(),kill(),and _exit().

    Process Descriptor :
      To manage processes,the kernel must have a clear picture of what each process is doing.
      <linux/sched.h>
        struct task_struct;
	/*  task_struct - linux process descriptor whose fields contain all the information related to a single process.  */
	/*  some important members :
	 *    volatile long state;
	 *    struct thread_info *thread_info;
	 *    struct mm_struct *mm, *active_mm;
	 *    struct tty_struct *tty;
	 *    struct fs_struct *fs;
	 *    struct files_struct *files;
	 *    struct signal_struct *signal;
	 */

    Process State :
      the @state field of the process descriptor describes what is currently happening to the process.
      it consists of an array of flags,each of which describes a possible process state.
      process states :
        TASK_RUNNING
	  -  running on CPU or wating to be executed.
	TASK_INTERRUPTIBLE
	  -  suspended(sleeping) until some condition becomes true.
	TASK_UNINTERRUPTIBLE
	  -  like TASK_INTERRUPTIBLE,except that delivering a signal to the sleeping process leaves its state unchanged.
	TASK_STOPPED
	  -  execution has been stopped.
	TASK_TRACED
	  -  execution has been stopped by a debugger.
	EXIT_ZOMBIE (@state, @exit_state)
	  -  execution has been terminated,but the parent process has not yet issued a wait4() or waitpid() system call to
	     return information about the dead process.(status have to be reported)
	EXIT_DEAD   (@state, @exit_state)
	  -  the final state : the process is being removed by the system because the parent process has just issued a wait4()
	     or waitpid() system call for it.(status had been reported)

      <linux/sched.h>
        /*  set_task_state - set the task's state.
	 *  @tsk : a pointer points to the task_struct.
	 *  @state_value : state value.
	 */
        #define set_task_state(tsk, state_value)  set_mb((tsk)->state, (state_value))

	/*  set_current_state - set currently executing task's state.
	 *  @state_value : state value.
	 */
	#define set_current_state(state_value)    set_mb(current->state, (state_value))

	/*  set_mb - enable memory barrier before set value,disable it later.  */


    Identifying a Process :
      general rule : each execution context that can be independently scheduled must have its own process descriptor,
      	      	     even lightweight processes.
      Process ID : Unix-like operating systems allow users to identify processes by means of a number called PID.
      (task_struct.pid)
		
		   /*  by default,the maximum PID number is 32767(PID_MAX_DEFAULT - 1)[32-bit].
		    *  	  	      	      	  	    4194303[64-bit]
		    *  for change the maximum PID number dynamically,can write a new value into
		    *  /proc/sys/kernel/pid_max
		    */
      the kernel uses the pidmap_array bitmap to manages PID numbers,which denotes which are the PIDs currently assigned
      and which are the free ones.
      /*  32-bit,a page frame contains 32768 bits,so such structure is stored in a single page.
       *  64-bit,kernel can adds the additional pages to the bitmap,if the PID number is too large.
       */

      one process one Process ID !

      POSIX 1003.1c : all threads of a multithreaded application must have the same PID.
      thread groups : the identifier shared by the threads is the PID of the thread group leader,that is,the PID of the
      	     	      first lightweight process in the group.
		      (task_struct.tgid)[getpid() retrives the value of this field]

      Process descriptors handling :
        Processes are dynamic entities,Linux packs two different data structures in a single per-process memory area :
	  >  thread_info
	  >  the Kernel Mode process stack
	the length of this memory area is usually 8kB(2 pages),kernel stores the 8kB memory area in two consecutive page
	frames with the first page frame aligned to a multiple of 2^13.(turn out to be a problem when littel dynamic memory
	is available,but kernel could to be configured that use 4kB memory area to stores the stack and thread_info)

	the thread_info structure resides at the begining of the memory area,and the stack grows downward from the end(the end
	of the memory area) !
	[  thread_info.task -> task_struct ; task_struct.thread_info -> thread_info  ]
	/*  Linux task_struct is defined in <linux/sched.h>,it contains a field named "stack" which type is void* ,
	 *  kernel use this field to assocaited task_struct with thread_info object.
	 *  macro "#define task_thread_info(task) ((struct thread_info *)(task)->stack)" is used to retrive the
	 *  thread_info object which associated with current process.
	 */

	/*  <linux/sched.h>
	 *    union thread_union {
	 *            struct thread_info thread_info;
	 *            unsigned long stack[THREAD_SIZE/sizeof(long)];
	 *    };  /*  task_struct.stack will points to a thread_union object.  */
	 *        /*  <arch/x86/include/asm/page_32_types.h>:  #define THREAD_SIZE (PAGE_SIZE << THREAD_ORDER)  */
	 */

	/*  because the thread_info structure is 52B long,the kernel stack can expand up to 8140B  */

	the kernel uses the alloc_thread_info and free_thread_info macros to allocate and release the memory area storing a
	thread_info structure and a kernel stack :
	<kernel/fork.c>
	  /*  alloc_thread_info - sets thread_info field in struct task_struct.
	   *  @tsk : the process descriptor.
	   *  return - NULL or the address of the thread_info object former allocated.
	   */
	  static inline thread_info *alloc_thread_info(struct task_struct *tsk);

	  /*  free_thread_info - release the thread_info object.
	   *  @ti : the pointer points to a thread_info structure object.
	   */
	  static inline void free_thread_info(struct thread_info *ti);

      Identifying the current process :
        the kernel can easily obtain the address of the thread_info structure of the process currently running on a 
	CPU from the value of the esp register.
	  8kB thread_union :
	    the kernel masks out the 13 least significant bits of esp to obtain the base address of the thread_info structure.
	  4kB thread_union :
	    the kernel masks out the 12 least significant bits of esp to obtain the base address of the thread_info structure.

	<arch/x86/include/asm/thread_info.h>
	  /*  current_thread_info - retrive the address of the thread_info object which is owns to the running process.  */
	  static inline struct thread_info *current_thread_info(void)
	  {
	          return (struct thread_info *)(current_stack_pointer & ~(THREAD_SIZE - 1));
	  }

	the kernel can easily obtain the address of the task_struct structure of the process currently running on a CPU from
	the macro "current".
	  /*  this is can be simply achieved by thread_info.task  */
	  <arch/x86/include/asm/current.h>
	    /*  get_current - retrive the address of a task_struct object which is owns to the current running process.  */
	    static __always_inline struct task_struct *get_current(void);	
	    #define curren get_current()

        advantage of storing the process descriptor with the stack emerges on multiprocessor systems :
	  the correct current process for each hardware processor can be derived just by checking the stack.

	  /*  earlier versions of Linux did not store the kernel stack and the process descriptor together,instead,they
	   *  were forced to introduce a global static variable called current to identify the process descriptor of the
	   *  running process.On multiprocessor systems,it was necessary to define current as an array -- one element for
	   *  each available CPU.
	   *  #  Linux 2.6 introduced percpu data,macro "current" will retrives the percpu data for running task.
	   */

      Doubly linked lists :
        kernel data structure.
	<linux/list.h>
	  struct list_head {
	          struct list_head *next, *prev;
	  };	
	some methods :
	<linux/list.h>
	  list_add(n, p)	inserts @n to @p's next.
	  list_add_tail(n, p)	inserts @n to the tail of the list represented by @p.
	  list_del(p)	   	deletes an element pointed to by @p.
	  list_empty(p)		checks if the list specified by the address @p of its head is empty.
	  list_entry(p, t, m)	returns the address of the data structure of type @t in which the list_head field
	  		   	that has the name @m and the address @p is included.
	  list_for_each(p, h)	scans the elements of the list specified by the address @h of the head;in each
	  		   	iteration,a pointer to the list_head structure of the list element is returned in @p.
	  list_for_each_entry(p, h, m)
	  			similar to list_for_each,but returns the address of the data structure embedding the list_head
				structure rather than the address of the list_head structure itself.

	usage :
	  struct kobject {
	  	  ...
	          struct list_head list;
		  ...
	  };

	  struct kobject kobj;
	  INIT_LIST_HEAD(&kobj.list);

	  struct kobject *new = kmalloc(sizeof(struct kobject), GFP_KERNEL);
	  if (new) {
	          list_add(&new->list, &kobj.list);
	  }

	  ...
	  
	  list_for_each_entry(new, &kobj.list, list) {
	  	  printk(KERN_DEBUG "%p", new);
	          ...
	  }

	  ...

	  struct kobject *temp = NULL;
	  list_for_each_entry_safe(new, temp, &kobj.list, list) {
	          list_del(&new->list);
		  kfree(new);
	  }

	  ...


        another doubly linked list : hlist.
	<linux/list.h>
	  struct hlist_node {
	          struct hlist_node *next, **prev;
	  };
	  struct hlist_head {
	          struct hlist_node *first;
	  };

	methods are similar to list_head's all defined in <linux/list.h>

      The process list :
        a list that links together all existing process descriptors.(task_struct.tasks)
	the head of the process list is the init_task task_struct descriptor.

	useful macros :
	  SET_LINKS and REMOVE_LINKS macros are used to insert and to remove a process descriptor in the process 
	  list.(but did not find such macros were defined in Linux 2.6)

	  <linux/sched.h>
	    #define for_each_process(p)  \
	            for (p = &init_task; (p = next_task(p)) != &init_task)
	    /*  @p must be the pointer which is type of task_strcut  */

      The lists of TASK_RUNNING processes :
        only the TASK_RUNNING process is could to be runned on a CPU.

	#  earlier version :
	     all TASK_RUNNING processes are putted into runqueue,and scheduler have to scans the whole list in order
	     to select the "best" runnable process.(too costly to maintain the list ordered according to process priorities)
	
	Linux 2.6 :
	  the aim is to allow the scheduler to select the best runnable process in constant time,independently of the number
	  of runnable processes.

	  principle :
	    split the runqueue in many lists of runnable processes,one list per process priority.
	    each task_struct has a field run_list is type of list_head.if the process priority is equal to k,the
	    run_list field links the process descriptor into the list of runnable processes having priority k.
	    on SMP,each CPU has its own runqueue.

	    #  to make scheduler operations more efficient,the runqueue list has been split into 140(0 -- 139) different lists.
	       kernel must preserve a lot of data for every runqueue in the system,however,the main data structures of a runqueue
	       are the lists of process descriptors belonging to the runqueue;all these lists are implemented by a single 
	       prio_array_t data structure.(Linux 2.6 no such data structure)
	       /*
	        *  prio_array_t {
		*          int nr_active;		/*  the number of process descriptors linked into the lists  */
		*	   unsigned long bitmap[5];	/*  priority bitmap:each flag is set if and only if the correspoding
		*                                           priority list is not empty.  */
		*	   struct list_head queue[140]; /*  the 140 heads of the priority lists  */
		*  };
		*/

	  task_struct.prio stored the dynamic priority of the process.
	  task_struct.array is a pointer to the prio_array_t data structure of its current runqueue.(Linux 2.6.34.1 no such field)

	  <linux/sched.h>
	    void (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup, bool head);
	    void (*dequeue_task) (struct rq *rq, struct task_struct *p, int sleep);

	    /*  struct sched_class members.
	     *  enqueue_task - inserts @p to @rq,called when a task enters a runnable state.
	     *  dequeue_task - removes @p from @rq,called when a task is no longer runnable.
	     */

      Relationships Among Processes :
        processes created by a program have a parent/child relationship.
	and the children created by a program have sibling relationships.

	task_struct.real_parent; /*  type task_struct*  */
	task_struct.parent;	 /*  type task_struct*  */
	task_struct.children;	 /*  type list_head     */
	task_struct.sibling;	 /*  type list_head     */

	real_parent : the process which has been created @this,or @init process.
	parent : current parent,this is the process that must be signaled when the child process terminates.
	       	 as usual,real_parent == parent,but it may occasionally differ,such as when another process issues a
		 ptrace() system call requesting that it be allowed to monitor @this process.
        children : @this process's children.
	sibling : @this process's siblings.

	task_struct.group_leader;  /*  type task_struct  */

	#  a process can be a leader of a process group or of a login session,it can be a leader of a thread group,and
	   it can also trace the execution of other processes.

	(task_struct.signal)->leader_pid;	   /*  PID of the group leader  */
	task_struct.tgid;			   /*  PID of the thread group leader  */
	tasK_struct.sessionid;			   /*  ID of session associated now  */
	task_struct.ptrace_children;		   /*  the head of a list containing all children being traced by a debugger  */
						   /*  Linux 2.6 no such field  */
	task_struct.ptraced;			   /*  a list of tasks this task is using ptrace on  */
						   /*  Linux 2.6 no ptrace_list field  */

      The pidhash table and chained lists :
        kernel must be able to derive the process descriptor pointer corresponding to a PID.
	scanning the process list sequentially and checking the @pid fields of the process descriptors is feasible but rather
	inefficient.
	four hash tables are used to speed up such operation :
	  /*  the reason for multiple hash tables :
	   *    the process descriptor includes fields that represent different types of PID.
	   *    each type of PID requires its own hash table.
	   */

	  task_struct.pids[PIDTYPE_MAX];	/*  type pid_link  */

          [type]         [field]  [introduce]
	  PIDTYPE_PID    pid      PID of the process
	  PIDTYPE_TGID 	 tgid     PID of thread group leader process
	  PIDTYPE_PGID	 pgrp	  PID of the group leader process	/*  Linux 2.6 no such field  */
	  PIDTYPE_SID    session  PID of the session leader process	/*  Linux 2.6 no such field  */
	    /*  Linux 2.6,for retrive group leader's PID,use function task_pgrp(struct task_struct *) ,
	     *  for retrive session leader's PID,use function task_session(struct task_struct *) .
	     */

	  task_struct.pid;  /*  this field has type pid_t,such type from typedef __kernel_pid_t,
	  		     *  and __kernel_pid_t is type of int.
			     */
	  task_struct.pids; /*  an array of pid_link.
	  		     *  struct pid_link {
			     *          struct hlist_node node;
			     *          struct pid *pid;
			     *  };
			     */

          these four hash tables are dynamically allocated during the kernel initialization phase,the size of 
	  a single hash table depends on the amount of available RAM.

	  Linux 2.6 use find_pid_ns() and find_vpid() to get the pid object.
	  <linux/pid.h>
	    /*  find_pid_ns - find the pid in a specified pid_namespace.
	     *  @nr : pid value.
	     *  @second-arg : a pointer points to the pid_namespace which is used to find the pid.
	     *  return - struct pid *,or NULL.
	     */
	    extern struct pid *find_pid_ns(int nr, struct pid_namespace *);

	    /*  find_vpid - find the pid in current pid_namespace.  */
	    extern struct pid *find_vpid(int nr);

	  #define pid_hashfn(x)  hash_long((unsigned long)x, pidhash_shift)
	  /*  Linux 2.6 : defined in <kernel/pid.c>
	   *  #define pid_hashfn(nr, ns)  hash_long((unsigned long)nr + (unsigned long)ns, pidhash_shift)
	   *  this macro is used to transform PID into a table index.
	   *  @pidhash_shift stores the length in bits of a table index.
	   */

	  /*  hash_long() based on a multiplication of the index by a suitable large number.
	   *  the magic constant is 0x9e370001(2654404609).
	   *  unsigned long hash = val * 0x9e370001;
	   *  0x9e370001 is a prime near to (2^32) * ((square_root(5) - 1) / 2) that can also
	   *  easily multiplied by additions and bit shifts,because it is equal to 2^31 + 2^29 - 2^25 + 2^22 - 2^19 - 2^16 + 1
	   */

	  Linux uses chaining to handle colliding PIDs;each table entry is the head of a doubly linked list of colliding
	  process descriptors.(as usual,the number of processes in the system is far below 32768)

	  the data structures used in the PID hash tables are quite sophisticated,because they must keep track of the 
	  relationships between the processes.
	    >  if kernel wants to retrive all processes in a specified thread group,it must finds out all the processes
	       each tgid field == @tgid_value.
	       but use @tgid_value to find a process just returns one process descritpor,the thread group leader.
	       so kernel have to maintains a list of processes for each thread group!

	       the fields of the pid structure :
	         int nr;  /*  PID number  */
		 struct hlist_node pid_chain;	/*  hash chain list  */
		 struct list_head pid_list;	/*  the head of the per-PID list  */

	       Linux 2.6 use another definition of pid structure :
	         <linux/pid.h>
		   struct upid {
		           int nr;
			   struct pid_namespace	*ns;
			   struct hlist_node pid_chain;
		   };  /*  this structure represented the Identifier for the pid structure.  */
		   struct pid {
		           atomic_t count;
			   unsigned int level;
			   struct hlist_head tasks[PIDTYPE_MAX];  /*  tasks that use this pid  */
			   struct rcu_head rcu;
			   struct upid numbers[1];
		   };

          pid handling functions :
	    <linux/pid.h>
	      /*  do_each_pid_task - do-while loop head.  */
	      #define do_each_pid_task(pid, type, task)

	      /*  @pid : the pid structure pointer.
	       *  @type : pid type.
	       *  @task : task_struct pointer used to iterate all tasks in the same pid.
	       */

	      #define while_each_pid_task(pid, type, task)
	      /*  while_each_pid_task - do-while loop end.  */

	      /*  Linux 2.6 no such functions  */
	      #define find_task_by_pid_type(type, nr)
	      #define find_task_by_pid(nr)

	      /*  Linux 2.6 has these :
	       *    <linux/sched.h>
	       *      extern struct task_struct *find_task_by_vpid(pid_t nr);
	       *      extern struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns);
	       *      /*  use pid value or pid value with namespace to find the corresponding task.  */
	       */

	      /*  attach_pid - attach task with pid.
	       *  @task : the task pointer.
	       *  @type : pid type.
	       *  @pid : the pid attach to.
	       */
	      extern void attach_pid(struct task_struct *task, enum pid_type type, struct pid *pid);

	      /*  detach_pid - detach task with pid.
	       *  @task : the task pointer.
	       *  @second-arg : the pid type.
	       *  #  @task will attach to NULL pid structure.
	       *     and if pid_link[PIDTYPE].pid->tasks[ALL] is empty,then free_pid(pid).
	       */
	      extern void detach_pid(struct task_struct *task, enum pid_type);

	      <linux/sched.h>
	        /*  next_thread - retrive the next thread.
		 *  @p : the task in the specified thread group.
		 *  return - next task pointer in the thread group or @p.
		 */
	        static inline struct task_struct *next_thread(const struct task_struct *p);

      How Processes Are Organized :
        runqueue lists group all processes in a TASK_RUNNING state.

	processes in a TASK_STOPPED,EXIT_ZOMBIE,EXIT_DEAD state are not linked in specific lists.
	there is no need to group processes in any of these three states.

	processes in a TASK_INTERRUPTIBLE,TASK_UNINTERRUPTIBLE state are subdivided into many classes,
	each of which corresponds to a specific event.in this case,the process state does not provide enough
	information to retrive the process quickly,so it is necessary to introduce additional lists of processes,
	there are called wait queues.

	Wait queues :
	  wait queues used in kernel particularly for interrupt handling,process synchronization,timing.

	  conditional waits on events :
	    a process wishing to wait for a specific event places itself in the proper wait queue and
	    relinquishes control.
	    therefore,a wait queue represents a set of sleeping processes,which are woken up by the kernel
	    when some condition becomes true.

	  <linux/wait.h>
	    typedef struct __wait_queue wait_queue_t;
	    typedef int (*wait_queue_func_t)(wait_queue_t *wait, unsigned mode, int flags, void *key);
	    struct __wait_queue {
	            unsigned int flags;
            #define WQ_FLAG_EXCLUSIVE	0x01
		    void *private;			/*  private data,generally stored task_struct address */
		    wait_queue_func_t func;
		    struct list_head task_list;
	    };

	    struct __wait_queue_head {
	            spinlock_t lock;
		    struct list_head task_list;
	    };
	    typedef struct __wait_queue_head wait_queue_head_t;

	    wait_queue_head_t {
	            ... -> wait_queue_t.task_list - > task_list -> wait_queue_t.task_list {
		            ... -> wait_queue_t.task_list -> wait_queue_t.task_list -> wait_queue_t.task_list -> ...
		    };
	    };

	  there are two kinds of sleeping processes :
	    exclusive processes - wait_queue_t.flags == 1 are selectively woken up by the kernel;
	    nonexlusive processes - wait_queue_t.flags == 0 are always woken up by the kernel when the event occurs.

	    #  two kinds to prevent race for a resource accessing just allow one process on it.
	       a process waiting for a resource that can be granted to just one process at a time is a typical
	       exclusive process.
	       processes waiting for an event that may concern any of them are nonexclusive.

	  Handling wait queues :
	    <linux/wait.h>
	      !  use these two macro to initialize a wait queue head object which is declared statically or dynamically.

	      /*  DECLARE_WAIT_QUEUE_HEAD - declare a wait_queue_head_t object statically.  */	    
	      #define DECLARE_WAIT_QUEUE_HEAD(name)  \
	              wait_queue_head_t name = __WAIT_QUEUE_HEAD_INITIALIZER(name);

	      extern void __init_waitqueue_head(wait_queue_head_t *q, struct lock_class_key *);
	      /*  init_waitqueue_head - initialize a wait_queue_head_t object which is dynamically allocated.  */
	      #define init_waitqueue_head(q) \
	              do {		     \
		              static struct lock_class_key __key;	\
			      __init_waitqueue_head((q), &__key);	\
		      } while (0)


	      !  use these two functions to initialize a wait queue entry with @task or with @wake_up_func.

	      /*  init_waitqueue_entry - initialize a wait_queue element with task @p.
	       *  @q : the target to be initialized.
	       *  @p : task pointer.
	       *  #  @q->flags = 0;
	       *     @q->private = @p;
	       *     @q->func = default_wake_function;	/*  for nonexclusive process  */
	       */
	      static inline void init_waitqueue_entry(wait_queue_t *q, struct task_struct *p);

	      /*  init_waitqueue_func_entry - initialize @q with @func.
	       *  @q : the wait_queue_t object's address.
	       *  @func : the wake up function.
	       *  #  @q->flags = 0;
	       *     @q->private = NULL;
	       *     @q->func = func;
	       */
	      static inline void init_waitqueue_func_entry(wait_queue_t *q, wait_queue_func_t func);


	      !  use these two macros to put "current" into a wait queue and automatically remove it later.

	      /*  DEFINE_WAIT_FUNC - declare a wait_queue_t object @name and initialize it with @function,
	       *                     this object's private field will be initialized to "current".
	       */
	      #define DEFINE_WAIT_FUNC(name, function)
	      /*  DEFINE_WAIT - put "current" process into wait queue and automatically remove it at the time
	       *                it is woken up.
	       *  #  autoremove_wake_function() will invokes default_wake_function() at first,then remove this
	       *     element from the wait queue.
	       */
	      #define DEFINE_WAIT(name)  DEFINE_WAIT_FUNC(name, autoremove_wake_function)


	      !  use these three functions to complete INSERT | INSERT INTO EXCLUSIVE | REMOVE operations.

	      /*  add_wait_queue - insert @wait to @q.
	       *  @q : the head.
	       *  @wait : the element.
	       *  #  for nonexclusive processes.
	       */
	      extern void add_wait_queue(wait_queue_head_t *q, wait_queue_t *wait);

	      /*  add_wait_queue_exclusive - insert @wait to @q.
	       *  #  for exclusive processes.
	       */
	      extern void add_wait_queue_exclusive(wait_queue_head_t *q, wait_queue_t *wait);

	      /*  remove_wait_queue - remove @wait from @q.
	       *  #  this function does not use the @q parameter,but it as an identifier.
	       */
	      extern void remove_wait_queue(wait_queue_head_t *q, wait_queue_t *wait);

	      !  use this function to check if the wait queue is active.

	      /*  waitqueue_active - check if @q is activing(it is not empty) now.  */
	      static inline int waitqueue_active(wait_queue_head_t @q);

            Process wishing to wait :
	    <linux/wait.h>
	      /*  sleep_on - let "current" sleep on @q and it will enter TASK_UNINTERRUPTIBLE state.  */
	      extern void sleep_on(wait_queue_head_t *q);
	      /*  interruptible_sleep_on - "current" will enter TASK_INTERRUPTIBLE state.  */
	      extern void interruptible_sleep_on(wait_queue_head_t *q);

	      /*  sleep_on_timeout - timer version,@timeout is the maximum time to sleep.
	       *                     "current" will enter TASK_UNINTERRUPTIBLE state.
	       *                     if "current" is woken up before timer expire,then
	       *                     the left time will be returned.
	       */
	      extern long sleep_on_timeout(wait_queue_head_t *q, signed long timeout);
	      /*  interruptible_sleep_on - TASK_INTERRUPTIBLE version.  */
	      extern long interruptible_sleep_on_timeout(wait_queue_head_t *q, signed long timeout);

	      !  the sleep_on()-like functions cannot be used in the common situation where one has to
	         test a condition and atomatically put the process to sleep when the condition is not
		 verified.
		 they are a well-known source of race conditions,their use is discouraged.
	         #  sleep_on()-like functions were defined in <kernel/sched.c>.
		    for sleep_on(),it calls to sleep_on_common(),that function sets "current"'s state to
		    TASK_UNINTERRUPTIBLE,initializes a wait entry with "current",and insert it into @q,
		    calls schedule_timeout() with MAX_SCHEDULE_TIMEOUT left current process sleeping.

	      /*  prepare_to_wait - does the prepare works for "current" is going to wait.
	       *  @q : the wait queue head.
	       *  @wait : the wait entry.
	       *  @state : the state "current" will be.
	       *  #  this function will set "current"'s state to @state at first,then insert @wait
	       *     into @q.
	       *  #  this version is defined for nonexclusive process.
	       */
	      void prepare_to_wait(wait_queue_head_t *q, wait_queue_t *wait, int state);

	      /*  prepare_to_wait - exclusive version.  */
	      void prepare_to_wait_exclusive(wait_queue_head_t *q, wait_queue_t *wait, int state);

	      /*  finish_wait - finish "current" waiting.
	       *  @q : the wait queue head.
	       *  @wait : the wait entry.
	       *  #  this function will removes @wait from @q,and sets "current"'s state to
	       *     TASK_RUNNING(this will happens before removing).
	       */
	      void finish_wait(wait_queue_head_t *q, wait_queue_t *wait);

	      Usage for prepare_to_wait_*() and finish_wait() :
	        #define __wait_event(wq, condition)					\
		do {			 		      				\
		        DEFINE_WAIT(__wait);		      				\
							      				\
			for (;;) {			      				\
			        prepare_to_wait(wq, &__wait, TASK_UNINTERRUPTIBLE);	\
				if (condition)	    	     				\
				        break;		     				\
				schedule();		     				\
			}   	 			      				\
			finish_wait(wq, &__wait);	      				\
		} while (0)
		  

	      /*  wait_event - "current" waiting for @condition gets TRUE.
	       *  @wq : the wait queue head pointer.
	       *  @condition : the condition is a C expression.
	       *  #  "current" will enter TASK_UNINTERRUPTIBLE state.
	       *     @condition will be checked each time @wq is woken up.
	       *  #  function wake_up() is used to wake a wait_queue up.
	       */	      
	      #define wait_event(wq, condition)		\
	      do {					\
	              if (condition)			\
	                      break;			\
		      __wait_event(wq, condition);	\
	      } while (0)

	      /*  wait_event_timeout - timer version.
	       *                       if "current" is woken up before timer expired,
	       *                       the left time will be returned.
	       */
              #define wait_event_timeout(wq, condition, timeout)
	      
	      /*  wait_event_interruptible - TASK_INTERRUPTIBLE version.
	       *  return - interrupted,returns -ERESTARTSYS;
	       *           condition got TRUE,returns 0.
	       */
	      #define wait_event_interruptible(wq, condition)


            Kernel wake up the waiting processes :
	    <linux/wait.h>
	      /*  __wake_up* - the main procedure to wake up processes.
	       *               defined in <kernel/sched.c>
	       */	    
	      void __wake_up(wait_queue_head_t *q, unsigned int mode, int nr, void *key);
	      void __wake_up_locked(wait_queue_head_t *q, unsigned int mode);
	      void __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr);

	      /*  TASK_UNINTERRUPTIBLE processes.  */
	      #define wake_up(x)  __wake_up(x, TASK_NORMAL, 1, NULL)
	      #define wake_up_nr(x, nr)  __wake_up(x, TASK_NORMAL, nr, NULL)
	      #define wake_up_all(x)  __wake_up(x, TASK_NORMAL, 0, NULL)
	      #define wake_up_locked(x)  __wake_up_locked((x), TASK_NORMAL)

	      /*  TASK_INTERRUPTIBLE processes.  */
	      #define wake_up_interruptible(x)  __wake_up(x, TASK_INTERRUPTIBLE, 1, NULL)
	      #define wake_up_interruptible_nr(x, nr)  __wake_up(x, TASK_INTERRUPTIBLE, nr, NULL)
	      #define wake_up_interruptible_all(x)  __wake_up(x, TASK_INTERRUPTIBLE, 0, NULL)
	      #define wake_up_interruptible_sync(x)  __wake_up_sync((x), TASK_INTERRUPTIBLE, 1)

	      !  all macros wakeup all nonexclusive processes.
	      	 that is the parameter @nr to __wake_up*() functions is 0,just wake up everything;
		 (includes all nonexclusive processes and all exclusive processes)
		 if @nr == small + venumber,then wake up all nonexclusive processes and one
		 exclusive process.(that is what wake_up() to do)
		 e.g. (code)
		   if (curr->func(curr, mode, wake_flags, key) &&
		           (flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
	                   break;
			                                  /*  negative number also is TRUE  */

	      !  TASK_NORMAL is used to wake up TASK_UNINTERRUPTIBLE processes;
	         TASK_INTERRUPTIBLE is used to wake up TASK_INTERRUPTIBLE processes.
	      !  '_nr' suffix macros are used to wake up exclusive processes with the given @nr.
	      	 @nr => @nr_exclusive.
	      !  '_all' suffix macros will wake up all exclusive processes.
	      !  '_sync' suffix macro check whether the priority of any of the woken processes is
	      	 higher than that of the processes currently running in the systems and incoke
		 schedule() if necessary.(these checks is not made by this macro)
	      !  '_locked' suffix macro requires the wait_queue_head_t.lock has been held.
	      
      
      Process Resource Limits :
        each process has an associated set of resource limits,which specify the amount of system resources
	it can use.

	<linux/sched.h>
	  struct signal_struct {
	          ...
		  struct rlimit rlim[RLIM_NLIMITS];
		  ...
	  };

	  task_struct.signal->rlim;

	<linux/resource.h>
	  struct rlimit {
	          unsigned long rlim_cur;  /*  the current resource limit  */
		  unsigned long rlim_max;  /*  the maximum resource limit  */
	  };

	!  only the superuser(or,more precisely,a user who has the CAP_SYS_RESOURCE capability) can increase the 
	   @rlim_max or set the @rlim_cur to a value greater than the corresponding @rlim_max field.

	Kernel use the @index of @rlim member to identify the type of resource-limit.
	Toltal number of resource-limits in Linux 2.6 is 16,they are defined in <include/asm-generic/resource.h>.

	#define RLIMIT_CPU  0		        /*  if cpu time exceeds,kernel will send SIGXCPU to the process,  */
	  /*  CPU time in sec  */		/*  then,if the process does not terminate,SIGKILL will be sent.  */
	#define RLIMIT_FSIZE  1			/*  if process wish to enlarge filesize greater than this limit,  */
	  /*  maximum filesize  */		/*  kernel will send SIGXFSZ  signal.  */
	#define RLIMIT_DATA  2			/*  heap size in bytes  */
	  /*  max data size  */
	#define RLIMIT_STACK  3
	  /*  max stack size  */
	#define RLIMIT_CORE  4			/*  if limit == 0,kernel does not creates core dump file  */
	  /*  max core file size  */

	#ifndef ...
	#define RLIMIT_RSS  5
	  /*  max resident set size  */
	#define RLIMIT_NPROC  6
	  /*  max number of processes  */
	#define RLIMIT_NOFILE  7
	  /*  max number of open files  */
	#define RLIMIT_MEMLOCK  8		/*  size of nonswappable memory in bytes  */
	  /*  max locked-in-memory address space  */
	#define RLIMIR_AS  9	   	        /*  kernel checks this limit when malloc() was called  */
	  /*  address space limit  */
	#define RLIMIT_LOCKS  10
	  /*  maximum file locks held  */
	#define RLIMIT_SIGPENDING  11
	  /*  max number of pending signals  */
	#define RLIMIT_MSGQUEUE  12
	  /*  maximum bytes in POSIX mqueues  */
	#define RLIMIT_NICE  13
	  /*  max nice prio allowed to raise to  0-39 for nice level 19 .. -20  */
	#define RLIMIT_RTPRIO  14
	  /*  maximum realtime priority  */
	#define RLIMIT_RTTIME  15
	  /*  timeout for RT tasks in us  */
	#endif 

	#define RLIMIT_NLIMITS  16

	/*  resource limit value  */
	#ifndef RLIM_INFINITY
	#define RLIM_INFINITY  (~0UL)
	  /*  no user limit is imposed on the corresponding resource  */
	#endif


      Process Switch :
        kernel suspends the executing of the process that is running on the CPU and resume it later.
	kernel suspends a process and pick up another process to be executing.

	these different names all refers to the process switch : process switch, task switch, context switch

	!  Process Switch occurs only in Kernel Mode.

	Hardware Context :
	  the set of data that must be loaded into the registers before the process resumes its execution on the
	  CPU is called the "hardware context".it is the subset of the process execution context,which includes all
	  information needed for the process execution.

	  Linux,a part of hardware context is stored in process descriptor,while the remaining part is saved in the
	  Kernel Mode stack.

	  !  because process switches occur quite often,it is important to minimize the time spent in saving and 
	     loading hardware contexts.

	  /*  Old versions of Linux took advantage of the hardware support offered by the 80x86 architecture and
	   *  performed a process switch through a "far jmp" instruction to the selector of the 
	   *  Task State Segment Descriptor of the next process.
	   */

	  Linux 2.6 uses software to perform a process switch for the following reasons :
	    >  Step-by-step switching performed through a sequence of "mov" instructions allows better control
	       over the validity of the data being loaded.
	       (it is possible to check the values of the ds and es segmentation registers)

	    >  the amount of time required by the old approach and the new approach is about the same.
	       However,it is not possible to optimize a hardware context switch,while there might be room
	       for improving the current switching code.

	  Linux stores the contents of all registers used by a process in User Mode into the Kernel Mode
	  stack before performing process switching.(includes ss esp .etc)

        Task State Segment :
	  80x86 architecture includes a specific segment type called Task State Segment(TSS) to store hardware
	  contexts.Linux does not use hardware context switching,but it is still set up TSS for each CPU in the
	  system.
	  the reasons :
	    >  when an 80x86 CPU switches from User Mode to Kernel Mode,it fetches the address of the Kernel Mode
	       stack from the TSS.
	    >  when a User Mode process attempts to access an I/O port by means of an "in" or "out" instruction,
	       the CPU may need to access an I/O Permission Bitmap stored in the TSS to verify whether the process
	       is allowed to address the port.

	       #  a process executes an "in" or "out" I/O instruction in User Mode :
	            the control unit performs the following operations >
		      1>  checks the 2-bit IOPL field in the "eflags" register.
		      	  IOPL == 3, executes I/O instructions,
			  Otherwise, performs the next check.
	              2>  accesses the "tr" register to determine the current TSS,and thus the proper
		      	  I/O Permission Bitmap.
	              3>  checks the bit of the I/O Permission Bitmap corresponding to the I/O port
		      	  specified in the I/O instruction.
			  cleared => instruction is executed,
			  setted  => raises a "General protection" exception.

          the tss_struct structure describes the format of the TSS :
	    <arch/x86/include/asm/processor.h>
	      struct tss_struct {
	              struct x86_hw_tss	x86_tss;
		      /*  extra 1 is there because CPU will access an
		       *  additional byte,and the extra byte must be all 1 bits.
		       */
		      unsigned long io_bitmap[IO_BITMAP_LONGS + 1];
		      unsigned long stack[64];  /*  another 0x100 bytes for emergency kernel stack  */
	      } ____cachingline_aligned;

	  the init_tss array stores one TSS for each CPU on the system :
	    <arch/x86/include/asm/processor.h>
	      DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss);
	      /*  the invoked macro is defined in <linux/percpu-defs.h>  */

	  !  at each process switch,the kernel updates some fields of the TSS so that the corresponding
	     CPU's control unit may safely retrive the information it needs.
	     (TSS reflects the privilege of the current process on the CPU,but there is no need to maintain
	      TSSs for processes when they're not running.)

	  TSS <=> TSSD (Task State Segment Descriptor)
	    /*  in the Intel's original design,each process in the system should refer to its own TSS;
	     *  the second least significant bit of the Type field is called the Busy bit;
	     *  it is set to 1 if the process is being executed by a CPU,and to 0 otherwise.
	     *  in Linux design,there is just one TSS for each CPU,so the Busy bit is always set to 1.
	     */

	  TSSDs stored in GDT,whose base address is stored in the gdtr register of each CPU.
	  the tr register of each CPU contains the TSSD Selector of the corresponding TSS.
	  the register also includes two hidden,nonprogrammable fields :
	    the Base field of the TSSD
	    the Limit field of the TSSD
	  
	  The thread field :
	    at every process switch,the hardware context of the process being replaced must be saved in
	    somewhere,but it can not be stored in TSS,because in Linux Design,one TSS for one CPU.

	    /*  thread - task_struct member which type is thread_struct  */
	    task_struct.thread;	 /*  CPU-specific state of this task  */

	    <arch/x86/include/asm/processor.h>
	      struct thread_struct {
	              ...
	      };
	      /*  this structure represents the CPU-specific state for a task,
	       *  Linux saves hardware context in such object whenever the process is being switched out.
	       *  this structure contains fields for most of the CPU registers,except the general-purpose
	       *  registers such as eax,ebx, etc.,which are stored in the Kernel Mode stack.
	       */


        Performing the Process Switch :
	  A process switch may occur at just one well-defined point :
	    the schedule() function.(<include/linux/sched.h>, <kernel/sched.c>)
	    
	  every process switch consists of two steps :
	    1>  switching the Page Global Directory to install a new address space.
	    2>  switching the Kernel Mode stack and the hardware context,which provides all the
	    	information needed by the kernel to execute the new process,including the CPU registers.

          @prev -> the process been replaced.
	  @next -> the process being activated.

	  The switch_to macro :
	    <include/asm-generic/system.h>
              /*  switch_to - switch previous process to next process.
	       *  @prev : the previous process,it is often got by "current".
	       *  @next : the next process switch to.
	       *  @last : it is an output parameter that specifies a memory location
	       *  	  in which the macro writes the descriptor address of process C.
	       *	  (this is done after A resumes its execution)
	       *	  before the process switching,the macro saves in the eax CPU register
	       *	  the content of the variable identified by the first input parameter @prev,
	       *	  after the process switching,when A has resumed its execution,the macro
	       *	  writes the content of the eax CPU register in the memory location of A
	       *	  identified by the third output parameter @last.
	       *  #  this function will call to the architecture based __switch_to()
	       *     function to accomplishes the primary works,that function is
	       *     defined in <arch/"model"/asm/process_(32 | 64).h>
	       */
	      #define switch_to(prev, next, last)			\
	              do {						\
		              ((last) = __switch_to((prev), (next)));	\
		      } while (0)

	      !  there is another switch_to() function is defined in <arch/x86/include/asm/system.h>,
	      	 which has introduced the detail for how process switching be executed.
		 the switch_to() in <asm/system.h> is the architecture based switch_to() function,
		 not the asm-generic version.
		 for x86,this switch_to() will be called by context_switch() function which is defined
		 in <kernel/sched.c>.(because <linux/sched.h> includes <asm/systemd.h>)
		 the primary assembly :
		   pushfl			#  save eflags
		   pushl %%ebp			#  save base stack pointer
		   movl	 %%esp, %[prev_sp]	#  save  stack pointer
		   movl	 %[next_sp], %%esp	#  restore stack pointer
		   movl	 $1f, %[prev_ip]	#  save ip
		   pushl %[next_ip]		#  push ip into stack for ret instruction
		   __switch_canary		#  macro defined in <asm/system.h>
		   jmp   __switch_to		#  jump to __switch_to() function
		 1:	 			#  symbol
		   popl	 %%ebp			#  restore base stack pointer
		   popfl 			#  restore eflags


	      figure :
	        switch_to(A, B, A) {
		  A {
		    prev = A
		    next = B
		    eax = A
		    last = A
		  }

		  B {
		    prev = B
		    next = other
		    eax = A	/*  this register will be updated after function invocation completed
		    	  	 *  or process switching was occurred.
				 */
	            last = A
		  }
		}
		/*  there just one kernel existed,so the code for context switching is same between
		 *  processes.
		 */
		switch_to(C, A, C) {
		  C {
		    prev = C
		    next = A
		    eax = C
		    last = C
		  }

		  A {
		    prev = A
		    next = other
		    eax = C	/*  this register will be updated after function invocation completed
		    	  	 *  or process switching was occurred.
				 */
		    last = C
		  }
		}

		!  the process has been switched at the time that @next->thread.esp was loadded into
		   esp register.
		   (this operation is take affect when arch_end_context_switch(@next) was called)
		   the kernel stack of previous process is saved in @prev->thread.esp.
		   (this operation is accomplished when arch_start_context_switch(@prev) was called)
	   

	  The __switch_to() function :
	    <arch/x86/include/asm/system.h>
	    <arch/x86/kernel/process_(32 | 64).c>

	      /*  __switch_to - does the bulk of the process switch started by the switch_to() macro.
	       *  @prev_p : previous task pointer.
	       *  @next_p : next task pointer.
	       *  return -  @prev_p.
	       *  	    switch_to() macro will replaces esp register(ebp was not replaced,it was pushed
	       *            into the stack of @prev and popped up later) before jmp to __switch_to().
	       *	    but the arguments of __switch_to() are stored in CPU generic-purpose registers
	       *	    that is eax(@prev_p) and edx(@next_p),finally,it returns the value in eax register,
	       *	    so it is @prev_p.
	       *  #  __attribute__(regparm(3)) should be attached to __switch_to(),but it did not detect such
	       *     GCC attribute is used.
	       *     regparm(number) : this attribute just take affect only if x86-32 targers,it tell compiler
	       *     		       that,store the parameter from number one to @number in CPU registers
	       *		       eax,edx,ecx(so the maximum @number is 3) to instead store them on stack.
	       */
	      __notrace_funcgraph struct task_struct *
	      __switch_to(struct task_struct *prev_p, struct task_struct *next_p);

	      the works this function does :
	        /*  @prev_p -> previous task pointer.
		 *  @prev   -> previous task's thread structure.
		 *  @next_p -> next task pointer.
		 *  @next   -> next task's thread structure.
		 */
	        1>  get local CPU id and local tss.
		2>  check if the task @next_p is used math function,if it used and
		    @next_p->fpu_counter > 5,then set @preload_fpu to true.
		3>  call to "__unlazy_fpu(prev_p);" this macro optionally save the contents of the FPU,MMX,XMM of
		    @prev_p.
		4>  if @preload_fpu is T => prefetch(@next->xstate) .
		5>  call to "load_sp0(@tss, @next);",load @next->esp0 into @tss->esp0.
		    any future privilege level change from User Mode to Kernel Mode raised by a sysenter assembly
		    instruction will copy this address in the esp register.
		6>  call to "lazy_save_gs(@prev->gs);",this macro is defined through savesegment(gs, (v)),it stores
		    gs register in the @prev->gs.
		7>  call to "load_TLS(@next, @cpu);",loads in the Global Descriptor Table of the local CPU the 
		    Thread-Local Storage segments used by the @next_p process,the Segment Selectors are stored in
		    @next->tls_array member.
		8>  restore IOPL if needed.
		    in normal use,the flags restore in the switch assembly will handle this,but if the kernel
		    is running virtualized at a non-zero CPL,the popf will not restore flags,so it must be done
		    in a separate step.
		9>  handle debug registers and/or IO bitmaps.
		    if this is necessary,call to "__switch_to_xtra(@prev_p, @next_p, @tss);".
		10> if @preload_fpu is T => execute clts instruction.
		11> call to "arch_end_context_switch(@next_p);" to ensure context switching has been completed.
		12> if @preload_fpu is T => call to "__math_stat_restore();" restore math registers.
		13> restore gs if needed(if (@prev->gs || @next->gs) lazy_load_gs(@next->gs); ).
		14> update percpu data via "percpu_write(current_task, @next_p);").
		15> return @prev_p.


          Saving and Loading the FPU,MMX,XMM registers :
	    from Intel 80486DX,the arithmetic floating-point unit(FPU) has been integrated into the CPU.the name
	    'mathematical coprocessor' continues to be used in memory of the days when floating-point computations
	    were executed by an expensive special-purpose chip.
	    ESCAPE instructions(for compatible with older models) are instructions with a prefix byte ranging
	    between 0xd8 -- 0xdf,these instructions act on the set of floating-point registers included in the CPU.
	    !  if a process is using ESCAPE instructions,the contents of the FPU registers belong to its hardware
	       context,so they are should be saved and restored later.

	    MMX : new instructions were introduced on Pentium models,supposed to speed up the execution of multimedia
	    	  applications.
		  MMX instructions act on the FPU registers.
		  disadvantage : programmers can not mix FPU instructions and MMX instructions.
		  advantage    : for OS designer,save the FPU state is to save MMX state.
		  MMX introduced SIMD(single-instruction multiple-data) pipeline inside the processor.

		  !  Pentium III model extends that SIMD capability :
		       it introduces the SSE extensions(Streaming SIMD Extensions),which adds facilities for handling
		       floating-point values caontained in eight 128-bit registers called the XMM registers.
		       (XMM0 -- XMM7)
		       XMM registers do not overlap the FPU and MMX registers.(it is able to mix SSE and FPU/MMX)
		  !  Pentium 4 model introduces yet another feature : SSE2 Extensions.
		       which is basically an extension of SSE supporting higher-precision floating-point values,
		       it uses the same set of XMM registers as SSE.

            80x86 model do not save the FPU,MMX,XMM registers in the TSS automatically,but it enables kernel to do
	    that if necessary.

	    cr0.TS flag : TS(Task-Switching)
	    	   	  which obeys the following rules :
			    >  every time a hardware context switch is performed,the TS flag is set.
			    >  every time an ESCAPE,MMX,SSE,SSE2 instruction is executed when the TS flag is set,
			       the control unit raises a "Device not available" exception.

	      figure :
	        A is using mathematical coprocessor;
		switch occurs from A to B,cr0.TS = 1,saves floating-point registers to A.tss;
		B is not using mathmetical coprocessor => kernel do not need to restore floating-point registers;
		B try to use mathmetical coprocessor -> cr0.TS has been setted -> "Device not available" exception;
		kernel handle the exception => restore the floating-point registers from B.tss;

            FPU,MMX,XMM structures :
	      <arch/x86/include/asm/processor.h>
	        struct i387_fsave_struct;	/*  FPU,MMX state  */
		struct i387_fxsave_struct;	/*  SSE,SSE2 state */
		struct i387_soft_struct;	/*  older compatibility  */
		       				/*  it is used for the older CPU model which is no
						 *  mathmetical coprocessor.
						 */
		union thread_xstate {
		        struct i387_fsave_struct fsave;
			struct i387_fxsave_struct fxsave;
			struct i387_soft_struct soft;
			struct xsave_struct xsave;
		};  /*  thread_struct.xstate (pointer type)  */

            the process descriptor includes two additional flags :
	      task_struct.thread_info.status { TS_USEDFPU }
	      task_struct.flags { PF_USED_MATH }

	      TS_USEDFPU : it specifies whether the process used the FPU,MMX,XMM registers in the current execution run.
	      PF_USED_MATH : it specifies whether the contents of the thread_struct.xstate are significant,the flag is
	      		     cleared in two cases :
			       1>  when the process starts executing a new program by invoking an execve() system call,
			       	   because the control will never return to the former program,the data currently stored
				   in thread_struct.xstate is never used again.
			       2>  when a process that was executing a program in User Mode starts executing a signal
			       	   handler procedure,because signal handlers are asynchronous with respect to the program
				   execution flow,the floating-point registers could be meaningless to the signal handler.
				   however,the kernel saves the floating-point registers in thread_struct.xstate before
				   starting the handler and restores them after the handler terminates.(that is the signal
				   handler is allowed to use floating-point features)

            Saving the FPU registers :
	      <arch/x86/include/asm/i387.h>
	        /*  __unlazy_fpu - save fpu state.
		 *  @tsk : the target,which often is @prev in __switch_to().
		 *  return - none.
		 *  #  this function checks if TS_USEDFPU flag of @tsk's thread_info.status is set,
		 *     TS_USEDFPU == 1,then call to __save_init_fpu(@tsk),and call to stts().
		 *     TS_USEDFPU == 0,then @tsk->fpu_counter = 0.
		 */
	        static inline void __unlazy_fpu(struct task_struct *tsk);

		/*  __save_init_fpu - save fpu state and then initializes them.
		 *  @tsk : the target.
		 *  return - none.
		 *  #  this function maybe call to either xsave(@tsk) or fxsave(@tsk),that is determined by
		 *     task_thread_info(@tsk)->status & TS_XSAVE.
		 *     then call to clear_fpu_state(@tsk) to initializes fpu state to fixed values,
		 *     and clear TS_USEDFPU in the @tsk's thread_info.status.
		 */
		static inline void __save_init_fpu(struct task_struct *tsk);

              <arch/x86/include/asm/system.h>
	        /*  stts - a macro sets cr0.TS flag.  */
	        #define stts() write_cr0(read_cr0() | X86_CR0_TS)

            Loading the FPU registers :
	      the contents of the floating-point registers are not restored right after the @next process
	      resumes execution.(but __switch_to() restored it if @preload_fpu is TRUE)
	      however,the TS flag of cr0 has been set by __unlazy_fpu(),thus,the first time the @next process
	      tries to execute an ESCAPE,MMX,SSE/SSE2 instruction will traps an exception,then handler calls to
	      math_state_restore() to restore the contents.

	      <arch/x86/include/asm/i387.h>
	        /*  math_state_resotre - exception handler to deal with use mathematical coprocessor when TS == 1.
		 *  #  this function checks @task is used math at first(PF_USED_MATH flag),
		 *     if it is not,then call to init_fpu() to initializes FPU(PF_USED_MATH set to 1) before
		 *     next execution;
		 *     call to clts() to clear cr0.TS,then invoke __math_state_restore() to do primary works.
		 *     (if TS == 1,execute ESCAPE,MMX,SSE/SSE2 instruction will traps exception)
		 */
	        extern asmlinkage void math_state_restore(void);

		/*  __math_state_restore - restores fpu for @tsk and set TS_USEDFPU.
		 *  #  @tsk = thread->task;  =>
		 *     @thread = current_thread_info();
		 *     call to restore_fpu_checking(@tsk),(this function execute "fxrstor" instruction)
		 *     set TS_USEDFPU,
		 *     @tsk->fpu_counter++.
		 */
		extern void __math_state_restore(void);

            Using the FPU,MMX,and SSE/SSE2 units in Kernel Mode :
	      !  IF IT IS NOT NECESSARY,DO NOT USE x87 IN KERNEL MODE.
	      use x87 in Kernel Mode is more expensive than User Mode.
	      
	      if kernel use FPU,it should avoid interfering with any computation carried on by the current 
	      User Mode process.

	      <arch/x86/include/asm/i387.h>
	        /*  kernel_fpu_begin - kernel ready to use FPU.
		 *  #  disable preempt,
		 *     checks if current thread is used FPU(TS_USEDFPU),then save the state,
		 *     if it is not,just clts().
		 */
	        static inline void kernel_fpu_begin(void);

		/*  kernel_fpu_end - kernel end use FPU.
		 *  #  set cr0.TS by stts(),
		 *     enable preempt.
		 */
		static inline void kernel_fpu_end(void);

              because the FPU state of User Mode process has been saved,so math_state_restore() will be called
	      later when the process tries to execute an ESCAPE,MMX,SSE/SSE2 instruction.

	      !  the kernel uses FPU only in a few places,typically when moving or clearing large memory areas
	      	 or when computing checksum functions.


