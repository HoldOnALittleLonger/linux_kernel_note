                                                Understanding the Linux Kernel (Third Edition)
Linux kernel version : 2.6.34.1

Chapter 1 : Introducation
    Linux joins System V Release 4 (SVR4),developed by AT&T.
    other UNIX OS or UNIX-like OS :
      4.4 BSD
      Digital UNIX
      AIX
      HP-UX
      Solaris
      Mac OS X
      ...

      (opensource)
      FreeBSD
      NetBSD
      OpenBSD
      Linux
      ...

    The common attributes between Linux and well-known commercial Unix kernels :
      monolithic kernel
      compiled and statically linked traditional Unix kernels
      (of course support module for dynamically load and unload)
      kernel threading
      multithreaded application support(lightweight process LWP supporting)
      preemptive kernel
      multiprocessor support(SMP)
      filesystem

      streams(but Linux has no analogue to the STREAMS I/O subsystem introduced in SVR4)  

    The advantages of Linux :
      Linux is cost-free
      Linux is fully customizable in all its components
      Linux runs on low-end,inexpensive hardware platforms
      Linux is powerful
      Linux developers are excellent programmers
      The linux kernel can be very small and compact
      Linux is highly compatible with many common operating systems
      Linux is well supported

    Basic Operating System Concepts :
      the operating system must fulfill two main objectives :
        >  interact with the hardware components,servicing all low-level programmable elements
           included in the hardware platform.
        >  provide an execution environment to the applications that run on the computer system.
    
      modern OS does not allow user program interact with hardware directly and forbid them to
      access arbitrary memory locations.
      in particular,the hardware introduces at least two different execution modes for the CPU :
        a nonprivileged mode for user programs
        a privileged mode for the kernel
        /*  UNIX calls these
         *  User Mode and Kernel Mode
         */

    Multiuser Systems :
      A multiuser system is a computer that is able to concurrently and independently execute several
      applications belonging to two or more users.

      Concurrently :
        applications can be activated at the same time and contend for various resources such as CPU,memory,
        hard disks,and so on.
      Independently :
        each application can perform its task with no concern for what the applications of the other uses are
        doing.

      Multiuser operating systems must include several features :
        >  An authentication mechanism for verifying the user's identify
        >  A protection mechanism against buggy user programs that could block other applications running 
           in the system
        >  A protection mechanism against malicious user programs that could interfere with or spy on the 
           activity of other users
        >  An accounting mechanism that limits the amount of resource units assigned to each user

      To ensure safe protection mechanisms,operating systems must use the hardware protection associated 
      with the CPU privileged mode.
      Unix is a multiuser system that enforces the hardware protection of system resources.

    Users and Groups :
      in a multiuser system,each user has a private space on the machine,the operating system must ensure that
      the private portion of a user space is visible only to its owner.Unix-like system use UserID or UID as
      user identifier,it is a unique number.
      to selectively share material with other users,each user is a member of one or more user groups,which 
      are identified by a unique number called a user group ID.each file is associated with exactly one group.
      any Unix-like operating system has a special user called root or superuser,the root user can do almost
      everything,because the operating system does not apply the usual protection mechanism to it.

    Processes :
      A process can be defined either as "an instance of a program in execution" or  as the "execution context"
      of a running program.
      in traditional operating systems,a process executes a single sequence of instructions in an address space;
      the address space is the set of memory addresses that the process is allowed to reference.

      multiple processes environment :
        in modern operating system allow processes with multiple execution flows,that is,multiple sequences
        of instructions executed in the same address space.
        system that allow concurent active processes are said to be multiprogramming or multiprocessing.

      multiuser systems must enforce an multiple processes environment,but some multiprocessing operating
      systems are not multiuser.

      it is important to distinguish programs from processes;several processes can execute the same program
      concurently,while the same process can execute several programs sequentially.

      scheduler is an important system compoent in multiprocessing system,which process should be hold the 
      CPU to execute is determined by scheduler.
    
      preemptable :
        some operating systems allow only nonpreemptable processes,which means that the scheduler is invoked
        only when a process voluntarily relinquishes the CPU.but process of a multiuser system must be
        preemptable.

        UNIX is a multiuer and multiprocessing operating system with  preemptable processes.

        UNIX-like operating systems adopt a process/kernel model.each process has the illusion that it is the
        only process on the machine,and it has exclusive access to the operating system services.

    Kernel Architecture :
      monolithic kernel,each kernel layer is integrated into the whole kernel program and runs in Kernel Mode
      on behalf of the current process.

      microkernel operating systems demand a very small set of functions from the kernel,generally including a 
      few synchronization primitives,a simple scheduler,and an interprocess communication mechanism.
      several system processes that run on top of the microkernel implement other operating system layer functions,
      like memory allocators,device drivers,and system call handlers.

      comparison between monolithic kernel and microkernel :
        1>  microkernel is slower than monolithic kernel,because kernel layer communication has a cost.
        2>  microkernel force the system programmers to adopt a modularized approach,that means microkernel
            is very modularized.every relativly kernel layer is independent program that must interact with
            the other layers through well-defined and clean software interfaces.
        3>  microkernel can be easily ported to other architectures fairly easily.all hardware-dependent components
            are generally encapsulated in the microkernel code.
        4>  microkernel tend to make better use of random access memory than monolithic kernel,system processes
            that are not implementing needed functionalities might be swapped out or destroyed.

        UNIX and UNIX-like operating system almost satisfy all advantages of microkernel.

      the main advantages of using modules include :
        >  a modularized approach.
        >  platform independence
        >  frugal main memory usage
        >  no performance penalty

    An Overview of Unix Filesystem :
      the UNIX operating system design is centered on its filesystem,which has several interesting characteristics.

        Files - A Unix file is an information container structured as a sequence of bytes;the kernel does not
                interpret the contents of a file.

        Hard and Soft Links - A filename included in a directory is called a file hard link,or more simply,a link.
                              Soft links also called symbolic links,symbolic links are short files that contain an
                              arbitrary pathname of another file.

        Hard links limitations :
          it is not possible to create hard links for directories.
          links can be created only among files included in the same filesystem.

        File Types - File type represent what the file is.
          Unix files may have one of the following types :
            Regular file
            Directory
            Symbolic link
            Block-oriented device file
            Character-oriented device file
            Pipe and named pipe
            Socket

        File Descriptor and Inode - A file descriptor is a number greater than or equal to zero,the number
                                    associared with a file data structure which represent the file opened by
                                    process.
                                    All information needed by the filesystem to handle a file is included in a 
                                    data structure called an inode.each file has its own inode,which the filesystem
                                    uses to identify the file.

        Access Rights and File Mode - there are three types of access rights :
          read
          write
          execute
          /*  access rights occupied nine bits  */
                      
        Three additonal flags :
          suid(Set User ID)
          sgid(Set Group ID)
          sticky - an executable file with the sticky flag set corresponds to request to
                   the kernel to keep the program in memory after its execution terminates.

        File Mode is a mode mixed Access Rights and Additonal flags.

        File-Handling System Calls -  kernel provide some primitives to user mode to help user interact with actual
                                      file stored in block device.
                                      opening a file - open
                                      accessing an opened file - read write lseek ...
                                      closing a file - close
                                      renaming and deleting a file - rename unlink link ...
                       
    An Overview of Unix Kernels :
      Unix kernels provide an execution environment in which applications may run.
      
      The Process/Kernel Mode - Actually,some CPUs can have more than two execution states,but all standard Unix
                                kernels use only Kernel Mode and User Mode.
        A program usually executes in User Mode and switches to Kernel Mode only when requesting
        a service provided by the kernel.when the kernel has satisfied the program's request,it
        puts the program back in User Mode.the way is system calls.
        after sets up parameters of syscall,then executes the hardware-dependent CPU instruction to
        switch from User Mode to Kernel Mode.
        Unix systems include a few privileged processes called kernel threads with the following characteristics :
          >  they run in Kernel Mode in the kernel address space
          >  they do not interact with users,and thus do not require terminal devices
          >  they are usually created during system startup and remain alive until the system
             is shut down.
        Unix kernels do much more than handle system calls;in fact,kernel routines can be activated in several ways :
          >  A process invokes a system call
          >  the CPU executing the process signals an exception,which is an unusual condition such as
             an invalid instruction.the kernel handles the exception on behalf of the process that 
             caused it.
          >  A peripheral device issues an interrupt signal to the CPU to notify it of an event such
             as a request for attention,a status change,of the completion of an I/O operation.
             each interrupt signal is dealt by a kernel program called an interrupt handler.
          >  A kernel thread is executed.because it runs in Kernel Mode,the corresponding program
             must be considered part of the kernel.

      Process Implementation - To let the kernel manage processes,each process is represented by a process descriptor that
                               includes information about the current state of the process.
                               that include :
                                 >  the program counter(PC) and stack pointer(SP) registers
                                 >  the general purpose registers
                                 >  the floating point registers
                                 >  the processor control registers(Processor Status Word) containing information about
                                    the CPU state
                                 >  the memory management registers used to keep track of the RAM accessed by the process

      When a process is scheduled,then kernel use the former stored information to recover process status,
      and set IP to the next instruction.(it also known as PC,process counter,but IP is a instruction register)

      Reentrant Kernels - All Unix kernels are reentrant.this means that several processes may be executing in Kernel Mode at
                          the same time.of course,on uniprocessor systems,only one process can progress,but many can be blocked
                          in Kernel Mode when waiting for the CPU or the completion of some I/O operation.
        reentrant functions : the functions they modify only local variables and do not alter global data structures.
                              (reentrant functions,that is how some real-time kernels are implemented)
                              but kernel can include nonreentrant functions and use locking mechanisms to ensure that only one process
                              can execute a nonreentrant function at a time.

              generial kernel control path :
                Run process -> Timer interrupt -> Deal with interrupt -> Schedule tasks -> Run process -> ...

              when one of the following events occurs,the CPU will enter a new path to do something :
                >  a process executing in User Mode invokdes a system call.if the service would not be satisfied,
                   then scheduler pick next process to run,the former process enter sleeping until condition is
                   satisifed or wake up by a signal.
                   (system calls is triggered via soft interrupt,on x86,it is "int 0x80")
                >  the CPU detects an exception.CPU must starts the execution of a suitable procedure.
                   after the procedure terminates,CPU return to the path before the exception is detected.
                >  a hardware interrupt occurs,and the interrupts enabled.
                   CPU must starts processing another kernel control path to handle the interrupt.
                >  an interrupt occurs while the CPU is running with kernel preemption enabled,and a higher priority
                   process is runnable.(lower intterupt could be preempted by higher interrupt)

              three states of CPU current be :
                >  running a process in User Mode                   (process context)
                >  running an exception or a system call handler    (process context)
                >  running an interrupt handler                     (interrupt context)

      Process Address Space - Each process runs in its private address space.A process running in User Mode refers to private stack,
                              data,and code areas.when running in Kernel Mode,the process addresses the kernel data and code areas
                              and uses another private stack.
                              reentrant kernel,each kernel control path refers to its own private kernel stack.
                              sometimes,kernel shares process address space to another same program,certainly that is code area.
                              but process also can initiativly shares its address space to another different program.(IPC)

      Synchronization and Critical Regions - reentrant kernel requires the use of synchronization.
                                             if a kernel control path is suspended while acting on a kernel data structure,no other
                                             kernel control path should be allowed to act on the same data structure unless it has
                                             been reset to a consistent state.
                                             critical region :
                                               any section of code that should be finished by each process that begins it before
                                               another process can enter it.

      Kernel preemption disabling - A synchronization mechanism applicable to preemptive kernel consists of disabling kernel 
                                    preemption before entering a critical region and reenabling it right after levaing the region.
                                    nonpreemptability is not enough for multiprocessor systems,because two kernel control paths running
                                    on different CPUs can concurrently access the same data structure.

      Interrupt disabling - Another synchronization mechanism for uniprocessor systems consists of disabling all hardware interrupts
                            before entering a critical region and reenabling them right after leaving it.
                            if critical region is too large,this will down system efficiency;moreover,on a multiprocessor system,
                            disabling interrupts on the local CPU is not sufficient.

      Semaphores - A semaphore is simply a counter associated with a data structure;it is checked by all kernel threads before they
                   try to access the data structure.
                   each semaphore may be viewed as an object composed of :
                     an integer variable
                     a list of waiting processes
                     two atomic method : down() and up()

                     down() decrease semaphore value,up() increase semaphore value.if its value is less than 0,process have to wait on
                     it until semaphore is available.

      Spin locks - some kernel data structures should be protected from being concurrently accessed by kernel control paths that run
                   on different CPUs.in this case,if the time required to update the data structure is short,a semaphore could be very
                   inefficient.
                   a spink lock is very similar to semaphore,but it has no process list;when a process finds the lock closed by another
                   process,it "spins" around repeatedly,executing a tight instruction loop until the lock becomes open.
                   of course,spin locks are useless in a uniprocessor environment.

      Avoiding deadlocks - the simplest case of deadlock occurs when process p1 gains access to data structure a and process p2 gains
                           access to b,but p1 then waits for b and p2 waits for a.
                           Several operating systems,including Linux,avoid this problem by requesting locks in a predefined order.

    Signals and Interprocess Communication :
      Unix signals provide a mechanism for notifying processes of system events.
      there are two kinds of system events :
        Asynchronous notifications
        Synchronous notifications

      POSIX standard defines about 20 different signals,2 of which are user-definable and may be used as a primitive mechanism for
      communication and synchronization among processes in UserMode.

      process may ignore the signal or asynchronously execute a specific procedure(signal handler),kernel provides the default
      signal handler for every signal.
      the default actions are :
        terminate the process
        write the execution context and the contents of the address space in a file(coredump) and terminate the process
        ignore the signal
        suspend the process
        resume the process's execution,if it was stopped

      SystemV IPC(AT&T's Unix System V) - semaphores, message queues, shared memory
      POSIX standard also defined some IPCs - posix semaphores, posix message queues, posix shared memory

    Process Management : 
      Unix makes a neat distinction between the process and the program it is executing.
      process is the program which is loadded into memory,it contains the resources all the program needs.
      the program it is executing,that means it had been loadded and the process is TASK_RUNNING.
      a parent process is such process it has been called fork() syscall,fork() would create a new process and its resources
      is duplicated to parent process,the process is child process.

      Copy-On-Write - this approach defers page duplication until the last moment(i.e., until the parent or the child is
                      required to write into a page).
                      the naive implementation of fork() was quite time consuming.

      Zombie process - a process would exited if it called _exit(),and kernel send SIGCHLD to its parent.
                       the zombie process is such process it had been exited but its status had not been checked,generally,
                       this work should be completed by its parent.
                       when the process exited,some resources are still effect and saved in its process address space,until
                       parent calls wait() systemcall to wait the process,that would destroy all resources.
                       if parent exited before wait its child,then kernel process init will adopts childs,that procedure calls
                       wait() periodically on its child processes.

      Process groups and login sessions - modern Unix operating systems introduce the notion of process groups to represent a 
                                          "job" abstraction.
                                          a job,it might be combined by several processes,and they are in the same process group,
                                          the group leader is the process whose PID is equal to the group ID.
                                          a new process would be inserted into its parent's group initially.
                                          normally,a job is treated as a single entity.
                                          moder Unix kernels also introduce login sessions.
                                          informally,a login session contains all processes that are descendants of the process that
                                          has started a working session on a specific terminal--usually,the first command shell process
                                          created for the user.
                                          all processes in a process group must be in the same login session,a login session may have
                                          several process groups active simultaneously;one of these process groups is always in the
                                          foreground,which means that it has access to the terminal.others are in the background.

    Memory Management :
      Virtual memory -  all recent Unix systems provide a useful abstraction called virtual memory.
                        virtual memory acts as a logical layer between the application memory requests and the hardware
            Memory Management Unit.(MMU)
            its puposes and advantages :
              several processes can be executed concurrently.
              it is possible to run applications whose memory needs are larget than the available physical memory.
              processes can execute a program whose code is only partially loaded in memory.
              each process is allowed to access a subset of the available physical memory.
              processes can share a single memory image of a library or program.
              programs can be relocatable--that is,they can be placed anywhere in physical memory.
              programmers can write machine-independent code,because they do not need to be concerned about physical
              memory organization.
            the main ingredient of a virtual memory subsystem is the notion of virtual address space.

      Random access memory usage - all Unix operating systems clearly distinguish between two portions of the
                                   random access memory(RAM).a few megabytes are dedicated to storing the kernel
                                   image.the remaining portion of RAM is usually handled by the virtual memory
                                   system and is used in three possible ways :
                                     to satisfy kernel requests for buffers,descriptors,and other dynamic kernel data structures.
                                     to satisfy process requests for generic memory areas and for memory mapping of files.
                                     to get better performance from disks and other buffered devices by means of caches.

      Kernel Memory Allocator - the kernel Memory Allocator(KMA) is a subsystem that tries to satisfy the requests fof memory areas
                                from all parts of the system.
                                a good KMA should have the following features :
                                  it must be fast.actually,this is the most crucial attribute,because it is invoked by all kernel
                                  subsystems.
                                  it should minimize the amount of wasted memory.
                                  it should try to reduce the memory fragmentation problem.
                                  it should be able to cooperate with the other memory management subsystems to borrow and release
                                  page frames from them.
                                  all recent Unix operating systems adopt a memory allocation strategy called demand pagine.
                                  with demand paging,a process can start program execution with none of its pages in physical memory.

      Caching - a good part of the available physical memory is used as cache for hard disks and other block devices.this is because
                hard drives are very slow.
                data read previously from disk and no longer used by any process continue to stay in RAM,and defer writing to disk as
                long as possible.
                the sync() system call forces disk synchronization by writing all of the "dirty" buffers into disk.to avoid data loss,
                all operating systems take care to periodically write dirty buffers back to disk.

    Device Drivers :
      the kernel interacts with I/O devices by means of device drivers.device drivers are included in the kernel and consist of
      data structures and functions that control one or more devices.
      each driver interacts with the remaining part of the kernel through a specific interface,this approach has the following
      advantages :
        >  device-specific code can be encapsulated in a specific module.
        >  vendors can add new devices without knowing ther kernel source code,only the interface specifications must be known.
        >  the kernel deals with all devices in a uniform way and accesses them through the same interface.
        >  it is possible to write a device driver as a module that can be dynamically loaded in the kernel without requiring
           the system to be rebooted.it is also possible to dynamically unload a module that is no longer needed,therefore
           minimizing the size of the kernel image strored in RAM.


/*  END OF CHAPTER1  */


Chapter 2 : Memory Addressing
    Memory Address :
      the three kinds of addresses on 80x86 microprocessors >
        1>  logical address
            included in the machine language instructions to specify the address of an operand or of an instruction.
            this type of address embodies the well-known 80x86 segmented architecture.
            each logical address consists of a segment and an offset that denotes the distance from the start of
            the segment to the actual address.

        2>  linear address(also known as virtual address)
            a signle 32-bit unsigned integer that can be used to address up to 4GB.
            linear addresses are usually represented in hexadecimal notation,their values range from
            0x00000000 -- 0xffffffff

        3>  physical address
            used to address memory cells in memory chips.
            they correspond to the electrical signals sent along the address pins of the microprocessor to the memory bus.
            they are represented as 32-bit or 36-bit unsigned integers.

      MMU Memory Management Unit,it transforms a logical address into a linear address by means of a hardware circuit called
      a segmentation unit.
      Page Unit,it transforms the linear address into a physical address.

      In multiprocessor systems,RAM chips may be accessed concurrently(CPUs share same memory).
      Memory Arbiter is inserted between the bus and every RAM chip,it is used to ensure serially operations perform.
      (if the RAM chip is free or it is busy servicing a request by another CPU(in this case,it delay the CPU's request))
      even uniprocessor systems use memory arbiters,because they include specialized processors called DMA(Direct Memory Access)
      controllers that operate concurrently with the CPU.
      (GPU might use DMA zone)

      /*  Memory Arbiter is unvisible to program  */

    Segmentation in Hardware :
      (80386 model)
      Intel microprocessors perform address translation in two different ways called "real mode" and "protected mode".

      Segment selectors and Segmentation registers :
        a logical address consists of two parts : a segment identifier and an offset that specifies the relative address
        within the segment.

        Logical Address : (Segment Identifier , Offset)
        segment identifier : a 16-bit field called the Segment Selector.
        offset : a 32-bit field associated the segment identifier(Segment Selector).

        Segment Selector { index (15-3), TI (2), RPL (1-0) }
        /*  TI : Table Indicator
         *  RPL : Requestor Privilege Level
         */

        processor provides six segmentation registers for retrieve segment selector quickly :
          cs, ss, ds, es, fs, gs
          /*  the only purpose is to hold Segment Selectors  */
        program could reuse some segmentation registers,but have to store its contents in memory,and then
        restore it later.

        cs : code segment register,which points to a segment containing program instructions.
        ss : stack segment register,which points to a segment containing the current program stack.
        ds : data segment register,which points to a segment containing global and static data.
        es,fs,gs : these are extra segment registers available for far pointer addressing like video
                   memory and such.
            
        the remaining three segmentation registers are general purpose and may refer to arbitrary data segments.

        cs register has another important function :
          it includes a 2-bit field that specifies the Current Privilege Level(CPL) of the CPU.
          value 0 denotes the highest privilege level,value 3 denotes the lowest one.
          (Linux only use 0(kernel mode RING0) and 3(user mode RING3))

          Segment Descriptors : 
            Global Descriptor Table(GDT)
            Local Descriptor Table(LDT)

        each segment is represented by an 8-byte segment descriptor,they are stored either in GDT or LDT.
        program is allowed to have its own LDT,if it have to stores some segments besides those stored in GDT.

        gdtr : gdtr control register,it contains the address and size of the GDT in main memory.
        ldtr : ldtr control register,it contains the address and size of the current LDT in main memory.

    Format of Segment Descriptor : (every 8-byte 64-bit)
      Fields >
        Base :      contains the linear address of the first byte of the segment.
        G :         granularity flags,if it is cleared(0),the segment size is expressed in bytes;otherwise,it is
                    expressed in multiples of 4096 bytes.
        Limit :     holds the offset of the last memory cell in the segment,thus binding the segment length.
        S :         system flag,if it is cleared(0),the segment is a system segment that stores critical data 
                    structures such as the Local Descriptor Table;otherwise it is a normal code or data segment.
        Type :      Characterizes the segment type and its access rights.
        DPL :       Descriptor Privilege Level,used to restrict accesses to the segment.it represents the minimal
                    CPU privilege level requested for accessing the segment.
        P :         Segment-Present flag,it is equal to 0 if the segment is not stored currently in main memory.
                    Linux always sets this flag (bit 47) to 1,because it never swaps out whole segments to disk.
        D or B :    called D or B depending on whether the segment contains code or data.
                    its meaning is slightly different in the two cases,but it is basically set(equal to 1) if the
                    addresses used as segment offsets are 32 bits long,and it is cleared if they are 16 bits long.
        AVL :       may be used by the operating system,but it is ignored by Linux.

      Composing :
        0-15 : LIMIT (0-15)
        16-31 : BASE (0-15)
        32-39 : BASE (16-23)
        40-43 : TYPE
        44 : S
        45-46 : DPL
        47 : P
        48-51 : LIMIT (16-19)
        52 : AVL
        53 : none
        54 : D or B
        55 : G
        56-63 : BASE (24-31)

    The Segment Descriptors widely used in Linux :
      Code Segment Descriptor -
        it may be included either GDT or LDT,the descriptor has the S flag set.
      Data Segment Descriptor - 
        data segment.included either GDT or LDT,has S flag set.
        stack segments are implemented by means of generic data segments.
      Task State Segment Descriptor(TSSD) -
        task state segment,it is a segment used to save the contents of the processor registers;it can appear
        only in the GDT.The corresponding Type field has the value 11 or 9,depending on whether the corresponding
        process is currently executing on a CPU.has no S flag.
      Local Descriptor Table Descriptor(LDTD) -
        a segment containing an LDT;it can appear only in the GDT.
        the corresponding Type field has the value 2.has no S flag.

    Fast Access to Segment Descriptors :
      80x86 processor provides an additional nonprogrammable register for each of the six programmable segmentation registers.
      each nonprogrammable register contains the 8-byte segment descriptor specified by the segment selector contained in the
      corresponding segmentation register.
      every time a segment selector is loaded in a segmentation register,the corresponding segment descriptor is loaded from
      memory into the matching nonprogrammable CPU register.

        segs1 --> cs  &&  segd1 --> unprogrammable(cs)

      CPU only need to access GDT or LDT is the time to change the contents of the segmentation registers!

      segment selector fields :
        index - identifies the segment descriptor entry contained in the GDT or in the LDT.
        TI    - table indicator,specifies whether the segment descriptor is inclued in the GDT(TI = 0) or
                in the LDT(TI = 1).
        RPL   - requestor privilege level,specifies the current privilege level of the CPU when the corresponding
                segment selector is loaded in to the cs register;it also may be used to selectively weaken the
                processor privilege level when accessing data segments.

      segment descriptor index = GDT + segment selector index field * 8 (every segment descriptor occupy 8-byte)

      ! the first entry in GDT always set to 0,this ensures that logical address with a null segment selector will be considered
        invalid,thus causing a processor exception.
      ! maximum of number of segment descriptors in GDT is 8191(2^13 - 1).

    Segmentation Unit :
      segmentation unit handle address translation.
      translate logical address to linear address : (in : read, out : write)
      /*  logical addres : (composed by) segment identifer, offset  */

        if register changed,then in segment register
        else in nonprogrammable register
        ->
        if TI == 1,  in LDT(ldtr)
        else TI == 0,  in GDT(gdtr)
        ->
        in index
        ->
        segment descriptor = index * 8 + GDT
        ->
        in segment descriptor
        ->
        in BASE field
        ->
        linear address = BASE + offset (LIMIT determine the length of the segment)

    Segmentation in Linux :
      in fact,segmentation and paging do same work,linear address to physical space.
      linux prefers paging to segmentation for these reasons :
        >  memory management is simpler when all process use the same segment register values,
           that is,when they share the same set of linear addresses.
        >  one of the design objectives of Linux is portability to a wide range of architectures;
           RISC architectures in particular have limited support for segmentation.
        /*  linux 2.6 use segmentation only when required by the 80x86 architecture.  */

      Linux processes running in User Mode use the same pair of segments to address instructions and data.
        user code segment AND user data segment (Segment Selector : __USER_CS, __USER_DS /* macros */)
      Linux processes running in Kernel Mode use the same pair of segments to address instructions and data.
        kernel code segment AND kernel data segment (Segment Selector : __KERNEL_CS, __KERNEL_DS /* macros */)

      CS means cs register, DS means ds register.

      all processes either in User Mode or in Kernel Mode,may use the same logical address.
      (linear address associated such segment start at 0,and reach the addressing limit of (2^32 - 1))

      Linux,logical address coincide with linear address.(so all segments start at 0x00000000)

      about RPL :
        when CPL of cs is changed,ds and ss have to correspondingly updated.
        e.g.
          CPL = 3, ds -> user data segment, ss -> user stack inside user data segment
          CPL = 0, ds -> kernel data segment, ss -> kernel stack inside kernel data segment

        ! When switching from User Mode to Kernel Mode,Linux always makes sure that the 
          ss register contains the Segment Selector of the kernel data segment.
          
      when saving a pointer to an instruction or to a data structure,the kernel does not need to store the
      Segment Selector component of the logical address,because the ss register contains the current 
      Segment Selector.
      /*  a function,it has a stack frame.and ss contains the Segment Selector used to get Segment Descriptor
       *  of the stack.so a pointer just contains the offset for the Segment Descriptor(in unprogrammable register).
       *  in the case for instruction is same,but use the cs register.
       */

    The Linux GDT :
      uniprocessor : one GDT
      multiprocessor : every processor one GDT

      all GDTs are stored in the "cpu_gdt_table" array,while the addresses and sizes of the GDTs are stored in the 
      "cpu_gdt_descr" array.(2.6.34.1,use struct "desc_struct" to represent segment descriptor)

      each GDT includes 18 segment descriptors and 14 null,unused,or reserved entries.unused entries are inserted
      on purpose so that Segment Descriptors usually accessed together are kept in the same 32-byte line of hardware
      cache.
      the 18 segment descriptors included in each GDT point to the following segments :
        >  four user and kernel code and data segments
        >  a task state segment(TSS),different for each processor in the system.
           the linear address space corresponding to a TSS is a small subset of the linear address space corresponding
           to the kernel data segment.
           TSS stored in the "init_tss" array.
           in particualr :
             BASE -> index in init_tss
             G = 0 ; if LIMIT = 0xeb (because TSS is 236 byte)
             TYPE = 9 || 11
             DPL = 0
        >  a segment including the default Local Descriptor Table(LDT),usually shared by all processes.
        >  three Thread-Local Storage(TLS) segments :
             multithreaded applications could make used of up to TLSs containing data local to each thread.
            syscall set_thread_area() and get_thread_area() create and release a TLS segment for the
            executing process.
        >  three segments related to Advanced Power Management(APM) :
             the BIOS code makes use of segments,so when the Linux APM driver invokes BIOS functions to get
             or set the status of APM devices,it may use custom code and data segments.
        >  five segments related to Plug and Play(PnP) BIOS services.
           Linux PnP driver invokes BIOS functions to detect the resources used by PnP devices.
           (custom data and data segments).
        >  a special TSS used by the kernel to handle "Double fault" exceptions.

      each processor has its own TSS.
      a few entries in the GDT may depend on the process that the CPU is executing(i.e. TLS).
      in some cases a processor may temporarily modify an entry in its copy of the GDT(i.e. invoke APM's BIOS procedure).

    The Linux LDTs :
      most Linux User Mode applications do not make use of a Local Descriptor Table.
      thus,kernel defines a default LDT to be shared by most processes,it is stored in the "default_ldt" array.
      default_ldt includes five entries,and two of them are used by kernel effectively :
        a call gate for iBCS executables;
        a call gate for Solaris/x86 executables;

      /*  Call gate is a mechanism provided by 80x86 to change CPU privilege while invoking a predefined function */

      process may require to set up their own LDT,via syscall modify_ldt().(i.e. Wine)
      any custom LDT created by modify_ldt() also requires its own segment.
      if the CPU is executing a process which has a custom LDT,then the LDT entry in CPU GDT also be changed accordingly.

      /*  User Mode applications also may allocate new segments by means of modify_ldt()  */
      /*  kernel never use these segments and do not keep track of the corresponding Segment Descriptor  */

    Paging in Hardware :
      the paging unit translates linear address into physical ones,it must to check the requested access type against the
      access rights of the linear address.if the memory access is not valid,it generates a Page Fault Exception.

      linear addresses are grouped in fixed-length intervals called page :
        contiguous linear addresses within a page are mapped into contiguous physical addresses.
        in this way,kernel can specify the physical address and the access rights of a page instead
        of those of all the linear addresses included in it.

      page unit thinks of all RAM as partitioned into fixed-length page frames(or physical pages),each page frame contains a page.
      a page frame is a constituent of main memory,and hence it is a storage area.
      
      ! the data structures that map linear to physical addresses are called page tables,they are stored in memory and kernel will
        initializes them before enabling page unit.        

      ! start with 80386,80x86 processors :
          if cr0.PG = 1,enable page;
          if cr0.PG = 0,linear addresses are interpreted as physical addresses;

    regular paging :
      start with 80386,a page handles 4kB memory.
    32bit linear address : Directory(10bit), Table(10bit), Offset(12bit)
                           (most significant) (middle)     (least significant)
    translation of linear address(based on type of translation table) :
      the physical address of the Page Directory in use is stored in a control register named cr3.
      get Page Directory from cr3 ->
      use Directory field to determine the Page Table associated with the linear address in Page Directory ->
      use Table field to determine the Page Frame in the Directory ->
      use Offset field to determine the relative position within the Page Frame

      cr3 { Page Directory address }
      Page Directory {
        ...
        Page Table
        Page Table {    /*  10bit Directory as index  */
          ...
          Page Frame
          Page Frame {      /*  10bit Table as index  */
            address
            address  /*  12bit Offset as index for addresses in a page frame  */
            ...
          }
        }
      }

      physical address = retrievePageFrame(retrievePageTable(cr3, linear.Directory), linear.Table) + Offset

      /*  12bit Offset,each page consists of 4kB of data.  */
      /*  10bit Directory,so Page Directory include 1024 entries;10bit Table,so Page Table include 1024 entries,
       *  so a Page Directory can address up to 1024x1024x4096 = 2^32 memory cells.
       */

    the entries of Page Directories and Page Tables have the same structure,each entry includes the following fields :
      Present flag :
        it = 1,the referred-to page(or Page Table) is contained in main memory;
        it = 0,the referred-to page(or Page Table) is not contained in main memory,the remainder bits may be used by OS for
        its own purpose;
        it = 0 and access physical address via page unit,page unit will stores the linear address in cr2,and generates 
        exception 14 : Page Fault

      Field containing the 20 most significant bits of a page frame physical address :
        a page frame has a 4-kB capacity,so its physical address must be multiple of 4096,so the 12 least significant
        bits are always equal to 0.
        it -> Page Directory,the page frame contains a Page Table;
        it -> Page Table,the page frame contains a page of data;

      Accessed flag :
        page unit set each time accesses the corresponding page frame.(page unit never reset it,OS have to do this)
        OS may be use it when selecting pages to be swapped out.
        
      Dirty flag :
        Page Table entry only,it is set each time a write operation is performed on the page frame.
        page unit never reset it,OS have to do this.

      Read/Write flag :
        contains the access right of the page or of the Page Table.
        
      User/Supervisor flag :
        contains the privilege level required to access the page or Page Table.

      PCD and PWT flags :
        controls the way the page or Page Table is handled by the hardware cache.

      Page Size flag :
        Page Directory entry only,if it = 1,then entry refers to a 2MB- or 4MB-long page frame.

      Global flag :
        Page Table entry only,this flag was introduced in the Pentium Pro to prevent frequently used pages from being
        flushed from the TLB cache.it works only if the Page Global Enable(PGE) flag of register cr4 is set.
        
      extended paging :
        starting with the pentium model,80x86 microprocessors introduce extended paging.
        extended paging allows page frames to be 4MB.in this case,Page Table is no longer need,thus save memory and 
        preserve TLB entries.
        extended paging : Directory(10bit), Offset(22bit)

    Page Directory entries for extended paging are the same as for normal paging,except that :
      >  the Page Size flag must be set.
      >  Only the 10 most significant bits of the 20-bit physical address field are significant.

    extended paging coexists with regular paging;it is enabled by setting the PSE flag of the cr4 processor register.

    Hardware Protection Scheme :
        only two privilege levels are associated with pages and Page Tables,it is indicated by User/Supervisor field.
        when User/Supervisor == 0,the page can be addressed only when the CPL is less than 3.
        when User/Supervisor == 1,the page can always be addressed.

        segmentation has three types of access rights,but only two types of access rights are associated with pages.
        if Read/Write == 0,the corresponding Page Table or page can only be read;otherwise it can be read and written.
        (Read Page Directory to get Page Table,read Page Table to get page frame)

    An Example of Regular Paging :
        a process is allowed to access linear addresses from 0x20000000 - 0x2003ffff.
        Directory field is 0010000000 for all addresses,
        Table field contain values in 0000000000 - 0000111111 (0 - 63)
        Offset field contain values in 000000000000 - 111111111111

        Directory is 0x80 or 128 decimal,so the 129th entry in Page Directory will be selected,
        the 129th entry contains the physical address of the Page Table of current process.
        Table is 0 - 63,so all the remaining 1024 - 64(960) entries are filled with zeros,so
        only the first Page to 63th Page in Page Table is valid.
        Finally,use Offset to access Page Frame.

        Suppose that the process needs to read the byte at linear address 0x20021406,this address is handled by the paging
        unit as follows :
          1>  use 0x80 to select the 129th entry in Page Directory.
          2>  the Table field 0x21 is used to select entry 0x21 of the Page Table.
          3>  Finally,the Offset field 0x406 is used to select the byte at offset 0x406 in the desired page frame.

        if process accessed a linear address which is outside to its linear address space,because the address is not in its
        address space,thus the address's Present flag is 0,cleared,Page Unit will issues an exception.
        all addresses are not valid for process in its linear address space,Present flag of each address will be cleared.

    The Physical Address Externsion (PAE) Paging Mechanism :
        normal CPU only supports 4GB RAM (from 80386 to the Pentium),which is used 32-bit physical addresses.
        in pratice,due to the linear address space requirements of User Mode processes,the kernel cannot directly address
        more than 1GB of RAM.
        and server needs more than 4GB RAM.
        Intel introduced a mechanism called Physical Address Extension(PAE)<36-bit physical address> on Pentium Pro processor.

        PAE is activated by setting the Physical Address Externsion flag in the cr4 control register.
        if PAE is open,then the Page Size flag in page directory entry will set to 1.

    PAE paging mechanism :
      >:
      the 64 GB of RAM are split into 2^24 distinct page frames,and the physical address field of Page Table entries has
      been expanded from 20 to 24 bits.
      the Page Table entry size has been doubled from 32bits to 64 bits,as a result,a 4-kB PAE Page Table includes 512
      entries instead of 1024.

      >:
      a new level of Page Table called the Page Directory Pointer Table(PDPT) consisting of four 64-bit entries has
      been introduced.

      >:
      the cr3 control register contains a 27-bit Page Directory Pointer Table base address field.because PDPTs are stored
      in the first 4GB of RAM and aligned to a multiple of 32 bytes,27 bits are sufficient to represent the base address
      of such tables.

      >:
      when mapping linear addresses to 4kB pages,the 32 bits of a linear address are interpreted in the following way :
        cr3 Points to a PDPT
        
        bits 31-30
          point to 1 of 4 possible entries in PDPT

        bits 29-21
          point to 1 of 512 possible entries in Page Directory

        bits 20-12
          point to 1 of 512 possible entries in Page Table

        bits 11-0
          Offset of 4-kB page

        linear-address : { PDPT Index(31-30), Page Directory Index(29-21), Page Table Index(20-12), Offset(11-0) }

      >:
      when mapping linear addresses to 2-MB pages(PS flag open),the 32 bits of a linear address are interpreted in
      the following way :
        cr3 Points to a PDPT

        bits 31-30
          point to 1 of 4 possible entries in PDPT

        bits 29-21
          point to 1 of 512 possible entries in Page Directory

        bits 20-0
          Offset of 2-MB page

        linear-address : { PDPT Index(31-30), Page Directory Index(29-21), Offset(20-0) }

        once cr3 is set,it is possible to address up to 4 GB of RAM,if we want to address more RAM,we will have to put 
        a new value in cr3 or change the content of the PDPT.
        PAE only extend physical address,User Mode processes are still address 4 GB linear address space(still 32 bits).
        
      Paging for 64-bit Architectures : 
        Paging levels in some 64-bit architectures >
        Platform name         Page size        address bit      paging levels       linear address splitting
        alpha                 8kB              43               3                   10 + 10 + 10 + 13
        ia64                  4kB              39               3                   9 + 9 + 9 + 12
        ppc64                 4kB              41               3                   10 + 10 + 9 + 12
        sh64                  4kB              41               3                   10 + 10 + 9 + 12
        x86_64                4kB              48               4                   9 + 9 + 9 + 9 + 12

        two levels paging :
          Page Directory  <level 1>
          Page Table      <level 2>

          cr3 get Page Directory address.
          PD[Directory] -> Page Table
          PT[Table] -> Page Frame
          PF[Offset] -> page of data

    Hardware Cache : 
        Hardware cache memories were introduced to reduce the speed mismatch between CPU and RAM.
        they are based on the well-known locality principle,which holds both for programs and data structures.
        it makes sense to introduce a smaller and faster memory that contains the most recently used code and data.
        for this purpose,a new unit called the "line" was introduced into the 80x86 architecture.

        DRAM : dynamic RAM
        SRAM : static RAM,it is more faster than DRAM and it is on-chip

        the cache is subdivided into subsets of lines.
        there are some different strategy to determine how to store cache :
          1>  the cache can be direct mapped,in which case a line in main memory is always stored at the exact
              same location in the cache.
          2>  the cache is fully associative,meaning that any line in memory can be stored at any location in the cache.
          3>  degree N-way set associative,where any line of main memory can be stored in any one of N lines of the cache.

                          DRAM Main Memory
                               |
                               v
        CPU { SRAM cache -> Cache controller <- Paging unit }

        SRAM stores the actual lines of memory.(it is the cache memory)
        Cache controller stores an array of entries,every entry is the line of the cache memory.
        Each entry includes a tag and a few flags that describe the status of the cache line,the tag consists of some
        bits that allow the cache controller to recognize the memory location currently mapped by the line.

        The bits of the memory's physical address : { TAG, SUBSET INDEX, OFFSET }

        when accessing a RAM memory cell,the CPU extracts the subset index from the physical address and compares the tags
        of all lines in the subset with the high-order bits of the physical address.
        if a line with the same tag as the high-order bits of the address is found,the CPU has a cache hit;otherwise,it
        has a cache miss.

          subset_index := SUBSET_INDEX(physical_address)
          tag := TAG(physical_address)

          hardware_cache_subset := get_subset(subset_index)
          line := try_hit_cache_line(hardware_cache_subset, tag)

          if is_hit(line)
                  if is_expired(line)
                          update_cache_line(&line, physical_address, hardware_cache_subset)
                  return get_cache(line, OFFSET(physical_address))

          line := make_cache_line(fetch_from_RAM(physical_address))
          new_cache_entry := make_cache_entry(tag, hardware_cache_subset, line)

          insert_hardware_cache_entry(new_cache_entry, hardware_cache_subset)
          map_cache_line(line)
          

        when a cache hit occurs,the cache controller behaves differently,depending on the access type :
          READ >
            controller selects the data from the cache line and transfers it into a CPU register.

          WRITE >
            write-through :
              write data into both cache line and mapped RAM.

            write-back :
              just write data into cache line,the controller updates RAM only when the CPU executes an instruction
              requiring a flush of cache entries or then a FLUSH hardware signal occurs.

        when a cache miss occurs,the cache line is written to memory,if necessary,and the correct line is fetched from
        RAM into the cache entry.

        Multiprocessor system :
          there must have an additional hardware circuitry to synchronize the cache contents.
          whenever a CPU modifies its hardware cache,it must check whether the same data is contained in the other
          hardware cache;if so,it must notify the other CPU to update it with the proper value.(cache snooping)

          new model have more cache,L1-cache,L2-cache,L3-cache,etc.
          linux ignore hardware details,and assumes there is a single cache.

          the CD flag of the cr0 control register is used to enable or diable the cache circuitry.
          the NW flag,in the same register,specifies whether the write-through or the write-back strategy is used for
          the caches.

          some processors allow OS associate a different cache management policy with each page frame,that is PCD flag
          in Page Directory and Page Table;and PWT(Page Write-Through),which specifies whether the write-back or the 
          write-through strategy must be applied while writing data into the page frame.
          (Linux default clear these flags)

   Translation Lookaside Buffers(TLB) :
     80x86 processors include another cache called Translation Lookaside Buffers(TLB) to speed up linear address
     translation.
     when a linear address is used for the first time,the corresponding physical address is computed through slow
     accesses to the Page Tables in RAM.the physical address is then stored in a TLB entry for further accessing.
        
     in a multiprocessor system,each CPU has its own TLB,called the local TLB of the CPU.contrary to the hardware
     cache,these local TLB need not to be synchronized,because processes running on the existing CPUs may associate
     the same linear address with different physical ones.

     when the cr3 control register of a CPU is modified,the hardware automatically invalidates all entries of the 
     local TLB,because a new set of page tables is in use and the TLBs are pointing to old data.

    Paging in Linux :
      before 2.6.11,linux paging model has three level,
      starting with 2.6.11,linux paging model has four level.
      (linux adopts a common paging model that fits both 32-bit and 64-bit architectures.)

      paging level :
        Page Global Directory
        Page Upper Directory
        Page Middle Directory
        Page Table

        PGD { PUDs }
        PUD { PMDs }
        PMD { PTs }
        PT -> Page Frame (a page frame)
    
        cr3 -> Address of PGD

      on 32-bit architecture(no PAE),PUD and PMD fields will be zero.(code still same,so it is work on 64-bit)
      but kernel keeps a position for the PUD and PMD by setting the number of entries in them to 1 and mapping
      these two entries into the proper entry of the PGD.

      on 32-bit architecture(PAE),PUD is eliminated.
      PGD -> 80x86's Page Directory Pointer Table
      PMD -> 80x86's Page Directory
      PT  -> 80x86's Page Table

      on 64-bit architecture,three or four levels of paging are used depending on the linear address bit splitting
      performed by the hardware.

      Linux's handling of processes relies heavily on paging :
        linear address to physical address,design >
          < assign a different physical address space to each process,ensuring an efficient protection against
            addressing errors.
          < distinguish pages(groups of data) from page frames(physical ddress in main memory),this allows the same
            page to be stored in a page frame,then saved to disk and later reloaded in a different page frame,
            this is "the basic ingredient" of the virtual memory mechanism.

      The Linear Address Fields :
        the macros simplify Page Table handling >
      PAGE_SHIFT
        specifies the length in bits of the Offset field;
        this macro is used by PAGE_SIZE to return the size of the page,finally,
        the PAGE_MASK macro yields the value 0xfffff000 and is used to mask all the bits of the Offset field.

      PMD_SHIFT
        the total length in bits of the Offset and Table fields of a linear address;
        the logarithm of the size of the area a Page Middle Directory entry can map.
        the PMD_MASK macro is used to mask all the bits of the Offset and Table fields.
        PAE -> off
          PMD_SHIFT = 22 (12 Offset + 10 Table)
          PMD_SIZE = 2^22 (4MB)
          PMD_MASK = 0xffc00000

        PAE -> on
          PMD_SHIFT = 21 (12 Offset + 9 Table)
          PMD_SIZE = 2^21 (2MB)
          PMD_MASK = 0xffe00000

        !  large pages do not make use of the last level of page tables,thus LARGE_PAGE_SIZE which yields
           the size of a large page,is equal to PMD_SIZE(2PMD_SHIFT)
           LARGE_PAGE_MASK is used to mask all the bits of the Offset and Table fields in a large page address,
           is equal to PMD_MASK.

      PUD_SHIFT
        determines the logarithm of the size of the area a Page Upper Directory entry can map.
        PUD_SIZE macro computes the size of the area mapped by a single entry of the Page Global Directory,
        PUD_MASK macro is used to mask all the bits of the Offset,Table,Middle Air,and Upper Air fields.

      PGDIR_SHIFT
        determines the logarithm of the size of the area that a Page Global Directory entry can map.
        PGDIR_SIZE macro computes the size of the area mapped by a single entry of the Page Global Directory,
        the PGDIR_MASK macro is used to mask all the bits of the Offset,Table,Middle Air,and Upper Air fields.
        PAE -> off
          PGDIR_SHIFT = 22 (the same value yielded by PMD_SHIFT and by PUD_SHIFT)
          PGDIR_SIZE = 2^22 (4MB)
          PGDIR_MASK = 0xffc00000

        PAE -> on
          PGDIR_SHIFT = 30 (12 Offset + 9 Table + 9 Middle Air)
          PGDIR_SIZE = 2^30 (1GB)
          PGDIR_MASK = 0xc0000000

      PTRS_PER_PTE PTRS_PER_PMD PTRS_PER_PUD PTRS_PER_PGD
        compute the number of entries in the Page Table,Page Middle Directory,Page Upper Directory,Page Global Directory.
        they yield the values 1024, 1, 1, 1024, respectively; (PAE disabled)
        they yield the values 512, 512, 1, 4, respectively; (PAE enabled)

      Page Table Handling :
        pte_t -> Page Table entry
        pmd_t -> Page Middle Directory entry
        pud_t -> Page Upper Directory entry
        pgd_t -> Page Global Directory entry
        pgprot_t -> the protection flags associated with a single entry        
        (64-bit PAE on,32-bit PAE off)

    conversion macros :
      pte_t __pte(unsigned int)
      pmd_t __pmd(unsigned int)
      pud_t __pud(unsigned int)
      pgd_t __pgd(unsigned int)
      pgprot_t __pgprot(unsigned int)

      unsigned int pte_val(pte_t)
      unsigned int pmd_val(pmd_t)
      unsigned int pud_val(pud_t)
      unsigned int pgd_val(pgd_t)
      /*  reverse casting  */

    macros for RW a page table entry operations :
      unsigned pte_none(pte_t)
      unsigned pmd_none(pmd_t)
      unsigned pud_none(pud_t)
      unsigned pgd_none(pgd_t)
      /*  return 0 if @arg == 1,return 1 if @arg == 0  */

      pte_clear(mm, addr, ptep)
      pmd_clear(pmd)
      pud_clear(pud)
      pgd_clear(pgd)
      /*  clear an entry of the corresponding page table  */
      /*  forbidding a process to use the linear addresses mapped by the page table entry  */

      static inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr, pte_t *ptep);
      /*  clears a Page Table entry and returns the previous value  */

      set_pte(ptep, pte)
      set_pmd(pmdp, pmd)
      set_pud(pudp, pud)
      set_pgd(pgdp, pgd)
      /*  write a given value into a page table entry  */

      set_pte_atomic(ptep, pte)
      /*  atomic operation  */
      /*  if PAE on,it also ensures that the 64-bit value is written atomically  */

      static inline int pte_same(pte_t, pte_t);
      /*  returns 1 if @arg1 == @arg2,specify @arg1.privileges == @arg2.privileges,otherwise retuns 0  */

      static inline int pmd_large(pmd_t pte);
      /*  returns 1 if the Page Middle Directory entry refers to a large page(2MB or 4MB),0 otherwise  */

      static inline int pmd_bad(pmd_t pmd);
      /*  it is used by functions to check Page Middle Directory entries passed as input parameters.
       *  returns 1 if the entry points to a bad Page Table :
       *    Present flag cleared
       *    Read/Write flag cleared
       *    Accessed or Dirty cleared
       *    0 otherwise.
       */

      static inline int pud_bad(pud_t pud);
      static inline int pgd_bad(pgd_t pgd);
      /*  these macros always yield 0.  */

      no pte_bad() macro is defined,because no Present,no Read/Write,no Accessed or no Dirty is legal for
      Page Table entry.

      pte_present(pte_t)
      /*  returns 1 if Present == 1 or Page Size == 1,0 otherwise  */

      Page Size flag in Page Table entries has no meaning for the paging unit of the microprocessor,
      but kernel marks Present = 0 and Page Size = 1 for the pages present in main memory but without read,
      write,or execute privileges.
      any access to such pages will triggers a Page Fault exception because Present is cleared,but kernel
      is able to detect that the Page Fault exception is not due to a missing page by checking the value of
      Page Size!

      pmd_present(pmd_t pmd)
      /*  returns 1 if the Present == 1,0 otherwise  */

      pud_present(pud_t pud)
      pgd_present(pgd_t pgd)
      /*  same as above  */

    query or setting functions for Page Table entry's value of any of the flags :
      Page flag reading :
        pte_user()        /*  User/Supervisor  */
        pte_read()        /*  User/Supervisor,pages on the 80x86 processor cannot be protected against reading  */
        pte_write()       /*  Read/Write  */
        pte_exec()        /*  User/Supervisor,pages on the 80x86 processor cannot be protected against code execution  */
        pte_dirty()       /*  Dirty  */
        pte_young()       /*  Accessed  */
        pte_file()        /*  Dirty,when the Present is cleared and the Dirty flag is set,the page belongs to a
                           *  non-linear disk file mapping.
                           */

      Page flag setting :
        mk_pte_huge()               /*  Page Size and Present  */
        pte_wrprotect()             /*  Read/Write */
        pte_rdprotect()             /*  User/Supervisor  */
        pte_exprotect()             /*  User/Supervisor  */
        pte_mkwrite()               /*  Read/Write  */
        pte_mkread()                /*  User/Supervisor  */
        pte_mkexec()                /*  User/Supervisor  */
        pte_mkclean()               /*  Dirty  */
        pte_mkdirty()               /*  Dirty  */
        pte_mkold()                 /*  Accessed  */
        pte_mkyoung()               /*  Accessed  */
        pte_modify(p, v)            /*  Sets all access rights in a Page Table entry @p to a specified value @v  */
        ptep_set_wrprotect()        /*  pointer version  */
        ptep_set_access_flags()     /*  if the Dirty == 1,sets the page's access rights to a specified value and
                                     *  invokes flush_tlb_page()
                                     */
        ptep_mkdirty()              /*  pointer version  */
        ptep_test_and_clear_dirty() /*  like pte_mkclen() but acts on a pointer to a Page Table entry and returns
                                     *  the old value of the flag
                                     */
        ptep_test_and_clear_young() /*  like pte_mkold() but acts on a pointer to a Page Table entry and returns 
                                     *  the old value of the flag
                                     */

    macros acting on Page Table entries :
      pgd_index(addr)     
      /*  yields the index of the entry in Page Global Directory that maps the linear address @addr  */

      pgd_offset(mm, addr)
      /*  yields the linear address of the entry in a Page Global Directory that corresponds to the 
       *  address @addr,the Page Global Directory is found through a pointer within the memory descriptor
       */

      pgd_offset_k(addr)
      /*  yields the linear address of the entry in the master Kernel Page Global Directory that corresponds
       *  to the address @addr
       */

      pgd_page(pgd)
      /*  yields the page descriptor address of the page frame containing the Page Upper Directory referred to
       *  by the Page Global Directory entry @pgd,in a two or three-level paging system,this macro is
       *  equivalent to pud_page() applied to the folded Page Upper Directory entry
       */

      pud_offset(pgd, addr)
      /*  yields the linear address of the entry in a Page Upper Directory that corresponds to @addr,in a 
       *  two- or three-level paging system,this macro yields @pgd,the address of a Page Global Directory entry
       */

      pud_page(pud)
      /*  yields the linear address of the Page Middle Directory referred to by the Page Upper Directory entry @pud,
       *  in a two- or three-level paging system,this macro is equivalent to pmd_page() applied to the foled
       *  Page Middle Directory entry
       */

      pmd_index(addr)
      /*  yields the index of the entry in the Page Middle Directory that maps the linear address @addr  */
      
      pmd_offset(pud, addr)
      /*  yields the address of the entry in a Page Middle Directory that corresponds to @addr,
       *  in a two- or three-level paging system,it yields @pud,the address of a Page Global Directory entry
       */

      pmd_page(pmd)
      /*  yields the page descriptor address of the Page Table referred to by the Page Middle Directory entry @pmd,
       *  in a two- or three-level paging system,@pmd is actually an entry of a Page Global Directory
       */

      mk_pte(p, prot)
      /*  use a page descriptor @p and a group of access rights @prot to builds the corresponding Page Table entry  */

      pte_index(@addr)
      /*  yields the index of the entry in the Page Table that maps the linear address @addr  */

      pte_offset_kernel(dir, addr)
      /*  yields the linear address of the Page Table that corresponds to the linear address @addr mapped by the 
       *  Page Middle Directory @dir,used only on the master kernel page tables
       */

      pte_offset_map(dir, addr)
      /*  yields the linear address of the entry in the Page Table that corresponds to the linear address @addr,
       *  if the Page Table is kept in high memory,the kernel establishes a temporary kernel mapping,to be released
       *  by means of pte_unmap.
       *  the macros pte_offset_map_nested() and pte_unmap_nested() are identical,but they use a different temporary
       *  kernel mapping.
       */

      pte_page(x)
      /*  returns the page descriptor address of the page referenced by the Page Table entry @x  */

      pte_to_pgoff(pte)
      /*  extracts from the content @pte of a Page Table entry the file offset corresponding to a page belonging to
       *  a non-linear file memory mapping
       */

      pgoff_to_pte(offset)
      /*  sets up the content of a Page Table entry for a page belonging to a non-linear file memory mapping  */

    page allocation functions :
      pgd_alloc(mm)
      /*  allocates a new Page Global Directory;if PAE is on,it also allocates the three children Page Middle Directories
       *  that map the User Mode linear addresses.the argument @mm is ignored on the 80x86 architecture.
       */

      pgd_free(pgd)
      /*  releases the Page Global Directory at address @pgd;if PAE is on,it also releases the three Page Middle
       *  Directories that map the User Mode linear addresses
       */

      pud_alloc(mm, pgd, addr)
      /*  in a two- or three-level paging system,this function does nothing;it simply returns the linear address of
       *  the Page Global Directory entry pgd
       */

      pud_free(x)
      /*  in a two- or three-level paging system,this macro does nothing  */

      pmd_alloc(mm, pud, addr)
      /*  defined so generic three-level paging systems can allocate a new Page Middle Directory for the linear address
       *  @addr;if PAE is off,the function simply returns the @pud -- that is,the address of the entry in the
       *  Page Global Directory;if PAE is on,the function returns the linear address of the Page Middle Directory entry
       *  that maps the linear address @addr,the argument cw is ignored
       */

      pmd_free(x)
      /*  does nothing,because Page Middle Directories are allocated and deallocated together with their parent
       *  Page Global Directory.
       */

      pte_alloc_map(mm, pmd, addr)
      /*  returns the address of the Page Table entry corresponding to @addr,if the Page Middle Directory entry is null,
       *  the function allocates a new Page Table by invoking pte_alloc_one().
       *  if a new Page Table is allocated,the entry corresponding to @addr is initialized and the User/Supervisor is set,
       *  if the Page Table is kept in high memory,the kernel establishes a temporary kernel mapping,to be released by
       *  pte_unmap().
       */

      pte_alloc_kernel(mm, pmd, addr)
      /*  if @pmd associated with the address @addr is null,the function allocates a new Page Table,it then returns the 
       *  linear address of the Page Table entry associated with @addr,used only for master kernel page tables
       */

      pte_free(pte)
      /*  releases the Page Table associated with the @pte page descriptor pointer  */
      
      pte_free_kernel(pte)
      /*  equivalent to pte_free(),but used for master kernel page tables  */

      clear_page_range(mmu, start, end)
      /*  clears the contents of the page tables of a process from linear address @start to @end by iteratively
       *  releasing its Page Tables and clearing the Page Middle Directory entries
       */

      because the Page Table that is supposed to contain it might not exist,in such cases,it is necessary to allocate
      a new page frame,fill it with zeros,and add the entry.
      if PAE on,the kernel uses three-level paging,when the kernel creates a new Page Global Directory,it also allocates
      the four corresponding Page Middle Directories;these are freed only when the parent Page Global Directory is released.
      when two or three-level paging is used,the Page Upper Directory entry is always mapped as a single entry within the
      Page Global Directory.
    
      
      Physical Memory Layout : 
        kernel must build a physical addresses map that specifies which physical address ranges are usable by the kernel and 
        which are unavailable(hardware device I/O shared memory or BIOS data).

      reserved page frames :
        those falling in the unavailable physical address ranges
        those containing the kernel's code and initialized data structures
        (such page frames can never be dynamically assigned or swapped to disk)

        /*  general rule,linux kernel is installed in RAM starting from the physical address 0x00100000,
         *  from the second megabyte.
         *  the total number of page frames required depends on how the kernel is configured.
         */    

        why kernel loaded starting with the second megabyte?
        :  the PC architecture has several peculiarities that must be taken into account.
           PF0 (BIOS data,system hardware configuration detected during POST(Power-On Self Test)).
           0x000a0000 -- 0x000fffff are usually reserved to BIOS rountines and to map the internal memory of ISA graphics
           cards.
           additional page frames within the first megabyte may be reserved by specific computer models.

    in the early stage of the boot sequence,the kernel queries the BIOS and learns the size of the physical memory,
    later,kernel executes the machine_specific_memory_setup() rountine,which builds the physical address map.
    (such functions is named default_machine_specific_memory_setup() in arch/x86/kernel/e820.c)

    /*  kernel builds this table on the basis of the BIOS list,if this is available,otherwise the kernel builds the table
     *  following the conservative default setup:
     *    all page frames with numbers from 0x9f(LOWMEMSIZE()) to 0x100(HIGH_MEMORY) are marked as reserved.
     */

    /*  POST stage,BIOS writes information about the system hardware devices into the proper page frames,
     *  and initialization stage,kernel will copies such data into the suitable kernel data structures,
     *  then consider these page frames usable.
     *  BIOS may not provide information for some physical address ranges,in such case,linux kernel assumes
     *  such ranges are not usable.
     */

     after machine_specific_memory_setup(),the function setup_memory() will be invoked,it analyzes the table
     of physical memory regions and initializes a few variables that describe the kernel's physical memory
     layout.
       the vairables :  (thest variables are declared in *.c files under the directory mm/)
         num_physpages      --  page frame number of the highest usable page frame
         totalram_pages     --  total number of usable page frames
         min_low_pfn        --  page frame number of the first usable page frame after the kernel image in RAM
         max_pfn            --  page frame number of the last usable page frame
         max_low_pfn        --  page frame number of the last page frame directly mapped by the kernel(low memory)
         totalhigh_pages    --  total number of page frames not directly mapped by the kernel(high memory)
         highstart_pfn      --  page frame number of the first page frame not directly mapped by the kernel
         highend_pfn        --  page frame number of the last page frame not directly mapped by the kernel

     /*  under x86,function setup_arch()<kernel/setup.c> invokes setup_memory_map()<kernel/e820.c>,this function
      *  will copies the e820 data structure object from &e820 to &e820_saved,and setup_arch() will updates 
      *  some of the variables,another will be updated by the functions in the files under mm/ .
      */
         
     /*  linux kernel prefers to skip the first megabyte of RAM to ensure it can never be loaded into groups of
      *  noncontiguous page frames.
      *  generally,kernel keeps initialized data structures right after kernel code,and the uninitialized data
      *  structures follows it.
      */

    Process Page Tables :
      two parts of the linear address space of a process :
        1>  linear address 0x00000000 -- 0xbfffffff can be addressed either User or Kernel Mode.
        2>  linear address 0xc0000000 -- 0xffffffff can only be addressed in Kernel Mode.

    macro PAGE_OFFSET yields value 0xc0000000,this is the offset in the linear address space of a process
    where the kernel lives.
    (in some cases,the process is running in Kernel Mode maybe need to access the linear address space of 
     User Mode for retrieve or store data.)

    !  the content of the first entries of the Page Global Directory that map linear address lower than
       0xc0000000(768 entries with PAE disabled,3 entries with PAE enabled),depends on the specific process.

    Kernel Page Tables :
      the kernel maintains a set of page tables for its own use,rooted at a so-called master kernel Page Global
      Directory.
      after these page tables were initialized,they will never be directly accessed by any process or kernel thread.
      (the highest entries of the master kernel Page Global Directory are the reference model for the corresponding
      entries of the Page Global Directories of every regular process in the system.)

    kernel initializes these page tables with two-phase activity :
      1>  the kernel creates a limited address space including the kernel's code and data segments,the initial
          Page Tables,and 128KB for some dynamic data structures.
          the minimal address space is just large enough to install the kernel in RAM and to initialize its core
          data structures.
      2>  the kernel takes advantage of all of the existing RAM and sets up the page tables properly.

    (right after kernel image loaded,CPU is stil running in real mode,thus,paging is not enabled)

    Provisional kernel Page Tables :
      A provisional Page Global Directory is initialized statically during kernel compilation,while the provisonal
      Page Tables are initialized by startup_32() assembly function defined in <arch/x86/kernel/head_32.S>.

      the Provisional Page Global Directory is contained in swapper_pg_dir variable,the Provisional Page Tables
      are stored starting from pg0.pg0 is the Page Frame which number is 0,it is the first Page Frame,right after
      the end of the kernel's uninitialized data segments(symbol _end).

      suppose all the limited address space fit in the first 8MB of RAM.there,kernel just required two Page Tables,
      but each Page Table is points to a Page Frame,and Page Frame holds one Page,each Page Frame consisting with
      a Page size is 4KB(1024(entries) * 2(Page Tables) * 4KB(Page Size) = 8MB).(at this time,PAE is off)

      for easily addressed both in real mode and protected mode to the 8MB,kernel must create a mapping from
      both the linera addresses [0x00000000 -- 0x007fffff] and the linera addresses [0xc0000000 -- 0xc07fffff]
      into the physical address [0x00000000 -- 0x007fffff].
        linear [0x00000000 -- 0x007fffff] -> [0x00000000 -- 0x007fffff] physical
        linear [0xc0000000 -- 0xc07fffff] -> [0x00000000 -- 0x007fffff] physical

      kernel create the desired mapping by filling all the swapper_pag_dir entries(1024) with zeroes,except for
      entries 0, 1, 0x300(768), 0x301(769) . 
      entries 0x300 and 0x301 will span all linear addresses between [0xc000000 -- 0xc07fffff].
      /*  head_32.S defined swapper_pg_dir :
       *    ENTRY(swapper_pg_dir)
       *    .fill 1024,4,0    #  .fill repear,size(byte),value
       */

      initialization :
        > the address field of entries 0 and 0x300 is set to the physical address of pg0,while the address field
          of entries 1 and 0x301 is set to the physical address of the page frame following pg0(it is pg1).
        > the Present, Read/Write, User/Supervisor flags are set in all four entries(on -> 1).
        > the Accessed, Dirty, PCD, PWD, and Page Size flags are cleared in all four entries(off -> 0).

      start_32() copies the address of swapper_pg_dir to cr3,and enables paging unit(PG flag of the cr0).
      /*  A part of code  */
        movl     $swapper_pg_dir-0xc0000000,%eax
        movl     $eax,%cr3            #  set he page table pointer
        movl     %cr0,%eax            #  get value of cr0
        orl      $0x80000000,%eax     #  open PG
        movl     %eax,%cr0            #  write back

        #  0xc0000000 is the __PAGE_OFFSET
        #  addresses in [0x00000000, 0xc00000000) is used for User Mode.

    Final Kernel Page Table :
      Final Kernel Page Table when RAM size is less than 896MB >
        the final mapping provided by the kernel page tables must transform linear addresses starting
        from 0xc0000000 into physical addresses starting from 0.

        <arch/x86/include/asm/page.h>
        #define __pa(x)  __phys_addr((unsigned long)(x))
        /*  convert a linear address starting from PAGE_OFFSET to the corresponding physical address  */

        #define __va(x)  ((void *)((unsigned long)(x) + PAGE_OFFSET))
        /*  convert a physical address to the corresponding linear address starting from PAGE_OFFSET  */

        the master kernel Page Global Directory is still stored in swapper_pg_dir,it is initialized by
        the paging_init() which is declared in <arch/x86/include/asm/pgtable_32.h>,and defined in
        <arch/x86/mm/init_32.c> with the prototype "void __init paging_init(void);" .
        it executes these works :
          invoke pagetable_init() to set up the Page Table entries properly;
          writes the physical address of swapper_pg_dir in the cr3;
          if the CPU supports PAE and if the kernel is compiled with PAE support,sets the PAE flag in
          the cr4;
          invokes __flush_tlb_all() to invalidate all TLB entries;

        /*  body of it  */
        void __init paging_init(void)
        {
                pagetable_init();
                __flush_tlb_all();
                kmap_init();
                sparse_init();
                zone_sizes_init();
        }

        /*  this routines also unmaps the page at virtual kernel address 0,so
         *  that we can trap those pesky NULL-reference erros in the kernel.
         */

        the actions performed by pagetable_init() depend on both the amount of RAM  present and on the CPU
        model.(suppose less than 896MB)
        /*  the highest 128MB of linear address are left available for several kinds of mapping,the kernel
         *  address space left for mapping the RAM is thus 1GB - 128MB = 896MB
         */

        the identity mapping of the first megabytes of physical memory(8MB,the supposed size) built by startup_32()
        is required to complete the initialization phase of the kernel,when this mapping is no longer necessary,
        the kernel clears the corresponding page table entries by invoking the zap_low_mappings() .

      Final Kernel Page Table when RAM size is between 896MB and 4096MB >
        in this case,the RAM can not be mapped entirely into the kernel linear address space.
        Linux does a mapping to map a RAM window of 896MB into the kernel linear address space,if a program
        need to address other parts of the existing RAM,some other linear address interval must be mapped to
        the required RAM,this implies changing the value of some page table entries.
        (that is the remaining RAM(4096-896) is left unmapped status and handled by dynamic remapping)

      Final Kernel Page Table when RAM size is more than 4096MB >
        in this case,CPU supports PAE,and kernel is compiled with PAE support.
        even PAE is on,linear address is till 32-bit,but physical address is 36-bit.
        the main difference with the previous case is that,at this time,three-level paging model is used.
        (pgd -> pmd -> pte)
        /*  but kernel still directly maps 896MB RAM window  */

        the setups :
          > kernel initializes the first three entries(pgd_index(PAGE_OFFSET) = 3) in the Page Global Directory
            corresponding to the user linear address space with the address of an empty page(empty_zero_page).
          > kernel sets the fourth entry with the address of a Page Middle Directory(pmd) allocated by
            invoking alloc_bootmem_low_pages().
          > kernel sets the first 448 entries(896MB / 2MB(PAGE_SIZE)) in the Page Middle Directory(PAE on,512 entries)
            are filled with the physical address of the first 896MB of RAM.
            /*  phys_addr = 0x00000000;
             *  for(j = 0; j < PTRS_PER_PMD && phys_addr < max_low_pfn * PAGE_SIZE; ++j) {
             *          ...
             *          phys_addr += PTRS_PER_PTE * PAGE_SIZE;  /*  PAE on,512  */
             *  }
             */
          > kernel copies the fourth Page Global Directory entry into the first entry,so as to mirror the mapping
            of the low physical memory in the first 896MB of the linear address space.

            /*  this mapping is required in order to complete the initialization of SMP  */
            (if these page table entries is no longer neccesary,kernel will call zap_low_mappings() to clears them.)

      
      Fix-Mapped Linear Address :
        the initial part of the fourth gigabyte of kernel linear address maps the physical memory of the system,
        however,at least 128MB of linear addresses are always left available because the kernel uses them to implement
        noncontiguous memory allocation and fix-mapped linear addresses.
        (noncontiguous memory allocation is just a special way to dynamically allocate and release pages of memory)

        basically,a fix-mapped linear address is a constant linear address like 0xffffc000 whose corresponding physical
        address does not have to be the linear address minus 0xc0000000,but rather a physical address set in an arbitrary
        way.thus,each fix-mapped linear address maps one page frame of the physical memory.
        fix-mapped linear address is similiar to the linear address that map the first 896MB conceptually,but it can map
        any physical address,while the mapping established by the linear address in the initial portion of the fourth
        gigabyte is linear.

        each fix-mapped linear address is represented by a small integer index defined in the enum fixed_addresses 
        data structure :
          <arch/x86/include/asm/fixmap.h>
          enum fixed_addresses {
                  FIX_HOLE,
                  FIX_VSYSCALL,
                  FIX_APIC_BASE,
                  FIX_IO_APIC_BASE_0,
                  [...]
                  __end_of_fixed_addresses
          };

          fix-mapped linear addresses are placed at the end of the fourth gigabyte of linear addresses,
          fix_to_virt() function computes the constant linear address starting from the index.
          <arch/x86/include/asm/fixmap.h>
            /*  fix_to_virt - computes the constant linear address starting from the index in fixed_addresses.
             *  @idx : the index.
             *  return - the constant linear address.
             *  #  if idx is not in fixed_addresses,then an error "undefined symbol __this_fixmap_does_not_exist"
             *     will be reported by compiler.
             */
            static __always_inline unsigned long fix_to_virt(const unsigned int idx);

            /*  set_fixmap - associates a fix-mapped address with a physical address.
             *  @idx : the index in fixed_address.
             *  @phys : the physical address have to be associated.
             */
            #define set_fixmap(idx, phys)  __set_fixmap(idx, phys, PAGE_KERNEL)

            /*  set_fixmap_nocache - nocache version.
             *    this function will sets PCD flag of the Page Table entry,thus disabling the hardware cache when
             *    accessing the data in the page frame.
             */
            #define set_fixmap_nocache(idx, phys)  __set_fixmap(idx, phys, PAGE_KERNEL_NOCACHE)

            /*  clear_fixmap - clear fix-mapped,removes the linking between a fix-mapped linear address
             *                 and the physical address.
             *  @idx:          the index of member in fixed_addresses.
             */
            #define clear_fixmap(idx)  __set_fixmap(idx, 0, __pgprot(0))

    
      Handling the Hardware Cache and the TLB :
        Handling the hardware cache >
          macro L1_CACHE_BYTES yields the size of a cache line in bytes.
          <arch/x86/include/asm/cache.h>
            #define L1_CACHE_BYTES  (1 << L1_CACHE_SHIFT)

          to optimize the cahce hit rate,the kernel considers the architecture in making the following decisions :
            1>  the most frequently used fields of a data structure are placed at the low offset within the 
                data structure,so they can be cached in the same line.
            2>  when allocating a large set of data structures,the kernel tries to store each of them in memory
                in such a way that all cache lines are used uniformly.

                #  80x86 microprocessors does cache synchronization automatically,linux need not to care about anymore.
                   the kernel does provide,however,cache flushing interfaces for processors that does not synchronize caches.

    Handling the TLB >
      TLB is used keep records about mapping between linear addresses and physical addresses,
      so processors can not synchronize their own TLB cache automatically.(kernel determines if the mapping is invalid)

      the methods linux provides for TLB synchronization :
        <arch/x86/include/asm/tlbflush.h>
          /*  flush_tlb_all - flushes all TLB entries even refer to global pages.
           *  #  typically used when changing the kernel page table entries.
           */
          #define flush_tlb_all()  __flush_tlb_all()

          /*  flush_tlb_kernel_range - flushes all TLB entries in a given range of linear addresses,
           *                           even refer to global pages.
           *  @start : range start.
           *  @end   : range end.
           */
          static inline void flush_tlb_kernel_range(unsigned long start, unsigned long end);

          /*  flush_tlb - flushes all TLB entries of the non-global pages owned by the current process.
           *              (current mm struct TLBs)
           */
          #define flush_tlb()  __flush_tlb()

          /*  flush_tlb_mm - flushes all TLB entries of the non-global pages owned by a given process.
           *  @mm : a pointer points to a mm_struct object.
           *  #  typically used when forking a new process.
           */
          static inline void flush_tlb_mm(struct mm_struct *mm);

          /*  flush_tlb_range - flushes the TLB entries corresponding to a linear address interval of a given process.
           *  @vma : the virtual memory area of current process.
           *  @start : range start.
           *  @end : range end.
           *  #  typically releasing a linear address interval of a process.
           */
          static inline void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);

          /*  flush_tlb_pgtables - flushes the TLBs of a given contiguous subset of page tables of a given process.  */
          flush_tlb_pgtables
          !  some architecure does not offer such function,i.e. x86.
          #  80x86 architecure nothing has to be done when a page table is unlinked from its parent table.

          /*  flush_tlb_page - flushes the TLB of a single Page Table entry of a given process.
           *  @vma : the virtual memory area of the process.
           *  @addr : specified address.
           *  #  typically used processing a Page Fault.
           */
          static inline void flush_tlb_page(struct vm_area_struct *vma, unsigned long addr);

          every microprocessor usually offers a far more restricted set of TLB-invalidating assembly language instructions,
          Intel microprocessors offers only two TLB-invalidating techniques :
            >  all Pentium models automatically flush the TLBs relative to non-global pages when a value is loaded into cr3.
            >  in Pentium Pro and later models,the "invlpg" assembly language instruction invalidates a single TLB mapping a
               given linear address.

      Linux macros that exploit such hardware technique :
        <arch/x86/include/asm/tlbflush.h>
          /*  __flush_tlb - rewrites cr3 register back into itself.  */
          #define __flush_tlb()  __native_flush_tlb()

          /*  __flush_tlb_global - disables global pages by clearing the PGE flag of cr4,
           *                       rewrites cr3 register back into itself,and sets again the PGE flag.
           */
          #define __flush_tlb_global()  __native_flush_tlb_global()

          /*  __flush_tlb_single - executes "invlpg" assembly language instruction with parameter @addr.  */
          #define __flush_tlb_single(addr)  __native_flush_tlb_single(addr)

        #  for SMP : a processor sends a interprocessor interrupt to other to forces synchronizing.

      kernel avoids TLB flushes :
        >  when performing a process switch between two regular processes that use the same set of page tables.
        >  when performing a process switch between a regular process and a kernel thread.

        #  when the kernel assigns a page frame to a User Mode process and stores its physical address into Page Table
           entry,it must flush any local TLB entry that refers to the corresponding linear address,on SMP,synchronization
           have to be done between CPUs.

      Lazy TLB mode : (kernel use this strategy to avoid useless TLB flushing in SMP)
        the basica idea is :
          if several CPUs are using same page tables and a TLB entry must be flushed on all of them,then TLB flushing
          may,in some cases,be delayed on CPUs running kernel threads.

        when some CPUs start running a kernel thread,the kernel sets it into lazy TLB mode.
        each CPU in lazy TLB mode does not flush the corresponding entries;however,the CPU remembers that its current
        process is running on a set of page tables whose TLBs for the User Mode addresses are invalid.
        as soon as the CPU in lazy TLB mode switches to a regular process with a different set of page tables,the
        hardware automatically flushes the TLBs,and the kernel sets the CPU back in non-lazy TLB mode.
        but,if a CPU in lazy TLB mode switches to a regular process that owns the same set of page tables used by the
        previously running kernel thread,then any deferred TLB invalidation must be effectively applied by the kernel.

        data structures associated to lazy TLB mode :
          @cpu_tlbstate variable is a static array of @NR_CPUS structures consisting of an @active_mm filed pointing to the
          memory descriptor of the current process,and a @state flag that can assume only two values : TLBSTATE_OK | TLBSTATE_LAZY
          each memory descriptor includes a @cpu_vm_mask field that stores the indices of the CPUs that should receive
          Interprocessor Interrupts related to TLB flushing.(it is meaningful just for current process is executing)
          (the CPU has relative to the active memory,in default,indices of all CPUs of the system will be stored into
           @cpu_vm_mask,including the CPU is lazy TLB mode)

        if a CPU recevied a Interprocess interrupt related to TLB flushing,kernel have to checks @state field of its
        @cpu_tlbstate element is equal to TLBSTATE_LAZY,in this case,the kernel refuses to invalidate the TLBs and
        removes the CPU index from the @cpu_vm_mask filed of the memory descriptor.
        two consequences :
          >  as long as the CPU remains in lazy TLB mode,it will note receive other Interprocessor Interrupts related to
             TLB flushing.

          >  if the CPU switches to another process that is using the same set of page tables as the kernel thread that
             is being replaced,the kernel invokes __flush_tlb() to invalidate all non-global TLBs of the CPU.


/*  END OF CHAPTER2  */


Chapter 3 : Processes
    processes are often called "tasks" or "threads" in the Linux source code.

    Processes,Lightweight Processes,and Threads :
      a process is an intance of a program in execution!
      kernel's point of view :
        the purpose of a process is to act as an entity to which system resources are allocated.

      multithreaded applications :
        user programs having many relatively independent execution flows sharing a large portion of the application data
        structures.
      kernel's point of view :
        a multithreaded application was just a normal process.

      "Linux uses lightweight processes to offer better support for multithreaded applications."
      Basically,two lightweight processes may share some resources,if they were associated.

      NPTL - Native POSIX Thread Library
      NGPT - Next Generation POSIX Threading Package

      thread groups :
        in Linux,a thread group is basically a set of lightweight processes that implement a multithreaded application
        and act as a whole with regards to some system calls such as getpid(),kill(),and _exit().

    Process Descriptor :
      To manage processes,the kernel must have a clear picture of what each process is doing.
      <linux/sched.h>
        struct task_struct;
        /*  task_struct - linux process descriptor whose fields contain all the information related to a single process.  */
        /*  some important members :
         *    volatile long state;
         *    struct thread_info *thread_info;
         *    struct mm_struct *mm, *active_mm;
         *    struct tty_struct *tty;
         *    struct fs_struct *fs;
         *    struct files_struct *files;
         *    struct signal_struct *signal;
         */

    Process State :
      the @state field of the process descriptor describes what is currently happening to the process.
      it consists of an array of flags,each of which describes a possible process state.
      process states :
        TASK_RUNNING
          -  running on CPU or wating to be executed.
        TASK_INTERRUPTIBLE
          -  suspended(sleeping) until some condition becomes true.
        TASK_UNINTERRUPTIBLE
          -  like TASK_INTERRUPTIBLE,except that delivering a signal to the sleeping process leaves its state unchanged.
        TASK_STOPPED
          -  execution has been stopped.
        TASK_TRACED
          -  execution has been stopped by a debugger.
        EXIT_ZOMBIE (@state, @exit_state)
          -  execution has been terminated,but the parent process has not yet issued a wait4() or waitpid() system call to
             return information about the dead process.(status have to be reported)
        EXIT_DEAD   (@state, @exit_state)
          -  the final state : the process is being removed by the system because the parent process has just issued a wait4()
             or waitpid() system call for it.(status had been reported)

      <linux/sched.h>
        /*  set_task_state - set the task's state.
         *  @tsk : a pointer points to the task_struct.
         *  @state_value : state value.
         */
        #define set_task_state(tsk, state_value)  set_mb((tsk)->state, (state_value))

        /*  set_current_state - set currently executing task's state.
         *  @state_value : state value.
         */
        #define set_current_state(state_value)    set_mb(current->state, (state_value))

        /*  set_mb - enable memory barrier before set value,disable it later.  */


    Identifying a Process :
      general rule : each execution context that can be independently scheduled must have its own process descriptor,
                     even lightweight processes.
      Process ID : Unix-like operating systems allow users to identify processes by means of a number called PID.
                  (task_struct.pid)
                  /*  by default,the maximum PID number is 32767(PID_MAX_DEFAULT - 1)[32-bit].
                   *                                       4194303[64-bit]
                   *  for change the maximum PID number dynamically,can write a new value into
                   *  /proc/sys/kernel/pid_max
                   */
                  the kernel uses the pidmap_array bitmap to manages PID numbers,which denotes which are the PIDs currently assigned
                  and which are the free ones.
                  /*  32-bit,a page frame contains 32768 bits,so such structure is stored in a single page.
                   *  64-bit,kernel can adds the additional pages to the bitmap,if the PID number is too large.
                   */

                  one process one Process ID !

      POSIX 1003.1c : all threads of a multithreaded application must have the same PID.
      thread groups : the identifier shared by the threads is the PID of the thread group leader,that is,the PID of the
                      first lightweight process in the group.
                      (task_struct.tgid)[getpid() retrieves the value of this field]

    Process descriptors handling :
      Processes are dynamic entities,Linux packs two different data structures in a single per-process memory area :
        >  thread_info                      /*  the structure is defined in <arch/x86/include/asm/thread_info.h>  */
        >  the Kernel Mode process stack


      the length of this memory area is usually 8kB(2 pages),kernel stores the 8kB memory area in two consecutive page
      frames with the first page frame aligned to a multiple of 2^13.(turn out to be a problem when littel dynamic memory
      is available,but kernel could to be configured that use 4kB memory area to stores the stack and thread_info)

      the thread_info structure resides at the begining of the memory area,and the stack grows downward from the end(the end
      of the memory area) !
      [  thread_info.task -> task_struct ; task_struct.thread_info -> thread_info  ]
      /*  Linux task_struct is defined in <linux/sched.h>,it contains a field named "stack" which type is void* ,
       *  kernel use this field to assocaited task_struct with thread_info object.
       *  macro "#define task_thread_info(task) ((struct thread_info *)(task)->stack)" is used to retrieve the
       *  thread_info object which associated with current process.
       *  @current's thread_info => asm("esp") & ~(THREAD_SIZE - 1)
       */

      /**
       *           +---------+  -->  high address
       *           |         |
       *           | process |
       *           | kernel  |
       *           | STACK   |
       *           |         |
       *           +---------+  -->  esp /* C Compiler prefer to use ebp instead of esp(convention) */
       *           |         |           /* so "push $0x1" => "movb $0x1, -2(%ebp)" */
       *           | process |           /* and the memory unit (%esp) saves caller's %ip when invocation occurred */
       *           | thread  |           /* when process switch happens,kernel checkout to */
       *           | info    |           /* the segment contains the process's kernel stack */
       *           |         |           /* BE AWARE,PROCESS SWITCH IS NOT FUNCTION CALLING! */
       *           +---------+  -->  low address
       */

       /*  <linux/sched.h>
        *    union thread_union {
        *            struct thread_info thread_info;
        *            unsigned long stack[THREAD_SIZE / sizeof(long)];
        *    };  /*  task_struct.stack will points to a thread_union object.  */
        *        /*  <arch/x86/include/asm/page_32_types.h>:  #define THREAD_SIZE (PAGE_SIZE << THREAD_ORDER)  */
        */

       /*  because the thread_info structure is 52B long,the kernel stack can expand up to 8140B  */

      the kernel uses the alloc_thread_info and free_thread_info macros to allocate and release the memory area storing a
      thread_info structure and a kernel stack :
      <kernel/fork.c>
        /*  alloc_thread_info - sets thread_info field in struct task_struct.
         *  @tsk : the process descriptor.
         *  return - NULL or the address of the thread_info object former allocated.
         */
        static inline thread_info *alloc_thread_info(struct task_struct *tsk);

        /*  free_thread_info - release the thread_info object.
         *  @ti : the pointer points to a thread_info structure object.
         */
        static inline void free_thread_info(struct thread_info *ti);

    Identifying the current process :
      the kernel can easily obtain the address of the thread_info structure of the process currently running on a 
      CPU from the value of the esp register.
      8kB thread_union :
        the kernel masks out the 13 least significant bits of esp to obtain the base address of the thread_info structure.
      4kB thread_union :
        the kernel masks out the 12 least significant bits of esp to obtain the base address of the thread_info structure.

      <arch/x86/include/asm/thread_info.h>
        /*  current_thread_info - retrieve the address of the thread_info object which is owns to the running process.  */
        static inline struct thread_info *current_thread_info(void)
        {
                return (struct thread_info *)(current_stack_pointer & ~(THREAD_SIZE - 1));
        }

        the kernel can easily obtain the address of the task_struct structure of the process currently running on a CPU from
        the macro "current".
        /*  this is can be simply achieved by thread_info.task  */
        <arch/x86/include/asm/current.h>
          /*  get_current - retrieve the address of a task_struct object which is owns to the current running process.  */
          static __always_inline struct task_struct *get_current(void);    
          #define curren get_current()

        advantage of storing the process descriptor with the stack emerges on multiprocessor systems :
          the correct current process for each hardware processor can be derived just by checking the stack.

          /*  earlier versions of Linux did not store the kernel stack and the process descriptor together,instead,they
           *  were forced to introduce a global static variable called current to identify the process descriptor of the
           *  running process.On multiprocessor systems,it was necessary to define current as an array -- one element for
           *  each available CPU.
           *  #  Linux 2.6 introduced percpu data,macro "current" will retrieves the percpu data for running task.
           */

    Doubly linked lists :
      kernel data structure.
      <linux/list.h>
        struct list_head {
                struct list_head *next, *prev;
        };    
        some methods :
      <linux/list.h>
        list_add(n, p)        inserts @n to @p's next.
        list_add_tail(n, p)   inserts @n to the tail of the list represented by @p.
        list_del(p)           deletes an element pointed to by @p.
        list_empty(p)         checks if the list specified by the address @p of its head is empty.
        list_entry(p, t, m)   returns the address of the data structure of type @t in which the list_head field
                              that has the name @m and the address @p is included.
        list_for_each(p, h)   scans the elements of the list specified by the address @h of the head;in each
                              iteration,a pointer to the list_head structure of the list element is returned in @p.
        list_for_each_entry(p, h, m)
                              similar to list_for_each,but returns the address of the data structure embedding the list_head
                              structure rather than the address of the list_head structure itself.

        usage :
          struct kobject {
                  ...
                  struct list_head list;
                  ...
          };

          struct kobject kobj;
          INIT_LIST_HEAD(&kobj.list);

          struct kobject *new = kmalloc(sizeof(struct kobject), GFP_KERNEL);
          if (new) {
                  list_add(&new->list, &kobj.list);
          }

          ...
          
          list_for_each_entry(new, &kobj.list, list) {
                  printk(KERN_DEBUG "%p", new);
                  ...
          }

          ...

          struct kobject *temp = NULL;
          list_for_each_entry_safe(new, temp, &kobj.list, list) {
                  list_del(&new->list);
              kfree(new);
          }

          ...


      another doubly linked list : hlist.
        <linux/list.h>
          struct hlist_node {
                  struct hlist_node *next, **prev;
          };
          struct hlist_head {
                  struct hlist_node *first;
          };

        methods are similar to list_head's all defined in <linux/list.h>

    The process list :
      a list that links together all existing process descriptors.(task_struct.tasks)
      the head of the process list is the init_task task_struct descriptor.

      useful macros :
        SET_LINKS and REMOVE_LINKS macros are used to insert and to remove a process descriptor in the process 
        list.(but did not find such macros were defined in Linux 2.6)

      <linux/sched.h>
        #define for_each_process(p)  \
                for (p = &init_task; (p = next_task(p)) != &init_task)
        /*  @p must be the pointer which is type of task_strcut  */

      The lists of TASK_RUNNING processes :
        only the TASK_RUNNING process is could to be runned on a CPU.

      #  earlier version :
           all TASK_RUNNING processes are putted into runqueue,and scheduler have to scans the whole list in order
           to select the "best" runnable process.(too costly to maintain the list ordered according to process priorities)
    
      Linux 2.6 :
        the aim is to allow the scheduler to select the best runnable process in constant time,independently of the number
        of runnable processes.

      principle :
        split the runqueue in many lists of runnable processes,one list per process priority.
        each task_struct has a field run_list is type of list_head.if the process priority is equal to k,the
        run_list field links the process descriptor into the list of runnable processes having priority k.
        on SMP,each CPU has its own runqueue.

        #  to make scheduler operations more efficient,the runqueue list has been split into 140(0 -- 139) different lists.
           kernel must preserve a lot of data for every runqueue in the system,however,the main data structures of a runqueue
           are the lists of process descriptors belonging to the runqueue;all these lists are implemented by a single 
           prio_array_t data structure.(Linux 2.6 no such data structure)
           /*
            *  prio_array_t {
            *          int nr_active;                /*  the number of process descriptors linked into the lists  */
            *          unsigned long bitmap[5];      /*  priority bitmap:each flag is set if and only if the correspoding
            *                                            priority list is not empty.  */
            *          struct list_head queue[140];  /*  the 140 heads of the priority lists  */
            *  };
            */

      task_struct.prio stored the dynamic priority of the process.
      task_struct.array is a pointer to the prio_array_t data structure of its current runqueue.(Linux 2.6.34.1 no such field)

      <linux/sched.h>
        void (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup, bool head);
        void (*dequeue_task) (struct rq *rq, struct task_struct *p, int sleep);

        /*  struct sched_class members.
         *  enqueue_task - inserts @p to @rq,called when a task enters a runnable state.
         *  dequeue_task - removes @p from @rq,called when a task is no longer runnable.
         */

    Relationships Among Processes :
      processes created by a program have a parent/child relationship.
      and the children created by a program have sibling relationships.

      task_struct.real_parent; /*  type task_struct*  */
      task_struct.parent;      /*  type task_struct*  */
      task_struct.children;    /*  type list_head     */
      task_struct.sibling;     /*  type list_head     */

      real_parent : the process which has been created @this,or @init process.
      parent : current parent,this is the process that must be signaled when the child process terminates.
               as usual,real_parent == parent,but it may occasionally differ,such as when another process issues a
               ptrace() system call requesting that it be allowed to monitor @this process.
      children : @this process's children.
      sibling : @this process's siblings.

      task_struct.group_leader;  /*  type task_struct,thread group leader  */

      #  a process can be a leader of a process group or of a login session,it can be a leader of a thread group,and
         it can also trace the execution of other processes.

      (task_struct.signal)->leader_pid;   /*  PID of the group leader in the thread group  */
      task_struct.pid                     /*  Process ID  */
      task_struct.tgid;                   /*  PID of the thread group leader  */
      tasK_struct.sessionid;              /*  ID of session associated now  */
      task_struct.ptrace_children;        /*  the head of a list containing all children being traced by a debugger  */
                                          /*  Linux 2.6 no such field  */
      task_struct.ptraced;                /*  a list of tasks this task is using ptrace on  */
                                          /*  Linux 2.6 no ptrace_list field  */

    The pidhash table and chained lists :
      kernel must be able to derive the process descriptor pointer corresponding to a PID.
      scanning the process list sequentially and checking the @pid fields of the process descriptors is feasible but rather
      inefficient.
      four hash tables are used to speed up such operation :
        /*  the reason for multiple hash tables :
         *    the process descriptor includes fields that represent different types of PID.
         *    each type of PID requires its own hash table.
         */

        task_struct.pids[PIDTYPE_MAX];    /*  type pid_link  */

        [type]         [field]  [introduce]
        PIDTYPE_PID    pid      PID of the process
        PIDTYPE_TGID   tgid     PID of thread group leader process  /*  Linux 2.6 no such PIDTYPE,but has this field  */
        PIDTYPE_PGID   pgrp     PID of the group leader process    
        PIDTYPE_SID    session  PID of the session leader process    
        /*  Linux 2.6 does not exist @pgrp and @session fields in task_struct,there is a field named 'pids' which type
         *  is array of struct pid_link and represents the four type pid hashlist.
         */
        /*  Linux 2.6,for retrieve group leader's PID,use function task_pgrp(struct task_struct *) ,
         *  for retrieve session leader's PID,use function task_session(struct task_struct *) .
         */

        <linux/pid.h>
          enum pid_type {
                PIDTYPE_PID,
                PIDTYPE_PGID,
                PIDTYPE_SID,
                PIDTYPE_MAX
          };  /*  Linux 2.6 only has three types of pid.  */

        task_struct.pid;  /*  this field has type pid_t,such type from typedef __kernel_pid_t,
                           *  and __kernel_pid_t is type of int.
                           */
        task_struct.pids; /*  an array of pid_link.
                           *  struct pid_link {
                           *          struct hlist_node node;
                           *          struct pid *pid;
                           *  };
                           */

        these four hash tables are dynamically allocated during the kernel initialization phase,the size of 
        a single hash table depends on the amount of available RAM.

      Linux 2.6 use find_pid_ns() and find_vpid() to get the pid object.
      <linux/pid.h>
        /*  find_pid_ns - find the pid in a specified pid_namespace.
         *  @nr : pid value.
         *  @second-arg : a pointer points to the pid_namespace which is used to find the pid.
         *  return - struct pid *,or NULL.
         */
        extern struct pid *find_pid_ns(int nr, struct pid_namespace *);

        /*  find_vpid - find the pid in current pid_namespace.  */
        extern struct pid *find_vpid(int nr);

        #define pid_hashfn(x)  hash_long((unsigned long)x, pidhash_shift)
        /*  Linux 2.6 : defined in <kernel/pid.c>
         *  #define pid_hashfn(nr, ns)  hash_long((unsigned long)nr + (unsigned long)ns, pidhash_shift)
         *  this macro is used to transform PID into a table index.
         *  @pidhash_shift stores the length in bits of a table index.
         */

        /*  hash_long() based on a multiplication of the index by a suitable large number.
         *  the magic constant is 0x9e370001(2654404609).
         *  unsigned long hash = val * 0x9e370001;
         *  0x9e370001 is a prime near to (2^32) * ((square_root(5) - 1) / 2) that can also
         *  easily multiplied by additions and bit shifts,because it is equal to 2^31 + 2^29 - 2^25 + 2^22 - 2^19 - 2^16 + 1
         */

      Linux uses chaining to handle colliding PIDs;each table entry is the head of a doubly linked list of colliding
      process descriptors.(as usual,the number of processes in the system is far below 32768)

      the data structures used in the PID hash tables are quite sophisticated,because they must keep track of the 
      relationships between the processes.
        >  if kernel wants to retrieve all processes in a specified thread group,it must finds out all the processes
           each tgid field == @tgid_value.
           but use @tgid_value to find a process just returns one process descritpor,the thread group leader.
           so kernel have to maintains a list of processes for each thread group!

           the fields of the pid structure :
             int nr;  /*  PID number  */
             struct hlist_node pid_chain;    /*  hash chain list  */
             struct list_head pid_list;    /*  the head of the per-PID list  */

           Linux 2.6 use another definition of pid structure :
             <linux/pid.h>
               struct upid {
                       int nr;
                       struct pid_namespace *ns;
                       struct hlist_node pid_chain;
               };  /*  this structure represented the Identifier for the pid structure.  */
               struct pid {
                       atomic_t count;
                       unsigned int level;
                       struct hlist_head tasks[PIDTYPE_MAX];  /*  tasks that use this pid  */
                       struct rcu_head rcu;
                       struct upid numbers[1];
               };

      pid handling functions :
        <linux/pid.h>
          /*  do_each_pid_task - do-while loop head.  */
          #define do_each_pid_task(pid, type, task)

          /*  @pid : the pid structure pointer.
           *  @type : pid type.
           *  @task : task_struct pointer used to iterate all tasks in the same pid.
           */

          #define while_each_pid_task(pid, type, task)
          /*  while_each_pid_task - do-while loop end.  */

          /*  Linux 2.6 no such functions  */
          #define find_task_by_pid_type(type, nr)
          #define find_task_by_pid(nr)

          /*  Linux 2.6 has these :
           *    <linux/sched.h>
           *      extern struct task_struct *find_task_by_vpid(pid_t nr);
           *      extern struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns);
           *      /*  use pid value or pid value with namespace to find the corresponding task.  */
           */

          /*  attach_pid - attach task with pid.
           *  @task : the task pointer.
           *  @type : pid type.
           *  @pid : the pid attach to.
           */
          extern void attach_pid(struct task_struct *task, enum pid_type type, struct pid *pid);

          /*  detach_pid - detach task with pid.
           *  @task : the task pointer.
           *  @second-arg : the pid type.
           *  #  @task will attach to NULL pid structure.
           *     and if pid_link[PIDTYPE].pid->tasks[ALL] is empty,then free_pid(pid).
           */
          extern void detach_pid(struct task_struct *task, enum pid_type);

        <linux/sched.h>
          /*  next_thread - retrieve the next thread.
           *  @p : the task in the specified thread group.
           *  return - next task pointer in the thread group or @p.
           */
          static inline struct task_struct *next_thread(const struct task_struct *p);

    How Processes Are Organized :
      runqueue lists group all processes in a TASK_RUNNING state.

      processes in a TASK_STOPPED,EXIT_ZOMBIE,EXIT_DEAD state are not linked in specific lists.
      there is no need to group processes in any of these three states.

      processes in a TASK_INTERRUPTIBLE,TASK_UNINTERRUPTIBLE state are subdivided into many classes,
      each of which corresponds to a specific event.in this case,the process state does not provide enough
      information to retrieve the process quickly,so it is necessary to introduce additional lists of processes,
      there are called wait queues.

      Wait queues :
        wait queues used in kernel particularly for interrupt handling,process synchronization,timing.

        conditional waits on events :
          a process wishing to wait for a specific event places itself in the proper wait queue and
          relinquishes control.
          therefore,a wait queue represents a set of sleeping processes,which are woken up by the kernel
          when some condition becomes true.

        <linux/wait.h>
          typedef struct __wait_queue wait_queue_t;
          typedef int (*wait_queue_func_t)(wait_queue_t *wait, unsigned mode, int flags, void *key);
          struct __wait_queue {
                  unsigned int flags;
                  #define WQ_FLAG_EXCLUSIVE  0x01
                  void *private;            /*  private data,generally stored task_struct address */
                  wait_queue_func_t func;
                  struct list_head task_list;
          };

          struct __wait_queue_head {
                  spinlock_t lock;
                  struct list_head task_list;
          };
          typedef struct __wait_queue_head wait_queue_head_t;

          wait_queue_head_t {
                  ... -> wait_queue_t.task_list - > task_list -> wait_queue_t.task_list {
                          ... -> wait_queue_t.task_list -> wait_queue_t.task_list -> wait_queue_t.task_list -> ...
                  };
          };

        there are two kinds of sleeping processes :
          exclusive processes - wait_queue_t.flags == 1 are selectively woken up by the kernel;
          nonexlusive processes - wait_queue_t.flags == 0 are always woken up by the kernel when the event occurs.

          #  two kinds to prevent race for a resource accessing just allow one process on it.
             a process waiting for a resource that can be granted to just one process at a time is a typical
             exclusive process.
             processes waiting for an event that may concern any of them are nonexclusive.

      Handling wait queues :
        <linux/wait.h>
          !  use these two macro to initialize a wait queue head object which is declared statically or dynamically.

          /*  DECLARE_WAIT_QUEUE_HEAD - declare a wait_queue_head_t object statically.  */        
          #define DECLARE_WAIT_QUEUE_HEAD(name)  \
                  wait_queue_head_t name = __WAIT_QUEUE_HEAD_INITIALIZER(name);

          extern void __init_waitqueue_head(wait_queue_head_t *q, struct lock_class_key *);
          /*  init_waitqueue_head - initialize a wait_queue_head_t object which is dynamically allocated.  */
          #define init_waitqueue_head(q)                     \
                  do {                                       \
                      static struct lock_class_key __key;    \
                      __init_waitqueue_head((q), &__key);    \
                  } while (0)


          !  use these two functions to initialize a wait queue entry with @task or with @wake_up_func.

          /*  init_waitqueue_entry - initialize a wait_queue element with task @p.
           *  @q : the target to be initialized.
           *  @p : task pointer.
           *  #  @q->flags = 0;
           *     @q->private = @p;
           *     @q->func = default_wake_function;    /*  for nonexclusive process  */
           */
          static inline void init_waitqueue_entry(wait_queue_t *q, struct task_struct *p);

          /*  init_waitqueue_func_entry - initialize @q with @func.
           *  @q : the wait_queue_t object's address.
           *  @func : the wake up function.
           *  #  @q->flags = 0;
           *     @q->private = NULL;
           *     @q->func = func;
           */
          static inline void init_waitqueue_func_entry(wait_queue_t *q, wait_queue_func_t func);


          !  use these two macros to put "current" into a wait queue and automatically remove it later.

          /*  DEFINE_WAIT_FUNC - declare a wait_queue_t object @name and initialize it with @function,
           *                     this object's private field will be initialized to "current".
           */
          #define DEFINE_WAIT_FUNC(name, function)
          /*  DEFINE_WAIT - put "current" process into wait queue and automatically remove it at the time
           *                it is woken up.
           *  #  autoremove_wake_function() will invokes default_wake_function() at first,then remove this
           *     element from the wait queue.
           */
          #define DEFINE_WAIT(name)  DEFINE_WAIT_FUNC(name, autoremove_wake_function)


          !  use these three functions to complete INSERT | INSERT INTO EXCLUSIVE | REMOVE operations.

          /*  add_wait_queue - insert @wait to @q.
           *  @q : the head.
           *  @wait : the element.
           *  #  for nonexclusive processes.
           */
          extern void add_wait_queue(wait_queue_head_t *q, wait_queue_t *wait);

          /*  add_wait_queue_exclusive - insert @wait to @q.
           *  #  for exclusive processes.
           */
          extern void add_wait_queue_exclusive(wait_queue_head_t *q, wait_queue_t *wait);

          /*  remove_wait_queue - remove @wait from @q.
           *  #  this function does not use the @q parameter,but it as an identifier.
           */
          extern void remove_wait_queue(wait_queue_head_t *q, wait_queue_t *wait);

          !  use this function to check if the wait queue is active.

          /*  waitqueue_active - check if @q is activing(it is not empty) now.  */
          static inline int waitqueue_active(wait_queue_head_t @q);

      Process wishing to wait :
        <linux/wait.h>
          /*  sleep_on - let "current" sleep on @q and it will enter TASK_UNINTERRUPTIBLE state.  */
          extern void sleep_on(wait_queue_head_t *q);
          /*  interruptible_sleep_on - "current" will enter TASK_INTERRUPTIBLE state.  */
          extern void interruptible_sleep_on(wait_queue_head_t *q);

          /*  sleep_on_timeout - timer version,@timeout is the maximum time to sleep.
           *                     "current" will enter TASK_UNINTERRUPTIBLE state.
           *                     if "current" is woken up before timer expire,then
           *                     the left time will be returned.
           */
          extern long sleep_on_timeout(wait_queue_head_t *q, signed long timeout);
          /*  interruptible_sleep_on - TASK_INTERRUPTIBLE version.  */
          extern long interruptible_sleep_on_timeout(wait_queue_head_t *q, signed long timeout);

          !  the sleep_on()-like functions cannot be used in the common situation where one has to
             test a condition and atomatically put the process to sleep when the condition is not
             verified.
             they are a well-known source of race conditions,their use is discouraged.
             #  sleep_on()-like functions were defined in <kernel/sched.c>.
                for sleep_on(),it calls to sleep_on_common(),that function sets "current"'s state to
                TASK_UNINTERRUPTIBLE,initializes a wait entry with "current",and insert it into @q,
                calls schedule_timeout() with MAX_SCHEDULE_TIMEOUT left current process sleeping.

          /*  prepare_to_wait - does the prepare works for "current" is going to wait.
           *  @q : the wait queue head.
           *  @wait : the wait entry.
           *  @state : the state "current" will be.
           *  #  this function will set "current"'s state to @state at first,then insert @wait
           *     into @q.
           *  #  this version is defined for nonexclusive process.
           */
          void prepare_to_wait(wait_queue_head_t *q, wait_queue_t *wait, int state);

          /*  prepare_to_wait - exclusive version.  */
          void prepare_to_wait_exclusive(wait_queue_head_t *q, wait_queue_t *wait, int state);

          /*  finish_wait - finish "current" waiting.
           *  @q : the wait queue head.
           *  @wait : the wait entry.
           *  #  this function will removes @wait from @q,and sets "current"'s state to
           *     TASK_RUNNING(this will happens before removing).
           */
          void finish_wait(wait_queue_head_t *q, wait_queue_t *wait);

          Usage for prepare_to_wait_*() and finish_wait() :
            #define __wait_event(wq, condition)                                  \
            do {                                                                 \
                    DEFINE_WAIT(__wait);                                         \
                                                                                 \
                    for (;;) {                                                   \
                            prepare_to_wait(wq, &__wait, TASK_UNINTERRUPTIBLE);  \
                            if (condition)                                       \
                                    break;                                       \
                            schedule();                                          \
                    }                                                            \
                    finish_wait(wq, &__wait);                                    \
            } while (0)
          

          /*  wait_event - "current" waiting for @condition gets TRUE.
           *  @wq : the wait queue head pointer.
           *  @condition : the condition is a C expression.
           *  #  "current" will enter TASK_UNINTERRUPTIBLE state.
           *     @condition will be checked each time @wq is woken up.
           *  #  function wake_up() is used to wake a wait_queue up.
           */          
          #define wait_event(wq, condition)     \
          do {                                  \
                  if (condition)                \
                          break;                \
                  __wait_event(wq, condition);  \
          } while (0)

          /*  wait_event_timeout - timer version.
           *                       if "current" is woken up before timer expired,
           *                       the left time will be returned.
           */
          #define wait_event_timeout(wq, condition, timeout)
          
          /*  wait_event_interruptible - TASK_INTERRUPTIBLE version.
           *  return - interrupted,returns -ERESTARTSYS;
           *           condition got TRUE,returns 0.
           */
          #define wait_event_interruptible(wq, condition)


      Kernel wake up the waiting processes :
        <linux/wait.h>
          /*  __wake_up* - the main procedure to wake up processes.
           *               defined in <kernel/sched.c>
           */        
          void __wake_up(wait_queue_head_t *q, unsigned int mode, int nr, void *key);
          void __wake_up_locked(wait_queue_head_t *q, unsigned int mode);
          void __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr);

          /*  TASK_UNINTERRUPTIBLE processes.  */
          #define wake_up(x)  __wake_up(x, TASK_NORMAL, 1, NULL)
          #define wake_up_nr(x, nr)  __wake_up(x, TASK_NORMAL, nr, NULL)
          #define wake_up_all(x)  __wake_up(x, TASK_NORMAL, 0, NULL)
          #define wake_up_locked(x)  __wake_up_locked((x), TASK_NORMAL)

          /*  TASK_INTERRUPTIBLE processes.  */
          #define wake_up_interruptible(x)  __wake_up(x, TASK_INTERRUPTIBLE, 1, NULL)
          #define wake_up_interruptible_nr(x, nr)  __wake_up(x, TASK_INTERRUPTIBLE, nr, NULL)
          #define wake_up_interruptible_all(x)  __wake_up(x, TASK_INTERRUPTIBLE, 0, NULL)
          #define wake_up_interruptible_sync(x)  __wake_up_sync((x), TASK_INTERRUPTIBLE, 1)

          !  all macros wakeup all nonexclusive processes.
             that is the parameter @nr to __wake_up*() functions is 0,just wake up everything;
            (includes all nonexclusive processes and all exclusive processes)
             if @nr == small + venumber,then wake up all nonexclusive processes and one
             exclusive process.(that is what wake_up() to do)
             e.g. (code)
               if (curr->func(curr, mode, wake_flags, key) &&
                       (flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
                           break;
                                                  /*  negative number is also TRUE  */

          !  TASK_NORMAL is used to wake up TASK_UNINTERRUPTIBLE processes;
             TASK_INTERRUPTIBLE is used to wake up TASK_INTERRUPTIBLE processes.
          !  '_nr' suffix macros are used to wake up exclusive processes with the given @nr.
             @nr => @nr_exclusive.
          !  '_all' suffix macros will wake up all exclusive processes.
          !  '_sync' suffix macro check whether the priority of any of the woken processes is
             higher than that of the processes currently running in the systems and incoke
             schedule() if necessary.(these checks is not made by this macro)
          !  '_locked' suffix macro requires the wait_queue_head_t.lock has been held.
          
      
    Process Resource Limits :
      each process has an associated set of resource limits,which specify the amount of system resources
      it can use.

      <linux/sched.h>
        struct signal_struct {
                ...
                struct rlimit rlim[RLIM_NLIMITS];
                ...
        };

        task_struct.signal->rlim;

      <linux/resource.h>
        struct rlimit {
                unsigned long rlim_cur;  /*  the current resource limit  */
                unsigned long rlim_max;  /*  the maximum resource limit  */
        };

      !  only the superuser(or,more precisely,a user who has the CAP_SYS_RESOURCE capability) can increase the 
         @rlim_max or set the @rlim_cur to a value greater than the corresponding @rlim_max field.

      Kernel use the @index of @rlim member to identify the type of resource-limit.
      Toltal number of resource-limits in Linux 2.6 is 16,they are defined in <include/asm-generic/resource.h>.

      #define RLIMIT_CPU  0            /*  if cpu time exceeds,kernel will send SIGXCPU to the process,  */
        /*  CPU time in sec  */        /*  then,if the process does not terminate,SIGKILL will be sent.  */
      #define RLIMIT_FSIZE  1          /*  if process wish to enlarge filesize greater than this limit,  */
        /*  maximum filesize  */       /*  kernel will send SIGXFSZ  signal.  */
      #define RLIMIT_DATA  2           /*  heap size in bytes  */
        /*  max data size  */
      #define RLIMIT_STACK  3
        /*  max stack size  */
      #define RLIMIT_CORE  4           /*  if limit == 0,kernel does not creates core dump file  */
        /*  max core file size  */

      #ifndef ...
      #define RLIMIT_RSS  5
        /*  max resident set size  */
      #define RLIMIT_NPROC  6
        /*  max number of processes  */
      #define RLIMIT_NOFILE  7
        /*  max number of open files  */
      #define RLIMIT_MEMLOCK  8        /*  size of nonswappable memory in bytes  */
        /*  max locked-in-memory address space  */
      #define RLIMIR_AS  9             /*  kernel checks this limit when malloc() was called  */
        /*  address space limit  */
      #define RLIMIT_LOCKS  10
        /*  maximum file locks held  */
      #define RLIMIT_SIGPENDING  11
        /*  max number of pending signals  */
      #define RLIMIT_MSGQUEUE  12
        /*  maximum bytes in POSIX mqueues  */
      #define RLIMIT_NICE  13
        /*  max nice prio allowed to raise to  0-39 for nice level 19 .. -20  */
      #define RLIMIT_RTPRIO  14
        /*  maximum realtime priority  */
      #define RLIMIT_RTTIME  15
        /*  timeout for RT tasks in us  */
      #endif 

      #define RLIMIT_NLIMITS  16

      /*  resource limit value  */
      #ifndef RLIM_INFINITY
      #define RLIM_INFINITY  (~0UL)
        /*  no user limit is imposed on the corresponding resource  */
      #endif


    Process Switch :
      kernel suspends the executing of the process that is running on the CPU and resume it later.
      kernel suspends a process and pick up another process to be executing.

      these different names all refers to the process switch : process switch, task switch, context switch

      !  Process Switch occurs only in Kernel Mode.

      Hardware Context :
        the set of data that must be loaded into the registers before the process resumes its execution on the
        CPU is called the "hardware context".it is the subset of the process execution context,which includes all
        information needed for the process execution.

        Linux,a part of hardware context is stored in process descriptor,while the remaining part is saved in the
        Kernel Mode stack.

        !  because process switches occur quite often,it is important to minimize the time spent in saving and 
           loading hardware contexts.

        /*  Old versions of Linux took advantage of the hardware support offered by the 80x86 architecture and
         *  performed a process switch through a "far jmp" instruction to the selector of the 
         *  Task State Segment Descriptor of the next process.
         */

        Linux 2.6 uses software to perform a process switch for the following reasons :
          >  Step-by-step switching performed through a sequence of "mov" instructions allows better control
             over the validity of the data being loaded.
             (it is possible to check the values of the ds and es segmentation registers)

          >  the amount of time required by the old approach and the new approach is about the same.
             However,it is not possible to optimize a hardware context switch,while there might be room
             for improving the current switching code.

      Linux stores the contents of all registers used by a process in User Mode into the Kernel Mode
      stack before performing process switching.(includes ss esp .etc)

      Task State Segment :
        80x86 architecture includes a specific segment type called Task State Segment(TSS) to store hardware
        contexts.Linux does not use hardware context switching,but it is still set up TSS for each CPU in the
        system.
        the reasons :
          >  when an 80x86 CPU switches from User Mode to Kernel Mode,it fetches the address of the Kernel Mode
             stack from the TSS.
          >  when a User Mode process attempts to access an I/O port by means of an "in" or "out" instruction,
             the CPU may need to access an I/O Permission Bitmap stored in the TSS to verify whether the process
             is allowed to address the port.

        #  a process executes an "in" or "out" I/O instruction in User Mode :
           the control unit performs the following operations >
             1>  checks the 2-bit IOPL field in the "eflags" register.
                 IOPL == 3, executes I/O instructions,
                 Otherwise, performs the next check.
             2>  accesses the "tr" register to determine the current TSS,and thus the proper
                 I/O Permission Bitmap.
             3>  checks the bit of the I/O Permission Bitmap corresponding to the I/O port
                 specified in the I/O instruction.
                 cleared => instruction is executed,
                 setted  => raises a "General protection" exception.

        the tss_struct structure describes the format of the TSS :
          <arch/x86/include/asm/processor.h>
            struct tss_struct {
                    struct x86_hw_tss x86_tss;
                    /*  extra 1 is there because CPU will access an
                     *  additional byte,and the extra byte must be all 1 bits.
                     */
                     unsigned long io_bitmap[IO_BITMAP_LONGS + 1];
                     unsigned long stack[64];  /*  another 0x100 bytes for emergency kernel stack  */
            } ____cachingline_aligned;

        the init_tss array stores one TSS for each CPU on the system :
          <arch/x86/include/asm/processor.h>
            DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss);
            /*  the invoked macro is defined in <linux/percpu-defs.h>  */
        !  at each process switch,the kernel updates some fields of the TSS so that the corresponding
           CPU's control unit may safely retrieve the information it needs.
           (TSS reflects the privilege of the current process on the CPU,but there is no need to maintain
            TSSs for processes when they're not running.)

        TSS <=> TSSD (Task State Segment Descriptor)
        /*  in the Intel's original design,each process in the system should refer to its own TSS;
         *  the second least significant bit of the Type field is called the Busy bit;
         *  it is set to 1 if the process is being executed by a CPU,and to 0 otherwise.
         *  in Linux design,there is just one TSS for each CPU,so the Busy bit is always set to 1.
         */

        TSSDs stored in GDT,whose base address is stored in the gdtr register of each CPU.
          the tr register of each CPU contains the TSSD Selector of the corresponding TSS.
          the register also includes two hidden,nonprogrammable fields :
          the Base field of the TSSD
          the Limit field of the TSSD
      
      The thread field :
        at every process switch,the hardware context of the process being replaced must be saved in
        somewhere,but it can not be stored in TSS,because in Linux Design,one TSS for one CPU.

        /*  thread - task_struct member which type is thread_struct  */
        task_struct.thread;     /*  CPU-specific state of this task  */

        <arch/x86/include/asm/processor.h>
          struct thread_struct {
                  ...
          };
          /*  this structure represents the CPU-specific state for a task,
           *  Linux saves hardware context in such object whenever the process is being switched out.
           *  this structure contains fields for most of the CPU registers,except the general-purpose
           *  registers such as eax,ebx, etc.,which are stored in the Kernel Mode stack.
           */


      Performing the Process Switch :
        A process switch may occur at just one well-defined point :
          the schedule() function.(<include/linux/sched.h>, <kernel/sched.c>)
        
        every process switch consists of two steps :
          1>  switching the Page Global Directory to install a new address space.
          2>  switching the Kernel Mode stack and the hardware context,which provides all the
              information needed by the kernel to execute the new process,including the CPU registers.

        @prev -> the process been replaced.
        @next -> the process being activated.

        The switch_to macro :
          <include/asm-generic/system.h>
            /*  switch_to - switch previous process to next process.
             *  @prev : the previous process,it is often got by "current".
             *  @next : the next process switch to.
             *  @last : it is an output parameter that specifies a memory location
             *          in which the macro writes the descriptor address of process C.
             *          (this is done after A resumes its execution)
             *  #  before the process switching,the macro saves in the eax CPU register
             *     the content of the variable identified by the first input parameter @prev,
             *     after the process switching,when A has resumed its execution,the macro
             *     writes the content of the eax CPU register in the memory location of A
             *     identified by the third output parameter @last.
             *  #  this function will call to the architecture based __switch_to()
             *     function to accomplishes the primary works,that function is
             *     defined in <arch/"model"/asm/process_(32 | 64).h>
             */
            #define switch_to(prev, next, last)                 \
                  do {                                          \
                      ((last) = __switch_to((prev), (next)));   \
              } while (0)

          !  there is another switch_to() function is defined in <arch/x86/include/asm/system.h>,
             which has introduced the detail for how process switching be executed.
             the switch_to() in <asm/system.h> is the architecture based switch_to() function,
             not the asm-generic version.
             for x86,this switch_to() will be called by context_switch() function which is defined
             in <kernel/sched.c>.(because <linux/sched.h> includes <asm/systemd.h>)
             the primary assembly :
               pushfl                 #  save eflags
               pushl %%ebp            #  save base stack pointer
               movl %%esp, %[prev_sp] #  save  stack pointer
               movl %[next_sp], %%esp #  restore stack pointer
               movl $1f, %[prev_ip]   #  save ip(the resume point of current)
               pushl %[next_ip]       #  push ip into stack for ret instruction
               __switch_canary        #  macro defined in <asm/system.h>
               jmp __switch_to        #  jump to __switch_to() function
               1:                     #  symbol
               popl %%ebp             #  restore base stack pointer
               popfl                  #  restore eflags


          figure :
            switch_to(A, B, A) {
              A {
                      prev = A
                      next = B
                      eax = A
                      last = A
              }

              B {
                      prev = B
                      next = other
                      eax = A    /*  this register will be updated after function invocation completed
                                  *  or process switching was occurred.
                                  */
                      last = A
              }
            }
            /*  there just one kernel existed,so the code for context switching is same between
             *  processes.
             */
            switch_to(C, A, C) {
              C {
                      prev = C
                      next = A
                      eax = C
                      last = C
              }

              A {
                      prev = A
                      next = other
                      eax = C    /*  this register will be updated after function invocation completed
                                  *  or process switching was occurred.
                                  */
                      last = C
              }
            }

        !  the process has been switched at the time that @next->thread.esp was loadded into
           esp register.
           (this operation is take affect when arch_end_context_switch(@next) was called)
           the kernel stack of previous process is saved in @prev->thread.esp.
           (this operation is accomplished when arch_start_context_switch(@prev) was called)
       

        The __switch_to() function :
          <arch/x86/include/asm/system.h>
          <arch/x86/kernel/process_(32 | 64).c>

            /*  __switch_to - does the bulk of the process switch started by the switch_to() macro.
             *  @prev_p : previous task pointer.
             *  @next_p : next task pointer.
             *  return -  @prev_p.
             *            switch_to() macro will replaces esp register(ebp was not replaced,it was pushed
             *            into the stack of @prev and popped up later) before jmp to __switch_to().
             *            but the arguments of __switch_to() are stored in CPU generic-purpose registers
             *            that is eax(@prev_p) and edx(@next_p),finally,it returns the value in eax register,
             *            so it is @prev_p.
             *  #  __attribute__(regparm(3)) should be attached to __switch_to(),but it did not detect such
             *     GCC attribute is used.
             *     regparm(number) : this attribute just take affect only if x86-32 targers,it tell compiler
             *                       that,store the parameter from number one to @number in CPU registers
             *                       eax,edx,ecx(so the maximum @number is 3) to instead store them on stack.
             */
            __notrace_funcgraph struct task_struct *
            __switch_to(struct task_struct *prev_p, struct task_struct *next_p);

            the works this function does :
              /*  @prev_p -> previous task pointer.
               *  @prev   -> previous task's thread structure.
               *  @next_p -> next task pointer.
               *  @next   -> next task's thread structure.
               */
              1>  get local CPU id and local tss.
              2>  check if the task @next_p is used math function,if it used and
                  @next_p->fpu_counter > 5,then set @preload_fpu to true.
              3>  call to "__unlazy_fpu(prev_p);" this macro optionally save the contents of the FPU,MMX,XMM of
                  @prev_p.
              4>  if @preload_fpu is T => prefetch(@next->xstate) .
              5>  call to "load_sp0(@tss, @next);",load @next->esp0 into @tss->esp0.
                  any future privilege level change from User Mode to Kernel Mode raised by a sysenter assembly
                  instruction will copy this address in the esp register.
              6>  call to "lazy_save_gs(@prev->gs);",this macro is defined through savesegment(gs, (v)),it stores
                  gs register in the @prev->gs.
              7>  call to "load_TLS(@next, @cpu);",loads in the Global Descriptor Table of the local CPU the 
                  Thread-Local Storage segments used by the @next_p process,the Segment Selectors are stored in
                  @next->tls_array member.
              8>  restore IOPL if needed.
                  in normal use,the flags restore in the switch assembly will handle this,but if the kernel
                  is running virtualized at a non-zero CPL,the popf will not restore flags,so it must be done
                  in a separate step.
              9>  handle debug registers and/or IO bitmaps.
                  if this is necessary,call to "__switch_to_xtra(@prev_p, @next_p, @tss);".
              10> if @preload_fpu is T => execute clts instruction.
              11> call to "arch_end_context_switch(@next_p);" to ensure context switching has been completed.
              12> if @preload_fpu is T => call to "__math_stat_restore();" restore math registers.
              13> restore gs if needed(if (@prev->gs || @next->gs) lazy_load_gs(@next->gs); ).
              14> update percpu data via "percpu_write(current_task, @next_p);").
              15> return @prev_p.


        Saving and Loading the FPU,MMX,XMM registers :
          from Intel 80486DX,the arithmetic floating-point unit(FPU) has been integrated into the CPU.the name
          'mathematical coprocessor' continues to be used in memory of the days when floating-point computations
          were executed by an expensive special-purpose chip.
          ESCAPE instructions(for compatible with older models) are instructions with a prefix byte ranging
          between 0xd8 -- 0xdf,these instructions act on the set of floating-point registers included in the CPU.
          !  if a process is using ESCAPE instructions,the contents of the FPU registers belong to its hardware
             context,so they are should be saved and restored later.

          MMX : new instructions were introduced on Pentium models,supposed to speed up the execution of multimedia
                applications.
          MMX instructions act on the FPU registers.
          disadvantage : programmers can not mix FPU instructions and MMX instructions.
          advantage    : for OS designer,save the FPU state is to save MMX state.
          MMX introduced SIMD(single-instruction multiple-data) pipeline inside the processor.

          !  Pentium III model extends that SIMD capability :
               it introduces the SSE extensions(Streaming SIMD Extensions),which adds facilities for handling
               floating-point values caontained in eight 128-bit registers called the XMM registers.
               (XMM0 -- XMM7)
               XMM registers do not overlap the FPU and MMX registers.(it is able to mix SSE and FPU/MMX)
          !  Pentium 4 model introduces yet another feature : SSE2 Extensions.
               which is basically an extension of SSE supporting higher-precision floating-point values,
               it uses the same set of XMM registers as SSE.

          80x86 model do not save the FPU,MMX,XMM registers in the TSS automatically,but it enables kernel to do
          that if necessary.

          cr0.TS flag : TS(Task-Switching)
            which obeys the following rules :
              >  every time a hardware context switch is performed,the TS flag is set.
              >  every time an ESCAPE,MMX,SSE,SSE2 instruction is executed when the TS flag is set,
                 the control unit raises a "Device not available" exception.

          figure :
            A is using mathematical coprocessor;
            switch occurs from A to B,cr0.TS = 1,saves floating-point registers to A.tss;
            B is not using mathmetical coprocessor => kernel do not need to restore floating-point registers;
            B try to use mathmetical coprocessor -> cr0.TS has been setted -> "Device not available" exception;
            kernel handle the exception => restore the floating-point registers from B.tss;

          FPU,MMX,XMM structures :
            <arch/x86/include/asm/processor.h>
              struct i387_fsave_struct;    /*  FPU,MMX state  */
              struct i387_fxsave_struct;   /*  SSE,SSE2 state */
              struct i387_soft_struct;     /*  older compatibility  */
                                           /*  it is used for the older CPU model which is no
                                            *  mathmetical coprocessor.
                                            */
              union thread_xstate {
                struct i387_fsave_struct fsave;
                struct i387_fxsave_struct fxsave;
                struct i387_soft_struct soft;
                struct xsave_struct xsave;
              };  /*  thread_struct.xstate (pointer type)  */

            the process descriptor includes two additional flags :
            task_struct.thread_info.status { TS_USEDFPU }
            task_struct.flags { PF_USED_MATH }

            TS_USEDFPU : it specifies whether the process used the FPU,MMX,XMM registers in the current execution run.
            PF_USED_MATH : it specifies whether the contents of the thread_struct.xstate are significant,the flag is
                           cleared in two cases :
                             1>  when the process starts executing a new program by invoking an execve() system call,
                                 because the control will never return to the former program,the data currently stored
                                 in thread_struct.xstate is never used again.
                             2>  when a process that was executing a program in User Mode starts executing a signal
                                 handler procedure,because signal handlers are asynchronous with respect to the program
                                 execution flow,the floating-point registers could be meaningless to the signal handler.
                                 however,the kernel saves the floating-point registers in thread_struct.xstate before
                                 starting the handler and restores them after the handler terminates.(that is the signal
                                 handler is allowed to use floating-point features)

          Saving the FPU registers :
            <arch/x86/include/asm/i387.h>
              /*  __unlazy_fpu - save fpu state.
               *  @tsk : the target,which often is @prev in __switch_to().
               *  return - none.
               *  #  this function checks if TS_USEDFPU flag of @tsk's thread_info.status is set,
               *     TS_USEDFPU == 1,then call to __save_init_fpu(@tsk),and call to stts().
               *     TS_USEDFPU == 0,then @tsk->fpu_counter = 0.
               */
              static inline void __unlazy_fpu(struct task_struct *tsk);

              /*  __save_init_fpu - save fpu state and then initializes them.
               *  @tsk : the target.
               *  return - none.
               *  #  this function maybe call to either xsave(@tsk) or fxsave(@tsk),that is determined by
               *     task_thread_info(@tsk)->status & TS_XSAVE.
               *     then call to clear_fpu_state(@tsk) to initializes fpu state to fixed values,
               *     and clear TS_USEDFPU in the @tsk's thread_info.status.
               */
              static inline void __save_init_fpu(struct task_struct *tsk);

            <arch/x86/include/asm/system.h>
              /*  stts - a macro sets cr0.TS flag.  */
              #define stts() write_cr0(read_cr0() | X86_CR0_TS)

          Loading the FPU registers :
            the contents of the floating-point registers are not restored right after the @next process
            resumes execution.(but __switch_to() restored it if @preload_fpu is TRUE)
            however,the TS flag of cr0 has been set by __unlazy_fpu(),thus,the first time the @next process
            tries to execute an ESCAPE,MMX,SSE/SSE2 instruction will traps an exception,then handler calls to
            math_state_restore() to restore the contents.

            <arch/x86/include/asm/i387.h>
              /*  math_state_resotre - exception handler to deal with use mathematical coprocessor when TS == 1.
               *  #  this function checks @task is used math at first(PF_USED_MATH flag),
               *     if it is not,then call to init_fpu() to initializes FPU(PF_USED_MATH set to 1) before
               *     next execution;
               *     call to clts() to clear cr0.TS,then invoke __math_state_restore() to do primary works.
               *     (if TS == 1,execute ESCAPE,MMX,SSE/SSE2 instruction will traps exception)
               */
              extern asmlinkage void math_state_restore(void);

              /*  __math_state_restore - restores fpu for @tsk and set TS_USEDFPU.
               *  #  @tsk = thread->task;  =>
               *     @thread = current_thread_info();
               *     call to restore_fpu_checking(@tsk),(this function execute "fxrstor" instruction)
               *     set TS_USEDFPU,
               *     @tsk->fpu_counter++.
               */
              extern void __math_state_restore(void);

          Using the FPU,MMX,and SSE/SSE2 units in Kernel Mode :
            !  IF IT IS NOT NECESSARY,DO NOT USE x87 IN KERNEL MODE.
            use x87 in Kernel Mode is more expensive than User Mode.
          
            if kernel use FPU,it should avoid interfering with any computation carried on by the current 
            User Mode process.

            <arch/x86/include/asm/i387.h>
              /*  kernel_fpu_begin - kernel ready to use FPU.
               *  #  disable preempt,
               *     checks if current thread is used FPU(TS_USEDFPU),then save the state,
               *     if it is not,just clts().
               */
              static inline void kernel_fpu_begin(void);

              /*  kernel_fpu_end - kernel end use FPU.
               *  #  set cr0.TS by stts(),
               *     enable preempt.
               */
              static inline void kernel_fpu_end(void);

            because the FPU state of User Mode process has been saved,so math_state_restore() will be called
            later when the process tries to execute an ESCAPE,MMX,SSE/SSE2 instruction.

            !  the kernel uses FPU only in a few places,typically when moving or clearing large memory areas
               or when computing checksum functions.


    Creating Processes :
      Traditional Unix systems treat all processes in the same way :
        resources owned by the parent process are duplicated in the child process.
        !  this approach makes process creation very slow and inefficient.

      Modern Unix kernels solve this problem by introducing three different mechanisms :
        1>  Copy-On-Write
        2>  Lightweight processes allow both the parent and the child to share many perprocess kernel data
            structures,such as the paging tables,the open file tables,and the signal dispositions.
        3>  the vfork() system call creates a process that shares the memory address space of its parent,
            to prevent the parent from overwriting data needed by the child,the parent's execution is blocked
            until the child exits or executes a new program.

      The clone(),fork(),and vfork() System Calls :
        Lightweight processes are created in Linux by using a function named clone().
        <linux/sched.h>
          /*  clone - Linux system call,create a lightweight process.
           *  @fn : the function to be executed.
           *  @child_stack : User Mode stack pointer was assigned to esp register.
           *  @flags : flags,the low byte specifies the signal number to be sent to the parent process when
           *           child terminates,the SIGCHLD signal is generally selected;
           *           remaining three bytes encode a group of clone flags.
           *  @arg : arguments to @fn.
           *  @ptid : address of a User Mode variable of the parent process that will hold the PID of the 
           *          new lightweight process.meaningful only if the CLONE_PARENT_SETTID flag is set.
           *  @newtls : address of a data structure that defines a Thread Local Storage segment for the new
           *            lightweight process,meaningful only if the CLONET_SETTLS flag is set.
           *  @ctid : address of a User Mode variable of the new lightweight process that will hold the PID
           *          of such process,meaningful only if the CLONE_CHILD_SETTID flag is set.
           *  return - lighetweight process's thread ID will be returned,if succeed.
           *           Otherwise,-1 will be returned,and no lightweight process has been created.
           *         error value will be set appropriately.
           */
          int clone(int (*fn)(void *), void *child_stack, int flags, void *arg,
                    /*  pid_t *ptid, void *newtls, pid_t *ctid  */ ...);
          /*  System-Call  */

          clone flags :
            CLONE_VM - shares the memory descriptor and all Page Tables.
            CLONE_FS - shares the table that identifies the root directory and the current working directory,
                       as well as the value of the bitmask used to mask the initial file permissions of a new
                       file.
            CLONE_FILES - shares the table that identifies the open files.
            CLONE_SIGHAND - shares the tables that identify the signal handlers and the blocked and pending signals.
                            if this flag is true,the CLONE_VM flag must also be set.
            CLONE_PTRACE - if traced,the parent wants the child to be traced too,furthermore,the debugger may want
                           to trace the child on its own,in this case,the kernel forces the flag to 1.
            CLONE_VFORK - set when the system call issued is a vfork().
            CLONE_PARENT - set the parent of the child to the parent of the calling process.
            CLONE_THREAD - inserts the child to the same thread group of the parent,and forces the child to share
                           the signal descriptor of the parent,the child's tgid and group_leader fields are set
                           accordingly.if the flag is true,the CLONE_SIGHAND flag must also be set.
            CLONE_NEWNS - set if the clone needs its own namespace,that is,its own view of the mounted filesystems;
                          it is not possible to specify both CLONE_NEWNS and CLONE_FS.
            CLONE_SYSVSEM - shares the SystemV IPC undoable semaphore operations.
            CLONE_SETTLS - creates a new Thread Local Storage segment for the lightweight process.
            CLONE_PARENT_SETTID - writes the PID of the child into the User Mode variable of the parent pointed to
                                  by the @ptid parameter.
            CLONE_CHILD_CLEARTID - when set,the kernel sets up a mechanism to be triggered when process will exit or
                                   when it will start executing a new program.in these cases,the kernel will clear the
                                   User Mode variable pointed to by the @ctid parameter and will awaken any process
                                   waiting for this event.
            CLONE_DETACHED - a legacy flag ignored by the kernel.
            CLONE_UNTRACED - set by the kernel to overried the value of the CLONE_PTRACE flag.
            CLONE_CHILD_SETTID - writes the PID of the child into the User Mode Variable of the child pointed by the
                                 @ctid parameter.
            CLONE_STOPPED - forces the child to start in the TASK_STOPPED state.

          !  the clone flags were defined in kernel have more values,these values were used by kernel routine.

          !  the clone() in C library is actually a wrapper of the sys_clone() defined in <arch/x86/kernel/process.c>.
             and sys_clone() will call to do_fork() function.
             sys_clone() does not have the @fn and @arg parameters,in fact,the wrapper function saves the pointer @fn
             into the child's stack position corresponding to the return address of the wrapper function itself;
             the pointer @arg is saved on the child'stack right below @fn.

          !  sys_fork() similar to sys_clone(),but flag only the SIGCHLD was set and all clone flags are cleared,
             stack pointer is point to the parent's stack,remaining arguments set to NULL.

          !  sys_vfork() similar to sys_clone(),but flag is set to CLONE_VFORK | CLONE_VM | SIGCHLD combination,
             stack pointer is point to the parent's stack,remaining arguments set to NULL.


      The do_fork() function :
        <linux/sched.h> <kernel/fork.c>
          /*  do_fork - the main fork routine.
           *  @clone_flags : same as the @flags parameter of clone().
           *  @stack_start : the stack's start point,same as the @child_stack parameter of clone().
           *  @regs : pointer to the values of the general purpose registers saved into the Kernel Mode stack
           *          when switching from User Mode to Kernel Mode.
           *  @stack_size : unused parameter,always set to 0.
           *  @parent_tidptr : same as the @ptid parameter of the clone().
           *  @child_tidptr : same as the @ctid parameter of the clone().
           *  return - pid of the forked process returned on successful,otherwise returns error code.
           *           return from fork is different between parent and child!(but do_fork() always called 
           *           by kernel in parent's context)
           */
          extern long do_fork(unsigned long clone_flags, unsigned long stack_start, struct pt_regs *regs,
                              unsigned long stack_size, int __user *parent_tidptr, int __user *child_tidptr);

        the primary works that do_fork() do :
          /*  do_fork() call to copy_process() to finish process copying,and copy_process() call to copy_thread()
           *  to finish thread copying.
           */
          1>  allocates a new PID for the child by looking in the @pidmap_array bitmap.

          2>  checks the ptrace field of the parent(current->ptrace) :
              ptrace != 0 => the parent process is being traced by another process,thus do_fork() checks whether
              the debugger wants to trace the child on its own(independently of the value of the CLONE_PTRACE
              flag specified by the parent),in this case,if the child is not a kernel thread(CLONE_UNTRACED
              flag cleared),the function sets the CLONE_PTRACE flag.

          3>  invokes copy_process() to make a copy of the process descriptor.

          4>  if either the CLONE_STOPPED flag is set or the child process must be traced,that is,the PT_PTRACED flag
              is set in @p->ptrace,do_fork() set the state of the child to TASK_STOPPED and adds a pending SIGSTOP
              signal to it.
        
          5>  if the CLONE_STOPPED flag is not set,it invokes the wake_up_new_task() function,which performs the following
              operations :
                >  adjusts the scheduling parameters of both the parent and the child.

                >  if the child will run on the same CPU as the parent,and parent and child do not share the same set of
                   page tables(CLONE_VM flag cleared),it then forces the child to run before the parent by inserting it 
                   into the parent's runqueue right before the parent.(if child flushes its address space and executes a new
                   program right after the forking,this way will prevent unnecessary cost for COPY-ON-WRITE)
                   otherwise(child run on the same CPU as parent,or CLONE_VM flag set),it inserts the child in the last
                   position of the parent's runqueue.

          6>  CLONE_STOPPED == 1 => put @child -> TASK_STOPPED.

          7>  if the parent process is being traced,it stores the PID of the child in the current->ptrace_message,
              and invokes ptrace_notify().(this function was called by ptrace_event()<linux/ptrace.h>,and ptrace_event() was
              called by tracehook_report_clone_complete()<linux/tracehook.h>)
              ptrace_notify() stops the current process and sends a SIGCHLD signal to its parent,the "grandparent" of the child
              is the debugger that is tracing the parent;the SIGCHLD signal notifies the debugger that current has forked a child,
              whose PID can be retrieved by looking into the  current->ptrace_message field.
              /*  Linux 2.6,ptrace_notify() was called in ptrace_event() with ((event << 8) | SIGTRAP),
               *  a siginfo_t structure was allocated in ptrace_notify() and @info.si_signo set to SIGTRAP,ptrace_notify() then
               *  call to ptrace_stop(),which set current->last_siginfo = @info,and put current into TASK_TRACED(let debugger run).
               *  I DID NOT FIND OUT THE SIGCHLD WAS SENT TO THE @current's PARENT FROM THE SOURCE CODE!
               *  ptrace_notify() JUST CALLED WITH THE PARAMETER 100000101 BY ptrace_event()(SIGTRAP == 5,PTRACE_EVENT_FORK == 1).
               */

          8>  if the CLONE_VFORK flag is specified,it inserts the parent process in a wait queue and suspends it until the child
              releases its memory address space.

          9>  terminates by returning the PID of the child.


      The copy_process() function :
        the copy_process() function sets up the process descriptor and any other kernel data structure required for a child's
        execution.
        <kernel/fork.c>
          /*  copy_process - create a new process as a copy of the old one,but does not actually start it yet.
           *  @clone_flag:   clone flags same as do_fork().
           *  @stack_start:  same as do_fork() @stack_start.
           *  @regs:         same as do_fork() @regs.
           *  @stack_size:   same as do_fork() @stack_size,it is unused.
           *  @child_tidptr: same as do_fork() @child_tidptr.
           *  @pid:          a pointer points to an object which type is struct pid,this parameter is used
           *                 to save child's pid.
           *  @trace:        for ptrace mechanism.
           *  return -       the task_struct pointer of the child process,ERR_PTR would be returned if 
           *                 any error occurred.
           */
          static struct task_struct *copy_process(unsigned long clone_flag, unsigned long stack_start,
                                                  struct pt_regs *regs, unsigned long stack_size, 
                                                  int __user *child_tidptr, struct pid *pid, int trace);

        the primary works of copy_process() :
          1>  checks whether the flags passed in the clone_flags parameter are compatible.
              in particular,it returns an error code in the following cases :
                <  CLONE_NEWNS == 1 AND CLONE_FS == 1
                <  CLONE_THREAD == 1 AND CLONE_SIGHAND == 0
                <  CLONE_SIGHAND == 1 AND CLONE_VM == 0
        
          2>  performs any additional security checks by invoking security_task_create(),this function call to
              security_ops->task_create().

          3>  invokes dup_task_struct() to get the process descriptor for the child.
              this function performs the following actions :
                <  invokes __unlazy_fpu() on the current process to save,if necessary.
                   later,dup_task_struct() will copy these values in the thread_info structure of the child.
                <  executes the alloc_task_struct() macro to get a process descriptor for the new process,and
                   stores its address in the @tsk local variable.
                <  executes the alloc_thread_info macro to get a free memory area to store the thread_info structure
                   and the Kernel Mode stack of the new process,and saves its address in the @ti local variable.
                <  copies the contents of the @current's process descriptor into @tsk,then sets @tsk->thread_info
                   to @ti.
                <  copies the contents of the @current's thread_info descriptor into @ti,then sets @ti->task to @tsk.
                <  sets the usage counter of the new process descriptor(@tsk->usage) to 2 to specify that the process
                   descriptor is in use and that the corresponding process is alive.
                <  returns the process descriptor pointer of the new process(@tsk).

          4>  checks whether the value stored in @current->signal->rlim[RLIMIT_NPROC].rlim_cur is smaller than the
              current number of process owned by the user.
              if it is not true,an error code is returned,unless 'capable(CAP_SYS_ADMIN) OR capable(CAP_SYS_RESOURCE) OR
              @p->real_cred->user-> == INIT_USER)'.
              this function gets the current number of processes owned by the user from a per-user data structure named
              user_struct,this data structure can be found through a pointer in the user field of the process descriptor.
              (task_struct->real_cred->user->processes)

          5>  increase the usage counter of the user_struct structure(user_struct.__count) and the counter of the process
              owned by the user(user_struct.processes).

          6>  checks that the number of processes in the system does not exceed the value of the max_threads variable.
              /*  write a new value into /proc/sys/kernel/threads-max file is able to change this limit dynamically.  */

          7>  if the kernel functions implementing the execution domain and the executable format of the new process
              are included in the kernel modules,it increase their usage counters.

          8>  sets a few crucial fields related to the process state :
                <  @tsk->lock_depth = -1         (no lock hold)
                <  @tsk->did_exec = 0            (no exec did)
                <  @tsk->flags = @current->flags & ~PF_SUPERPRIV | PF_FORKNOEXEC

          9>  @tsk->pid = pid_nr(pid)  /*  @pid is allocated by alloc_pid()  */
        
          10> if @clone_flags & CLONE_PARENT_SETTID => *@parent_tidptr = @tsk->pid
              this work is did by do_fork() through put_user(Linux 2.6)

          11> initializes the list_head data structures and the spin locks included in the child's process descriptor,
              and sets up several other fields related to pending signals,timers,and time statistics.

          12> invokes copy_semundo(),copy_files(),copy_fs(),copy_sighand(),copy_signal(),copy_mm(),copy_namespace()
              to create new data structures and copy into them the values of the corresponding parent process
              data structures unless specified differently by the @clone_flags.

          13> invokes copy_thread() to initialize the Kernel Mode stack of the child process with the values contained
              in the CPU registers when the clone() system call was issued.(these values have been saved in the Kernel
              Mode stack of parent)
              However,the function forces the value 0 into the field corresponding to the eax register(child process
              fork() return or clone() system-call).
              @thread.esp = the base address of the child's Kernel Mode stack
              @thread.eip = address of ret_from_fork()  /*  an assembly function  */
              if parent makes use of an I/O Permission Bitmap,the child gets a copy of such bitmap.
              if @clone_flags & CLONE_SETTLS => child gets the TLS segment specified by the User Mode data structure
              pointed to by the @tls parameter of the clone() system-call.

          14> if @clone_flags & CLONE_CHILD_SETTID => @tsk->set_child_tid = @child_tidptr
              if @clone_flags & CLONE_CHLID_CLEARTID => @tsk->clear_child_tid = @child_tidptr
              these flags specify that the value of the variable pointed to by @child_tidptr in the User Mode address
              space of the child has to be changed,although the actual write operations will be done later.

          15> turns off the TIF_SYSCALL_TRACE flag in the thread_info structure of the child.(ret_from_fork() will not
              notify the debugging process about the system-call termination)
              (system-call tracing is controled by PTRACE_SYSCALL flag in @tsk->ptrace)

          16> @tsk->exit_signal = (@clone_flags & CLONE_THREAD) ? -1 : (@clone_flags & CSIGNAL)
              only the death of the last member of a thread group(usually,the thread group leader) causes a signal
              notifying the parent of the thread group leader.

          17> invokes sched_fork() to complete the initialization of the scheduler data structure of the new process.
              the function also sets the state of the new process to TASK_RUNNING and sets the preempt_count field
              of the thread_info structure to 1,thus disabling kernel preemption.
              Moreover,in order to keep process scheduling fair,the function shares the remaining timeslice of the 
              parent between the parent and the child.

          18> sets the field in the thread_info structure of @tsk to the number of the local CPU returned by
              smp_processor_id().

          19> initializes the fields that specify the parenthood relationships.
              if @clone_flags & (CLONE_PARENT | CLONE_THREAD) =>
                @tsk->real_parent = @tsk->parent = @current->real_parent
              else
                @tsk->real_parent = @tsk->parent = @current
              /*  ! I DID NOT FIND OUT THE CODE SETS @TSK->PARENT = @CURRENT->REAL_PAREN,
               *    DUP_TASK_STRUCT() DID @TSK = @CURRENT.
               */

          20> if the child does not need to be traced(@clone_flags & ~CLONE_PTRACE),it sets the @tsk->ptrace to 0.

          21> executes the SET_LINKS macro to insert the new process descriptor in the process list.

          22> if the child must be traced(PT_PTRACED),it sets @tsk->parent to @current->parent and inserts the child
              into the trace list of the debugger.
              this work is accomplished by tracehook_finish_clone().

          23> invokes attach_pid() to insert the PID of the new process descriptor in the @tsk->pids[PIDTYPE_PID] hash table.

          24> if the child is a thread group leader(@clone_flags & ~CLONE_THREAD) :
                <  @tsk->tgid = @tsk->pid
                <  @tsk->group_leader = @tsk
                <  invokes three times attach_pid() to insert the child in the PID hash tables of type
                   PIDTYPE_TGID,PIDTYPE_PGID,PIDTYPE_SID.
                   /*  !  I DID NOT FIND OUT CODE THAT ATTACHED PIDTYPE_TGID IN COPY_PROCESS(),BECAUSE LINUX 2.6
                    *     NO SUCH PIDTYPE HAD BEEN DEFINED.
                    */

          25> otherwise,if the child belongs to the thread group of its parent(@clone_flags & CLONE_THREAD) :
                <  @tsk->tgid = @current->tgid
                <  @tsk->group_leader = @current->group_leader
                <  invokes attach_pid() to insert the child in the PIDTYPE_TGID hash table
                   (more specifically,in the per-PID list of the @current->group_leader process)

          26> a new process has now been added to the set of processes : ++nr_threads

          27> ++total_forks to keep trace of the number of forked processes.

          28> terminated by returning the child's process descriptor pointer(@tsk).
    

    Kernel Threads :
      What is the Kernel Thread :
        the system process running only in Kernel Mode and deal with some critical tasks such flash cache,swap pages,serving
        network connections,modern operating systems delegate their functions to Kernel threads.
        such Kernel Thread are not encumbered with the unnecessary User Mode context.

      Differ to regular process :
        >  Kernel threads run only in Kernel Mode,while regular processes run alternatively in Kernel Mode and in User Mode.
        >  Because kernel threads run only in Kernel Mode,they use only linear addresses greater than PAGE_OFFSET(as above,
           PAGE_OFFSET is 0xc0000000).
           Regular processes,on the other hand,use all four gigabytes of linear addresses,in either User Mode or Kernel Mode.

      Creating a kernel thread :
        <arch/x86/kernel/process.c>
          /*  kernel_thread - this is the kernel thread create function depends on architecture.
           *  @fn:            the function to execute.
           *  @arg:           the argument of @fn.
           *  @flags:         the clone_flags,because this function call to do_fork().
           *  return -        returns what the do_fork() return.
           */
          int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags) EXPORT_SYMBOL(kernel_thread);

          /*  !  this is the artecture depending function,so when developer developing kernel at the part it is not
           *     depend on architecture should use function kthread_create() to instead invocation of kernel_thread().
           */

        <linux/kthread.h>
          /*  kthread_create - kthread helper used to create a kernel thread,the kernel thread will be stopped,
           *                   use wake_up_process() to start it.
           *  @threadfn:       the function to execute.
           *  @data:           argument of @threadfn.
           *  @namefmt:        const char pointer point to a printf-style string.
           *  @additional:     parameters used to resolve @namefmt.
           *  return -         task_struct pointer points to the kthread or ERR_PTR(-ENOMEM).
           */
          struct task_struct *kthread_create(int (*threadfn)(void *data), void *data, const char namefmt[], ...);

          !  @threadfn can either call do_exit() directly if it is a standalone thread for which noone will call
             kthread_stop(),or return when 'kthread_should_stop()' is true(means kthread_stop() has been called)
             its return value should be a zero value or a negative error code,kthread_stop() will returns what
             @threadfn returned.

          /*  kthread_run - macro used to create a kernel thread and start it.  */
          #define kthread_run(threadfn, data, namefmt, ...)

        kernel_thread() call to do_fork() with (@clone_flags | CLONE_VM | CLONE_UNTRACED),avoids duplicate page tables
        and ensure no process is able to trace the new kernel thread(even if the calling process is being traced).
        @regs to do_fork() is the initial value of CPU registers for a new thread(copy_thread() function sets the
        CPU registers with this initial value for new thread,too)
        /*  kernel_thread() builds up this stack area so that :
         *    >  ebx and edx will be set by copy_thread() to the values of the parameters @fn and @arg,respectively.
         *    >  eip wii be set to the address of the following assembly language fragment :
         *         movl  %edx,%eax
         *         pushl %edx
         *         call  %ebx
         *         pushl %eax
         *         call  do_exit
         *         #  Linux 2.6,regs.ip = (unsigned long)kernel_thread_helper,so kernel_thread_helper() will run before
         *            @fn.
         */

    Process 0 :
      ancestor of all process called "process 0",the "idle process",or,for historical reasons,the "swapper process",is a 
      kernel thread created from scratch during the initialization phase of Linux.
      the statically allocated data structured used by process 0 :
        >  a process descriptor stored in the init_task variable,which is initialized by the INIT_TASK macro.
        >  a thread_info descriptor and a Kernel Mode stack stored in the init_thread_union variable and initialized
           by the INIT_THREAD_INFO macro.
        >  the following tables,which the process descriptor points to :
             init_mm             (INIT_MM)       /*  initialization macro  */
             init_fs             (INIT_FS)
             init_files          (INIT_FILES)
             init_signals        (INIT_SIGNALS)
             init_sighand        (INIT_SIGHAND)
        >  the master kernel Page Global Directory stored in swapper_pg_dir.

      <linux/start_kernel.h> <init/main.c>
        /*  start_kernel - architecture indepening kernel starting routine.
         *  #  on x86_64 platform,this function is called by x86_64_start_kernel_reservations()<arch/x86/kernel/head64.c>.
         */
        extern asmlinkage void __init start_kernel(void);

        start_kernel() initializes all the data structures needed by the kernel,then call to rest_init(),which create 
        a kernel thread named "kernel_init" via kernel_thread(),@clone_flags = CLONE_FS | CLONE_SIGHAND.
        "kthreadadd" kernel thread also created by rest_init(),which deal with new kthread add into system,finally,
        rest_init() enable preempt without resched,next call to schedule() let the process 1 get CPU.
        if rest_init() get CPU later by scheduler assigned,it just call to cpu_idle() without preempt enabled,
        cpu_idle() is a function depends on architecture,on x86 platform,it defined in <kernel/process_64.c>.

        /*  process 0 selected by scheduler only when there are no other processes in the TASK_RUNNING state.
         *  and for SMP,there is a process 0 for each CPU.
         */

        kernel_init() defined in <init/main.c> as a static function,which initializes SMP and SMP Scheduler,does some
        basic works,then open /dev/console(sys_dup(0) called twice for set up STDOUT and STDERR),finally call to
        init_post()<init/main.c>.
        init_post() execute call to run_init_process()<init/main.c> on one of @ramdisk_execute_command,@execute_command,"/sbin/init",
        "/etc/init","/bin/init","/bin/sh",at least one of these commands must be succeed,otherwise,kernel panic.
        run_init_process() call to kernel_execve()<arch/x86/kernel/sys_i386_32.c>,which do a system call(execve) from kernel
        instead of calling sys_execve.

        !  console maybe open six "tty"s(on my computer,this is default).

    Process 1 :
      Has descripted above.
      as a result,the init kernel thread becomes a regular process(kernel_execve() called) having its own per-process
      kernel data structure.
      the process started by run_init_process() will stays alive until the system is shut down.

    Other kernel Threads :
      Linux uses many other kernel threads,some of them are created in the initialization phase and run until shutdown;
      others are created "on demand",when the kernel must execute a task that is better performed in its own execution
      context.
      >
        keventd(also called events)
          executes the functions in the @keventd_wq workqueue.
        
        kapmd
          handles the events related to the Advanced Power Management(APM).
          /*  the newer OS use APIC to manages computer power.  */

        kswapd
          reclaims memory.

        pdflush
          flushes "dirty" buffers to disk to reclaim memory.
          
        kblockd
          executes the function in the @kblockd_workqueue workqueue,essentially,it periodically activates the block
          device drivers.

        ksoftirqd
          runs the tasklets(bottom of interrupt);there is one of these kernel threads for each CPU in the system.
          /*  for example,get informations through the shell command :
           *    ps ax | grep -E '\[.*\]' | grep ksoftirqd
           */

    Destroying Processes :
      process "die" is the process terminates executing the code suppose it run.
      when this occurs,the kernel must be notified so that it can release the resources owned by the process.
      The usual way for a process to terminate is to invoke the exit() C library function.
      this function can be called explicitly by programmer or be called implicitly by compiler.

      Alternaitively,the kernel may force a whole thread group to die,this typically occurs when a process in the
      group has received a signal that it cannot handle or ignore or when an unrecoverable CPU exception has been
      raised in Kernel Mode while the kernel was running on behalf of the process.

      Process Termination :
        In Linux 2.6 there are two system calls that terminate a User Mode application :
          >  <linux/syscalls.h> /*  Kernel code tree  */
               asmlinkage long sys_exit_group(int error_code);
             <linux/unistd.h>   /*  C Library encapsulate  */
               void exit_group(int status);

             the function terminates a full thread group,that is,a whole multithreaded application.
             the main kernel function that implements this system call is called do_group_exit().
             and this system call should be called by exit().

          >  <linux/syscalls.h>
               asmlinkage long sys_exit(int error_code);
             <linux/unistd.h>
               void _exit(int status);
             <stdlib.h>
               void exit(int status);

             the _exit() system call,while terminates a single process,regardless of any other process in the
             thread group of the victim.
             the main kernel function that implements this system call is called do_exit().
             for example,this system call invoked by the pthread_exit() function of the Linux POSIX Thread Library.

        The do_group_exit() function :
          <linux/sched.h> <kernel/exit.c>
            /*  do_group_exit - terminates all threads belong to the thread group of "current".
             *  @exit_code:     the exit code will pass to do_exit(),its value maybe changed in some case.
             *                  the value is passed by exit_group() system call,or an error code supplied by the kernel.
             *                  (abnormal termination)
             *  #  before call to do_exit(),it will sets SIGNAL_GROUP_EXIT up in signal_struct.flags.(if no this flag had
             *     been setted up before)
             */    
            extern NORET_TYPE do_group_exit(int exit_code);

            The function executes the following operations :
              1>  checks whether the SIGNAL_GROUP_EXI flag of current->signal->flags has been setted up.
                  which means that the kernel already started an exit procedure for this thread group,in this case,
                    exit_code = current->signal->group_exit_code;
                    goto invoke do_exit(exit_code);
              
              2>  if signal_group_sig(current->signal) == F
                    sets SIGNAL_GROUP_EXIT flag
                    current->signal->group_exit_code = exit_code

              3>  invokes the zap_other_threads() function to kill the other processes in the thread group of current,
                  if any.
                  in order to do this,the function scans the per-PID list in the PIDTYPE_TGID hash table corresponding
                  to current->tgid,for each process in the list different from current,it sends a SIGKILL signal to it.
                  as a result,all such processes will eventually execute the do_exit() function.
                  /*  because Linux 2.6 no PIDTYPE_TGID was existed,so this function call to next_thread() to retrieve
                   *  the next thread in the thread group.
                   */

              4>  invokes the do_exit() function passing to it the process termination code(@exit_code).
                  do_exit() terminates the process and call to schedule().  /*  no return  */

        The do_exit() function :
          <linux/kernel.h> <kernel/exit.c>
            /*  do_exit - terminates the current task.
             *  @error_code:  the exit code return to User Mode or an error code reported by kernel.
             *  #  do_exit() do not return,and @tsk->exit_code will be setted up with the value @error_code /*  code  */.
             *     (schedule() will be called,so this routine no return,and process switching will be executed by scheduler.)
             */
            NORET_TYPE do_exit(long error_code /*  long code  */) ATTRIB_NORET;

            /*  The C Library function exit() is call to this function via sys_exit() system call  */

            the essential actions are executed as the following :
              1>  sets the PF_EXITING flag in the @flag field of the process descriptor to indicate that the process is being
                  eliminated.
                  if PF_EXITING have being setted up in @flags of current when do_exit() is executing,do_exit() will fix
                  recursive fault and sets up PF_EXITDONE in @flags,then put process state to TASK_UNINTERRUPTIBLE.
                  (in this case,reboot is needed)

              2>  removes,if necessary,the process descriptor from a dynamic timer queue via the del_timer_sync() function.

              3>  detaches from the process descriptor the data structures related to paging,semaphores,filesystem,open file
                  descriptors,namespaces,and I/O Permission Bitmap,respectivly,with the exit_mm(),exit_sem(),exit_files(),
                  exit_fs(),exit_namespace(),and exit_thread() functions.
                  these functions also remove each of these data structures if no other processes are sharing them.

              4>  if the kernel functions implementing the execution domain and the executable format of the process being killed
                  are included in kernel modules,the function decreases their usage counters.
              
              5>  sets the @exit_code of the process descriptor to the process termination code.

              6>  invokes the exit_notify() function to perform the following operations :
                    a>  updates the parenthood relationships of both the parent process and the child processes.
                        all child processes created by the terminating process become children of another process in the
                        same thread group,if any is running,or otherwise of the "init" process.

                    b>  checks whether the @exit_signal process descriptor field of the process being terminated is different
                        from -1,and whether the process is the last member of its thread group.
                        in this case,the function sends a signal(usually SIGCHLD) to the parent of the process being terminated
                        to notify the parent about a child's death.
        
                    c>  otherwise,if the @exit_signal field is equal to -1 or the thread group includes other processes,the
                        function sends a SIGCHLD signal to the parent only if the process is being traced.
                        (informed of the death of the lightweight process)

                    d>  if the @exit_signal process descriptor field is equal to -1 and the process is not being traced,
                        it sets the @exit_state field of the process descriptor to EXIT_DEAD,and invokes release_task()
                        to reclaim the memory of the remaining process data structures and to decrease the usage counter
                        of the process descriptor.
                        the usage counter becomes equal to 1,so that the process descriptor itself is not released right
                        away.

                    e>  otherwise,if the @exit_signal process descriptor field is not equal to -1 or the process is being
                        traced,it sets the @exit_state field to EXIT_ZOMBIE.
                
                    f>  sets the PF_DEAD flag in the @flags field of the process descriptor.

              7>  invokes the schedule() function to select a new process to run.
                  because a process in an EXIT_ZOMBIE state is ignored by the scheduler,the process stops executing right after
                  the switch_to macro in schedule() is invoked.
                  the scheduler will check the PF_DEAD flag and will decrease the usage counter in the descriptor of the
                  zombie process being replaced to denote the fact that the process is no longer alive.
              
    Process Removal :
      The Unix operating system allows a process to query the kernel to obtain the PID of its parent process or the execution
      state of any of its children.
      Unix kernels are not allowed to discard data included in a process descriptor field right after the process terminates.
      They are allowed to do so only after the parent process has issued a wait()-like system call that refers to the 
      terminated process.

      EXIT_ZOMBIE : although the process is technically dead,its descriptor must be saved until the parent process is notified.
      
      If parent dead before children,then system forcing all orphan processes to become children of the init process.
      init process issues wait()-like system call period.

      The release_task() function detaches the last data structures from the descriptor of a zombie process,it is applied on a
      zombie process in two possible ways :
        1>  by the do_exit() function if the parent is not interested in receiving signals from the child(SIGCHLD).
        2>  by the wait4() or waitpid() system calls after a signal has been sent to the parent.

        #   the default action was taken by a process for the signal SIGCHLD is ignores it,but this signal is able to be handled.

        FOR 1> : the memory reclaiming will be done by the scheduler.
        FOR 2> : the memory reclaiming will be progressed immediately.
                 /*  libc wait system call wrapper call to system call wait4(),which is defined in <kernel/exit.c>,
                  *  wait4() call to do_wait() complete the primary recycle routine.

        <linux/sched.h>
          /*  release_task - recycle the resources have holden by @p.
           *  @p:            the task to be released.
           */
          extern void release_task(struct task_struct *p);

          This function executes the following steps :
            1>  decreases the number of processes belonging to the user owner of the terminated process.
                (in user_struct)

            2>  if the process is being traced,the function removes it from the debugger's ptrace_children list and
                assigns the process back to its original parent.

            3>  invokes __exit_signal() to cancel any pending signal and to release the signal_struct descriptor of the 
                process.
                if the descriptor is no longer used by other lightweight processes,the function also removes this data
                structure.
                Moreover,the function invokes exit_itimers() to detach any POSIX interval timer from the process.

            4>  invokes __exit_sighand() to get rid of the signal handlers.

            5>  invokes __unhash_process(),which in turn :
                  a>  decreases by 1 the nr_threads variable.
                  b>  invokes detach_pid() twice to remove the process descriptor from the pidhash hash tables of type
                      PIDTYPE_PID and PIDTYPE_TGID.
                  c>  if the process is a thread group leader,invokes again detach_pid() twice to remove the process
                      descriptor from the PIDTYPE_PGID and PIDTYPE_SID hash tables.
                  d>  use the REMOVE_LINKS macro to unlink the process descriptor from the process list.

                  /*  __unhash_process() is called by __exit_signal(),and timer detaching also have done by it.  */
                  /*  Linux 2.6 have no PIDTYPE_TGID,so __unhash_proces() call to detach_pid() thrice.  */

            6>  if the process is not a thread group leader,the leader is a zombie,and the process is the last member
                of the thread group,the function sends a signal to the parent of the leader to notify it of the death
                of the process.(via do_notify_patent())

            7>  invokes the sched_exit() function to adjust the timeslice of the parent process.
                (similar to copy_process() where sched_fork() was called)

            8>  invokes pus_task_struct() to decrease the process descriptor's usage counter(via call_rcu());
                if the counter becomes zero,the function drops any remaining reference to the process :
                  a>  decreases the usage counter(@__count field) of the user_struct data structure of the user that owns
                      the process,and release that data structure if the usage counter becomes zero.
                  b>  releases the process descriptor and the memory area used to contain the thread_info descriptor and
                      the Kernel Mode stack.
                

/*  END OF CHAPTER3  */
            

Chapter 4 : Interrupts and Exceptions
    An interrupt is usually defined as an event that alters the sequence of instructions executed by a processor,
    such events correspond to electrical signals generated by hardware circuits both inside and outside the CPU chip.

    The type of interrupts :
      Synchronous interrupts : 
        produced by the CPU control unit while executing instructions and are called synchronous because the control
        unit issues them only after terminating the execution of an instruction.(int $0x80)
      
      Asynchronous interrupts :
        generated by other hardware devices at arbitrary times with respect to the CPU clock signals.

    #  Intel microprocessor manuals designate synchronous and asynchronous interrupts as exceptions and interrupts,
       respectively.

    The Role of Interrupt Signals :
      interrupt signals provide a way to divert the processor to code outside the normal flow of control.
      when an interrupt signal arrives,the CPU must stop what it is currently doing and switch to a new activity;
      it does this by saving the current value of the program counter in the Kernel Mode stack and by placing an
      address related to the interrupt type into the program counter.

      difference between interrupt handling and process switching :
        the code executed by an interrupt or by an exception handler is not a process.

      interrupt handling is one of the most sensitive tasks performed by the kernel,because it must satisfy the following
      constraints :
        >  interrupts can come anytime,when the kernel may want to finish something else it was trying to do.the kernel's
           goal is therefore to get the interrupt out of the way as soon as possible and defer as much processing as it can.
           so,an interrupt is divided into a critical urgent part that the kernel executes right away and a deferrable part
           that is left for later.(Linux,interrupt top half part and bottom half part)

        >  an interrupt maybe come when kernel is handling another interrupt.this should be allowed as much as possible,
           because it keeps the I/O devices busy.
           the interrupt handlers must be coded so that the corresponding kernel control paths can be executed in a nested
           manner.when the last kernel control path terminates,the kernel must be able to resume execution of the interrupted
           process or switch to another process if the interrupt signal has caused a rescheduling activity.

        >  although,the kernel may accept a new interrupt signal while handling a previous one,some critical regions exist
           inside the kernel code where interrupts must be disabled,such critical regions must be limited as much as possible,
           because according to the previous requirement,the kernel,and particularly the interrupt handlers,should run most of
           the time with the interrupts enabled.

    Interrupts and Exceptions :
      the Intel documentation classifies interrupts and exceptions as follows :
        Interrupts >
          Maskable interrupts :
            all Interrupt Requests(IRQs) issued by I/O devices give rise to maskable interrupts.
            a maskable interrupt can be in two states :
              masked or unmasked
                a masked interrupt is ignored by the control unit as long as it remains masked

          Nonmaskable interrupts :
            only a few critical events(such hardware failures) give rise to nonmaskable interrupts.
            nonmaskable interrupts are always recognized by the CPU.

        Exceptions >
          Processor-detected exceptions :
            Generated when the CPU detects an anomalous condition while executing an instruction.
            there are further divided into three groups,depending on the value of the eip register
            that is saved on the Kernel Mode stack when the CPU control unit raises the exception.
            
            Faults :
              can generally be corrected;once corrected,the program is allowed to restart with no loss of continuity.
              the saved value of eip is the address of the instruction that caused the fault,and hence that instruction
              can be resumed when the exception handler terminates.

            Traps :
              reported immediately following the execution of the trapping instruction;after the kernel returns control
              to the program,it is allowed to continue its execution with no loss of continuity.
              the saved value of eip is the address of the instruction that should be executed after the one that caused
              the trap.
              a trap is triggered only when there is no need to reexecute the instruction that terminated.the main use of 
              traps is for debugging purpose.the role of the interrupt signal in this case is to notify the debugger that
              a specific instruction has been executed.(such breakpoint feature)

            Aborts :
              a serious error occurred;the control unit is in trouble,and it may be unable to store in the eip register
              the precise location of the instruction causing the exection.
              aborts are used to report severe errors,such as hardware failures and invalid or inconsistent values in
              system tables.
              the interrupt signal sent by the control unit is an emergency signal used to switch control to the corresponding
              abort exception handler.this handler has no choice but to force the affected process to terminate.

        Programmed exceptions >
          occur at the request of the programmer.they are triggered by 'int' or 'int3' instructions;the 'into'(check for overflow)
          and 'bound'(check on address bound) instructions also give rise to a programmed exception when the condition they are
          checking is not true.
          programmed exceptions are handled by the control unit as traps;they are often called "software interrupts",such
          exceptions have two common uses:
            to implement system calls
            to notify a debugger of a specific event.

        #  each interrupt or exception is identified by a number ranging from 0 -- 255,Intel calls this 8-bit unsigned number
           a vector.the vectors of nonmaskable interrupts and exceptions are fixed,while those of maskable interrupts can be
           altered by programming the Interrupt Controller.

    IRQs and Interrupts :
      (PCI,Peripheral Component Interconnect)

      each hardware device controller capable of issuing interrupt requests usually has a single output line designated as
      the Interrupt ReQuest(IRQ) line(more sophisticated devices use several IRQ lines,i.e. PCI card).
      all existing IRQ lines are connected to the input pins of a hardware circuit called the Programmable Interrupt Controller.
      the actions that Programmable Interrupt Controller takes :
        1>  Monitors the IRQ lines,checking for raised signals.If two or more IRQ lines are raised,selects the one having the
            lower pin number.
        2>  If a raised signal occurs on an IRQ line :
              >  Converts the raised signal received into a corresponding vector.(IRQ vector)
              >  Stores the vector in an Interrupt Controller I/O port,thus allowing the CPU to read it via the data bus.
              >  Sends a raised signal to the processor INTR pin--that is,issues an interrupt.
              >  Waits until the CPU acknowledges the interrupt signal by writing into one of the Programmable Interrupt Controllers
                 (PIC) I/O ports;when this occurs,clears the INTR line.
        3>  Goes back to step 1.

      IRQ lines start from 0,therefore,the first IRQ line is usually denoted as IRQ0.
      Intel's default vector associated with IRQn is n+32,but such mapping can be modified by issuing suitable I/O instructions
      to the Interrupt Controller ports!(i.e. IRQ7 <=> vector 81)

      IRQ lines is selectively disabled/enabled via PIC,then that IRQ line will no longer issues interrupts.
      Disable interrupts are not lost;the PIC sends them to the CPU as soon as they are enabled again.

      !  Selectively enabling/disabling of IRQs is not the same as global masking/unmasking of maskable interrupts.
         eflags.IF flag controls that CPU whether ignores the maskable interrupt.

      !  Traditional PICs are implemented by connecting "in cascade" two 8259A-style external chips,each chip can handle up to
         eight different IRQ input lines.because the INT output line of the slave PIC is connected to the IRQ2 pin of the master
         PIC,the number of available IRQ lines is limited to 15.
         e.g.
           A{ IRQ0 ... IRQ7 }-(INT output line)-->{B.IRQ2 B{ IRQ0 IRQ1 IRQ3 ... IRQ7 }}-(INT output line)-->CPU INTR pin

    The Advanced Programmable Interrupt Controller(APIC) :
      INT output line straightforward connect to INTR pin of the CPU is only valid on a single CPU platform.
      Being able to deliver interrupts to each CPU in the system is crucial for fully exploiting the parallelism of the SMP
      architecture.

      I/O Advanced Programmable Interrupt Controller(I/O APIC) introduced from Intel Pentium III.
      80x86 microprocessors include a local APIC which has 32-bit registers,an internal clock,a local timer device,and two
      additional IRQ lines LINT0,LINT1 reserved for APIC interrupts.
      All local APICs are connected to an external I/O APIC,giving rise to a multi-APIC system.
      scheme :
        CPU0.local APIC{local IRQs : LINT0, LINT1}          CPU1.local APIC{local IRQs : LINT0, LINT1}  ...
                |                                                   |
            -------------Interrupt Controller Communication(ICC) bus----------
                                        |
                                    I/O APIC
                                        |
                                    external IRQs(IRQs from hardware)

      The I/O APIC consists of a set of 24 IRQ lines,a 24-entry Interrupt Redirection Table,programmable registers,and a 
      message unit for sending and receiving APIC messages over the APIC bus.
      interrupt priority is not related to pin number(8259A relating to pin number) :
        each entry in the Redirection Table can be individually programmed to indicate the interrupt vector and priority,
        the destination processor,and how the processor is selected.
        the information in the Redirection Table is used to translate each external IRQ signal into a message to one or
        more local APIC units via the APIC bus.

      external IRQs distributing :
        static distribution >
          the IRQ signal is delivered to the local APICs listed in the corresponding Redirection Table entry,the interrupt
          is delivered to one specific CPU,to a subset of CPUs,or to all CPUs at once(broadcast mode).

        dynamic distribution >
          the IRQ signal is delivered to the local APIC of the processor that is executing the process with the lowest
          priority.

          #  every local APIC has a programmable task priority register(TPR),which is used to compute the priority of the
             currently running process.(it is modified when process switch occurred,kernel modifies it)

          if two or more CPUs share the lowest priority,the load is distributed between them using a technique called
          "arbitration" :
            each CPU is assigned a different arbitration priority ranging from 0(lowest) to 15(highest) in the arbitration
            priority register of the local APIC.
            each time an interrupt is delivered to a CPU,its corresponding arbitration priority is automatically set to 0,
            while the arbitration priority of any other CPU is increased.when the arbitration priority register becomes
            greater than 15,it is set to the previous arbitration priority of the winning CPU increased by 1.
                previous_priority := WINNING_CPU.priority
                if ++(this.priority) > 15
                then
                    this.priority := ++previous_priority
          
          !  Pentium 4 local APIC does not have an arbitration priority register,the mechanism is hidden in the bus
             arbitration circuitry.

      the multi-APIC system allows CPUs to generate "interprocessor interrupts" :
        when a CPU wishes to send an interrupt to another CPU,it stores the interrupt vector and the identifier of the
        target's local APIC in the Interrupt Command Register(ICR) of its own local APIC,a message is then sent via the
        APIC bus to the target's local APIC,which therefore issues a corresponding interrupt to its own CPU.

        !  IPIs are actively used by Linux to exchange messages among CPUs.

      multi-APIC for uniprocessor system :
        it include an I/O APIC chip,which may be configured in two distinct ways :
          1>  as a standard 8259A-style external PIC connected to the CPU.
              the local APIC is disabled and the two LINT0 and LINT1 local IRQ lines
              are configured,respectively,as the INTR and NMI pins.
          2>  as a standard external I/O APIC.
              the local APIC is enabled,and all external interrupts are received through the I/O APIC.

    Exceptions :
      80x86 microprocessor introduced 20 different exceptions,kernel must provide a dedicated exception handler for each
      exception type.
      sometimes,a hardware error code is generated by CPU and stored in Kernel Mode stack before start the exception handler.

      About exceptions :
        0  -  Divide error (fault) : integer division by 0
        1  -  Debug (trap or fault) : Raised when the eflags.TF == 1 or when the address of an instruction or operand falls
                                      within the range of an active debug register.
        2  -  Not used : reserved for nonmaskable interrupts(NMI pin).
        3  -  Breakpoint (trap) : caused by an "int3(breakpoint)" instruction.
        4  -  Overflow (trap) : an "into" instruction has been executed while the eflags.OF == 1.
        5  -  Bounds check (fault) : a "bound" instruction is executed with the operand outside of the valid address bounds.
        6  -  Invalid opcode (fault) : CPU execution unit has detected an invalid opcode.
        7  -  Device not avaiable (fault) : an ESCAPE,MMX,or SSE/SSE2 instruction has been executed with the cr0.TS == 1.
        8  -  Double fault (abort) : raised when processor failed to handle exceptions serially.
                                     normally,when the CPU detects an exception while trying to call the handler for a prior
                                     exception,the two exceptions can be handled serially.
        9  -  Coprocessor segment overrun (abort) : problems with the external mathmeatical coprocessor.
        10 -  Invalid TSS (fault) : CPU has attempted a context switch to a process having an invalid Task State Segment.
        11 -  Segment not present (fault) : a reference was made to a segment not present in memory.
        12 -  Stack segment fault (fault) : the instruction attempted to exceed the stack segment limit,or the segment 
                                            identified by "ss" is not present in memory.
        13 -  General protection (fault) : one of the protection rules in the protected mode of the 80x86 has been violated.
        14 -  Page Fault (fault) : the addressed page is not present in memory,the corresponding Page Table entry is null,or
                                   a violation of the paging protection mechanism has occurred.
        15 -  Reserved by Intel
        16 -  Floating-point error (fault) : floating-point unit integrated into the CPU chip has signaled an error condition,
                                             such as numeric overflow or division by 0.
        17 -  Alignment check (fault) : the address of an operand is not correctly aligned.
        18 -  Machine check (abort) : a machine-check mechanism has detected a CPU or bus error.
        19 -  SIMD floating point exception (fault) : the SSE or SSE2 unit integrated in the CPU chip has signaled an error 
                                                      condition on a floating-point operation.

        #  value 20 -- 31 reserved for future development.

      Signals and Exception handlers on Linux :
        0       Divide error                divide_error()                  SIGFPE
        1       Debug                       debug()                         SIGTRAP
        2       NMI                         nmi()                           None
        3       Breakpoint                  int3()                          SIGTRAP
        4       Overflow                    overflow()                      SIGSEGV
        5       Bounds check                bounds()                        SIGSEGV
        6       Invalid opcode              invalid_op()                    SIGILL
        7       Device not available        device_not_available()          None
        8       Double fault                doublefault_fn()                None
        9       Coprocessor segment overrun coprocessor_segment_overrun()   SIGFPE
        10      Invalid TSS                 invalid_TSS()                   SIGSEGV
        11      Segment not present         segment_not_present()           SIGBUS
        12      Stack segment fault         stack_segment()                 SIGBUS
        13      General protection          general_protection()            SIGSEGV
        14      Page Fault                  page_fault()                    SIGSEGV
        15      Intel-reserved              None                            None
        16      Floating-point error        coprocessor_error()             SIGFPE
        17      Alignment check             alignment_check()               SIGBUS
        18      Machine check               machine_check()                 None
        19      SIMD floating point         simd_coprocessor_error()        SIGFPE

        #  Linux exceptions relating code is defined in <arch/x86/include/asm/traps.h>,and implemented in
           <arch/x86/kernel/traps.c>.

    Interrupt Descriptor Table :
      a system table called Interrupt Descriptor Table(IDT) associates each interrupt or exception vector with the address of 
      the corresponding interrupt or exception handler.it must be properly initialized before kernel enables interrupt.
      
      IDT is similar to GDT and LDT,each entry is a 8-byte descriptor,thus,a maximum of 256 * 8 = 2048 bytes are required to
      store IDT.

      idtr register allows the IDT to be located anywhere in memory :
        it specifies both the IDT base linear address and its limit(maximum length).
        this register must be initialized before enabling interrupts by using the "lidt" assembly language instruction.

      Three types of descriptors may included by IDT :
        1>  Task gate
              includes the TSS selector of the process that must replace the current one when an interrupt signal occurs.
              [0, 15] : RESERVED
              [16, 31] : TSS SEGMENT SELECTOR
              [32, 39] : RESERVED
              40 : 1
              41 : 0
              42 : 1
              43 : 0
              44 : 0
              [45, 46] : DPL
              47 : P
              [48, 63] : RESERVED

        2>  Interrupt gate
              includes the segment selector and the offset inside the segment of an interrupt or exception handler.
              while transferring control to the proper segment,the proprocessor clears the eflags.IF flag,thus disabling
              further maskable interrupts.
              [0, 15] : OFFSET (0--15)
              [16, 31] : SEGMENT SELECTOR
              [32, 36] : RESERVED
              37 : 0
              38 : 0
              39 : 0
              40 : 0
              41 : 1
              42 : 1
              43 : 1
              44 : 0
              [45, 46] : DPL
              47 : P
              [48, 63] : OFFSET(16--31)

        3>  Trap gate
              similar to an interrupt gate,except that while transferring control to the proper segment,the processor 
              does not modify the IF flag.
              [0, 15] : OFFSET(0--15)
              [16, 31] : SEGMENT SELECTOR
              [32, 36] : RESERVED
              37 : 0
              38 : 0
              39 : 0
              40 : 1
              41 : 1
              42 : 1
              43 : 1
              44 : 0
              [45, 46] : DPL
              47 : P
              [48, 63] : OFFSET(16--31)

        #  In particular,the value of the Type field encoded in the bits 40-43 identifies the descriptor type.
        #  Linux use interrupt gate to handle interrupts,and use trap gate to handle exceptions.

    Hardware Handling of Interrupts and Exceptions :
      /*  Suppose Kernel has been initialized,and CPU running on protected mode  */

      Before CPU executes the next instruction,it checks if an interrupt or an exception occurred while it executed the
      previous instruction.if it is,then CPU control unit does the following :
        1>  determines the vector i(0 <= i <= 255) associated with the interrupt or the exception.(Send by PIC)
        2>  reads the i_th entry in IDT through idtr.
        3>  gets the base address of the GDT from the gdtr register and looks in the GDT to read the Segment Descriptor
            identified by the selector in the IDT entry.this descriptor specifies the base address of the segment that
            includes the interrupt or exception handler.
        4>  makes sure the interrupt was issued by an authorized source.
            first
              compares the CPL which is stored in the two least significant bits of the cs register with the DPL of the
              Segment Descriptor included in the GDT.
              if cs.CPL < GDT.Segd.DPL
                    Raises a "General protection" exception     /*  interrupt handler cannot have a lower privilege than  */
                                                                /*  the program that caused the interrupt.  */
            second (for programmed exceptions)
              makes a further security check :
                compares the CPL with the DPL of the gate descriptor included in the IDT.
              if cs.CPL > IDT.entry.DPL
                    Raises a "General protection" exception
              
              !  prevent access by user applications to specific trap of interrupt gates.
        5>  checks whether a change of privilege level is taking place -- if CPL is different from the selected
            Segment Descriptor's DPL.(selected Segd,but cs.CPL has changed)
            if cs.CPL != GDT.Segd.DPL
              start using the stack that is associated with the new privilege level
              {
                a>  reads the tr register to access the TSS segment of the running process.
                b>  loads the ss and esp registers with the proper values for the stack segment and stack pointer
                    associated with the new privilege level.these values are found in the TSS.
                c>  in the new stack,it saves the privous values of ss and esp,which define the logical address of the
                    stack associated with the old privilege level.
              }
        6>  if a fault has occurred,it loads cs and eip with the logical address of the instruction that caused the 
            exception so that is can be executed agian.
        7>  saves the contents of eflags,cs,and eip in the stack.
        8>  if the exception carries a hardware error code,it saves it on the stack.
        9>  loads cs and eip,respectively,with the Segment Selector and the Offset fields of the Gate Descriptor stroed
            in the i_th entry of the IDT.these values define the logical address of the first instruction of the 
            interrupt or exception handler.

      After the interrupt or exception is processed,the corresponding handler must relinquish control to the interrupted
      process by using the "iret" instruction.
      "iret" forces the CPU control unit to does :
        1>  load the cs,eip,and eflags registers with the values saved on the stack.
            if a hardware error code has been pushed in the stack on top of the eip contents,it must be popped
            before executing iret.
        2>  check whether the CPL(cs.CPL) of the handler is equal to the value contained in the two least significant bits of
            cs(the content of cs is saved on the stack).
            if cs.CPL == stack.cs.CPL
              the interrupted process was running at the same privilege level as the handler,
              "iret" concludes execution.
            else
              load the ss and esp registers from the stack and return to the stack associated with the old privilege level.
              examine the contents of the ds,es,fs,and gs segment registers :
                if any of them contains a selector that refers to a Segment Descriptor whose DPL values is lower than CPL,
                clear the corresponding segment register.
                /*  The CPU control unit does this to forbid User Mode programs that run with a CPL equal to 3 from using
                 *  segment registers previously used by Kernel routines(with a DPL equal to 0).
                 */

    Nested Execution of Exception and Interrupt Handlers :
      Kernel control paths may be arbitrarily nested;an interrupt handler may be interrupted by another interrupt handler.
      the last instructions of a kernel control path that is taking care of an interrupt do not always put the current
      process back into User Mode :
        if the level of nesting is greater than 1,these instructions will put into execution the kernel control path that
        was interrupted last,and the CPU will continue to run in Kernel Mode.

      For kernel control path is able to nested,an interrupt handler must never block !
      all the data needed to resume a nested kernel control path is stored in the Kernel Mode stack,which is tightly bound
      to the current process.(interrupt handler shares stack to interrupted process)

      An interrupt handler may preempt both other interrupt handlers and exception handlers,Conversely,an exception handler
      never preempts an interrupt handler !

      !  THE ONLY EXCEPTION THAT CAN BE TRIGGEED IN KERNEL MODE IS "Page Fault".
         /*  Page Fault exception have vector 14
          *  the coressponding exception handler page_fault() is inserted to IDT by set_intr_gate()
          *  page_fault() as an entry included in <entry_32.S>,the primary function do_page_fault() is
          *  defined in <arch/x86/mm/fault.c>.
          */

      !  Interrupt handlers never perform operations that can induce page faults,and thus,potentially,a process switch.
      !  In contrast to exceptions,interrupts issued by I/O devices do not refer to data structures specific to the 
         current process.

      Linux interleaves kernel control paths for two major reasons :
        1>  to improve the throughput of programmable interrupt controllers and device controllers.
            /*  device controller issues a signal on an IRQ line,
             *  PIC transforms it into an external interrupt,
             *  both PIC and device controller wait for CPU send an acknowledgment.
             *  interleaves kernel control paths can let kernel send the acknowledgment while it is 
             *  handling the previous interrupt.
             */
        2>  to implement an interrupt model without priority levels.
            /*  no longer some predefined levels between interrupts are necessary.  */

      !  On multiprocessor systems,several kernel control paths may execute concurrently,moreover,a kernel control
         path associated with an exception may start executing on a CPU and, due to a process switch,migrate to
         another CPU.

    Initializing the Interrupt Descriptor Table :
      before kernel enables interrupts,it must load the initial address of IDT into idtr register and initializes all
      entries of IDT.
      "int" instruction allows a User Mode process to issue an interrupt signal that has an arbitrary vector ranging
      from 0 to 255,kernel must prevent illegal interrupts and exceptions simulated by User Mode process via "int".
      /*  set DPL of gate descriptor up to 0,cs.CPL == 3 for User Mode process.  */

      !  in a few cases,a User Mode process must be able to issue a programmed exception,for this,must set the
         corresponding gate descriptor's DPL to 3.(that is as high as possible)

      Interrupt,Trap,and System Gates :
        Linux uses a slightly different breakdown and terminology from Intel when classifying the interrupt
        descriptors included in the Interrupt Descriptor Table :
          >  Interrupt gate
               an Intel interrupt gate that cannot be accessed by a User Mode process.(DPL = 0)
               all Linux interrupt handlers are activated by means of Interrupt gates,and all are
               restricted to Kernel Mode.

          >  System gate
               an Intel trap gate that can be accessed by a User Mode process.(DPL = 3)
               the three Linux exception handlers associated with the vectors 4,5,128 are activated by means of
               System gates,so the three assembly language instructions "into","bound",and "int $0x80" can be
               issued in User Mode.

          >  System interrupt gate
               an Intel interrupt gate that can be accessed by a User Mode process.(DPL = 3)
               the exception handler associated with the vector 3 is activated by means of a System interrupt gate,
               so the assembly language instruction "int3" can be issued in User Mode.

          >  Trap gate
               an Intel trap gate that cannot be accessed by a User Mode process.(DPL = 0)
               most Linux exception handlers are activated by means of Trap gates.

          >  Task gate
               an Intel task gate that cannot be accessed by a User Mode process.(DPL = 0)
               the Linux handler for the "Double fault" exception is activated by means of a Task gate.

      Architecture-dependent functions(just a little) were used to manipulate IDT :
        the gate entity is defined in <arch/x86/include/asm/desc_defs.h> !
        <arch/x86/include/asm/desc.h>
          /*  set_intr_gate - insert an Interrupt gate entry into IDT.
           *  @n:             index of IDT.
           *  @addr:          address of interrupt handler.
           */
          static inline void set_intr_gate(unsigned int n, void *addr);

          /*  set_system_trap_gate - insert a System trap gate entry into IDT.
           *  @n:                    index of IDT.
           *  @addr:                 trap handler address.
           */
          static inline void set_system_trap_gate(unsigned int n, void *addr);

          /*  set_system_intr_gate - insert a System interrupt gate entry into IDT.
          static inline void set_system_intr_gate(unsigned int n, void *addr);

          /*  set_trap_gate - insert a Trap gate entry into IDT.
          static inline void set_trap_gate(unsigned int n, void *addr);

          /*  set_task_gate - insert a Task gate entry into IDT.
           *  @n:             index of IDT.
           *  @gdt_entry:     the index of TSS which is contained in GDT and the function
           *                  to be activated is inside this TSS.
           *                  TSSD can appear only in GDT,when this gate is called,use the
           *                  index to pick the corresponding TSSD from GDT,and access to the TSS.
           *                  the Segment Selector inside Task gate stores the index value.
           *  #  I FOUND OUT Linux 2.6 ONLY USE TASK GATE TO DEAL WITH "Double fault" EXCEPTION,
           *     AND "GDT_ENTRY_DOUBLEFAULT_TSS" IS DEFINED WITH VALUE 31.
           *     OBJECT "doublefault_tss" IS DEFINED IN <arch/x86/kernel/doublefault_32.c>,IP
           *     REGISTER IS POINT TO "doublefault_fn"(DEFINED IN SAME FILE).
           *     THE GATE IS PLACED IN IDT WITH INDEX 8,AND THE SEGMENT DESCRIPTOR IS PLACED IN THE
           *     32nd ENTRY IN GDT.
           */
          static inline void set_task_gate(unsigned int n, unsigned int gdt_entry);

        #  all gate can be accessed only in Kernel Mode is attached __KERNEL_CS as the 
           Segment Selector.

      Preliminary Initialization of the IDT :
        IDT is initialized and used by the BIOS routines while the computer still operates in Real Mode.
        However,the IDT is moved to another area of RAM and initialized a second time after Linux take over
        control.(because Linux does not use any BIOS routine)

        IDT is stored in the @idt_table table,which includes 256 entries,the 6-byte @idt_descr variable stores
        both the size of the IDT and its address and is used in the system initialization phase when the kernel
        sets up the idtr register with the lidt assembly language instruction.

        assembly language function setup_idt() is used to initializes @idt_table,the 256 entries are filled with
        ignore_int and the corresponding interrupt gate.
        which function is defined in <arch/x86/kernel/head_32.S> :
          setup_idt:
                lea ignore_int,%edx                 #  load address of ignore_int into edx
                movl $(__KERNEL_CS << 16),%eax      #  kernel code segment
                movw %dx,%ax                        #  move the content in dx to ax
                movw $0x8E00,%dx                    #  interrupt gate - dpl = 0,present

                lea idt_table,%edi                  #  load address of idt_table into edi
                mov $256,%ecx                       #  set up counter

          rp_sidt:                                  #  repeat setup idt
                movl %eax,(%edi)                    #  the 4-byte content from 0--31
                movl %edx,4(%edi)                   #  the 4-byte content from 32--63
                addl $8,%edi                        #  iterating
                dec %ecx
                jne rp_sidt

          ...

          ignore_int:
                cld
                iret 

        ignore_int() :
          a null handler.
          #ifdef  CONFIG_PRINTK
            ignore_int saves the content of some registers in the stack ->
            invokes printk() to print an "Unknown interrupt" system message ->
            restores the register contents from the stack ->
            executes an "iret" instruction to restart the interrupted program.

          !  the ignore_int() handler should never be executed.if it is executed,that denotes either a 
             hardware problem(such an I/O devices is issuing unforeseen interrupts) or a kernel problem
             (such an interrupt or exception is not being handled properly).

        kernel replaces some of the null handlers with meaningful trap and interrupt handlers at the second
        phase to set up IDT.

        !  once this is done,the IDT includes a specialized interrupt,trap,or system gate for each different
           exception issued by the control unit and for each IRQ recognized by the interrupt controller.

    Exception Handling :
      most exceptions issued by the CPU are interpreted by Linux as error conditions.
      for example,CPU raises a "Divide error" exception,and the corresponding exception handler sends
      a SIGFPE signal to the process caused this exception,then it takes the necessary steps to recover or abort.

      Linux exploits CPU exceptions to manage hardware resources more efficiently :
        >  Linux use "Device not available" exception and cr0.TS flag give rise to load the floating point registers
           of the CPU with new values.
        >  Linux use "Page Fault" exception to defer allocating new page frames to the process until the last
           possible moment.(that is,schduler will take a place in the case)
           !  "Page Fault" exception handler is very complex,because this exception may or may not denote an
              error condition.

      Standard structure of steps that exception handler takes :
        1>  save the contents of most registers in the Kernel Mode stack(coded in assembly language)
        2>  handle the exception by means of a high-level C function.
        3>  exit from the handler by means of the ret_from_exception() function.

      trap_init() :
        Linux use this function to setup IDT entries that refer to nonmaskable interrupts and exceptions.

        <arch/x86/kernel/traps.c>
          /*  trap_init -  the x86 architecture specific trap initialization routine.
           *  #  this function is call to set_trap_gate(),set_intr_gate(),set_system_gate(),
           *     set_system_intr_gate(),set_task_gate() .etc to accomplishes trap initialization.
           *     finally,call to x86_init_irqs.trap_init() to finishes the architecture dependent
           *     trap initializing works.
           */
          void __init trap_init(void);

          !  "Double fault" exception handler is set up by cpu_init() function and which is called by trap_init().
             the "Double fault" exception is handled by means of a task gate instead of a trap or system gate,
             because it denotes a serious kernel misbehavior.thus,the exception handler that tries to print out the
             register values does not trust the current value of the esp register.
             when this exception occurred,CPU executes the doublefault_fn() exception handler on its own private stack
             (specified by TSS)

      Saving the Registers for the Exception Handler : 
        each exception handler starts with the following assembly language instructions :
          @handler_name:
                pushl $0                    #  pad stack with null value,if control unit is not insert a hardware error code
                                            #  on the stack automatically when the exception occurs.
                pushl $do_handler_name      #  address of high-level C function
                jmp error_code              #  the assembly code of "error_code" label will call to the function "do_##name()".

        /*  defined in <arch/x86/kernel/entry_32.S>  */
        /*  entry definition in <entry_64.S> is diff to <entry_32.S>,
         *  the assembly macro "errorentry"/"paranoidentry"/"zeroentry" are used to define 
         *  exception entry points.
         */
        
        the assembly language fragment labeled as "error_code" is the same for all exception handlers except the one for 
        the "Device not available" exception.(Linux 2.6,it is same as others)
        the code performs the following steps : (these are the default actions in Linux 2.6 for all exception handlers)
          1>  saves the registers that might be used by the high-level C function on the stack.
          2>  execute "cld".
          3>  copied the hardware error code saved in the stack at location esp+36 in edx.
              stores the value -1 in the same stack location,this value is used to separate 0x80 exceptions
              from other exceptions.
          4>  loads edi with the address of the high-level "do_##name()" C function saved in the stack at the
              location esp+32,writes the contents of es in that stack location.
          5>  loads in the eax register the current top location of the Kernel Mode stack.
              this address identifies the memory cell containing the last register value saved in step1.
          6>  loads the user data Segment Selector into the ds and es registers.
          7>  invokes the high-level C function whose address is now stored in edi.

        #  the invoked function receives its arguments from the eax and edx registers rather than from the stack.
        #  all "do_##name()" high-level handlers are call to do_trap() function,which is defined in 
           <arch/x86/kernel/traps.c>

      Entering and Leaving the Exception Handler :
        Kernel enters an exception handler through IDT,and then the correpsonding assembly function
        is going to invoke "do_##name()" the high-level C function,it finally call to do_trap() function.

        do_trap() :
          <arch/x86/kernel/traps.c>
            /*  do_trap - generic exception handling interface.
             *  @trapnr:  number of excpetion.
             *  @signr:   the signal must send to the process which cause the exception.
             *  @str:     information.
             *  @regs:    registers contents.
             *  @error_code:  error code.
             *  @info:    signal information.
             *  #  @regs is passed by the exception handler which is in assembly language.
             *  #  DO_ERROR() and DO_ERROR_INFO() macros are used to define "do_##name()" the
             *     high-level C functions.
             */
            static void __kprobes do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
                                          long error_code, siginfo_t *info);
  
          do_trap() will does :
            1>  retrieve current process's task_struct object.
            2>  @tsk->thread.error_code = @error_code;
                @tsk->thread.trap_no = @trapnr;
                if !@info
                      force_sig(@signr, @tsk);
                else
                      force_sig_info(@signr, @info, @tsk);
                the signal will be handled either in User Mode by the process's own signal handler or
                in Kernel Mode(default action).
            3>  it is always checks whether the exception occurred in User Mode or in Kernel Mode.
                in the Kernel Mode,have to checks whether it was due to an invalid argument passed to
                a system call.(different system call deal with invalid argument is difference)
                any other exception raised in Kernel Mode is due to a kernel bug,this denote kernel
                is misbehaving,then call to die() function to prevent data corruption and prints the 
                contents of all CPU registers on the console(this dump is called kernel oops),
                the current process have to be terminated by calling do_exit().
  
          #  when the C function that implements the exception handling terminates,the code performs a
             "jmp" instruction to the ret_from_exception() function.(return to the interrupted control path)
               
    Interrupt Handling :
      Interrupt handling is different to Exception Handling,so it would make no sense to send a Unix signal to the 
      current process.

      Interrupt handling depends on the type of interrupt :
        I/O interrupts :
          An I/O device requires attenton;the corresponding interrupt handler must query the device to determine the
          proper course of action.

        Timer interrupts :
          some timer,either a local APIC timer or an external timer,has issued an interrupt;this kind of interrupt
          tells the kernel that a fixed-time interval has elapsed.

        Interprocessor interrupts :
          A CPU issued an interrupt to another CPU of a multiprocessor system.

      I/O Interrupt Handling :
        in general,an I/O interrupt handler must be flexible enough to service several devices at the same time.
        some devices might share a vector and a IRQ line.(vector 43 for USB port and Sound Card)

        Interrupt handler flexibility is achieved in two distinct ways :
          1>  IRQ sharing
                the interrupt handler executes several interrupt service routines(ISRs).
                each ISR is a function related to a single device sharing the IRQ line.
                it is not possible to know in advance which particular device issued the IRQ,each ISR is
                executed to verify whether its device needs attention;if so,the ISR performs all the operations
                that need to be executed when the device raises an interrupt.

          2>  IRQ dynamica allocation
                An IRQ line is associated with a device driver at the last possible moment.
                that is IRQ line is allocated on demand(the time need to access the device).
                in this way,the same IRQ vector may be used by several hardware devices even if they cannot
                share the IRQ line;of course,the hardware devices cannot be used at the same time.

        not all actions to be performed when an interrupt occurs have the same urgency.
        when an interrupt is handling,the signals on the corresponding IRQ line are temporarily ignored,
        the interrupted process stay in the TASK_RUNNING state,a system freeze is possible to occur.

        Long noncritical operations should be deferred,blocking procedure must to be prevented.

        Actions for interrupt on Linux:
          1>  Critical
                actions such as acknowledging an interrupt to the PIC,reprogramming the PIC or the device controller,
                or updating data structures accessed by both the device and the processor.
                such actions must be performed as soon as possible(these can be executed quickly and are critical).
                critical actions are executed within the interrupt handler immediately,with maskable interrupts disabled.

          2>  Noncritical
                actions such as updating data structures that are accessed only by the processor.
                these actions can also finish quickly,so they are executed by the interrupt handler immediately,with
                the interrupts enabled.

          3>  Noncritical deferrable
                actions such as copying a buffer's contents into the address space of a process.
                these may be delayed for a long time interval without affecting the kernel operations;
                the interested process will just keep waiting for the data.

        Four basic actions all I/O interrupt handlers do :
          1>  save the IRQ value and the register's contents on the Kernel Mode stack.
          2>  send an acknowledgment to the PIC that is servicing the IRQ line,thus allowing it to issue
              further interrupts.
          3>  execute the interrupt service routines(ISRs) associated with all the devices that share the IRQ.
          4>  terminate by jumping to the ret_from_intr() address. 

      Interrupt vectors :
        physical IRQs may be assigned any vector in the range 32-238.however,Linux uses vector 128 to implement
        system calls.

        #  IBM-compatible PC architecture requires that some devices be statically connected to specific IRQ lines.
           in particular :
             interval timer device --> IRQ 0 line
             slave 8259A PIC --> IRQ 2 line
             external mathematical coprocessor --> IRQ 13 line
             an I/O device can be connected to a limited number of IRQ lines.

        interrupt vectors in Linux :
          vector range                      use
          [0, 19](0x0 - 0x13)               nonmaskable interrupts and exceptions
          [20, 31](0x14 - 0x1f)             intel-reserved
          [32, 127](0x20 - 0x7f)            external interrupts (IRQs)
          [128](0x80)                       programmed exception for system calls
          [129, 238](0x81 - 0xee)           external interrupts (IRQs)
          [239](0Xef)                       local APIC timer interrupt
          [240](0xf0)                       local APIC thermal interrupt
          [241, 250](0xf1 - 0xfa)           reserved by Linux for future use
          [251, 253](0xfb - 0xfd)           interprocessor interrupts
          [254](0xfe)                       local APIC error interrupt
          [255](0xff)                       local APIC spurious interrupt

        three ways to select a line for an IRQ-configurable device :
          1>  by setting hardware jumpers
          2>  by a utility program shipped with the device and executed when installing it.
              such a program may either ask the user to select an available IRQ number or probe the
              system to determine an available number by itself.
          3>  by a hardware protocol executed at system startup.
              peripheral devices declare which interrupt lines they are ready to use;the final values
              are then negotiated to reduce conflicts as much as possible.
              once this is done,each interrupt handler can read the assigned IRQ by using a function
              that accesses some I/O ports of the device.

        !  kernel must discover which I/O device corresponding to the IRQ number before enabling interrupts.

      IRQ data structures :
        <linux/irq.h>
          typedef (*irq_flow_handler_t)(unsigned int irq, struct irq_desc *desc);

          /*  struct irq_desc - the irq descriptor structure represents an interrupt.
           *  @irq:             interrupt number for this descriptor.
           *  @handler_irq:     high-level irq-events handler [if NULL, __do_IRQ()].
           *  @action:          the irq action chain.
           *  @status:          status information.
           */
          struct irq_desc {
                unsigned int irq;
                ...
                irq_flow_handler_t handler_irq;
                ...
                struct irqaction *action;
                unsigned int status;
                ...                
          };
        <kernel/irq/handle.c>
          /*  irq_desc - the irq_desc structure array used to establishes relationship
           *             between IRQ vector and IRQ descriptor.
           *  @NR_IRQS:  number of IRQs in the system,architecture dependent,
           *             if undefined,then it is defined with 64.
           *  #  the @handler_irq field for per-irq_desc is setup to "handle_bad_irq",
           *     and @status field is setup to "IRQ_DISABLED".
           */
          struct irq_desc irq_desc[NR_IRQS];

          /*  Linux 2.6 NO "irq_desc_t" DEFINITION.  */

        An interrupt is "unexpected" if it is not handled by the kernel,that is,either if there is
        no ISR associated with the IRQ line,or if no ISR associated with the line recognizes the interrupt
        as raised by its own hardware device.
        in this case,kernel checks the IRQ line number which raised "unexpected" interrupt and disable the line.
        (prevent a faulty hardware device keeps raising an interrupt)
        /*  because the IRQ line can be shared among several devices,the kernel does not disable the line as soon as
         *  it detects a single unhandled interrupt,rather,the kernel stores in the @irq_count and @irqs_unhandled fields
         *  of the "irq_desc" structure the total number of interrupts and the number of unexpected interrupts,respectively.
         *  #  kernel checks these fields,if the value exceed the threshold,kernel disable the IRQ line.
         */

        sample graph :
          +------+  +-----------+
          |(dev1)|  |  (dev2)   |
          | IRQ0 |  | IRQ0 IRQ1 |  (external devices' IRQ lines,physical circuit)
          +------+  +-----------+
             |            |
             |      =============
             |         |
             V         V
           +---+     +---+
           |PIC|     |PIC|         (hardware Programmable Interrupt Controller)
           +---+     +---+
             |         |           (INT output line)
          ====I/O APIC======
                 |
                 V
          ====ICC===========
            |         |
          +------+  +------+
          | CPU1 |  | CPU2 |
          |lAPIC |  |lAPIC |    (local APIC)
          +------+  +------+
            |         |
          ==================
            |         |   
            |         |   
            V         V
           +--+      +--+
           |37|      |82|       (dev1.IRQ0 <=> vector 82, dev2.IRQ0 <=> vector 82, dev2.IRQ1 <=> vector 37)
           +--+      +--+
            |         |
          +-------+  +-------+
          |irqd_37|  |irqd_82|
          +-------+  +-------+
            |         |
          +----+      |
          |ISR9|     =========  (ISR9 handles vector = 37 from dev2)
          +----+      |     |
                      |     |
                   +----+  +----+
                   |ISR4|  |IRS7|  (ISR4 handles vector = 82 from dev1, ISR7 handles vector = 82 from dev2)
                   +----+  +----+

          #  the mapping between IRQ line number and the vector is able to be altered via issue an
             I/O instruction to PIC.
          #  the vector assignable in the scope [32, 238] is because some nonmaskable interrupts and exceptions
             have the fixed vector.
        
        Status of an IRQ line :
          IRQ_INPROGRESS                a handler for the IRQ is being executed
          IRQ_DISABLED                  the IRQ line has been deliberately disbled by a device driver
          IRQ_PENDING                   an IRQ has occurred on the line,its occurrence has been acknowledged to the PIC,
                                        but it has not yet been serviced by the kernel
          IRQ_REPLAY                    the IRQ line has been disabled but the previous IRQ occurrence has not yet been
                                        acknowledged to the PIC
          IRQ_AUTODETECT                the kernel is using the IRQ line while performing a hardware device probe
          IRQ_WAITING                   the kernel is using the IRQ line while performing a hardware device probe,
                                        moreover,the corresponding interrupt has not been raised
          IRQ_LEVEL                     not used on the 80x86 architecture(IRQ level triggered)
          IRQ_MASKED                    not used(IRQ masked - should not be seen again)
          IRQ_PER_CPU                   not used on the 80x86 architectur(IRQ is per-cpu)
                                        /*  if an interrupt request is per CPU type,then disable it is only
                                         *  disabled on local cpu,another were not affected.
                                         */

          @depth field and IRQ_DISABLED specify whether the IRQ line is enabled or disabled.
          <linux/interrupt.h>
            /*  disable_irq - disable IRQ line,wait until all interrupt handlers for IRQ_irq
             *                have completed before running.
             *  @irq:         the IRQ line number.
             */
            extern void disable_irq(unsigned int irq);

            /*  disable_irq_nosync - asynchronous version.  */
            extern void disable_irq_nosync(unsigned int irq);

            #  these two functions are let @depth increase,if @depth == 0,the functions disable theIRQ line
               and set its IRQ_DISABLED flag(this is give rise before @depth increase). 

            /*  enable_irq - enable a IRQ line.
             *  #  this function decrease @depth,if @depth == 0,then enable the IRQ line and clean IRQ_DISABLED flag.
             */
            extern void enable_irq(unsigned int irq);

          @status of each element in irq_desc array is initialized to IRQ_DISABLED during system initializing,this is
          happens in "init_IRQ()" it is defined in <arch/x86/kernel/irqinit.c>(this function also initializes IDT).

        Linux supports several PIC,so kernel designer have used object-oriented approach to encapsulate PIC object.
        In older kernel,it is named "hw_interrupt_type",but in Linux 2.6,it is named "irq_chip".
        <linux/irq.h>
          /*  struct irq_chip - hardware interrup chip descriptor.
           *  @name:            name for /proc/interrupts .
           *  @startup:         start up the interrupt.
           *  @shutdown:        shut down the interrupt.
           *  @enable:          enable the interrupt.
           *  @disable:         disable the interrupt.
           *  @ack:             start of a new interrupt.
           *  ...
           *  @typename:        obsoleted by name,kept as migration helper. 
           */
          struct irq_chip {
                const char *name;
                unsigned int (*startup)(unsigned int irq);
                void (*shutdown)(unsigned int irq);
                void (*enable)(unsigned int irq);
                void (*disable)(unsigned int irq);
                void (*ack)(unsigned int irq);
                ...
                const char *typename;
          };  /*  each irq_desc.chip is points to a irq_chip object.  */
          /*  if @startup is nullptr,then @startup = @enable,
           *  if @shutdown is nullptr,then @shutdown = @disable
           */

        Multiple devices can share a single IRQ,so kernel maintains "irqaction" structure for a
        specific hardware device and a specific interrupt.
        <linux/interrupt.h>
          typedef irqreturn_t (*irq_handler_t)(int, void *);

          /*  struct irqaction - per interrupt action descriptor.
           *  @handler:          interrupt handler function.
           *  @flags:            flags(IRQF_*).
           *  @name:             name of the device.
           *  @dev_id:           cookie to identify the device.
           *  @next:             next irqaction for shared interrupts.
           *  @irq:              interrupt number.
           *  @dir:              pointer to the /proc/irq/NN/name entry.
           *  @thread_fn:        interrupt handler function for threaded interrupts.
           *  @thread:           thread pointer for threaded interrupts.
           *  @thread_flags:     flags related to @thread.
           *  #  Linux use this descriptor to represents an ISR.
           *     the devices sharing the single IRQ line,every interrupt handler
           *     for the device is encapsulated in an "irqaction" object.
           */
          struct irqaction {
                irq_handler_t handler;
                unsigned long flags;
                const char *name;
                void *dev_id;
                struct irqaction *next;
                int irq;
                struct proc_dir_entry *dir;
                irq_handler_t thread_fn;
                struct task_struct *thread;
                unsigned long thread_flags;
          };

          some values for irqaction.flags :
            IRQF_DISABLED           keep irqs disabled when calling the action handler
            IRQF_SHARED             allow sharing the irq among several devices
            IRQF_SAMPLE_RANDOM      irq is used to feed the random generator

        Kernel use a structure named "irq_cpustat_t" to keep track of what each CPU is currently doing,
        and an array named "irq_stat" save "irq_cpustat_t" structure object the size is consistent to NR_CPUS.
        <arch/x86/include/asm/hardirq.h>
          /*  struct irq_cpustat_t - structure used by kernel to keep track of CPU in the system.
           *  @__softirq_pending:    whether there is a softirq is pending.
           *  @__nmi_count:          number of occurrences of NMI interrupts.
           *  @irq0_irqs:            nmi watchdog.
           *  @apci_timer_irqs:      number of occurrences of local APIC timer interrupts.
           *  ...
           */
          typedef struct {
                unsigned int __softirq_pending;  /*  per-CPU 32-bit mask describing the pending softirqs  */
                unsigned int __nmi_count;
                unsigned int irq0_irqs;
                unsigned int apic_timer_irqs;  /*  #ifdef CONFIG_X86_LOCAL_APIC  */
                ...
          } ____cacheline_aligned irq_cpustat_t;

        <kernel/softirq.c>  <linux/irq_cpustat.h>
        #ifndef __ARCH_IRQ_STAT
          /*  irq_stat - irq_cpustat_t array for per-cpu.  */
          irq_cpustat_t irq_stat[NR_CPUS] ____cacheline_aligned;
          EXPORT_SYMBOL(irq_stat);
        #endif

      IRQ distribution in multiprocessor systems :
        when a hardware device raises an IRQ signal,the multi-APIC system selects one of the CPUs
        and delivers the signal to the corresponding local APIC,which in turn interrupts its CPU,no other
        CPUs are notified of the event.

        during system bootstrap,the booting CPU executes the "static void __init setup_IO_APIC_irqs(void)"
        which is defined in <arch/x86/kernel/apic/io_apic.c>,this function initializes the I/O APIC chip,
        the Interrupt Redirection Table of the chip is filled(24-entry).
        
        during system bootstrap,after I/O APIC chip had initialized,all CPUs execute the
        "void __cpu_init setup_local_APIC(void)" which is defined in <arch/x86/kernel/apic/apic.c>,
        this function initializes the local APICs.the task priority register(TPR) of each chip is 
        initialized to a fixed value,the CPU is willing to handle every kind of IRQ signal,regardless of 
        its priority,and the linux kernel never modifies the value after its initialization.

        #  all of these works are done by the hardware.
           but in some cases,the IRQs is distributed in a unfair way to the CPUs by hardware.
           Linux 2.6 makes use of a special kernel thread called kirqd to correct,if necessary,the 
           automatic assignment of IRQs to CPUs.

        <arch/x86/kernel/apic/io_apic.c>
          /*  set_ioapic_affinity_irq - set affinity for a specific irq.
           *  @irq:                     irq vector.
           *  @mask:                    the CPUs that can receive the IRQ.
           *  #  system administrator is able to change the irq affinity through write a 
           *     new CPU bitmap mask into the "/proc/irq/@vector/smp_affinity" file.
           */
          static int set_ioapic_affinity_irq(unsigned int irq, const struct cpumask *mask);

        #  kirqd periodically executes the "do_irq_balance()" function,which keep track of the
           number of interrupt occurrences received by every CPU.
           if imbalance has detected,then "move" IRQ from one CPU to another least loaded CPU,
           or rotates all IRQs among all existing CPUs.

      Multiple Kernel Mode stacks :
        task_struct.stack -> thread_union := { thread_info, stack }(4kB or 8kB)
        task_struct.thread_info => points to the thread_info object associated to current task_struct

        if the size of the thread_union union is 8kB,the Kernel Mode stack of the current process
        is used for every type of kernel control path :
          exceptions, interrupts, deferrable functions

        if the size of the thread_union union is 4kB,the kernel makes use of three types of Kernel Mode
        stacks :
          >  the exception stack is used when handling exceptions(including system call).
             different process has different thread_union,and the stack is contained in it,thus
             kernel makes use of a different exception stack for each process in the system.

          >  the hard IRQ stack is used when handling interrupts.
             there is one hard IRQ stack for each CPU in the system,and each stack is contained in a
             single page frame.

          >  the soft IRQ stack is used when handling deferrable functions.
             there is one soft IRQ stack for each CPU in the system,and each stack is contained in a 
             single page frame.

        #  the per-cpu data @hardirq_stack is type of "union irq_ctx",the union is defined in
           <arch/x86/kernel/irq_32.c>
             union irq_ctx {
                    struct thread_info tinfo;               /*  higher address  */
                    u32 stack[THREAD_SIZE / sizeof(u32)];   /*  lower address  */
             } __attribute__((aligned(PAGE_SIZE)));
           and per-cpu data @softirq_stack also is type of "union irq_ctx".
           @hardirq_stack is used for hard IRQ,and @softirq is used for deferrable functions.
           stack grows towards lower address.

      Saving the registers for the interrupt handler :
        saving registers is the first task of the interrupt handler.
        the address of the interrupt handler for IRQn is initially stored in the @interrupt[n] entry and
        then copied into the interrupt gate included in the proper IDT entry.

        the @interrupt array is build through assembly language instructions and which is defined in
        <arch/x86/kernel/entry_32.S>.
        the array contains NR_IRQS elements(Linux 2.6,NR_VECTORS = 256,FIRST_EXTERNAL_VECTOR = 32,so NR_IRQS = 256 - 32).
        the element at index @n in the array stores the address of the following two assembly language instructions :
          pushl $n-256
          jmp common_interrupt

          /*  Linux 2.6 constructs such entries via a more complex way :
           *  <linux/linkage.h>
           *    #define ENTRY(name)  \
           *        .global name;
           *        ALIGN;
           *        name:
           *
           *    #ifndef END
           *    #define END(name)    \
           *        .size name, .-name
           *    #endif
           *
           *    ----------------------
           *
           *  <arch/x86/kernel/entry_32.S>
           *    .section .init.rodata,"a"
           *    ENTRY(interrupt)
           *    .text
           *        ...             #  now in .init.text subsection
           *    ENTRY(irq_entries_start)
           *      ...
           *      vector=FIRST_EXTERNAL_VECTOR      #  vector = FIRST_EXTERNAL_VECTOR = 32
           *      .rept (NR_VECTORS-FIRST_EXTERNAL_VECTOR+6)/7  #  int(result) = 32
           *        .balign 32      #  position counter forwards to next address @p | 32
           *        .rept 7         #  repeat sequence of code line beween next ".endr" 7 times
           *          ...
           *    1:    pushl $(~vector+0x80)     #  vector number
           *          .if ((vector-FIRST_EXTERNAL_VECTOR)%7) <> 6   #  if (vector-32)%7 != 6 ?
           *            jmp 2f      #  '2' is a label,and suffix 'f' means 'f'orward search '2'
           *                        #  immediate value 2 => $2
           *                        #  address 2 => 0x02
           *                        #  near jump
           *          .endif
           *          .previous     #  continues processing of the previous section.
           *            .long 1b    #  'b'ackward search '1','1' is a label.
           *                        #  now in .init.rodata subsection
           *          .text         #  .text subsection
           *            vector=vector+1     #  update vector for next entry
           *                                #  now in .init.text subsection
           *          ...
           *        .endr
           *    2:      jmp common_interrupt    #  jump to common_interrupt program
           *      .endr
           *   END(irq_entries_start)
           *   .previous            #  now in .init.text subsection
           *   END(interrupt)
           *   #  pack 7 stubs into a single 32-byte chunk,and the pointer points to the handler
           *      for the chunk is attached right after it("jmp common_interrupt").
           *   #  .init.rodata has an array :
           *        [index0] := label 1  (an address of an opcode)  #  now @vector = 32
           *        [index1] := label 1  #  the opcode been compiled and @vector is updated.    
           *        ...                  #  the total number of elements in this array is 224.
           *                             #  elements in vector range 0--31 has been initialized by
           *                             #  trap_init().
           *   #  .init.text :
           *        irq_entries_start:  #  a symbol
           *          pushl $(~vector+0x80)     #  the first stub in the 32-byte chunk
           *          jmp 2f                    #  the first stub in the 32-byte chunk
           *          ...                       #  repeat 5 times
           *          pushl $(~vector+0x80)     #  the seventh stub in the 32-byte chunk
           *          jmp common_interrupt      #  the secenth stub in the 32-byte chunk
           *                                    #  ! AND 2f IS THE ADDRESS OF THIS OPCODE
           *        #  all 224 stubs.the exception special stubs had been initialized by trap_init()
           #        #  only external interrupts are placed there.
           */

        Linux use positive number to identify a system call,so the negative number is used to identify
        an interrupt.

        common_interrupt:
          addl $-0x80,(%esp)
          SAVE_ALL
          TRACE_IRQS_OFF
          movl %esp,%eax
          call do_IRQ
          jmp ret_from_intr
        ENDPROC(common_interrupt)   #  a macro function is defined in <linux/linkage.h>,
                                    #  it will have expended to 
                                    #    .type common_interrupt,@function
                                    #    END(common_interrupt) =>
                                    #      .size common_interrupt,.-common_interrupt
          SAVE_ALL :
            an assembly language macro.

            saves the contents of all registers that the interrupt handler maybe use into stack.

            movl $(__USER_DS),%edx  #  user data segment
            movl %edx,%ds
            movl %edx,%es
            movl $(__KERNEL_PERCPU),%edx
            movl %edx,%fs
            SET_KERNEL_GS %edx      #  movl $(__KERNEL_STACK_CANARY),%edx
                                    #  movl %edx,%gs
                                    #  __KERNEL_STACK_CANARY is defined in <arch/x86/include/asm/segment.h>,
                                    #  it is equal to GDT_ENTRY_STACK_CANARY * 8

            !  eflags,cs,eip,ss,and esp would not be saved,which are already saved automatically by the
               control unit.

          after SAVE_ALL accomplished,the stack be like :
            [contents of registers]     <=  esp
            [interrupt vector]
            [former contents]
            /*  [eip] if @common_interrupt is executed through "call" instruction,but it is not  */
            ...

      do_IRQ() function :
        <arch/x86/include/asm/irq.h> <arch/x86/kernel/irq.c>
          /*  do_IRQ - do interrupt request,@common_interrupt call to this function.
           *  @regs:   pointer points to pt_regs object,the structure saveing the
           *           contents of registers.
           *  return:  always return 1.
           *  #  because this function is called by @common_interrupt,thus,the content of eax
           *     is the content of esp,which is pointing the saved contents of registers by
           *     @SAVE_ALL.
           *     the top element on the stack is content of ebx,and the first member in pt_regs
           *     is named @bx with type of unsigned long.
           *     the member @orig_ax saved the vector of current irq,because its content is the 
           *     content of esp before @SAVE_ALL had called.
           */
          extern unsigned int __irq_entry do_IRQ(struct pt_regs *regs);

        the high-level IRQ handler enter point :
          1>  save the old registers in @old_regs.

          2>  get @vector from @orig_ax.

          3>  get @irq from a per-cpu data which is an array named @vector_irq,the elements in it
              is the @irq,and index is @vector.
              @vector_irq is initialized by "__setup_vector_irq()",it use a "for" cycle to initializes
              this array for a specific cpu.
              macro "for_each_irq_desc" is defined in <linux/irqnr.h>.
              @irq is start from 0 to NR_IRQS,but @vector maybe not,because the value of @vector as
              an index for @vector_irq is got from desc->chip_data,which is type of void pointer,and
              it will points to the device PIC cfg data.(through PIC,the vector is mutable)
              #  the vector is able to be used by devices is in the range [32, 238].(local APIC enabled)

          4>  call to "irq_enter()" start tracing.
              a counter representing the number of nested interrupt handles was increased.
              the counter is stored in the @preempt_count filed of the thread_info structure of 
              current process.

          5>  call to "handle_irq(@irq, @regs)" which is defined in <arch/x86/kernel/irq_32.c>.

          6>  if function "handle_irq()" was failed to retrieve irq descriptor via @irq,then returns
              "false",in this case,"ack_APIC_irq()" was called to acknowledge the @irq(it is not
              be handled,because no irq descriptor was find out).
              error message is printed.
          
          7>  call to "irq_exit()" stop tracing.
              decrease the interrupt counter and checks whether deferrable kernel functions are waiting
              to be executed.

          8>  restore registers.

          9>  return control to @common_interrupt,and "ret_from_intr()" will be called.

          #  if "handle_irq()" is succed to handles,the @irq must been acknowledged.
          #  "handle_irq()" checks whether on an interrupt stack now,if current stack is a IRQ stack,
             then keep using this IRQ stack,and call to desc->handle_irq(@irq, @desc);
             otherwise,switch to the IRQ stack for current CPU and then exchange ebx and esp,call to
             desc->handle_irq(@irq, @desc),exchange ebx and esp again after "handle_irq()" is completed.
             
             /*  it is on an interrupt stack now,that is,a hardirq handler is interrupted.  */
             /*  the current task is stored in @irqctx->tinfo.task
              *  the previous stack is stored in @irqctx->tinfo.previous_esp
              *  and the softirq bits in @preempt_count was copied,thus,the softirq checks work in
              *  the hardirq context
              */

          #  there is some difference for x86_64 platform about "handle_irq()" function.
             on x86_64,"handle_irq()" will calls to "generic_handle_irq_desc()" function which
             is defined in <linux/irq.h> as a static inline function.
             that function checks whether desc->handle_irq is NULL,if it is,then call to "__do_IRQ()"
             function;otherwise call to desc->handle_irq.
          #  the function "__do_IRQ()" is the original all in one high-level IRQ handler.
             but this function is deprecated.
             this function is existed only CONFIG_GENERIC_HARDIRQS_NO__DO_IRQ is off.                

        /*  The actions to take while desc->handle_irq is NULL :
         *    x86_32:  do nothing except to acknowledge PIC.
         *    x86_64:  call to __do_IRQ().
         */

      __do_IRQ() function : (Effectively with CONFIG_GENERIC_HARDIRQS_NO__DO_IRQ off)
        <linux/irq.h> <kernel/irq/handle.c>
          #ifndef CONFIG_GENERIC_HARDIRQS_NO__DO_IRQ
          /*  __do_IRQ - the generic hardware interrupt request handler.
           *  @irq:      IRQ number.
           *  return:    always return 1.
           */
          extern unsigned int __do_IRQ(unsigned int irq);
          #endif

        the works "__do_IRQ()" does as the following :
          1>  retrieve the corresponding interrupt descriptor via @irq.and increase the cpu_usage_stat.irq of current
              CPU via call to kstat_incr_irqs_this_cpu()<linux/kernel_stat.h>.

          2>  if this @irq is IRQ_PER_CPU and current CPU has not disabled it,then handles it through
              "handle_IRQ_event()"<kernel/irq/handle.c>.
              acknowledge maybe occurs before handle if desc->chip->ack is not NULL,otherwise it occurs after
              handle via desc->chip->end.
              finally return 1.

          3>  if this @irq is not IRQ_PER_CPU,that means the interrupt is global.
              then ready to handles it.

          4>  acquires spin lock of interrupt descriptor at first,
              if desc->chip->ack is not NULL,acknowledge it.
              clear IRQ_REPLAY | IRQ_WAITING in the @desc->status.
                IRQ_REPLAY is when Linux resends an IRQ that was dropped earlier.
                IRQ_WAITING is used by probe to mark irqs that are being tested.
              set IRQ_PENDING,ready to handle.

          5>  checks whether IRQ_DISABLED | IRQ_INPROGRESS is setted.
              if it is,then do not handle this @irq;
              otherwise,get ISR,clear IRQ_PENDING and set IRQ_INPROGRESS,that is,the @irq is ready to be
              serviced.

          6>  enter a loop to process ISR.
              before "handle_IRQ_event()" is called with the ISR,spin lock of the interrupt descriptor must be
              released.
              this allows the second @irq is occurred while in handler or in "do_IRQ()",so the second @irq will
              be serviced after the previous @irq is handled completely.
              BUT THIS LOOP ONLY HANDLE THE SECOND INSTANCE,NEITHER THE THIRD,NOR THE FOURTH...
              after "handle_IRQ_event()" returned,the kernel control path acquire the spin lock and check if
              IRQ_PENDING is setted,if it is,that means the SECOND came in,then continue to handles it.(clear
              IRQ_PENDING again)
              if there is no more @irq came in,loop will ends.

          7>  call to desc->chip->end to end an interrupt request.
              release spin lock.
              return 1.

        #  "handle_IRQ_event()" is run with local cpu irq disabled,that is "__do_IRQ()" is run with local cpu irq
           disabled.
           the CPU control unit automatically clears eflags.IF because the interrupt handler is invoked through an
           IDT's interrupt gate.(IF indicates if CPU receives maskable interrupt)
           but after "handle_IRQ_event()" finished,the local cpu irq will be enabled again.

        #  why not service the third or the fourth?
           because the acknowledge is deferred before __do_IRQ() exit.(no ack() was called while ISR running)
           so the local APIC of current CPU would not accept further interrupts.(I/O APIC)
           although further occurrences of this type of interrupt may be accepted by other CPU.
        #  everytime "handle_IRQ_event()" returned,the function "note_interrupt()" is called if @noirqdebug is false.
           that function updates some counter in the interrupt descriptor,and if the bad irq exceeded the threshold
           on this IRQ line,the kernel control path will disable the IRQ line.
           /*  <kernel/irq/spurious.c>  */

      Reviving a lost interrupt :
        an interrupt lost :
          CPU0 received an @irq,on IRQ56 =>
          CPU1 disabled IRQ56 before CPU0 acknowledge to the PIC the @irq from =>
          CPU0 call to "do_IRQ()",but it finds IRQ_DISABLED is setted =>
          "do_IRQ()" returned with nothing have done except acknowledge the @irq =>
          the interrupt request @irq lost.

        <linux/irq.h> <kernel/irq/manage.c>
          /*  enable_irq - enable an IRQ line which corresponding to the @irq.
           *  @irq:        IRQ number.
           *  #  this function call to __enable_irq(irq_to_desc(irq), irq, false) with held
           *     chip_bus_lock and raw_spin_lock_irqsave,and release them before exit.
           *  #  if desc is NULL,then nothing to do.
           */
          extern void enable_irq(unsigned int irq);  /*  exported  */

          /*  __enable_irq - internal routine for enable an irq.
           *  @desc:         interrupt descriptor.
           *  @irq:          irq number.
           *  @resume:       bool value indicates whether resume from suspended before enable.
           */
          void __enable_irq(struct irq_desc *desc, unsigned int irq, bool resume);  /*  unexported  */

          #  before enable,the @irq can not been suspended,and the @irq is only able to be enabled 
             when desc->depth == 1.
             if desc->depth > 1 => decrease it
             else if desc->depth < 1 => error
             if desc->depth == 1 => function "check_irq_resend()" enable @irq.

        <linux/irq.h> <kernel/irq/resend.c>
          /*  check_irq_resend - enable @irq,if any lost interrupt was found out,
           *                     revive it.
           *  @desc:             interrupt descriptor.
           *  @irq:              irq number.
           */
          void check_irq_resend(struct irq_desc *desc, unsigned int irq);

          the function detects that an interrupt was lost by checking the value of the IRQ_PENDING flag.
          because IRQ_PENDING is always cleared when leaving the interrupt handler,if the IRQ line is
          disabled but IRQ_PENDING is set,then an interrupt occurrence has been acknowledged but not
          yet serviced.
          thus,it call to desc->chip->enable to enables @irq.
          next,clear IRQ_PENDING and set IRQ_REPLAY,put @irq to the bits @irqs_resend,and schedule
          a tasklet which is named @resend_tasklet.@resend_tasklet is associated to a worker "resend_irqs()"
          which is static defined in "resend.c".
          the function "resend_irqs()" enter a loop until the bitmap @irqs_resend is null,for each @irq
          in the bitmap,it disable local irq and call to irq_to_desc(irq)->handle_irq,after returned
          from handler,it enable local irq then enter next cycle.

          #  IRQ_REPLAY : IRQ has been replayed but not acked yet.
             "__do_IRQ()" always clear it before handling,and it also clear IRQ_PENDING before exit.
          #  tasklet is a kind of softirq,and softirq is interprocessor interrupt.
          #  "check_irq_resend()" resend irqs must met the condition that desc->chip->retrigger is NULL or
             desc->chip->retrigger(irq) is false.
             retrigger is the hardware relative primitive for resend an IRQ to CPU.
             /*  subsection - PIC :
              *    Disable interrups are not lost;the PIC sends them to the CPU as soon as they are
              *    enabled again.
              */

      Interrupt Service Routines :
        the actions relate to an interrupt the handler have to takes named ISRs,and these actions are executed
        by the function "handle_IRQ_event()".ISR just specific to one type of device.

        <linux/irqreturn.h>
          enum irqreturn {
                IRQ_NONE,           /*  interrupt was not from this device  */
                IRQ_HANDLED,        /*  interrupt was handled by this device  */
                IRQ_WAKE_THREAD     /*  handler requests to wake the handler thread  */
          };
          typedef enum irqreturn irqreturn_t;

          /*  IRQ_RETVAL - if the interrupt is handled  */
          #define IRQ_RETVAL(X)  ((x) != IRQ_NONE)

        <linux/irq.h>
          typedef void (*irq_flow_handler_t)(unsgned int irq, struct irq_desc *desc);
          
          /*  handle_IRQ_event - the primary worker deal with a specific interrupt.
           *                     this function call to the actions associated to the IRQ line,
           *                     until one of ISR handled it.
           *  @irq:              IRQ number.
           *  @action:           ISRs.
           *  return:            a value is type of enum irqreturn.
           */
          extern irqreturn_t handle_IRQ_event(unsigned int irq, struct irqaction *action);

          the works "handle_IRQ_event()" does :
            1>  if IRQ_DISABLED in @action is not set,then call to "local_irq_enable_in_hardirq()" to
                enable @irq.thus,new interrupt is able to be raised on this IRQ line.
                ("sti" assembly instruction.and if an action for an interrupt must disabled maskable
                 interrupts before start handling,that is the action is critical.) 
            
            2>  enter a loop,iterator is @action,until @action is NULL.

            3>  start trace irq handling through "trace_irq_handler_entry(@irq, @action)",
                call to @action->handler(@irq, @action->dev_id),
                stop trace irq handling through "trace_irq_handler_exit(@irq, @action, ret)".

            4>  switch-cases :
                  ret = IRQ_WAKE_THREAD >
                    ret := IRQ_HANDLED
                    wake_up_process(action->thread)  /*  action->thread_fn must be not NULL,
                                                      *  IRQF_DIED is not set in @action->thread_flags.
                                                      */

                  ret = IRQ_HANDLED >
                    local var @status |= action->flags

                  default >
                    none

            5>
                local var @retval |= ret         /*  this variable will be return value and initialized to
                                                  *  IRQ_NONE at first.
                                                  *  there is the only place in the function @retval is changed.
                                                  */
                update @action to next ISR

            6>  exit loop

            7>  if IRQF_SAMPLE_RANDOM is set in @status,@irq will be used to feed random number generator.

            8>  call to "local_irq_disable()" to restore the state of local IRQ line,if it was not diabled
                early,this function does nothing.("cli" assembly instruction)
                return @retval.

          #  Attention :
                value of IRQ_NONE is zero.
                and,even there is an ISR handled current interrupt in the loop,the loop is still continues
                unitl @action is NULL.
                thus,if the interrupt is serviced,just @retval is not able to reveal how many ISRs was called.

          #  register informations had stored on the stack by @SAVE_ALL assembly macro function,and "do_IRQ()"
             writes the address about the top of stack that is where esp points to into a percpu data which
             is named @irq_regs.
             so call to ISR do not need to transmit register contents.

          #  suppose,ISR1 requests IRQ line is disabled,but ISR2 requests IRQ line is enabled,then ISR2 will
             be called with IRQ line disabled,because that loop do not check whether local IRQ line have to
             be enabled or disabled.
                  
      Dynamic Allocation of IRQ Lines :
        except to reserved vectors for specific devices,the remaining ones are dynamically handled.
        there is,a way in which the same IRQ line can be used by several hardware devices even if they do
        not allow IRQ sharing.
        !  THE TRICK IS TO SERIALIZE THE ACTIVATION OF THE HARDWARE DEVICES SO THAT JUST ONE OWNS THE
           IRQ LINE AT A TIME.

        <linux/interrupt.h>
        typedef irqreturn_t (*irq_handler_t)(int, void *);

        #ifdef CONFIG_GENERIC_HARDIRQS
          extern int __must_check request_threaded_irq(unsigned int irq, irq_handler_t handler, 
                                                       irq_handler_t thread_fn, unsigned long flags,
                                                       const char *name, void *dev);
          static inline int __must_check
          request_irq(unsigned int irq, irq_handler_t handler, unsigned long flags, const char *name, void *dev);
        #else
          extern int __must_check request_irq(unsigned int irq, irq_handler_t handler, unsigned long flags,
                                              const char *name, void *dev);
          extern int __must_check request_threaded_irq(unsigned int irq, irq_handler_t handler, 
                                                       irq_handler_t thread_fn, unsigned long flags,
                                                       const char *name, void *dev);
        #endif
        /*  request_irq - request to register an ISR on an IRQ line.
         *  @irq:         IRQ number.
         *  @handler:     handler.
         *  @flags:       flags.
         *  @name:        device specified information used in "/proc/irq" and "/proc/interrupts".
         *  @dev:         device specified data.
         *  return:       0 or error code.
         */ 

        /*  request_threaded_irq - request to register an ISR with threaded environment.
         *  @irq:                  IRQ number.
         *  @handler:              handler.
         *  @thread_fn:            thread function.
         *  @flags:                irqaction flags.
         *  @name:                 device specified information.
         *  @dev:                  device specified data.
         *  return:                0 or error code.
         */

        #  to insert an irqaction descriptor in the proper list,the kernel invokes the "setup_irq()" function,
           passing to it the parameters @irq IRQ number and an address about the new irqaction object.
           if there is device used @irq previously,then check whether IRQF_SHARED is set.if it is not setup,
           then register ISR on @irq will be refused.
           @new_irqaction is added into irq_desc[@irq]->action list.
           !  if no other device is sharing the same IRQ,the function("setup_irq()") clears the
              IRQ_DISABLED,IRQ_AUTODETECT,IRQ_WAITING,IRQ_INPROGRESS flags in the interrupt descriptor 
              corresponding to this @new_irqaction.

        !  IN THE CASE THAT CONFIG_GENERIC_HARDIRQS IS NOT DEFINED,FUNCTION "request_irq()" IS UNDEFINED,BUT
           "request_threaded_irq()" IS DEFINED AND EXPORTED.
           IN THE CASE THAT CONFIG_GENERIC_HARDIRQS IS DEFINED,FUNCTION "request_irq()" JUST A WRAPPER WHICH
           CALL TO "request_threaded_irq()" WITH NULL @thread_fn.

        flags for ISR :
          IRQF_DISABLED         keep irqs disabled when calling the action handler
          IRQF_SAMPLE_RANDOM    feed random number generator
          IRQF_SHARED           allow sharing the irq among several devices
          IRQF_PROBE_SHARED     set by callers when they expect sharing mismatches to occur
          IRQF_TIMER            mark this interrupt as timer interrupt
          IRQF_PERCPU           interrupt is percpu
          IRQF_NOBALANCING      exclude this interrupt from irq balancing
          IRQF_IRQPOLL          interrupt is used for polling
                                (only the interrupt that is registered first in an shared interrupt is
                                 considered for performance reasons)
          IRQF_ONESHOT          interrupt is not reenabled after the hardirq handler finished.
                                used by threaded interrupts which need to keep the irq line disabled
                                until the threaded handler has been run

          /*  free_irq - free a registered IRQ line.
           *  @irq:      the IRQ line.
           *  @dev:      identifier of the ISR in action list.(irq_desc[@irq]->action)
           *  #  if the ISR is the last interrupt service routine in the action list,
           *     then disable the IRQ line.(shut it down)
           */
          extern void free_irq(unsigned int irq, void *dev);

        !  DO NOT CALL TO "request_irq()" IN AN INTERRUPT CONTEXT,BECAUSE OF IT MIGHT ENTER SLEEP.

    Interprocessor Interrupt Handling :
      Interprocessor interrupts allow a CPU to send interrupt signals to any other CPU in the system.
      IPI is delivered not through an IRQ line,but directly as a message on the bus that connects the
      local APIC of all CPUs.

      Linux,IPI on multiprocessor system : (three kinds)
        CALL_FUNCTION_VECTOR(vector 0xfb)
          sent to all CPUs but the sender,forcing those CPUs to run a function passed by the sender.
          the corresponding interrupt handler is named "call_function_interrupt()".
          usually,this interrupt is sent to all CPUs except the CPU executing the calling function by
          means of the "smp_call_function()" facility function.
          /*  such function maybe stop the CPUs or force CPUs to set the contents of the
           *  Memory Type Range Registers(MTRRs).
           *  MTTRs : additional registers to easily customize cache operations.
           *          Linux may use them to disable the hardware cache for the address mapping the 
           *          frame buffer of a PCI/AGP graphic card while maintaining the "write combining"
           *          mode of operation : the paging unit combines write transfers into large chunks
           *          before copying them into the frame buffer.
           */

        RESCHEDULE_VECTOR(vector 0xfc)
          when a CPU receives this type of interrupt,the corresponding handler - named "reschedule_interrupt()" -
          limits itself to acknowledging the interrupt.
          rescheduling is done automatically when returning from the interrupt.
        
        INVALIDATE_TLB_VECTOR(vector 0xfd)
          send to all CPUs but the sender,forcing them to invalidate their Translation Lookaside Buffers.
          the corresponding handler,named "invalidate_interrupt()",flushes some TLB entries of the processor.

        /*  The assembly language macro "BUILD_INTERRUPT3(name, nr, fn)" is defined in
         *  <arch/x86/kernel/entry_32.S>,which is similar to "common_interrupt".(vector is still a negative number)
         *  The C language macro "BUILD_INTERRUPT(name, nr)" is defined in <arch/x86/kernel/entry_32.S>,which
         *  is used to define an interprocessor interrupt,actually,it is "BUILD_INTERRUPT3(name, nr, smp_##name)".
         *    e.g.  
         *      <arch/x86/include/asm/entry_arch.h>
         *      BUILD_INTERRUPT(call_function_interrupt, CALL_FUNCTION_VECTOR)
         *      BUILD_INTERRUPT(irq_move_cleanup_interrupt, IRQ_MOVE_CLEANUP_VECTOR)
         *      BUILD_INTERRUPT3(invalidate_interrupt0, INVALIDATE_TLB_VECTOR_START + 0)
         */

        source file <arch/x86/kernel/smp.c> defined some interprocessor interrupt handlers :
          e.g.
            /*  smp_reschedule_interrupt - handler for the IPI with vector RESCHEDULE_VECTOR.
             *  @regs:                     registers' contents saved on the stack via "SAVE_ALL".
             *                             @regs is the esp points to the top of stack.
             *  #  this function does nothing but acknowledge IPI,
             *     rescheduling is automatically done when returning from interrupt.
             */
            void smp_reschedule_interrupt(struct pt_regs *regs);
        
            /*  smp_call_function_interrupt - handler for the IPI with vector CALL_FUNCTION_VECTOR.
             *  @regs:                        registers' contents saved by "SAVE_ALL",@regs is the 
             *                                esp points to the top of stack.
             *  #  this function actually call to "generic_call_function_interrupt()" after it
             *     acknowledged the IPI.
             *     function "generic_call_function_interrupt()" is defined in <kernel/smp.c>.
             */
            void smp_call_function_interrupt(struct pt_regs *regs);
            
            /*  figure about smp_call_function_interrupt :
             *  smp_call_function_interrupt =>
             *  generic_smp_call_function_interrupt {
             *          traverse call_function list (defined in <kernel/smp.c>)
             *          for each entry is type of call_single_data (defined in <linux/smp.h>)
             *          retrieve its container is type of call_function_data (defined in <kernel/smp.c>)
             *          call to @data->csd.func(@data->csd.info)
             *          if @data->refs == 0
             *              delete current entry from list
             *      ####   
             *              struct call_function_data {
             *                struct call_single_data csd;
             *                atomic_t                refs;
             *                cpumask_var_t           cpumask;
             *              };  /*  one @csd corresponding to one @cfd  */
             *
             *              call_function.queue -> csd1 -> csd2 -> ... -> call_function.queue
             *                                      |       |
             *                                      V       V
             *                                     cfd     cfd
             *  }
             */


            !!  the following IPIs are existed only CONFIG_SMP is defined :
                  call_function_interrupt       =>  CALL_FUNCTION_VECTOR
                  reschedule_interrupt          =>  RESCHEDULE_VECTOR
                  irq_move_cleanup_interrupt    =>  IRQ_MOVE_CLEANUP_VECTOR
                  reboot_interrupt              =>  REBOOT_VECTOR
                  invalidate_interrupt{0..7}    =>  INVALIDATE_TLB_VECTOR_START + {0..7}

                  /*  the corresponding "smp_##name()" functions are defined either <arch/x86/kernel/smp.c>
                   *  or <kernel/smp.c>.
                   *  TLB relative function is defined in <arch/x86/mm/tlb.c>.
                   */

      IPI operating functions :
        <arch/x86/include/asm/apic.h>
          /*  struct apic - the generic APIC sub-arch data struct.in other word,the object is type of
           *                struct apic represents the I/O APIC.
           *  @send_IPI_mask:           send IPI with @vector to the CPUs determined by @mask.
           *  @send_IPI_allbutself:     send IPI with @vector to all CPUs except sender.
           *  @send_IPI_all:            send IPI with @vector to all CPUs include sender.
           *  @send_IPI_self:           send IPI with @vector to @self.
           */
          struct apic {
                ...
                void (*send_IPI_mask)(const struct cpumask *mask, int vector);
                ...
                void (*send_IPI_allbutself)(int vector);
                void (*send_IPI_all)(int vector);
                void (*send_IPI_self)(int vector);
                ...
          };

          extern struct apic *apic;     /*  global apic object pointer  */

          /*  the corresponding function pointers will be initialized by kernel across to the CPU model.
           *  different CPU has different apic object.
           */

          /*  Linux 2.6 DOES NOT CONTAIN THE WRAPPERS FOR THESE FUNCTIONS,IN ORDER TO SEND IPI,HAVE TO USE THE
           *  GLOBAL OBJECT @apic IS TYPE OF "struct apic *",FOR EXAMPLE,"apic->send_IPI_all(CALL_FUNCTION_VECTOR)".
           *  Linux SUPPORTS TO SEVERAL TYPES OF APIC,SO THE MEMBERS OF THE OBJECT MAYBE DIFFERENT.
           *  x86 DEFAULT APIC    : apic_default       <arch/x86/kernel/apic/probe_32.c>
           *  x86_64 DEFAULT APIC : apic_flat          <arch/x86/kernel/apic/apic_flat_64.c>
           */

          /*  call function interrupt outer interface :
           *  <kernel/smp.c>,
           *    smp_call_function - run a function on all other CPUs.
           *    @func:              the function to be executed.
           *    @info:              private data of @func.
           *    @wait:              T => wait until other CPUs completed function executing
           *                        F => does not waiting
           *    return:             always 0.
           *    #  can not call this function with disabled interrupts or from a hardirq handler or
           *      from a bottom half handler.
           *    int smp_call_function(void (*func)(void *), void *info, int wait);
           */

    Softirqs and Tasklets :
      non-critical tasks can be deferred for a long period of time,if necessary.
      ISRs are serialized,and often there should be no occurrence of an interrupt until the corresponding
      interrupt handler has terminated.
      deferrable tasks can execute with all interrupt enabled.

      !  Softirqs and tasklets cannot be interleaved on a givent CPU.

      Linux 2.6 non-urgent interruptible kernel functions :
        /*  deferrable functions  */
        /*  Intel manual called programmed exception as "softirq"  */

        softirqs
          statically allocated
          can run concurrently on several CPUs,even if they are of the same type(reentrant,the data
          structures that softirqs hold must be protected by spin lock)

        tasklets
          dynamically allocated
          the same type of tasklet cannot be executed by two CPUs at the same time(however,tasklets of
          different types can be executed concurrently on several CPUs)

        /*  softirqs and tasklets are strictly correlated,because tasklets are implemented on top of 
         *  softirqs.
         */

      Interrupt context :
        the kernel is currently executing either an interrupt handler or a deferrable function.

      The general operations can be performed on deferrable functions :
        Initialization
          defines a new deferrable function
          this is usually done when the kernel initializes iteself or a module is loaded

        Activation
          marks a deferrable function as "pending"
          it is to be run the next time the kernel schedules a round of executions of deferrable functions
          activation can be done at any time(even while handling interrupts)

        Masking
          selectively disables a deferrable function 
          it will not be executed by the kernel even if activated
        
        Execution
          executes a pending deferrable function together with all other pending deferrable functions of
          the same type
          execution is performed at well-specified times

        /*  a deferrable function that has been activated by a given CPU must be executed on the same CPU  */

      Softirq :
        ten kinds of softirqs > <linux/interrupt.h>
          HI_SOFTIRQ                handles high priority tasklets
          TIMER_SOFTIRQ             tasklets related to timer interrupts
          NET_TX_SOFTIRQ            transmits packets to network cards
          NET_RX_SOFTIRQ            recevies packets from network cards
          BLOCK_SOFTIRQ             block device softirq
          BLOCK_IOPOLL_SOFTIRQ      block device I/O poll softirq
          TASKLET_SOFTIRQ           handles regular tasklets
          SCHED_SOFTIRQ             scheduler softirq
          HRTIMER_SOFTIRQ           high-resolution timer
          RCU_SOFTIRQ               rcu lock

          NR_SOFTIRQS               /*  end of enumberated types  */
                                    /*  HI_SOFTIRQ = 0  */

        /*  Before Linux 2.6,the Bottom Half mechanism BH is used to handle non-critical works,
         *  the total number of BHs is 32.
         *  Since Linux 2.6,BH had abandoned,softirqs as a intead,and the maximum number of softirqs
         *  also is 32.all device drivers which used BH should convert it to tasklet.
         */

        data structures used for softirqs >
          <linux/interrupt.h>
            /*  struct softirq_action - softirq action wrapper.
             *  @action:                the deferrable function will be executed.
             */
            struct softirq_action {
                    void (*action)(struct softirq_action *);
                    /*  Linux 2.6 DID NOT DEFINE SUCH FIELD WHICH USED TO SAVE THE ARGUMENT OF
                     *  THE @action DEFERRABLE FUNCTION.
                     */
            };

          <kernel/softirq.c>
            /*  softirq_vec - an array contains all types of softirqs.
             *                the enumberated types defined in <linux/interrupt.h>
             *                as the vector for this array.
             */
            static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;

          thread_info.preempt_count :
            [0, 7]        preemption counter      /*  how many times preemption is explicitly disabled,max depth 256  */
            [8, 15]       softirq counter         /*  how many levels deep the disabling of deferrable functions is  */
            [16, 27]      hardirq counter         /*  specifies the number of nested interrupt handlers on the local CPU  */
            [28]          PREEMPT_ACTIVE flag     /*  allow preemption or refuse preemption  */
          
            /*  some bits in @preempt_count are used to keep track of kernel preemption and of nesting of kernel
             *  control paths.
             *  softirq counter = 0 => deferrable functions are enabled
             *  hardirq counter => irq_enter() increase it and irq_exit() decrease it
             */
          
            useful macro function to determine where the kernel path is now :
              <linux/hardirq.h>
                #define in_irq()  (hardirq_count())       /*  Is kernel running interrupt handler?  */
                #define in_softirq()  (softirq_count())   /*  Is kernel running deferrable function?  */
                #define in_interrupt()  (irq_count())     /*  Is kernel running interrupt handler or deferrable function?  */
  
                /*  if kernel is not make use of Kernel Mode stack
                 *    preempt_count get from current.thread_info
                 *  else
                 *    preempt_count get from irq_ctx union assocaited with the local CPU
                 */

            irq_cpustat_t.__softirq_pending :
              the field @__softirq_pending is a per-CPU 32-bit mask describing the pending softirqs.
              
              <linux/irq_cpustat.h>
                #define local_softirq_pending()  \
                        __IRQ_STAT(smp_processor_id(), __softirq_pending)

        Handling softirqs :
          linux softirq has defined two primitives for operating softirq mechanism.
            OPEN        -       install a softirq deferrable function.
            RAISE       -       raise a installed softirq.
                                /*  structure softirq_action is not chained,so there is only
                                 *  one deferrable function for one type of softirq at the
                                 *  same time.
                                 */

          <linux/interrupt.h>
            /*  open_softirq - install a softirq deferrable function @action at the
             *                 position @softirq_vec[@nr].
             *  @nr:           softirq vector.
             *  @action:       deferrable function.
             */
            extern void open_softirq(int nr, void (*action)(struct softirq_action *));

            /*  raise_softirq - raise a softirq have vector @nr.
             *  @nr:            softirq vector.
             *  #  this function will save the local IRQ flags(eflags.IF) and then disable
             *     local IRQ(through assembly language instruction "cli").
             *     call to "raise_softirq_irqoff()" which must run with irqs disabled,
             *     next,"__raise_softirq_irqoff()" will be called,which set the bit associated 
             *     to @nr in the pending softirqs on current CPU.check if current context
             *     is not interrupt context,if it is,wake up ksoftirqd.
             *     before "raise_softirq()" returns,"local_irq_restore()" is called for restore
             *     interrupt flags to the previously saved.
             */
            extern void raise_softirq(unsigned int nr);
            extern void raise_softirq_irqoff(unsigned int nr);

          checks for active(pending) softirqs should be performed periodically,but without inducing too much
          overhead,they are performed in a few points of the kernel code :
            >  when the kernel invokes the "local_bh_enable()" function to enable softirqs on the local CPU.
               /*  bh => bottom half,had removed from Linux kernel  */
            >  when the "do_IRQ()" function finishes handling an I/O interrupt and invokes the "irq_exit()" macro.
            >  if the system uses an I/O APIC,when the "smp_apic_timer_interrupt()" function finishes handling
               a local timer interrupt.
            >  in multiprocessor systems,when a CPU finishes handling a function triggered by a CALL_FUNCTION_VECTOR
               interprocessor interrupt(IPI).
            >  when one of the special "ksoftirqd/n" kernel threads is awakended.
               /*  on SMP,each CPU in the system has a kernel thread named "ksoftirqd" which deal with the
                *  softirq handling.
                *  Linux kernel make a balance between softirqs and User Mode processes through the task scheduler.
                */

          the do_softirq() function :
            <linux/interrupt.h> 
              /*  do_softirq - function deal with soft interrupt.
               *               defined in <kernel/softirq.c>
               */
              asmlinkage void do_softirq(void);

              the following actions this function will takes:
                1>  check whether in an interrupt context now,if it is,then return.
                    deferrable function can not be executed in an interrupt context.
                2>  call to local_irq_save(@flags) to saves the contents of eflags register to local
                    variable @flags.
                    /*  function local_irq_save() will disable local interrupt.  */
                3>  call to local_softirq_pending() to get the pending softirqs and saves them in
                    local variable @pending.
                4>  if there is a softirq pending,then call to __do_softirq(),which will does the
                    primary works.
                    /*  local interrupt still be disabled at the time control enter __do_softirq().  */
                5>  call to local_irq_restore(@flags) to restores the status of eflags.
                    /*  function local_irq_restore() restores eflags.if eflags.IF flag was not setted previously,
                     *  it will be unsetted after local_irq_restore() finished.
                     */

          the __do_softirq() function :
            <linux/interrupt.h>
              /*  __do_softirq - the main routine for softirq handling.
               *                 it is defined in <kernel/softirq.c>
               */
              asmlinkage void __do_softirq(void);

              /*  GCC feature:
               *    __builtin_return_address - GCC built-in function.
               *                               it returns the return address(eip) of the current function,or
               *                               of one of its callers.
               *    @level:                    it is number of frames to scan up the call stack.0 yields
               *                               the return address of the current function,and so forth.
               *    void *__builtin_return_address(unsigned int level);
               */

              the following actions this function will takes:
                1>  retrieve pending local softirqs.
                2>  account system vtime via account_system_vtime(current).
                3>  call to __local_bh_disable((unsigned long)__builtin_return_address(0)) to updates preempt_count()
                    with SOFTIRQ_OFFSET.
                    /*  preempt_count() => current_thread_info()->preempt_count
                     *  @ip parameter of __local_bh_disable() is used to tracing kernel control path.
                     *  before updating,eflags.IF will be cleared,and eflags will be restored after updated.
                     */
                4>  set_softirq_pending(0),clean the softirq pending bitmap.
                    enable local irq.
                    /*  do_softirq() disabled local interrupt before __do_softirq() was called.  */
                5>  let local variable @h points to @softirq_vec array.
                6>  traverse @pending until @pending == 0 (bits traversing).
                7>  if (@pending & 1)
                      prepare account and tracing;
                      call to h->action(h);
                    h++;
                    pending >>= 1;
                8>  disable local irq.
                9>  check again if new softirq was raised.
                10> if @pending != 0 and @max_restart != 0
                      restart to 4>
                    if @pending != 0 and @max_restart == 0
                      can not restart,wake up ksoftirqd
                11> stop tracing,update system vtime,call to __local_bh_enable().
                12> return control to do_softirq().

                /*  before action 4> the function lockdep_softirq_enter() is called,current->softirq_context++,
                 *  before action 12> the function lockdep_softirq_exit() is called,current->softirq_context++.
                 *  these two functions are defined in <linux/irqflags.h>
                 */

        The ksoftirqd kernel threads :
          the kernel ksoftirqd is represent a solution for a critical trade-off problem.
          the detail of it is defined in <kernel/softirq.c>.

          !  softirq functions may reactive themselves,in fact,both the networking softirqs and the
             tasklet softirqs do this,moreover,external events,such as packet flooding on a network card
             may activate softirqs at very high frequence.

          <kernel/softirq.c>
            /*  per-cpu data for per-cpu kernel thread [ksoftirqd]  */
            static DEFINE_PER_CPU(struct task_struct *, ksoftirqd);

            /*  wakeup_softirqd - wakeup the ksoftirqd kernel thread of local CPU.
             *  #  no indefinitely loop in this function to avoid userspace starvation.
             *     do_softirq() call to this function to deal with the new coming softirqs.
             */
            void wakeup_softirqd(void);

            /*  run_ksoftirqd - the main function to run for the kernel thread [ksoftirqd].
             *  @__bind_cpu:    the cpu associated to the [ksoftirqd],if the cpu is offline,
             *                  the corresponding [ksoftirqd] should to die.
             *  #  the main works this function does:
             *       >  set task state to TASK_INTERRUPTIBLE.
             *       >  enter a loop until kthread_should_stop() returns true.
             *       >  in the loop,set task state to TASK_RUNNING,disable kernel preempt and check local softirqs
             *          via local_softirq_pending().
             *       >  if any local softirqs are pending,call to do_softirq(),then enable preempt and call to
             *          cond_resched(),this function will call to schedule().(flag TIF_NEED_RESCHED of the current 
             *          thread_info set)
             *       >  if none of local softirqs are pending,then enable preempt and set task state to TASK_INTERRUPTIBLE,
             *          next call to schedule() to yield CPU time.
             */
            static int run_ksoftirqd(void *__bind_cpu);

            /*  spawn_ksoftirqd - init function which spawn the [ksoftirqd] kernel thread 
             *                    for current cpu.
             *  #  this function will be called at the time that is kernel initialization.
             *     it will call to cpu_callback()<kernel/softirq.c> to creates a kernel thread
             *     which is about to runs run_ksoftirqd() function.
             *     it call to cpu_callback() twice,the second of invocation is to wakeup [ksoftirqd].
             */
            static __init spawn_ksoftirqd(void) early_initcall(spawn_ksoftirqd);

          the potential for a continuous high-volume flow of softirqs creates a problem that is solved by
          introducing kernel threads.without them,developers are essentially faced with two alternative
          strategies:
            1>  ignoring new softirqs that occur while do_softirq() is running.
                the new coming softirqs will be deferred to next timer interrupt,this is can not be 
                acceptable for network system.
            2>  continuously rechecking for pending softirqs.
                the do_softirq() function could keep checking the pending softirqs and would terminate
                only when none of them is pending.
                userspace stravation will occur.(if a high-frequence flow of packets is received by a network card,
                or a softirq function keeps activating itself)

            !   [ksoftirqd] has low priority,the userspace programs will have a change to run.

        Tasklets :
          tasklets are the preferred way to implement deferrable functions in I/O drivers.
          tasklet is implemented through HI_SOFTIRQ or TASKLET_SOFTIRQ,several tasklets may be associated
          with the same softirq,each tasklet carrying its own function.
          do_softirq() deal with tasklet is type of HI_SOFTIRQ before the tasklet is type of TASKLET_SOFTIRQ.

          <linux/interrupt.h>
            /*  tasklet_struct - the structure represents a kernel tasklet.
             *  @next:           pointer to next tasklet.
             *  @state:          status of this tasklet.
             *                   value :
             *                     TASKLET_STATE_SCHED - pending
             *                     TASKLET_STATE_RUN   - running
             *  @count:          lock counter.
             *  @func:           function pointer to the function this tasklet carrying.
             *  @date:           date to the @func.
             */
            struct tasklet_struct {
                    struct tasklet_struct *next;
                    unsigned long state;
                    atomic_t count;
                    void (*func)(unsigned long);
                    unsigned long data;
            };

          <kernel/softirq.c>
            /*  tasklet_head - the structure represents a tasklet link-list.
             *  @head:         the head.
             *  @tail:         the tail(pointer to pointer).
             */
            struct tasklet_head {
                    struct tasklet_struct *head;
                    struct tasklet_struct **tail;
            };

            /*  --- per-cpu data ---  */

            /*  for TASKLET_SOFTIRQ  */
            static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);
            /*  for HI_SOFTIRQ       */
            static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);
            
          <linux/interrupt.h>
            /*  tasklet_init - initializes a tasklet_struct.
             *  @t:            pointer to the tasklet_struct structure.
             *  @func:         the function this tasklet_struct carrys.
             *  @data:         the data to @func.
             */
            extern void tasklet_init(struct tasklet_struct *t, void (*func)(unsigned long), unsigned long data);

            /*  tasklet_disable_nosync - disable a tasklet without synchronize.
             *  @t:                      the tasklet_struct object.
             *  #  this function atomic increase the data member tasklet_struct.count.
             *     for smp,the function smp_mb_after_atomic_inc() will be called to synchronize to another cpu.
             */
            static inline void tasklet_disable_nosync(struct tasklet_struct *t);

            /*  tasklet_disable - synchrounous version.
             *  @t:               the tasklet_struct object.
             *  #  this function just call to tasklet_disable_nosync() and then call to tasklet_unlock_wait(@t)
             *     to wait the tasklet gets end if it is running on some cpu.
             */
            static inline void tasklet_disable(struct tasklet_struct *t);

            /*  tasklet_enable - enable a disabled tasklet.
             *  @t:              the tasklet_struct object which had been disabled.
             *  #  this function atomic decrease the data member tasklet_struct.count .
             */
            static inline void tasklet_enable(struct tasklet_struct *t);

            /*  tasklet_schedule - schedule a tasklet with type of TASKLET_SOFTIRQ.
             *  @t:                the tasklet it should be scheduled.
             *  #  this function call to the __tasklet_schedule()<kernel/softirq.c>,
             *     before the calling,the @t->state will be setted to TASKLET_STATE_SCHED.
             */
            static inline void tasklet_schedule(struct tasklet_struct *t);

            /*  tasklet_hi_schedule - schedule a tasklet with type of HI_SOFTIRQ.
             *  @t:                   the tasklet it should be scheduled.
             *  #  this function call to the __tasklet_hi_schedule()<kernel/softirq.c>,
             *     before the calling,the @t->state will be setted to TASKLET_STATE_SCHED.
             */
            static inline void tasklet_hi_schedule(struct tasklet_struct *t);

            !  if @t->state == TASKLET_STATE_SCHED
               then
                 tasklet_schedule() and tasklet_hi_schedule() return without something have been done.
               else
                 call to the internal functions defined in <kernel/softirq.c>

            __tasklet_schedule() and __tasklet_hi_schedule() :
              these two functions almost do the same works:
                1>  store local irq flags.
                2>  @t->next = NULL.
                3>  push @t to the tasklet_head link-list.
                4>  call to raise_softirq_irqoff().
                5>  restore local irq flags.

              the difference:
                __tasklet_schedule() push @t to the tail of local @tasklet_vec link-list.
                __tasklet_schedule() call raise_softirq_irqoff() with TASKLET_SOFTIRQ.
                __tasklet_hi_schedule() push @t to the tail of local @tasklet_hi_vec link-list.
                __tasklet_hi_schedule() call raise_softirq_irqoff() with HI_SOFTIRQ.

          <kernel/softirq.c>
            /*  tasklet_action - action function for TASKLET_SOFTIRQ type.
             *                   it is installed through open_softirq() by softirq_init().
             *  @a:              the softirq action structure.
             */
            static void tasklet_action(struct softirq_action *a);

            /*  tasklet_hi_action - action function for HI_SOFTIRQ type.
             *                      it is installed through open_softirq() by softirq_init().
             *  @a:                 the softirq action structure.
             */
            static void tasklet_hi_action(struct softirq_action *a);

            the works these two functions do:
              1>  disable local irq and retrieve head of tasklet list.
              2>  empty the tasklet list.
              3>  enable local irq and enter a while-loop.
              4>  traverse the list to deal with each tasklet if it had not been disabled,
                  each tasklet only be activated once if it is not reactives itself.
                  if @t has been running on another cpu or it has been disabled,then it will
                  be push back to the tail of the tasklet list,raise_softirq_irqoff() will be
                  called before next loop.
              5>  traversed all tasklets in the list,function returns.
                  the new tasklet list will be deferred to the next time(it maybe contains
                  the running or disabled tasklets).

            !  while processing the tasklet,action function have to lock it(SMP) through tasklet_trylock() and
               unlock it after processed through tasklet_unlock().
               the two functions set tasklet_struct.state = TASKLET_STATE_RUN or unset it.
               /*  TASKLET_STATE_SCHED must be set before action by other kernel control path,
                *  otherwise,the bug occurred  */

          !  unless the tasklet function reactivates itself,every tasklet activation triggers at most one execution
             of the tasklet function.

        Work Queues :
          introduced in Linux 2.6 and replace a similar construct called "task queue" used in Linux 2.4.
          they allow kernel functions to be activated and later executed by special kernel threads called
          worker threads.

          the kernel functions in work threads are running in process context,it is not interrupt context.
          neither deferrable functions nor functions in a work queue can access to the User Mode address space
          of a process.

          <linux/workqueue.h>
            /*  work_func_t - a type definition for the worker function.  */
            typedef void (*work_func_t)(struct work_struct *work);

            /*  work_struct - it represents a work item.
             *  @data:        data to the @func.
             *  @entry:       works list.
             *  @func:        work function.
             */
            struct work_struct {
                    atomic_t data;
                    struct list_head entry;
                    work_func_t func;
            #ifdef CONFIG_LOCKDEP
                    struct lockdep_map lockdep_map;
            #endif
            };

            /*  delayed_work - represents a delayed work.
             *  @work:         the work.
             *  @timer:        timer.
             */
            struct delayed_work {
                    struct work_struct work;
                    struct timer_list timer;
            };

          <kernel/workqueue.c>
            <linux/wait.h> typedef struct __wait_queue_head wait_queue_head_t;

            /*  cpu_workqueue_struct - the per-CPU workqueue(if single thread, always use the first
             *                         possible cpu).
             *  @lock:                 spin lock.
             *  @worklist:             works list,@worklist.next-->@work_struct.entry .
             *  @more_work:            wait queue where the worker thread waiting for more work to be done sleeps.
             *  @current_work:         the work currently pending on the cpu.
             *  @wq:                   the workqueue.
             *  @thread:               the associated worker thread.
             */
            struct cpu_workqueue_struct {
                    spinlock_t lock;
                    struct list_head worklist;
                    wait_queue_head_t more_work;
                    struct work_struct *current_work;
                    struct workqueue_struct *wq;
                    struct task_struct *thread;
            } ____cacheline_aligned;

            /*  workqueue_struct - the externally visible workqueue abstraction is an array of
             *                     per-CPU workqueues.
             *  @cpu_wq:           associated per-CPU workqueue struct.
             *  @list:             workqueue list.
             *  @name:             workqueue name.
             *  @singlethread:     if singlethread.
             *  @freezeable:       freeze threads during suspend.
             *  @rt:               run times count.
             */
            struct workqueue_struct {
                    struct cpu_workqueue_struct *cpu_wq;
                    struct list_head list;
                    const char *name;
                    int singlethread;
                    int freezeable;
                    int rt;
            #ifdef CONFIG_LOCKDEP
                    struct lockdep_map lockdep_map;
            #endif
            };

          Detail :
            every work is represented by a work_struct structure.
            queue a work is queue it into a cpu_workqueue_struct.
            workqueue_struct is a externally visiable abstraction of cpu_workqueue_struct.
            cpu_workqueue_struct structures is created by linux kernel automatically.
            __create_workqueue_key() is used to creates several cpu_workqueue_struct structures,
            and each cpu_workqueue_struct structure is associated to a same workqueue_struct
            structure(new created by the function).then per-CPU worker thread is created and started.
            each new created @wq is added into a static link-list @workqueues(defined in <kernel/workqueue.c>).
            the interfaces operate workqueue needs a workqueue_struct object as its parameter,it
            specify which workqueue to operating.
            @workqueue_struct.list is a nested link-list structure each elements on it is a workqueue_struct
            which associated to a cpu_workqueue_struct,generally,one workqueue_struct to one cpu_workqueue_struct.

          <linux/workqueue.h> <kernel/workqueue.c>
            /*  __create_workqueue_key - create a workqueue,and several cpu_workqueue_struct structures also
             *                           will be created and associated to the new created workqueue.
             *  @name:                   workqueue name.
             *  @singlethread:           if singlethread?
             *  @freezeable:             if freezeable?
             *  @rt:                     rt.
             *  @key:                    lock_class_key pointer.
             *  @lock_name:              lock_name.
             *  return - NULL or the new created workqueue_struct pointer.
             *  #  singlethread == F
             *       creates several worker threads.
             */
            extern struct workqueue_struct *
            __create_workqueue_key(const char *name, int singlethread, int freezeable,
                                   int rt, struct lock_class_key *key, const char *lock_name);

            #define __create_workqueue(name, singlethread, freezeable, rt)  \
                    __create_workqueue_key((name), (singlethread), (freezeable), (rt), NULL, NULL)
            #define create_workqueue(name)  __create_workqueue((name), 0, 0, 0);

            /*  destroy_workqueue - destroy a workqueue.
             *  @wq:                the workqueue_struct to be destroyed.
             */
            extern void destroy_workqueue(struct workqueue_struct *wq);

            /*  queue_work - queue a work.
             *  @wq:         the workqueue to queue.
             *  @work:       the work.
             *  return - queued,1;work already on the queue or failed,0.
             */
            extern int queue_work(struct workqueue_struct *wq, struct work_struct *work);

            /*  queue_delayed_work - queue a delayed work.
             *  @wq:                 the workqueue to queue.
             *  @work:               the work.
             *  @delay:              delay.(expires = timer.jiffies + @delay)
             *  return - queued,1;work already on the queue or failed,0.
             */
            extern int queue_delayed_work(struct workqueue_struct *wq, struct work_struct *work,
                                          unsigned long delay);

            /*  cancel_delayed_work - <linux/workqueue.h> static function cancel a delayed work.
             *  @work:                the delayed work.
             *  return - @ret >= 0.
            static inline int cancel_delayed_work(struct delayed_work *work);

            /*  flush_workqueue - block current process until all pending works had been done,
             *                    do not care the new incoming works.
             *  @wq:              the workqueue.(this workqueue associated to several cpu_workqueue_struct)
             */
            extern void flush_workqueue(struct workqueue_struct *wq);

          the predefined work queue :
            <kernel/workqueue.c>
              /*  keventd_wq - the kernel thread "events"'s workqueue.
               *               kernel offers the predefined workqueue to deal with general
               *               works,prevent creating a whole set of worker threads in order
               *               to run a function.
               */
              static struct workqueue_struct *keventd_wq __read_mostly;

              /*  schedule_work - queue a work up to keventd_wq.
               *  @work:          the work.
               *  return - queued,1;work already on the queue or failed,0.
               */
              extern int schedule_work(struct work_struct *work);

              /*  schedule_delayed_work - queue a delayed work up to keventd_wq.
               *  @work:                  the delaying work.
               *  @delay:                 delay,expires = @timer.jiffies + delay.
               *  return - queued,1;work alread on the queue or failed,0.
               */
              extern int schedule_delayed_work(struct delayed_work *work, unsigned long delay);

              /*  schedule_delayed_work_on - similar to schedule_delayed_work(),but it is possible
               *                             to assign cpu-id to specify which cpu to run this work
               *                             via @cpu.
               */
              extern int schedule_delayed_work_on(int cpu, struct delayed_work* work, unsigned long delay);

              /*  flush_scheduled_work - similar to flush_workqueue(),but the target to flush is keventd_wq.  */
              extern void flush_scheduled_work(void);

            !  functions executed in the predefined work queue should not block for a long time,because the execution
               of the pending functions in the work queue list is serialized on each CPU,a long delay negatively
               affects the other users of the predefined work queue.

            !  [kblockd] kernel thread deal with the works on the @kblockd_workqueue work queue,
               the works from block device layer.

    Returning from interrupts and Exceptions :
      resume execution of some program,several issues must be considered beforing doing it :
        >  Number of kernel control paths being concurrently executed
             if these just one,the CPU must switch back to User Mode
        >  Pending process switch requests
             if there is any request,the kernel must perform process scheduling,otherwise,control
             is returned to the current process(the interrupted process)
        >  Pending signals
             if a signal is sent to the current process,it must be handled
        >  Single-step mode
             if a debugger is tracing the execution of the current process,single-step mode must be
             restored before switching back to User Mode
        >  Virtual-8086 mode
             if the CPU is in virtual-8086 mode,the current process is executing a legacy Real Mode program,
             thus it must be handled in a special way

      flags used to keep track of pending switch requests,of pending signals,and of single step execution :
        thread_info.flags >
          TIF_SYSCALL_TRACE         syscalls are being traced
          TIF_NOTIFY_RESUME         not used in the 80x86 platform
          TIF_SIGPENDING            the process has pending signals
          TIF_NEED_RESCHED          scheduling must be performed
          TIF_SINGLESTEP            restore single step execution on return to User Mode
          TIF_IRET                  force return from system call via "iret" rather than "sysexit"
          TIF_SYSCALL_AUDIT         system calls are being audited
          TIF_POLLING_NRFLAG        the idle process is polling the TIF_NEED_RESCHED flag
          TIF_MEMDIE                the process is being destroyed to reclaim memory

        /*  these flags are defined in <arch/x86/include/asm/thread_info.h>  */

      ret_from_intr and ret_from_exception :
        they are two entry points of a piece of kernel code.
        the general flow >
          ret_from_intr:
                  if <nested kernel control paths?>
                          if <virtual 8086 mode?>
                                  goto resume_userspace
                          goto resume_kernel  /*  eflags.VM = 0 and CPL < 3(USER_RPL)  */
                  goto resume_userspace
          resume_kernel:
                  cli
                  if <kernel preemption enabled?>
                          goto need_resched
                  goto restore_all
          need_resched:
                  if <need reschedule?>
                          if <resuming a kernel control path with IF = 0?>
                                  goto restore_all
                          preempt_schedule_irq()
                          goto need_resched
          restore_all:
                  restore hardware context
          resume_userspace:
                  cli
                  if <is there work to be done(rescheduling,signals,single step)?>
                          goto work_pending
                  goto  restore_all
          work_pending:
                  if <need to reschedule?>
                          goto work_resched
                  goto work_notifysig
          work_resched:
                  schedule()
                  goto resume_userspace
          work_notifysig:
                  if <virtual 8086 mode?>
                          save_v86_state()
                          do_notify_resume()
                          goto restore_all
                  do_notify_resume()
                  goto restore_all
          ret_from_exception:
                  cli
                  goto ret_from_intr
 
          /*  some function invoking might not returns,so "goto" means that function will switch
           *  to the next kernel control path.
           */
          /*  difference between ret_from_exception and ret_from intr when kernel preemption compilation
           *  option was selected :
           *    local interrupts(IF) are immediately disabled when returning from exceptions.
           */          
 
        !!  the assembly code of ret_from_intr and ret_from_exception is defined in <arch/x86/kernel/entry_32.S>
 
      resuming a kernel control path :
        resume_kernel:  /*  preemption support  */
          >  disables local interrupt
          >  if current->thread_info.preempt_count == 0
                     jmp need_resched
             else
                     jmp restore_all
          /*  preempt_count == 0 means preemption enabled  */

      checking for kernel preemption :
        need_resched:  /*  when the piece of code is executed,none of the unfinished kernel control path is an
                        *  interrupt handler,otherwise,the preempt_count field would be greater than zero.
                        */
          >  retrieve thread_info.flags
          >  if TIF_NEED_RESCHED != 1
                     jmp restore_all
             elif eflags.IF != 1
                     jmp restore_all
          >  call to preempt_schedule_irq
             /*  this function sets the PREEMPT_ACTIVE flag in the preempt_count field,enables the local interrupts,
              *  invokes schedule() to select another process to run.
              *  when the former process will resume,preempt_schedule_irq() clears the PREEMPT_ACTIVE flag,and disables
              *  local interrupts.
              *  preempt_schedule_irq() is defined in <kernel/sched.c>
              */
          >  jmp need_resched  /*  if the TIF_NEED_RESCHED flag of the current process is set,preempt_schedule_irq()
                                *  will be called again,thus,schedule() also be called again.
                                */
      
      resuming a User Mode program :
        resume_userspace:
          >  disbale local interrupts
          >  ends irq tracing
          >  retrieve thread_info.flags
          >  if TIF_WORK_MASK & thread_info.flags
                     jmp work_pending  /*  there has some works to be done before int/exception return  */
             else
                     jmp restore_all

        /*  work mask : TIF_SYSCALL_TRACE TIF_SYSCALL_AUDIT TIF_SINGLESTEP  */

      checking for rescheduling :
        work_pending:
          >  if thread_info.flags.TIF_NEED_RESCHED
                     jmp work_notifysig

        work_resched:
          >  call schedule
          >  disable local interrupts
          >  ends irq tracing
          >  retrieve thread_info.flags
          >  if thread_info.flags & TIF_WORK_MASK
                     if thread_info.flags & TIF_NEED_RESCHED
                             jmp work_resched
             jmp restore_all

      handling pending signals,virtual-8086 mode,and single stepping :
        work_notifysig:
          >  if eflags.VM == 1
                     jmp work_notify_x86
          >  call do_notify_resume
          >  jmp resume_userspace_sig

        work_notify_x86:
          >  save ti_flags /*  thread_info.flags  */
          >  call save_v86_state
          >  restore ti_flags
          >  call do_notify_resume
          >  jmp resume_userspace_sig

        /*  do_notify_resume() take care of pending signals and single stepping.  */
        /*  #ifdef CONFIG_VM86
         *  #define resume_userspace_sig  check_userspace
         *  #else
         *  #define resume_userspace_sig  resume_userspace
         */

      !  current->thread_info is stored in the process's kernel stack.


Chapter 5 : Kernel Synchronization
    How the Kernel Services Requests :
      two types of requests >
        1>  requests from processes
        2>  requests from devices
    
      the policy adopted by the kernel is the following >
        1>  if a dev-req comes while the kernel is idle,the kernel starts servicing.
        2>  if a dev-req comes while the kernel is servicing a process,the kernel
            stops servicing the process and start servicing the device.
        3>  if a dev-req comes while the kernel is servicing another device,the kernel
            stops servicing the first device and starts servicing the second one.
            when it finish servicing the new device,it resumes servicing the former one. 
        4>  one of the devices may induce the kernel to leave the process being currently
            services.
            after servicing the last request of the devices,the kernel may decide to drop
            temporarily the previous process and to pick up a new one.
            
        1--3>  Nested Execution of Exception and Interrupt Handlers
        4>     Kernel Preemption

      !  if CPU is not executing the code in the Kernel Mode,consider it is idle,or consider
         the kernel is idle and ready to service devices.
      !  dev-req correspond to interrupts(timer,keyboard,...)
         proc-req correspond to system calls or exception raised by User Mode processes

    Kernel Preemption : (Kernel Option : CONFIG_PREEMPT)
      Process Switch >
        >  a planned process switch :
             both in preemptive and nonpreemptive kernels,a process running in Kernel Mode can
             voluntarily relinquish the CPU(i.e. sleeping...)
           forced process switch :
             synchronous events cause a process switching,for instance,an interrupt handler that
             awakes a higher priority process.
             (react to such case,preemptive kernel is differs from nonpreemptive kernel)
  
        >  all process switches are performed by the switch_to macro.
           in both preemptive and nonpreemptive kernels,a process switch occurs when a process has
           finished some thread of kernel activity and the scheduler is invoked.
           however,in nonpreemptive kernels,the current process cannot be replaced unless it is about
           to switch to User Mode.
      
      !  the main characteristic of a preemptive kernel is that a process running in Kernel Mode can
         be replaced by another process while in the middle of a kernel function.

      !  nonpreemptive kernel forbids "forced process switch".
         /*  "forced process switch" also will happens when the process's time quantum expires.  */

      !  reason for making a kernel preemptive :
           reduce the "dispatch latency" of the User Mode processes.
           "dispatch latency" :
             the delay between the time processes become runnable and the time they actually begin running.
      
      kernel preemption is disabled when thread_info.preempt_count > 0,the following cases that preempt_count
      is greater than zero :
        1>  the kernel is executing an interrupt service routine
        2>  the deferrable functions are disabled(always true when the kernel is executing a softirq or tasklet)
        3>  the kernel preemption has been explicitly disabled by setting the preemption counter to a positive 
            value

        !  thus,the kernel can be preempted only when it is executing an exception handler(in particular a system call)
           and the kernel preemption has not been explicitly disabled.
           /*  eflags.IF must be set up,otherwise,kernel preemption is not performed.  */

      macros defined in <linux/preempt.h> to accessing and manipulating preempt_count :
        <linux/preempt.h>
          /*  preempt_count - returns the value of preempt_count.  */
          #define preempt_count()  (current_thread_info()->preempt_count)

          /*  preempt_disable - disable kernel preemption.  */
          #define preempt_disable() \
                  do { \
                          inc_preempt_count(); \
                          barrier();           \
                  } while (0)

          /*  preempt_enable - enable kernel preeption.
           *                   if thread_info.flags & TIF_NEED_RESCHED is T,
           *                   call to preempt_schedule().
           */
          #define preempt_enable() \
                  do { \
                          preempt_enable_no_resched(); \  /*  *_no_resched means no scheduler calling.  */
                          barrier();                   \
                          preempt_check_resched();     \
                  } while (0)

          /*  preempt_schedule() is declared in <linux/preempt.h> and defined in <kernel/sched.c>
           *    if preempt_count > 0 OR irq disabled
           *            do nothing,and return
           *    else
           *            do-while loop until need_resched() returns zero
           *                    set PREEMPT_ACTIVE
           *                    call schedule()
           *                    unset PREEMPT_ACTIVE
           *                    barrier(),wait synchronization
           */

      macros defined in <linux/smp.h> associated to preempt_count :
        <linux/smp.h>
          /*  get_cpu - get cpu id with kernel preemption disabled.  */
          #define get_cpu()  ({ preempt_disable(); smp_processor_id(); })

          /*  put_cpu - put cpu,that is enable kernel preemption.  */
          #define put_cpu()  preempt_enable()

        /*  Linux 2.6,no macro "put_cpu_no_resched()" was defined.  */

      !!  kernel preemption may happen either when a kernel control path is terminated,or when an exception
          handler reenables kernel preemption by means of preempt_enable().
          kernel preemption may happens when deferrable functions are enabled(preempt_count.softirq_count == 0).
      !!

    When Synchronization Is Necessary :
      A critical region is a section of code that must be completely executed by the kernel control path that
      enters it before another kernel control path can enter it.
      interleaving kernel control path,the critical regions may occur in exception handlers,or interrupt handlers,
      or deferrable functions,kernel threads.
      once a critical region has been identified,it must be suitably protected to ensure that any time at most
      one kernel control path is inside that region.
      /*  a single CPU system,a critical region the kernel threads will enter it,just disable interrupt as well;
       *  a single CPU system,a critical region the system call service routines will enter it,just disable
       *  kernel preemption as well.
       *  THINGS IS MORE COMPLICATED ON SMP!(interrupt handler,exception handler,softirq...)
       */

    When Synchronization Is Not Necessary :
      1>  Interrupt handlers and tasklets need not be coded as reentrant functions.
          /*  THAT IS,after CPU received a IRQ,the correspond IRQ line will be disabled by driver until
           *  the interrupt handler finishes execution;
           *  and the same type tasklets cannot be executed by two or more CPUs simultaneously.
           */
      2>  Per-CPU variable accessed by softirqs and tasklets only do not require synchronization.
          /*  Different CPU has different Per-CPU variable.
           *  Softirq can run concurrently on several CPUs,so each Per-CPU variable is not same.
           *  tasklets have same type can not run on two or more CPUs at same time;if there are
           *  two tasklets with different types,they can run on two CPUs at same time,but the
           *  Per-CPU variable is not same.
           *  If Per-CPU variable is accessed by kernel threads(preemptible),then it must be
           *  protected by some synchronizing-mechanisms. 
           */

      3>  A data structure accessed by only one kind of tasklet does not require synchronization.
          /*  same type tasklets can not run simultaneously on two or more CPUs.  */

    !  the best synchronization technique consists in designing the kernel so as to avoid the need for
       synchronization in the first place.

    Synchronization Primitives :
      Linux kernel synchronization techniques :
        1>  Pre-CPU variables (all CPUs)
            duplicate a data structure among the CPUs

        2>  Atomic operation (all CPUs)
            atomic read-modify-write instruction to a counter
            
        3>  Memory barrier (local CPU or all CPUs)
            avoid instruction reordering

        4>  Spin lock (all CPUs)
            lock with busy wait

        5>  Semaphore (all CPUs)
            lock with blocking wait(sleep)

        6>  Seqlocks (all CPUs)
            lock based on an access counter

        7>  Local interrupt disabling (local CPU)
            forbid interrupt handling on a single CPU

        8>  Local softirq disabling (local CPU)
            forbid deferrable function handling on a single CPU

        9>  Read-Copy-Update(RCU) (all CPUs)
            lock-free access to shared data structures through pointers

      Per-CPU Variables :
        the simplest and most efficient synchronization technique.
        basically,a per-CPU variable is an array of data structures,one element
        per each CPU in the system.(__per_cpu_offset[NR_CPUS] used to save the
        offset values for per-CPU variable,each offset value corresponding to
        a CPU in the system.)

        a CPU only be allowed to operates its own element,there is no race condition.
        but per-CPU variables can be used only in particular cases--basically,when
        it makes sense to logically split the data across the CPUs of the system.

        !  per-CPU variable aligned in main memory so that each data structures falls
           on a different line of the hardware cache,concurrent accesses to the per-CPU
           array do not result in cache line snooping and invalidation.

        !  per-CPU variable provides protection against concurrent accesses from several
           CPUs,but can not protects accesses from asynchronous functions,in theses cases,
           additional synchronization technique is required.

        !  per-CPU variable must be accessed while kernel preemption is disabled.

        per-CPU variables operating primitives :
          <linux/percpu-defs.h>

            /*  __PCPU_ATTRS - set up percpu attributes for current cpu.
             *  #  This macro would extends to assembly language.
             */
            #define __PCPU_ATTRS(sec)  \
                    __percpu_attributes__((section(PER_CPU_BASE_SECTION sec)))  \
                    PER_CPU_ATTRIBUTES
  
            /*  section for the DECLARE and for the DEFINE must match  */
  
            #if !defined(ARCH_NEED_WEAK_PER_CPU) && !defined(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)

            /*  normal per-CPU definitions  */
            /*  DECLARE_PER_CPU_SECTION - declare a section for percpu data.
             *                            the section had been defined on other place.
             */
            #define DECLARE_PER_CPU_SECTION(type, name, sec)  \
                    extern __PCPU_ATTRS(sec) __typeof__(type) name
  
            /*  DEFINE_PER_CPU_SECTION - define a section for percpu data.  */
            #define DEFINE_PER_CPU_SECTION(type, name, sec)  \
                    __PCPU_ATTRS(sec) PER_CPU_DEF_ATTRIBUTES \
                    __typeof__(type) name
                    /*  assembly data object in the section ".data" or ".data.percpu",
                     *  type is __typeof__(type),
                     *  symbol is @name.
                     *  ".data" and ".data.percpu" means switch to the corresponding section.
                     */
  
            /*  DECLARE_PER_CPU - declare a per-CPU variable @name with type is @type.  */
            #define DECLARE_PER_CPU(type, name)  \
                    DECLARE_PER_CPU_SECTION(type, name, "")
  
            /*  DEFINE_PER_CPU - define a per-CPU variable @name with type is @type.  */
            #define DEFINE_PER_CPU(type, name)   \
                    DEFINE_PER_CPU_SECTION(type, name, "")
  
            #endif

          <asm-generic/percpu.h>
            #ifdef CONFIG_SMP
            #define PER_CPU_BASE_SECTION  ".data.percpu"

            /*  per_cpu - get the per-CPU variable for local CPU.
             *            SHIFT_PERCPU_PTR() will vertify @var at first,then
             *            call to RELOC_HIDE(&var, __per_cpu_offset[cpu]),
             *            which returns "&var + @offset",the address is the
             *            per-CPU variable instance for the certain cpu @cpu.
             *            finally,derefer to the address for get the per-CPU variable.
             */
            #define per_cpu(var, cpu)  \
              (*SHIFT_PERCPU_PTR(&(var), per_cpu_offset(cpu)))

            /*  __get_cpu_var - select the per-CPU variable @var in local CPU.  */
            #define __get_cpu_var(var)  \
              (*SHIFT_PERCPU_PTR(&(var), my_cpu_offset))

            #else
            #define PER_CPU_BASE_SECTION  ".data"

            /*  NONSMP versions  */
            #define per_cpu(var, cpu)  (*((void)(cpu), &(var)))
            #define __get_cpu_var(var)  (var)

            #endif

          <linux/percpu.h>
            /*  get_cpu_var - get @var from current CPU with preemption disabling.  */
            #define get_cpu_var(var)  (*({  \
                    preempt_disable();       \
                    &__get_cpu_var(var); }))

            /*  put_cpu_var - put @var to current CPU and enable preemption.
             *  #  It is not really write-back value to @var in current CPU,just
             *     enable preemption.
             *     write a per-CPU variable :
             *       assume there is a per-CPU variable is named "pCPUv" which is type of int,
             *       in local CPU.
             *       per_cpu(pCPUv, my_cpu_offset) = 16;  //  okay,macro extending => *Pointer = 16
             */
            #define put_cpu_var(var)  do {  \
                    (void)&(var);           \
                    preempt_enable();       \
                    } while(0)

            /*  get_cpu() and put_cpu() respectively disable preemption and enable preemption.  */

            extern void __percpu *__alloc_percpu(size_t size, size_t align);
            /*  alloc_percpu - alloc a per-CPU variable.
             *  @type:         the type of the per-CPU variable.
             *  return:        pointer points to the per-CPU variable.
             */
            #define alloc_percpu(type)  \
                    (typeof(type) __percpu *)__alloc_percpu(sizeof(type), __alignof__(type)))

            /*  free_percpu - destroy a per-CPU variable pointer got previously from alloc_percpu().
             *  @__pdata:     the pointer to be destroyed.
             */
            extern void free_percpu(void __percpu *__pdata);

            #ifdef CONFIG_SMP

            /*  per_cpu_ptr - return a pointer is typeof(ptr),the address is equal to 
             *                A := ptr + __per_cpu_offset[cpu].
             *                __per_cpu_offset is an array of unsigned long and size is
             *                equal to "sizeof(unsigned long) * NR_CPUS".
             *  !!  per_cpu_offset() is the offset that has to be added to a percpu variable
             *      to get to the instance for a certain processor.
             *  !!  __per_cpu_offset[NR_CPUS] is defined in <arch/x86/kernel/setup_percpu.c>.
             */
            #define per_cpu_ptr(ptr, cpu)  SHIFT_PERCPU_PTR((ptr), per_cpu_offset((cpu)))

            #else

            #define per_cpu_ptr(ptr, cpu)  ({ (void)(cpu); (ptr); })

            #endif

      Atomic Operations : 
        race condition in "read-modify-write" :
          p1 read 0x7fffffff32 in 4 bytes -> p1 modify the value readed from the memory unit -> write back
          result to 0x7fffffff32 in 4 bytes.
          p2 do the some things.
          !  But p1 and p2 process interleaving.
             p1 read -> p2 read -> p2 modify -> p1 modify -> p1 write back -> p2 write back
             now incorrect value is in 0x7fffffff32.

        memory arbiter : a hardware circuit that serializes accesses to the RAM chips.

        atomic operation : such "read-modify-write" operations can not be interrupted by other kernel control path.

        80x86 atomic operation mechanisms :
          1>  assembly language instructions that make zero or one aligned memory access are atomic.
              /*  an unaligned memory access is not atomic.  */
          2>  read-modify-write assembly language instructions(such as inc or dec),
              no other processor is granted to takes the memory bus if there is processor has been readed before write.
              memory bus stealing never happens in a uniprocessor system.
          3>  assembly language instructions whose opcode is prefixed by a rep byte(0xf2,0xf3,which forces the control
              unit to repeat the same instruction several times) are not atomic.
              the control unit checks for pending interrupts before executing a new iteration.
              /*  register ecx as the counter  */

        atomic_t type :
          linux kernel provides a special atomic_t type(an atomically accessible counter) and some functions and macros
          that act on atomic_t variables and are implemented as single,atomic assembly language instructions.
          On multiprocessor systems,each such instruction is prefixed by a "lock" byte.
          /*  macro LOCK_PREFIX is defined in <arch/x86/include/asm/alternative.h> used to replace the LOCK and LOCK
           *  LOCK_PREFIX macro used everywhere in the source code tree.
           *  #  LOCK instruction prefix is introduced in x86 architecture.
           */

          /*  POSIX provides sig_atomic_t type,operations on it is guaranteed no interuppts in middle.
           *  introduced in header <signal.h>
           *  normally,operation on int type and pointer type all is atomic.
           */

          atomic operations :
            <asm-generic/atomic.h>  /*  architecture independ  */
            /*  <arch/x86/include/asm/atomic.h>  architecture depend  */

              atomic_read(v)                ->       return *v
              atomic_set(v, i)              ->       *v = @i
              atomic_add(i, v)              ->       *v += @i
              atomic_sub(i, v)              ->       *v -= @i
              atomic_sub_and_test(i, v)     ->       return (*v -= @i) ? 0 : 1
              atomic_inc(v)                 ->       ++*v
              atomic_dec(v)                 ->       --*v
              atomic_dec_and_test(v)        ->       return (--*v) ? 0 : 1
              atomic_inc_and_test(v)        ->       return (++*v) ? 0 : 1
              atomic_add_negative(i, v)     ->       return ((*v += @i) < 0) ? 1 : 0
              atomic_inc_return(v)          ->       return ++*v
              atomic_dec_return(v)          ->       return --*v
              atomic_add_return(i, v)       ->       return *v += @i
              atomic_sub_return(i, v)       ->       return *v -= @i

              atomic_clear_mask(mask, addr) ->       clear all bits of *addr specified by @mask
              atomic_set_mask(mask, addr)   ->       set all bits of *addr specified by @mask

            <arch/x86/include/asm/bitops.h>
              test_bit(nr, addr)            ->       return the value of the @nr_th bit of *addr
              set_bit(nr, addr)             ->       set the @nr_th bit of *addr
              clear_bit(nr, addr)           ->       clear the @nr_th bit of *addr
              change_bit(nr, addr)          ->       invert the @nr_th bit of *addr
              test_and_set_bit(nr, addr)    ->       set the @nr_th bit of *addr and return its old value
              test_and_clear_bit(nr, addr)  ->       clear the @nr_th bit of *addr and return its old value
              test_and_change_bit(nr, addr) ->       invert the @nr_th bit of *addr and return its old value

      Optimization and Memory Barriers :
        optimizing compiler may reorder assembly language instructions,and modern CPU may process
        instructions parallely(depend on CPU architecture) thus memory access order may be changed,
        these must be avoided when dealing with synchronization.

        optimization barrier primitive ensures that the assembly language instructions corresponding
        to C statements placed before the primitive are not mixed by the compiler with the instructions
        corresponding to the C statements after the primitive.

        Linux barrier() macro :
          <linux/compiler-gcc.h>
            /*  keyword 'memory' forces the compiler to assume that all memory locations in RAM have been
             *  changed by the assembly language instruction.
             *  thus,the compiler cannot optimize the code by using the values of memory locations stored
             *  in CPU registers before the "__asm__" instruction.
             *  Note : optimization barrier do not forbid CPU mix the assembly language instructions.
             *         this is only take affect to assembly language source code.
             *         this is memory barrier.
             */
            #define barrier()  __asm__ __volatile__("": : :"memory")

          e.g.
            int x = 3;
            int y = x * 2;
            int z = 0;

            z = z + x * 2 + y;
            fprintf(stdout, "%d", z);
            /*  the instructions before barrier() will be finished before the statement "z = 6;"
             *  start.
             */

            barrier();

            z = 6;
            /*  suppose,int is not atomic,and compiler place "z = 6;" front to "z = z + x * 2 + y;",
             *  that is 
             *    z = 6;
             *    fprintf(stdout, "%d", z);  /*  print 6,but we expect 12  */
             *    z = z + x * 2 + y;
             */
    
        80x86,the assembly language instructions are said to be "serizalizting" :
          1>  all instructions that operate on I/O ports.
          2>  all instructions prefixed by the "lock" byte.
          3>  all instructions that write into control registers,system registers,or
              debug registers.(cli, sti, eflags.IF)
          4>  the 'lfence', 'sfence', 'mfence' assembly language instructions.
                   RMB       WMB       WRMB
          5>  a few special assembly language instructions;among them,the 'iret' instruction
              that terminates an interrupt or exception handler.

        Linux memory barrier macros :
          <arch/x86/include/asm/system.h>  /*  architecture depends  */
          /*  <asm-generic/system.h>           architecture independs  */

          /*  the following are X86_64 macros,X86_32 macros also are defined in the same file.
           *  e.g.
           *    #define mb()  alternative("lock; addl $0,0(%%esp)", "mfence", X86_FEATURE_XMM2)
           */

          /*  mb - memory barrier for MP and UP.  */
          #define mb()      asm volatile("mfence":::"memory")
        
          /*  rmb - read memory barrier for MP and UP.  */
          #define rmb()     asm volatile("lfence":::"memory")

          /*  wmb - write memory barrier for MP and UP.
           *  # some intel microprocessors never reorder write memory access,so this macro may
           *    instead of "barrier()",but it is keep compiler do not reorder instructions.
           */
          #define wmb()     asm volatile("sfence":::"memory")

          /*  smp_mb - CONFIG_SMP,otherwise "barrier()".  */
          #define smp_mb()  mb()

          /*  smp_rmb - CONFIG_X86_PPRO_FENCE,otherwise "barrier()".  */
          #define smp_rmb() rmb()

          /*  smp_wmb - CONFIG_X86_OOSTORE,otherwise "barrier()".  */
          #define smp_wmb() wmb()

          /*  the macros prefixed "smp_" only take affect on multiprocessor system.
           *  no such prefix are take affect on multiprocessor system and uniprocessor system.
           */

      Spin Lock :
        spin locks are a special kind of lock designed to work in a multiprocessor environment.
        if a spin lock had been holden by another kernel control path,then current kernel control path
        who is waiting for the lock will repeatedly executing a tight instruction loop,until the lock
        is released.

        !  the instruction loop of spin locks represents a "busy wait".the waiting kernel control path
           keeps running on the CPU,even if it has nothing to do besides waste time.
           some works resource in kernel space may be locked a fraction of a millisecond only,spin lock
           would be far more time-conmusing to release the CPU and reacquire it later.

        !  kernel preemption is disabled in every critical region protected by spin locks.
           kernel preemption is still enabled in "busy wait" phase.

        <linux/spinlock_types.h>
          /*  raw_spinlock_t - raw spinlock type.
           *  @raw_lock:       it is the architecture depends spinlock structure,
           *                   which is defined in <arch/x86/include/asm/spinlock_types.h>,
           *                   typedef struct arch_spinlock arch_spinlock_t
           *                   it has member @slock is type of unsigned int .
           *  @break_lock:     flag signaling that a process is busy waiting for the lock.
           */
          typedef struct raw_spinlock {
                  arch_spinlock_t raw_lock;
          #ifdef CONFIG_GENERIC_LOCKBREAK
                  unsigned int break_lock;
          #endif
                  ...
          } raw_spinlock_t;

          typedef struct spinlock {
                  union {
                          struct raw_spinlock rlock;
                  };
          } spinlock_t;

          !!  UP and SMP have the different __ARCH_SPIN_LOCK_UNLOCKED macro definitions.
              UP :
                macro __ARCH_SPIN_LOCK_UNLOCKED is defined in <linux/spinlock_types_up.h>
                extend to { 1 } or { }  /*  CONFIG_DEBUG_SPINLOCK or !CONFIG_DEBUG_SPINLOCK  */
                on uniprocessor environment,spin lock is useless,because everytime only one
                kernel control path is executing.
                thus,acquire spin lock just decrease @slock in 1 and increase @slock in 1 to
                release spin lock.

              SMP :
                macro __ARCH_SPIN_LOCK_UNLOCKED is defined in <arch/x86/include/asm/spinlock_types.h>
                extend to { 0 }
                on Symmetrical Multi-Processor environment,spin lock is based ticket concept.a FIFO
                is constructed to queue the CPUs acquire the spin lock.@slock is encoded to represents
                the FIFO.



        <linux/spinlock.h>
          /*  spin_lock_init - initializer.
           *  @_lock:          a pointer to spinlock_t.
           */
          #define spin_lock_init(_lock)     \
          do {                              \
                  spinlock_check(_lock);    \
                  raw_spin_lock_init(&(_lock)->rlock);  \
          } while (0)

          /*  spin_lock - lock a spinlock.
           *  @lock:      a pointer point to spinlock_t object.
           */
          static inline void spin_lock(spinlock_t *lock);

          /*  spin_unlock - unlock.  */
          static inline void spin_unlock(spinlock_t *lock);

          /*  spin_unlock_wait - wait until spinlock gets unlocked.
           *                     it just query whether the spinlock
           *                     gets unlocked.
           *  #  arch_spin_unlock_wait() will be called,
           *     which enter a null cycle until the spinlock is released.
           */
          static inline void spin_unlock_wait(spinlock_t *lock);

          /*  spin_is_locked - it is locked?  */
          static inline void spin_is_locked(spinlock_t *lock);

          /*  spin_trylock - try to get spinlock.
           *                 return 1 -> succeed to get lock
           *                 return 0 -> failed to get lock               
           */
          static inline void spin_trylock(spinlock_t *lock);

          /*  all the functions above call to the corresponding raw_spin_*() functions,and them
           *  will call to _raw_spin_*() APIs,these APIs are declared in <linux/spinlock_api_smp.h>,
           *  they call to the primary functions with prefix "__raw_spin_".
           *  if CONFIG_INLINE_SPINLOCK was defined,then the primary functions are defined in the same
           *  header of _raw_spin_*(),otherwise,use the function definition in <kernel/spinlock.c> to
           *  instead.
           *  macron BUILD_LOCK_OPS(op, locktype) was defined in <kernel/spinlock.c>,this macro generate
           *  lockfunc such "__raw_##op##_lock(locktype##_t *lock)".
           */

        /*  very interesting things i had found out :
         *    the architecture specified functions in <arch/x86/include/asm/spinlock.h>
         *
         *      void __ticket_spin_lock(arch_spinlock_t *lock)
         *        asm volatile (
         *                LOCK_PREFIX "xaddw %w0, %1\n\t"  
         *                ...
         *                "cmpb %h0, %b0\n\t"
         *                "je 2f\n\t"  #  return
         *                ...
         *        );
         *        #  %0 -> @inc(0x0100) variable =Q (eax | ebx | ecx | edx)
         *           %h0 -> high 8-bits of the register  (i.e. ah)
         *           %b0 -> below b-bits of the register (i.e. al)
         *           %1 -> memory address of lock->slock
         *        #  xadd {  "w" a word
         *                   exchange %0 and %1
         *                   add %0 to %1,save result in %1
         *           }
         *        #  cmpb {
         *                   %b0 - %h
         *                   if result = 0 => ZF = 1
         *                   else => ZF = 0
         *           }
         *        #  default value of @lock->slock is zero,@lock->slock += 0x0100 to acquire lock
         *           high 8-bits equal to low-8-bits means no one is holding the lock
         *
         *      int __ticket_spin_trylock(arch_spinlock_t *lock)
         *        asm volatile (
         *                "movzwl %2, %0\n\t"  #  %2 -> &lock->slock, %0 -> tmp variable in eax
         *                                     #  zero-extend to eax,contents in ax is copied from
         *                                     #  @lock->slock
         *                "cmpb %h0, %b0\n\t"  #  same as above
         *                ...
         *        );
         *    !!  lock primitive waits until spin lock is able to be locked.
         *        trylock primitive does not wait.if it is can not get lock,return 0,otherwise,1.
         *
         *    these functions are implemented via inline assembly.
         *    ticket concept is introduced,that is :
         *      ticket is a queue has two parts,one is head,one is tail
         *      the @slock becomes a FIFO
         *      when need to acquire the lock,just atomically noting the tail and increase it by one
         *      then,wait until head becomes the initial value of the tail
         *
         *    so,acquire lock is not decrease @slock by one,it is increase @slock by 0x0100(256).
         *    unlock stay increase @slock by one.
         *
         *    Ticket Queue  <- @slock
         *    [high-32bits][high-16bits][high-8bits][low-8bits]  <= unsigned int
         *                               ah          al
         *                               TAIL        HEAD
         *      increment := 0x0100 -> 256     <- x86
         *      increment := 0x00000100 -> 256 <- x86-64
         *      ah in default : 00000000
         *      al in default : 00000000
         *  
         *      x86 =>
         *        kcpA try acquire lock
         *          ah = al => no one is holding the lock now
         *          ax += 0x0100
         *            =>  ah : 00000001
         *                al : 00000000
         *                ax : 0000000100000000
         *         
         *        kcpB try acquire lock
         *          ah != al => someone is holding the lock now
         *          set @condition_indicator := ax
         *            =>  ah : 00000001
         *                al : 00000000
         *          ax += 0x0100  -> queue itself to the tail of FIFO
         *            =>  ah : 00000010
         *                al : 00000000
         *          kcpB spin,enter a cycle
         *            movb @slock->al, @condition_indicator->al
         *            check if @condition_indicator->ah = @condition_indicator->al
         *       
         *        kcpA release lock
         *          @slock++
         *            =>  ah : 00000010
         *                al : 00000001
         *                ax : 0000001000000001
         *       
         *        kcpB finds out lock had been released
         *          movb @slock->al, @condition_indicator->al
         *            =>  ah : 00000001
         *            =>  al : 00000001
         *          lock is available
         *          @slock : 00000010 00000001
         *          return
         *
         *    even so,spin lock is stay allows one kernel control path to holding it at the same time.
         */

        The spin_lock macro with kernel preemption :
          when try to get a spinlock,the caller must call to spin_lock() function.
          which call to raw_spin_lock(),next the _raw_spin_lock() is called,finally,__raw_spin_lock() would
          be called.

          /*  kernel control path is not allowed to be preempted when it holds a spinlock.  */

          the following actions that __raw_spin_lock() takes :
            1>  enter a infinite for-cycle.
            2>  disable kernel preemption.
            3>  call to do_raw_spin_trylock(),thus,the architecture depended function arch_spin_lock()
                would be called,which is defined in <arch/x86/include/asm/spinlock.h> as a inline function.
            4>  if succeed to get the spinlock,break the cycle and return,otherwise,enable kernel preemption
                and get to the next.
            5>  set @lock->break_lock := 1,enter a nested cycle.
            6>  while !raw_spin_can_lock AND (lock)->break_lock
                        arch_spin_relax(&lock->raw_lock)
                /*  raw_spin_can_lock() checks whether the lock is able to be holden.
                 *  lock->break_lock = 1 means a kernel control path is waiting on the lock,but kernel
                 *  is allowed to be preempted.
                 *  arch_spin_relax() is equal to cpu_relax().
                 */
                
                if the lock can be holded,then break while-cycle and try to get the lock again.(jump to 2>)

        The spin_lock macro without kernel preemption(uniprocessor) :
          if no kernel preemption is configured,and no spinlock debug is configured too,
          then,the raw_spinlock_t is a NULL structure.
          for uniprocessor system,macro _raw_spin_lock() is macro __LOCK(),which just a compiler attribute
          "__context__(expression, in_context, out_context)".

          the __LOCK() just disable kernel preemption as well,it extended to :
            <linux/spinlock_api_up.h>
              /*  __LOCK - disable kernel preemption,check if lock is equal to 1(compiler checking).
               *  #  if kernel is not configured with preemption,then preempt_disable() do nothing,
               *     otherwise,the preempt count would increase and barrier() would be called.
               *  #  __acquire(x) is defined in <linux/compiler.h>,which is "__conext__(x,1)",
               *     conversely,__release(x) is defined in the same file,which is "__context__(x,-1)".
               */
              #define __LOCK(lock)  \
                do { preempt_disable(); __acquire(lock); (void)(lock); } while (0)

            /*  no CONFIG_PREEMPT on smp,it is same to CONFIG_PRREMPT configured on smp,but
             *  preempt_disable() and preempt_enable() do nothing as well.
             *  !  UP SPINLOCK DEFINITION IS DIFF TO SMP SPINLOCK DEFINITION.
             */

        The spin_unlock macro :
          it add 1 to spinlock_t object for release it(0->locked,1->unlocked),and then enable kernel preempt.
          /*  "lock" byte is not used,because write-only access always atomically executed by the current
           *  80x86 microprocessors.
           */

      Read/Write Spin Locks :
        rw spin lock,it is a lock allow more than one readers hold it for reading at same time;only one writer
        is allowed to holds it for writing at same time.

        <arch/x86/include/asm/rwlock.h>
          /*  RW_LOCK_BIAS - indicates the initializing state.no reader and no writer.  */
          #define RW_LOCK_BIAS        0x01000000

        <linux/rwlock_types.h>
          /*  rwlock_t - linux rw spin lock type.
           *  @raw_lock: architecture depends rw spin lock type.it is defined in
           *             <arch/x86/include/asm/spinlock_types.h>,
           *             it has a member @lock is type of unsigned int.
           *  @break_lock: whether there is a kernel control path waiting for the rw spin lock currently.
           *  #  @lock is encoded to includes two pieces of information :
           *       1>  bits [0, 23] -> the number of kernel control paths currently reading.
           *       2>  bit 24 -> unlock flag.1 -> unlocked, 0 -> locked.
           */
          typedef struct {
                  arch_rwlock_t raw_lock;
          #ifdef CONFIG_GENERIC_LOCKBREAK
                  unsigned int break_lock;
          #endif
                  ...
          } rwlock_t;

          /*  rwlock_init - initialize a rwlock_t object *@lock.
           *  @lock:        pointer points to an object is type of rw_lock_t.
           *  #  this version is no CONFIG_DEBUG_SPINLOCK defined.
           *  #  raw_lock.lock := RW_LOCK_BIAS
           *     break_lock := 0
           */
          #define rwlock_init(lock)        \
                  do { *(lock) = __RW_LOCK_UNLOCKED(lock); } while (0)

        Getting and releasing a lock for reading :
          <linux/rwlock.h>
            /*  read_lock - lock a rw spin lock for reading.
             *  @lock:      pointer points to an object is type of rwlock_t.
             */
            #define read_lock(lock)        _raw_read_lock(lock)

            /*  read_unlock - unlock a locked rwlock is for reading.
             *  @lock:        pointer points to an object is type  of rwlock_t which had been
             *                locked for reading.
             *  #  call list:
             *       _raw_read_unlock() -> __raw_read_unlock() {
             *                                     rwlock_release();
             *                                     do_raw_read_unlock();
             *                                     preempt_enable();
             *                             }
             */
            #define read_unlock(lock)      _raw_read_unlock(lock)

            /*  do_raw_read_trylock - try to get rwlock for reading.
             *  @rwlock:              pointer points to an object is type of rwlock_t.
             */
            #define do_raw_read_trylock(rwlock)        arch_read_trylock(&(rwlock)->raw_lock)

          <linux/rwlock_api_smp.h>
            /*  _raw_read_lock - internal rountine for get rwlock to reading.
             *  @lock:           rwlock_t pointer,can not be NULL.
             *  #  this functions is defined in <kernel/spinlock.c>,if no
             *     CONFIG_INLINE_READ_LOCK defined.in the case,this function
             *     calls to __raw_read_lock(),which is build by macro BUILD_LOCK_OPS().
             *     thus,__raw_read_lock() is similar to __raw_spin_lock(),
             *     do_raw_read_trylock() would be called for get the rwlock.
             */
            void __lockfunc _raw_read_lock(rwlock_t *lock)        __acquires(lock);

          <arch/x86/include/asm/spinlock.h>
            /*  arch_read_trylock - architecture depended trylock for rwlock to reading.
             *  @lock:              pointer points to an object is type of arch_rwlock_t.
             *  return:             1 -> got lock,0 -> failed.
             */
            static inline int arch_read_tyrlock(arch_rwlock_t *lock);

            /*  this function used atomic operations on @lock :
             *    1>  convert @lock to atomatic_t *,because the structure just one member arch_rwlock_t.lock,
             *        so there is no padding in the structure,and the member is placed in the head of structure.
             *    2>  call to atomic_dec_return(@lock),it does --*v .
             *    3>  if ret value @v >= 0,then it is succeed to hold the lock for reading.
             *        the value maybe 0x01000000 - 1 = 0x00ffffff,or 0x00ffffff - 1 ...
             *        !  if a negative value is returend(0x00000000 - 1),that means there is a writer holding
             *           the rwlock,then call to atomic_inc() to restore the value of @lock.
             *    4>  if no writing is holding this lock,return 1,otherwise,increase @v and then return 0.
             *    !!  EVEN arch_rwlock_t.lock IS TYPE OF unsigned int,BUT atomatic_t IS TYPE OF int,THUS NO
             *        INCORRECT RESULT WILL APPEAR.
             */

          !!  it is also forbid holding rwlock to sleep,thus kernel preempt will be disabled after got the lock.
              rwlock locking action is busy waiting,too.
          !!  uniprocessor version of _raw_read_lock() is defined in <linux/spinlock_api_up.h>,it is a macro would
              be insteaded by "__LOCK(lock)".
          !!  Linux-2.6.34.1,BUILD_LOCK_OPS() macro builds preemption-friendly version for spin lock functions.
              thus,even kernel preemption is not selected,__raw_read_lock() stay defined by this macro.(no
              CONFIG_INLINE_READ_LOCK)
              "understanding the linux kernel" indicates if no kernel preemption is configured,read_lock() macro
              would be expanded to some assembly language code.
              /*  ENTRY(__read_lock_failed) stay existed in <arch/x86/lib/rwlock_64.S>,and arch_read_lock() will
               *  jump to the entry if failed to got rwlock for reading.actually,arch_read_lock() is composed in
               *  some assembly language code,not in C language code.
               */

        Getting and releasing a lock for writing :
          it is similar to getting and releaing a lock for reading.
          <linux/rwlock.h>
            /*  write_lock - get rwlock for writing.
             *  @lock:       pointer points to an object is type of rwlock_t.
             *  #  call list:
             *       _raw_write_lock() -> __raw_write_lock() -> do_raw_write_trylock() -> arch_write_trylock()
             */
            #define write_lock(lock)        _raw_write_lock(lock)

            /*  write_unlock - unlock a locked rwlock is for writing.
             *  @lock:         pointer points to an object is type of rwlock_t which had been locked for
             *                 writing.
             *  #  call list:
             *       _raw_write_unlock() -> __raw_write_unlock() {
             *                                      rwlock_release();
             *                                      do_raw_write_unlock();
             *                                      preempt_enable();
             *                              }
            #define write_unlock(lock)      _raw_write_unlock(lock)

          <arch/x86/include/asm/spinlock.h>
            /*  arch_write_trylock - architecture depended trylock for rwlock to writing.
             *  @lock:               pointer points an object is type of arch_rwlock_t.
             *  return:              1 -> succeed,0 -> failed.
             *  #  this function is similar to arch_read_trylock(),but it call to
             *     atomic_sub_and_test(RW_LOCK_BIAS, @lock) and atomic_add(RW_LOCK_BIAS, @lock) to
             *     instead atomic_dec_return(),atomic_add(),respectively.
             *     if @lock = RW_LOCK_BIAS(0x01000000)
             *       atomic_sub_and_test() return 1  /*  (*v -= @i) ? 0 : 1  */
             *       that means no kernel control path is holding this rwlock now
             *     else
             *       ret value = 0
             *       then,atomic_add() restore @lock
             *       /*  x := @lock - RW_LOCK_BIAS
             *        *  @lock := x + RW_LOCK_BIAS
             *        */
             */
            static inline int arch_write_trylock(arch_rwlock_t *lock);

      Seqlocks :
        sequence lock is similar to rw spin lock,but writer has higher priority than readers.
        of course,only one writer is activing at the same time,and reader may have to reads the 
        share data structure several times for get the valid copy.

        <linux/seqlock.h>
          /*  seqlock_t - a structure type represent seqlock.
           *  @sequence:  sequence value,only writer is allowed to change it.
           *              even number -> no writer is activing
           *              odd number  -> a writer is activing
           *  @lock:      spin lock as the basic lock for seqlock.
           */
          typedef struct {
                  unsigned sequence;
                  spinlock_t lock;
          } seqlock_t;

          /*  SEQLOCK_UNLOCKED - initialize value for the seqlock.
           *                     it equals to {0, __SPIN_LOCK_UNLOCKED(old_style_seqlock_init)}.
           */
          #define SEQLOCK_UNLOCKED        __SEQLOCK_UNLOCKED(old_style_seqlock_init)

          /*  seqlock_init - macro for initialize a seqlock @x.
           *                 the state of @x after this macro completed is same as
           *                 *@x = SEQLOCK_UNLOCKED
           *  @x:            pointer of seqlock.
           */
          #define seqlock_init(x)

          /*  write_seqlock - lock function for writer on seqlock.
           *  @sl:            pointer of seqlock.
           *  #  this function increase @sl->sequence and lock @sl->lock .
           */
          static inline void write_seqlock(seqlock_t *sl);

          /*  write_sequnlock - unlock function for writer on seqlock.
           *  @sl:              pointer of seqlock.
           *  #  this function increase @sl->sequence and unlock @sl->lock .
           */
          static inline void write_sequnlock(seqlock_t *sl);

          /*  because the basic lock is spin lock,thus when writer lock a seqlock,the kernel
           *  preempt is automatically disabled,and automatically enabled when the writer
           *  unlock the seqlock.
           *  stack order : lock spin lock -> ++sequence (lock)
           *                ++sequence     -> unlock spin lock (unlock)
           */

          /*  read_seqbegin - seqlock routine for reader begin to access the share data structure
           *                  which is protected by a seqlock.
           *  @sl:            pointer of seqlock.
           *  return:         @sl->sequence and it is a even number.
           *  #  this function should be called only once when reader enter a critical region
           *     to access the shared data.
           */
          static __always_inline unsigned read_seqbegin(const seqlock_t *sl);

          /*  read_seqretry - seqlock routine for reader to check if @sl->sequence == @start.
           *  @sl:            pointer of seqlock.
           *  @start:         a value which should get from read_seqbegin().
           *  return:         @sl->sequence != @start
           *  #  this function is used check whether a writer is writing the shared data.
           *     if @sl->sequence == @start(from read_seqbegin(),a even number),that means no writer
           *     is activing.
           *     else,a writer is activing and the copy readed previously is not completed,reader
           *     should read it again.
           */
          static __always_inline int read_seqretry(const seqlock_t *sl, unsigned start);

          e.g.
            =>  seqlock_t *seqlock
            unsigned int ret = 0;
            do {
                    ret = read_seqbegin(seqlock);  /*  ret must be a even number,otherwise,
                                                    *  read_seqbegin() will waiting until the writer
                                                    *  leaved.
                                                    */
                    /*  critical region,read the copy of shared data  */
                    ...
            } while (read_seqretry(seqlock, ret));
                    /*  if @ret != @seqlock->sequence
                     *          a writer changed the shared data
                     *          OR
                     *          a writer is activing
                     *  each case,reader have to read shared data again.
                     */

        seqcount_t :
          sequence mechanism for the code has its own mutex to protects the critical region.

          <linux/seqlock.h>
            /*  seqcount_t - version of sequence only.  */
            typedef struct {
                    unsigned int sequence;
            } seqcount_t;

          operations :
            read_seqcount_begin()
            read_seqcount_retry()
            write_seqcount_begin()
            write_seqcount_end()

            /*  obviously,suffix for write functions is "begin" or "end",because no lock in seqcount_t.  */

        rules for seqlock usage :
          1>  the structure does not include pointers that are modified by the writers and
              dereferenced by the readers.
          2>  the code in the critical regions of the readers does not have side effects.
              (otherwise,multiple reads would have different effects from a single read)
          3>  the critical regions of the readers should be short and writers should seldom acquire the seqlock.
              (read_seqbegin() is busy wait)

      Read-Copy Update (RCU) :
        RCU is lock free,allows many readers and many writers to proceed concurrently.
        it is designed to protect data structures that are mostly accessed for reading by several CPUs.

        the key for RCU :
          1>  only data structures that are dynamically allocated and referenced by means of pointers
              can be protected by RCU.
          2>  no kernel control path can sleep inside a critical region protected by RCU!!

        <linux/rcupdate.h>
          /*  rcu_head - rcu structure,usually embedded inside the data structure to be freed.
           *  @next:     next update requests in a list.
           *  @func:     actual update function to call after the grace period.
           */
          struct rcu_head {
                  struct rcu_head *next;
                  void (*func)(struct rcu_head *head);
          };

          /*  rcu_read_lock - mark the beginning of an RCU read-side critical section.  */
          static inline void rcu_read_lock(void);

          /*  rcu_read_unlock - mark the end of an RCU read-side critical section.  */
          static inline void rcu_read_unlock(void);

          !!  RCU mechanism do very little for read-side to prevent race conditions,write-side work a bit more.
          !!  no rcu_write_lock() is there,because there is no way for writers to lock out RCU readers,
              but writers must coordinate with each other.(use spinlock or other protection mechanism)
              RCU does not care how the writers keep out of each others' way.

          /*  call_rcu - queue an RCU callback for invocation after a grace period.
           *  @head:     structure to be used for queueing the RCU updates.
           *  @func:     actual update function to be invoked after the grace period.
           *  #  callback maybe deferred until all read-side critical sections exited,
           *     and read-side critical section is allowed be nested.
           */
          extern void call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *head));

        <kernel/rcutree_plugin.h>
          /*  synchronize_rcu - wait until a grace period has elapsed.
           *  #  function return maybe deferred until all read-side critical sections exited.
           *  #  EXPORT_SYMBOL_GPL(synchronize_rcu)
           */
          void synchronize_rcu(void);

        RCU "quiescent state" in Linux Kernel :
          classic RCU read-side critical sections are confined to kernel code and are not permitted
          to block.this means that any time a given CPU is seen either blocking,in the idle loop,or
          exiting the kernel,we know that all RCU read-side critical sections that were previously
          running on that CPU must have completed.
          such states are called "quiescent states",and after each CPU has passed through at least
          one quiescent state,the RCU grace period ends.

        RCU mechanism initialization in Linux Kernel :
          Linux kernel supports two kinds of RCU,one is for SMP environment,another is for uniprocessor
          environment - "bloatwatch edition".the headers are <linux/rcupdate.h> and <linux/rcutiny.h>,
          respectively.
          kernel initializes RCU mechanism during system booting through function rcu_init().

          <linux/rcupdate.h>
          /**
           * rcu_init - initializing function for initializes RCU mechanism for mutual exclusion
           */
          extern void rcu_init(void);
          /* void __init rcu_init(void); - <kernel/rcutree.c> */

          !!  RCU nodes are combined together through robin-tree.

          what rcu_init() to does :
            1>  announce rcu bootup.
            2>  initialize rcu flavor - @rcu_sched_state with @rcu_sched_data AND @rcu_bh_state with @rcu_bh_data.
            3>  call to static function __rcu_init_preempt() defined in <kernel/rcutree_plugin.h> to initialize
                rcu flavor - @rcu_preempt_state with @rcu_preempt_data.
                /**
                 * assign leaf node pointers into each CPU's rcu_data structure.
                 */
            4>  register RCU_SOFTIRQ deferrable function "rcu_process_callbacks()".
            5>  register cpu notifier @rcu_cpu_notify.
            6>  for each online CPUs call once rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)@cpu).
                this will make CPUs to initialize per-CPU variables for sched state,bh state,and preempt state.

          <kernel/rcutree.c>
            /**
             * rcu_process_callbacks - the softirq RCU_SOFTIRQ handler
             */
            static void rcu_process_callbacks(struct softirq_action *unused);
            
            this function checks the quiescent state for @rcu_sched_state and @rcu_bh_state,if any callback is
            ready,then invoke it. (__rcu_process_callbacks())
            then deal with preemptable RCU @rcu_preempt_state.
            finally,check if any future RCU-related work will need to be done by current CPU.(through function
            rcu_needs_cpu_flush(),which call to rcu_needs_quick_check() to do primary checking.if there
            are some,then have to deal with them.even if none need be done immediately,returning 1 if so,but
            its return value is unused.)
                
        RCU principle :
          reader :
            read RCU data,it maybe old data or new data.

          writer :
            read RCU data -> make a copy -> modify copy -> write back.
            /*  writer back:
             *    use the new copy's address to instead the old address of the data structure
             *    it protected by RCU.(modify a pointer,operation on pointer is atomic)
             */

          usually,rcu_head is embedded in a data structure which is protected by RCU mechanism.
          each reader or writer sees either the old copy or the new one,no corruption in the
          data structure may occur.

          the old copy of the data structure cannot be freed right away when the writer updates
          the pointer.the old copy can be freed only afer all (potential) readers on the CPUs
          have executed the rcu_read_unlock() macro(exit read-side critical section).
          the kernel requires every potential reader to execute that macro before :
            1>  the CPU performs a process switch
            2>  the CPU starts executing in User Mode
            3>  the CPU executes the idle loop
            /*  in each of these cases,we say that the CPU has gone through a quiescent state.  */

          call_rcu() stores the @func in @head->func,and inserts @head in a per-CPU list of callbacks.
          when all CPUs have gone through a quiescent state,a local tasklet--whose descriptor is stored
          in the @rcu_tasklet per-CPU variable--executes all callbacks in the list.
          /*  kernel checks whether the local CPU has gone through a quiescent state,periodically.  */

        e.g.
          struct INTStruct {
                  int x;
                  struct rcu_head rcu;
          };

          INTStruct *pints = NULL;

          init:
            pints = kmalloc(sizeof(struct INTStruct, GFP_KERNEL));
            pints->x = 0;
            pints->rcu.next = NULL;
            pints->rcu.func = NULL;

          reader:
            rcu_read_lock();    /*  begin of RCU read-side section  */
            ...
            rcu_read_unlock();  /*  end of RCU read-side section  */

          writer:
            void pints_update_func(struct rcu_head *head)
            {
                    struct INTStruct *tmp = pints;

                    /*  if more than one writer will modify @pints concurrently,
                     *  there should have a synchronous mechanism to protect the
                     *  shared data structure. i.e. linux spin lock
                     */

                    pints = container_of(head, struct INTStruct, rcu);
                    kfree(tmp);  /*  free the old data structure  */
            }

            struct INTStruct *copy = kmalloc(sizeof(struct INTStruct), GFP_KERNEL);
            if (IS_ERR(copy))
              panic("RCU make copy was failed.");

            /*  make a copy  */
            rcu_read_lock();

            copy->x = pints->x;
            copy->rcu = pints->rcu;

            rcu_read_unlock();

            copy->x += 8;  /*  modify copy  */

            call_rcu(&copy->rcu, pints_update_func);  /*  queue an RCU callback  */

      Semaphores :
        Linux offers two kinds of semaphores :
          1>  Kernel semaphore,used by kernel control paths.
          2>  SystemV semaphore,used by User Mode processes.
          /*  3>  POSIX semaphore,used by User Mode processes.  */

        kernel semaphore is similar to spin lock,kernel semaphore does not allow a kernel control path
        to proceed unless the lock is open.
        kernel semaphores can be acquired only by functions that are allowed to sleep,that is kernel 
        control path would be blocked to wait for busy resource until it is released.

        <linux/semaphore.h>
          /*  semaphore - a structure represents the kernel semaphore.
           *  @lock:      kernel spin lock,concurrently writing protection for kernel semaphore.
           *              when sempahore APIs accessing to this structure,@lock will be locked and
           *              local interrupt request would be disabled until unlocked.
           *  @count:     @count > 0 => protected resource is free
           *              @count = 0 => semaphore busy,that is the resource is acquired by other.
           *                            "but no process is waiting for it"-(understanding linux kernel)
           *              "@count < 0 => resource is unavailable,and at least one process is waiting
           *                            for the resource"-(understanding linux kernel)
           *              #  but the type of @count is unsigned int,thus no negative value is possible
           *                 to exist(Linux 2.6.34.1)
           *              the initialized @count indicates how many processes are able to use the
           *              resource at the same time.
           *              because it is not recommend to access kernel semaphore members directly,
           *              thus,the initialized value of @count have to be determined when designing.
           *              #  creator is able to modify @count after initialized that is do not care
           *                 the suggestion or call up() once or more than once(@wait_list must be NULL).
           *                 IT IS NOT RECOMMEND,IT IS WORST,BECAUSE KERNEL DOES NOT PROVIDE SUCH
           *                 APIs TO CHANGE THE MAXIMUM NUMBER OF PROCESSES THAT ARE ALLOWED TO
           *                 ACCESS THE PROTECTED RESOURCE AT THE SAME TIME!!SUCH THING IS DETERMINED
           *                 AT DESIGNING.
           *  @wait_list: the sleeping processes on the kernel semaphore.
           *              if this list is NULL,that means no task is waiting for the semaphore.
           *  #  Must prevent access the member of this structure directly.
           *  #  if initialize a kernel semaphore with value 1,it will works as if a mutex.
           */
          struct semaphore {
                  spinlock_t lock;
                  unsigned int count;
                  struct list_head wait_list;
          };
            
          /*  DECLARE_MUTEX - declare and initialize a semaphore @name with value 1,it will works as if a
           *                  mutex.(@name.@count := 1)
           */
          #define DECLARE_MUTEX(name)        struct semaphore name = __SEMAPHORE_INITIALIZER(name, 1)

          /*  sema_init - initialize a kernel semaphore.
           *  @sem:       kernel semaphore pointer.
           *  @val:       the value.(@sem->@count := @val)
           */
          static inline void sema_init (struct semaphore *sem, int val);

          /*  init_MUTEX - initialize a mutex.  */
          #define init_MUTEX(sem)        sema_init(sem, 1)

          /*  init_MUTEX_LOCKED - initialize a mutex and lock it,at this time,
           *                      @sem->lock is unlocked,just @sem->count == 0(busy).
           */
          #define init_MUTEX_LOCKED(sem) sema_init(sem, 0)

        Getting and releasing semaphores :
          <linux/semaphore.h>
            /*  down - acquires the semaphore pointed by @sem.
             *  @sem:  kernel semaphore pointer.
             *  #  this function locks @sem->lock before checks @sem->count.
             *     if @sem->count > 0
             *       --(@sem->count)
             *     else
             *       call __down(@sem)
             *  #  if resource is unavailable,this function will put the task to sleep until
             *     the semaphore is released.
             *     this is,if semaphore is available,down() returns immediately with the side-effect
             *     "--(@sem->count)";otherwise,task is suspended until semaphore is available,in this
             *     case,"--(@sem->count)" would not occurs.of course,if a waiter is waiting for the
             *     semaphore,a up() calling would not process ++(@sem->count).
             *     another word,one leave and one enter,it is useless to restore value of semaphore.
             *  #  this function is deprecated.
             */
            extern void down(struct semaphore *sem);

            /*  down_trylock - try to acquire kernel semaphore without waiting.
             *  @sem:          kernel semaphore pointer.
             *  rerurn:        succeed to acquire -> 0
             *                 failed to acquire  -> 1
             */
            extern int __must_check down_trylock(struct semaphore *sem);

            /*  down_interruptible - acquire kernel semaphore,but set @current->state to
             *                       TASK_INTERRUPTIBLE.
             *  @sem:                kernel semaphore pointer.
             *  return:              0 OR -EINTR
             */
            extern int __must_check down_interruptible(struct semaphore *sem);

            /*  down_killable - acquire kernel semaphore,but set @current->state to
             *                  TASK_KILLABLE.
             *  @sem:           kernel semaphore pointer.
             *  return:         0 OR -EINR(interrupted by a fatal signal)
             */
            extern int __must_check down_killable(struct semaphore *sem);

            /*  down_timeout - acquire kernel semaphore,but have a timer to indicates
             *                 the maximum time to waiting for.
             *                 @current->state will be seted with TASK_UNINTERRUPTIBLE.
             *  @sem:          kernel semaphore pointer.
             *  @jiffies:      system timer jiffy.
             *                 schedule_timeout(@jiffies) would be called in assist routine.
             *  return:        0 OR -ETIME
             */
            extern int __must_check down_timeout(struct semaphore *sem, long jiffies);

            /*  up - releases the semaphore pointed by @sem.
             *  @sem:  kernel semaphore pointer.
             *  #  this function locks @sem->lock before checks @sem->count.
             *     if NULL(@sem->wait_list)
             *       ++(@sem->count)
             *     else
             *       call __up(@sem)
             *       in this case,the value of @sem->count will not increase,wake up the first 
             *       waiter in the waiting list to instead.
             */
            extern void up(struct semaphore *sem);

          <kernel/semaphore.c>
            /*  semaphore_waiter - structure represents a waiter on a specified kernel semaphore.
             *  @list:             linux list,every semaphore_waiter will be linked to a list
             *                     which is specified by @semaphore.list .
             *  @task:             struct task_struct *,the waiting process.
             *  @up:               indicate that whether a process which holding the protected resource
             *                     did up() calling.(that means someone released the kernel semaphore)
             */
            struct semaphore_waiter {
                    struct list_head list;
                    struct task_struct *task;
                    int up;
            };

            /*  __down_common - generic __down function.
             *  @sem:           the kernel semaphore.
             *  @state:         the task state expected to be seted when scheduling.
             *  @timeout:       timeout in jiffies.
             *  return:         0 OR -EINTR OR -ETIME
             *  #  other __down*() call to this function with different task state.
             *  #  this function create a semaphore_waiter{list, @current, 0},
             *     then list_add_tail @waiter into @sem->wait_list via @waiter->list.
             *     enter infinite for-cycle {
             *             if task is interrupted by signal,return -EINTR
             *             if expired,return -ETIME
             *             set task state with @state
             *             unlock @sem->lock
             *             call to schedule_timeout()
             *             lock @sem->lock
             *             if @waiter-> up == 1, return 0
             *     }
             *     !! before return,waiter would be removed from @sem->wait_list.
             */
            static inline int __sched __down_common(struct semaphore *sem, long state, long timeout);

            /*  __up - up function for the case one or more process is waiting for semaphore.
             *  @sem:  kernel semaphore pointer.
             *  #  this function retrieve the first waiter in @sem->wait_list,
             *     then set @waiter->up = 1,next wake up the task(@waiter->task).
             */
            static noinline void __sched __up(struct seamphore *sem);

          e.g.
            global namespace:
              DECLARE_MUTEX(ksem);

            kcp1:
              repeat:
                      if (!down_interruptible(&ksem)) {
                              ...
                              up(&ksem);
                      }
                      else
                              goto repeat;
              ...

            kcp2:
              if (!down_trylock(&ksem)) {
                      ...
                      up(&ksem);
              }
              else
                      printk(KERN_INFO "trylock kernel semaphore failed.");
              ...

          e.g.
            global namespace:
              struct semaphore ksem;
              sema_init(&ksem, 3);  /*  maximum processes number  */

            kpc1:  acquire ksem  /*  ksem.count -= 1 == 2  */
            kpc2:  acquire ksem  /*  ksem.count -= 1 == 1  */
            kpc3:  acquire ksem  /*  ksem.count -= 1 == 0  */
            kpc4:  acquire ksem => ksem.count <= 0 => __down(&ksem);

            ...

            kpc2:  release ksem => wake up kpc4  /*  ksem.count == 0  */
            kpc4:  waken up  /*  ksem.count == 0  */

            ...

            kpc1:  release ksem => no waiters => ++ksem.count  /*  ksem.count == 1  */
            kpc3:  release ksem => no waiters => ++ksem.count  /*  ksem.count == 2  */
            kpc4:  release ksem => no waiters => ++ksem.count  /*  ksem.count == 3  */

        !!  because semaphore is protected by kernel spin lock,thus kernel preemption is disabled
            when operating semaphore object(locked),kernel preemption is enabled after smeaphore
            is unlocked(operating finished).

        !!  kernel preemption is allowed when a kernel control path holding a semaphore.

      RW Semaphores :
        rw kernel semaphore is similar to kernel semaphore,but the operations are split to two kinds
        are read and write.

        <linux/rwsem-spinlock.h>
          /*  rw_semaphore - represent a kernel rw semaphore.
           *  @activity:     indicator.(default initialized to 0)
           *                 =  0 -> there are no active readers or writers
           *                 >  0 -> the number of active readers
           *                 = -1 -> one active writer
           *  @wait_lock:    rw_semaphore protector.
           *  @wait_list:    list of waiters.
           *  #  generic implement,the architecture-dependent implement is included in header
           *     <arch/x86/include/asm/rwsem.h> .
           */
          struct rw_semaphore {
                  __s32 activity;
                  spinlock_t wait_lock;
                  struct list_head wait_list;
          };

          /*  DECLARE_RWSEM - declare and initialize a local rw_semaphore object named with @name.  */
          #define DECLARE_RWSEM(name)  struct rw_semaphore name = __RWSEM_INITIALIZER(name)

          /*  init_rwsem - initialize a declared rw semaphore.
           *  @sem:        pointer of rw_semaphore.
           *  #  this function does same as DECLARE_RWSEM() macro.
           */
          #define init_rwsem(sem)

        rw_semaphore only allows a kernel control path holding a write lock at same time,but allows
        more than one kernel control paths to holding read lock.
        the default state of rw_semaphore is 0,means no readers and writers.
        if implementation is select to architecture-dependent(no CONFIG_RWSEM_GENERIC_SPINLOCK selected),
        then some macro definition would be used to represents the state what a rw_semaphore it is.
        for example :
          <arch/x86/include/asm/rwsem.h>
            RWSEM_ACTIVE_MASK  0xffffffffL(x86_64)  (0x0000ffffL x86)
            RWSEM_UNLOCKED_VALUE  0x00000000L
            RWSEM_ACTIVE_BIAS  0x00000001L
            RWSEM_WAITING_BIAS  (-RWSEM_ACTIVE_MASK - 1)
            RWSEM_ACTIVE_READ_BIAS  RWSEM_ACTIVE_BIAS
            RWSEM_ACTIVE_WRITE_BIAS  (RWSEM_WAITING_BIAS + RWSEM_ACTIVE_BIAS)

          /*  kernel rw semaphore is implemented as an library feature,thus the source code files
           *  under "top-level/lib"(i.e. rwsem.c),the artechiture-dependent source code files under 
           *  "arch/x86/lib"(i.e. rwsem_64.S).
           */

        it follow the strict FIFO rule,everytime wake-up always wake the first waiter up in the
        waiting list.
        up action is split two kinds that are up-read and up-write.
        after up-write,if the first waiter is waiting for read,then all read-waiter up to next
        write-waiter would be woken up,otherwise,the first waiter is waiting for write,then only
        it would be woken up in the waiting list.

        Operations :
          <linux/rwsem.h>
            /*  down_*() functions will put task to sleep,after the blocked task had woken up,
             *  the lock it acquired must be succeed,because,the blocking tasks enter
             *  TASK_UNINTERRUPTIBLE state,thus no signal is able to interrupt them.
             */
            extern void down_write(struct rw_semaphore *sem);
            extern void down_read(struct rw_semaphore *sem);

            extern void up_write(struct rw_semaphore *sem);
            extern void up_read(struct rw_semaphore *sem);

            /*  *trylock() functions try to acquire lock without blocking.
             *  return:  1 -> successful
             *           0 -> contention
             */
            extern int down_write_trylock(struct rw_semaphore *sem);
            extern int down_read_trylock(struct rw_semaphore *sem);

            /*  downgrade_write - downgrade a read semaphore which have holden by the task
             *                    to a write semaphore.if there is another reader is activing,
             *                    then block task until the last reader leaved.
             *  @sem:             rw_semaphore pointer.
             */
            extern void downgrade_write(struct rw_semaphore *sem);

        !!  rwsems is disallow to recurse(lock a rwsem multiple times),but it is allow that
            multiple locks of the same lock class might be taken,if the order of the locks 
            is always the same(prevent dead-lock).
            this feature is require CONFIG_DEBUG_LOCK_ALLOC,lockdep would be used for
            implement it,the associated APIs are suffix to *_nest .

      Completion :
        "understanding linux kernel" described that kernel synchronize mechanism "Completion"
        is imported for reslove kernel semaphore concurrently operating,but in Linux 2.6.34.1,
        the same kernel semaphore is disallow to be concurrently operated,kernel preemption and
        local interrupt request all are disabled when hold the spin lock embedded in the
        structure.thus the down() and up() running on different processes on different CPU would
        not bring dangling pointer problem.
        /*  process1-CPU0 destoryed kernel semaphore when process2-CPU1 is calling up() function.  */

        kernel Completion mechanism is based on wait queue and introduced for task wait for a
        condition is completed.
        only one task would be woken up after a condition is completed.in other words,only one
        kernel control path is allowed to enter critical region.

        !!  for wait queue see "How Processes Are Organized" .

        <linux/complete.h>
          /*  completion - structure represents a condition indicator.
           *  @done:       if the condition is completed.
           *               1 -> it is completed
           *               0 -> it is not completed
           *  @wait:       the tasks waiting for the condition.
           *  #  wait_queue_head_t contains a kernel spin lock and a kernel list as its own member.
           *     the items linked under wait_queue_head_t.task_list are wait instance each is 
           *     represents by structure wait_queue_t.
           */
          struct completion {
                  unsigned int done;
                  wait_queue_head_t wait;
          };

          /*  DECLARE_COMPLETION - declare and initialize a completion object.
           *                       initialized with {0, __WAIT_QUEUE_HEAD_INITIALIZER(@work.wait)} .
           */
          #define DECLARE_COMPLETION(work)  struct completion work = COMPLITION_INITIALIZER(work);

          /*  DECLARE_COMPLETION_ONSTACK - declare and initialize a completion object on kernel stack.  */
          #define DECLARE_COMPLETION_ONSTACK(work)  \
                  struct completion work = COMPLETION_INITIALIZER_ONSTACK(work)

          /*  init_completion - initialize a completion.
           *  @x:               pointer of the declared completion.
           *  #  does same thing as DECLARE_COMPLETION().
           */
          staic inline void init_completion(struct completion *x);

          /*  INIT_COMPLETION - reinitialize a completion.it is important after complete_all() is used.
           *  @x:               completion object,not a pointer.
           *  #  this macro does not modify wait list,just reset condition as well.
           */
          #define INIT_COMPLETION(x)  ((x).done = 0)

          /*  complete - signals a single thread waiting on this completion.
           *  @x:        completion pointer.
           *  #  this function hold spin lock with local interrupt request disabled,
           *     then does @x->done++,next call to __wake_up_common() with exclusive == 1.
           *     exclusive-wakeup : wake up one exclusive task and all non-exclusive tasks.
           */
          extern void complete(struct completion *x);

          /*  complete_all - signals all threads waiting on this completion.
           *  @x:            completion pointer.
           *  #  this function is similar to completion(),but it call to __wake_up_common() with
           *     exclusive == 0.
           *     non-exclusive-wakeup : wake everting up. 
           */
          extern void complete_all(struct completion *);

          /*  !!
           *  generally,the wait APIs add all non-exclusive threads to the head of wait queue,
           *  and add all exclusive threads to the tail of wait queue.
           *  but the wait queue of Completion mechanism only exclusive threads are queued.
           *  thus,do not mix Completion with wait queue APIs,just use Completion APIs only as well.
           *  !!
           */

          /*  wait_for_completion - let @current wait on @x and set task state to
           *                        TASK_UNINTERRUPTIBLE.
           *  @x:                   completion pointer.
           *  #  after queued completion waiting,@current will be put to sleep,another
           *     task would be selected by scheduler.
           */
          extern void wait_for_completion(struct completion *x);

          /*  TASK_INTERRUPTIBLE
           *  return:  1 -> condition completed
           *           -ERESTARTSYS -> interrupted by signal
           */
          extern int wait_for_completion_interruptible(struct completion *x);

          /*  TASK_KILLABLE
           *  return:  1 -> condition completed
           *           -ERESTARTSYS -> interrupted by signal
           */
          extern int wait_for_completion_killable(struct completion *x);

          /*  TASK_UNINTERRUPTIBLE
           *  @timeout:  time in jiffies.
           *  return:  1 -> condition completed
           *           0 -> timer expired
           */
          extern unsigned long wait_for_completion_timeout(struct completion *x,
                                                           unsigned long timeout);

          /*  TASK_INTERRUPTIBLE
           *  @timeout:  time in jiffies.
           *  return:  1 -> condition completed
           *           0 -> timer expired
           *           -ERESTARTSYS -> interrupted by signal
           */
          extern unsigned long wait_for_completion_interruptible_timeout(struct completion *x,
                                                                         unsigned long timeout);

          /*  try_wait_for_completion - try wait a condition completed without blocking.
           *  @x:                       completion pointer.
           *  return:                   0 -> decrement cannot be done without blocking
           *                            1 -> decrement succeed
           *  #  this function return 1,means the completion decrement succeed,thus the caller
           *     is able to use the condition right after control returned.
           *     because @x->done had been decreased,thus another task will see the condition
           *     is not completed(zero),they will wait for next completed.
           */
          extern bool try_wait_for_completion(struct completion *x);

          /*  completion_done - test to see if a completion has any waiters.
           *  @x:               completion pointer.
           *  return:           0 -> there are waiters
           *                    1 -> there are no waiters
           *  #  this function just checks if @x->done is equal to 0.
           *     if it is,that means a thread called to wait_for_completion*() did decrement to
           *     @x->done(become zero,it is in progress);
           *     else,@x->done is equal to 1,no any threads used the completed condition.
           */
          extern bool completion_done(struct completion *x);

      Local Interrupt Disabling :          
        interrupt disabling ensure that a sequence of kernel statements is treated as a critical section.
        the kernel control path entered the critical section,which will do not care about hardware event,
        thus providing an effective way to protect data structures that are also accessed by interrupt
        handlers.but this way in the case that interrupt handlers run on other CPUs(SMP) and access the
        protected data structures concurrently is useless,it is absolutely useful for uniprocessor system.

        for disable local interrupt,have to clear eflags.IF,the assembly instruction "cli" is take a charge
        to the feature.
        for enable local interrupt,have to set eflags.IF,the assembly instruction "sti" is take a charge to
        the feature.

        linux defined some macros or functions used to disable/enable local CPU interrupt :
          <linux/irqflags.h>
            /*  local_irq_disable - cli  */
            #define local_irq_disable()

            /*  local_irq_enable - sti  */
            #define local_irq_enable()

            /*  local_irq_save - save the older eflags state before disable local CPU interrupt
             *  @flags:          unsigned long type variable(not pointer) used to save the older
             *                   eflags state.
             */
            #define local_irq_save(flags)

            /*  local_irq_restore - restore eflags
             *  @flags:             the eflags state previously saved by local_irq_save().
             *  #  this function do not process "sti",it just call "popf" to restore eflags register.
             */
            #define local_irq_restore(flags)

            /*  irqs_disabled - return eflags.IF == 0  */
            #define irqs_disabled()

        !!  because interrupt is allowed to nest,eflags register may be changed by other interrupt handler,
            thus the kernel does not necessarily know what the IF flag was before the current control path
            executed.in the case,use local_irq_save() to save the older eflags state and restore it before
            exit critical region.

      Disabling and Enabling Deferrable Functions :
        deferrable functions do the work that is allowed to deferred to half-bottom of interrupt.
        the mechanism for half-bottom of interrupt contains :
          softirqs, tasklet, work_queue

        linux defined two macros used to disable or enable local softirqs :
          <linux/bottom_half.h>
            /*  local_bh_disable - increase preempt_count.softirq_counter  */
            extern void local_bh_disable(void);

            /*  local_bh_enable - decrease preempt_count.softirq_counter
             *  #  this function would checks
             *       if local_irq_pending() && !in_irq()
             *       true: call to do_softirq()
             *       false: decrease preempt_count.softirq_counter then call to preempt_check_resched()
             *     kernel preemption keep disabled until done with softirqs processing {
             *             preempt_count -= (255 = SOFTIRQ_OFFSET - 1)  bits[0, 7] => preempt count
             *             true-if
             *                     do_softirq()
             *             preempt_count -= 1    bits[8, 15] => softirq count
             *     }
             *     e.g.
             *       preempt_disable() => 000000001 => 1  /*/
             *       local_bh_disable() => 1 + 256 = 257 => 100000001
             *       preempt count = 257 - (SOFTIRQ_OFFSET - 1) = 255 => 2 => 000000010
             *       dec_preempt_count() => 2-- => 1 => 000000001  /*/
             */
            extern void local_bh_enable(void);

            /*  local_bh_enable_ip - enable local bh(bottom of interrupt),@ip is used to trace softirqs  */
            extern void local_bh_enable_ip(unsigned long ip);

        !!  tasklet is based softirq,thus local bottom disable/enable is take effect to tasklet,too.
        !!  preempt_disable() => increase preempt count; preempt_enable() => decrease preempt count
            preempt enabled if and only if preempt counter equals to zero,and preempt is able to be
            disabled more than once,each calling to preempt_disable() will increase preempt counter.

      Synchronizing Accesses to Kernel Data Structures :
        system performance may depends on which sychronization primitive is selected.
        there is a rule should to follow when select a synchronization primitive for protect
        kernel data structure: "always keep the concurrency level as high as possible in the system".

        the concurrency level in the system depends on two main factors:
          1>  the number of I/O devices that operate concurrently
          2>  the number of CPUs that do productive work

        !!  device IRQs is only can be disabled in a short periods of time.
        !!  CPUs in the system should avoid waste times on spin lock,that is do not select spin lock
            whenever possible.
            local IRQ disable is more efficiency than spin lock,it just clear eflags.IF .

        the examples that keep the concurrency level as high as possible :
          1>  a shared data is type of integer value it is accessed concurrently.
              declare it with atomic_t type,do not use spin lock or other different kernel synchronization
              primitives to protect it from concurrently access.

          2>  a shared linked list have to be updated.

              new->next = head->next;
              head->next = new;

              data corrupted to new->next when an interrupt handler modified head->next after the
              statement "new->next = head->next" finished.
                interrupt handler:
                  list_del(head->next);

                in this case,a kernel synchronization primitive is required:
                  unsigned long eflags;
                  local_irq_save(eflags);
                  new->next = head->next;
                  head->next = new;
                  local_irq_restore(eflags);

              data corrupted to these two statements there compiler optimizing or CPU out-of-order
              has taken a place:
                head->next = new => finished
                interrupt come
                interrupt handler access to head->next try to traverse list,but at this time,
                new->next == new
                /*  interrupt handler does not delete list item  */

                in this case,a memory write barrier is required:
                  new->next = head->next;
                  smp_wmb();
                  head->next = new;

      Choosing Among Spin Locks,Semaphores,and Interrupt Disabling :
        choosing the synchronization primitives depends on what kinds of kernel control paths access
        the data structure.

        kernel control path                   uniprocessor                      multiprocessor
        exceptions                            semaphore                         none
        interrupts                            local interrupt disabling         spin lock
        deferrable functions                  none                              none or spin lock
        exceptions + interrupts               local interrupt disabling         spin lock
        exceptions + deferrable functions     local softirq disabling           spin lock
        interrupts + deferrable functions     local interrupt disabling         spin lock
        exceptions + interrupts +             local interrupt disabling         spin lock
        deferrable functions

        /*  the primitives disable kernel preempt :
         *    spin lock
         *    local interrupt disable
         *    local bottom-half disable
         */

        protecting a data structure accessed by exceptions :
          the exception occurs in Kernel Mode only the "Page Fault".
          that is the subsequent "Page Fault" would be blocked until current is finished.
          the "Page Fault" handler(page_fault()) deal with it.

          a struct named mm_struct would be accessed by page_fault(),but it just accessed in read-only for
          scanning.  /*  page_fault() -> do_page_fault()  */

          /*  how synchronize to the shared data structures placed in the page for concurrently accessing
           *  is the work of kernel control paths,page_fault() does not care about,it does not access to
           *  such shared data structures,it just replaces pages.
           */

          the process in User Mode is able to call to system-call via "int" or "int3",and linux implement
          syscall in traps.(default the vector of system trap gate is 0x80,and handler is system_call())
          thus,the shared data structures accessed by syscall usually represents a resource that can be
          assigned to one or more processes.
          race condition occurs when more than one syscalls try to access the resource,such resource can be
          protected by semaphore primitive,because semaphore is allow processes to sleep until the resource
          is available.

          kernel preemption does not create problem when syscall owns a semaphore :
            process1 holds semaphore sem1 ->
            process2 preempt process1 ->
            process2 try to acquires sem1 ->
            sem1 is unavailable ->
            process2 entry sleep ->
            scheduler pick up next process ->
            ...
            scheduler pick up process1 to run ->
            process1 finished accessing to the resource ->
            sem1 is available now ->
            process2 is woken up

          !!  the kernel preemption must to be disabled in a syscall that is the syscall wants to
              access Per-CPU variable.

        protecting a data structure accessed by interrupts :

          data structure accessed by the top-half of interrupt:

            data structure only accessed by the interrupt handlers have the same type>
              do not need any synchronization,because such interrupt handlers are serialized.

            data structure accessed by interrupt handlers and each has the different type>
            /*  interrupt can be nested  */

              uniprocessor :
                enter any critical region in the interrupt handler must disable local irq at first.
                /*  semaphore brings blocking,but interrupt context forbid such action.
                 *  spin lock,interrupt handler holding a spin lock maybe interrupted by another
                 *  interrupt,and the interrupt handler request the same spin lock => dead-lock
                 */

              multiprocessor :
                1>  disable local irq before enter any critical region in the interrupt handler
                    /*  disable local irq won't interfere other CPU,the interrupt with the
                     *  same type may handled on other CPU
                     */
                2>  acquire a spin lock or a rw spin lock before access to the data structure
                    /*  acquire spin lock after local irq disabled,then no interrupt has highter
                     *  priority would interrupts current interrupt handler.
                     *  acquire spin lock disabling the concurrently access to the data structure
                     *  by interrupt handlers with the same type on other CPU.
                     *  SPIN LOCK MIGHT SPIN UNTIL UNLOCK,BUT TOP-HALF OF INTERRUPT MUST BE
                     *  SHORT AND FAST,THUS INTERRUPT HANDLER ACQUIRE SPIN LOCK WOULD NOT
                     *  FREEZE SYSTEM.
                     */

          linux macros coupled spin lock with local irq enabling/disabling :
            spin_lock_irq
            spin_unlock_irq
            spin_lock_bh
            spin_unlock_bh
            spin_lock_irqsave
            spin_unlock_irqrestore
            
            read_lock_irq                   read_seqbegin_irqsave
            read_unlock_irq                 read_seqretry_irqrestore
            read_lock_bh
            read_unlock_bh
            read_lock_irqsave
            read_unlock_irqrestore            
            
            write_lock_irq                  write_seqlock_irqsave
            write_unlock_irq                write_sequnlock_irqrestore
            write_lock_bh                   write_seqlock_irq
            write_unlock_bh                 write_sequnlock_irq
            write_lock_irqsave              write_seqlock_bh
            write_unlock_irqrestore         write_sequnlock_bh

        protecting a data structure accessed by deferrable functions : (bottom-half)
          !!  no race condition may exist in uniprocessor system in the case that data structure
              accessed only by deferrable functions.
              /*  deferrable function can not be interrupted by another,serialized.  */

          multiprocessor :
            softirq => softirqs has same type can run concurrently on several CPUs at a time
            tasklet => tasklets has same type can not run on more than one CPU at a time
                                                 
                                                 /*  protection  */
            softirqs            require          spin lock
            one tasklet         require          none
            many tasklets       require          spin lock

            /*  many tasklets might access to the same data structure concurrently,
             *  thus a synchronizing mechanism is demand.
             */

          !!  work_queue is dealt with by scheduler,in process context,thus any type of
              synchronization is OK except local irq disable(it is useless).

        protecting a data structure accessed by exceptions and interrupts :
          interrupt handler can not be interrupted by exceptions.          
          (non-reentrant)

          uniprocessor :
            just disable local irq as well,prevent interrupt handler is interrupted by other
            interrupts with different type.
            if just one kind of interrupt handler access to the data structure,do not need
            to disable local irq.

          multiprocessor :
            disable local irq
              =>  prevent interrupt handler interrupted by other
            select an appropriate kernel synchronizing mechanism to protect the data structure
              =>  prevent concurrently access to the data structure by I/E handlers on other CPU

            /*  on multiprocessor system,semaphore is preferable to replace spin lock.
             *    interrupt handler can not sleep
             *    exception handler can not sleep
             *    syscall is allowed to sleep
             *    only "Page Fault" occurs in Kernel Mode(only one type)
             *  thus,interrupt handler can use down_trylock() to acquire semaphore,it works like
             *  a spin lock.
             */

        protecting a data structure accessed by exceptions and deferrable functions :
          softirq AND tasklet in interrupt context
          exception handler "Page Fault" in interrupt context
          syscall in process context
          work_queue in process context

          multiprocessor :
            exception handler         require     disable bottom-half(or disable local irq)
                                                  spin lock

            deferrable function       require     spin lock
            /*  no exception can be raised when a deferrable
             *  function is running.
             *  exception can not interrupt interrupt handler
             */

          of course,semaphore can used to replace spin lock,but the handler in interrupt context
          must use down_trylock() to instead down().

        protecting a data structure accessed by interrupts and deferrable functions :
          interrupt handler can interrupt deferable function,but deferrable function can not
          interrupt an interrupt handler.

          multiprocessor :
            deferrable function        require        disable local irq
                                                      spin lock

        protecting a data structure accessed by exceptions,interrupts and deferrable functions :
          exception can not be raised when interrupt handler or deferrable function is running.
          interrupt handler can interrupt exceptions and deferrable functions.

          multiprocessor :
            deferrable function        require        disable local irq
                                                      spin lock
                                                      /*  dont care about exception  */

            exception handler          require        disable local irq
                                                      spin lock or semaphore
                                                      /*  disable local irq influence to
                                                       *  interrupt and softirq
                                                       */

            interrupt handler          require        disable local irq
                                                      spin lock

          !!  essentailly,deferrable function activated right after when interrupt handlers finished,
              thus exception handler do not to explicitly disable bh.

      Examples of Race Condition Prevention :

        Reference Counters >
          a reference counter is just an atomic_t counter associated with a specific resource such
          inode,file,memory page,module,etc.

          a kernel control path starts use the resource,increase reference counter;
          a kernel control path finishes use the resource,decrease reference counter;
          when reference counter is equals to zero,it can be released if necessary.

        Big Kernel Lock >
          In Linux 2.6.34.1,BKL is used to protect old code(functions to VFS or to several filesystems).
          BKL as an optional feature configured by CONFIG_LOCK_KERNEL.

          the corresponding APIs is defined in <linux/smp_lock.h>,and implementions are included in 
          <top-level/lib/kernel_lock.c> .

          In Linux 2.6.11,BKL is based on a meaningful semaphore,but in the earlier 2.6,it based on a
          spin lock.
          !!  LINUX 2.6.34.1,BKL IS BASED ON A SPIN LOCK NAMED @kernel_flag WHICH IS DEFINED IN
              <lib/kernel_lock.c> IS TYPE OF raw_spinlock_t.

          <linux/smp_lock.h>
            /*  lock_kernel - acquire BKL,the kernel preemption would be disabled after
             *                got BKL.kernel preempt is enabled during spin.
             *
             *  #  this macro call to _lock_kernel() defined in <lib/kernel_lock.c> .
             *  #  _lock_kernel() would checks whether @current->lock_depth is equal to -1.
             *       if it is,then call to __lock_kernel() which spin on @kernel_flag for
             *       get the spin lock.
             *     increase @current->lock_depth before return to called.
             */
            #define lock_kernel()

            /*  unlock_kernel - release BKL and enable kernel preemption.
             * 
             *  #  this macro call to _unlock_kernel() defined in <lib/kernel_lock.c> .
             *  #  must call to _unlock_kernel() when @current->lock_depth is greater than or
             *     equal to zero,otherwise,it is a BUG.
             *     this function decrease @current->lock_depth at first,
             *     if the value becomes less than zero,
             *       then call to __unlock_kernel() to release BKL.
             *     else does nothing.
             */
            #define unlock_kernel()

            /*  release_kernel_lock - release BKL,this macro used by scheduler.
             *                        after released,enable kernel preemption without
             *                        reschedule.
             *
             *  #  if @tsk->lock_depth >= 0,then call to __release_kernel_lock() .
             *  #  __release_kernel_lock() is defined in <lib/kernel_lock.c> ,which
             *     does not decrease @current->lock_depth,only decrease preempt counter,
             *     if preempt counter is equal to zero,that means kernep preempt enabled.
             */
            #define release_kernel_lock(tsk)

            /*  reacquire_kernel_lock - reacquire the BKL.after got lock,disable 
             *                          kernel preempt.
             *  @tsk:                   task pointer used to check @lock_depth
             *  return:                 -EAGAIN -> need reschedule but failed to
             *                                     acquire lock
             *                          0 -> succeed to acquire lock,before return,
             *                               kernel preempt would be disabled.
             *  #  this function checks whether @tsk->lock_depth is greater than or
             *     equal to zero first.
             *     if it is,call to __reacquire_kernel_lock()
             *     else return 0.
             */
            static inline int reacquire_kernel_lock(struct task_struct *tsk);

            !!  if a process is holding BKL,and call to schedule() to relinquish CPU,
                BKL is automatically unlocked during process switch by schedule(),
                but BKL would be automatically reacquired by schedule() when the process
                is selected to run by scheduler.

          <lib/kernel_lock.c>
            /*  kernel_flag - the BKL.  */
            static __cacheline_aligned_in_smp DEFINE_RAW_SPINLOCK(kernel_flag);

          !!  task_struct.lock_depth is crucial for allowing interrupt handlers,exception
              handlers,and deferrable functions to take the BKL,without it,every asynchronous
              function that tries to get the BKL could generate a deadlock if the current
              process already owns the BKL.
              /*  these three kinds of kernel control paths are use the process kernel stack.
               *  @lock_depth is process exclusive.
               *  @lock_depth == -1 -> unlocked
               *  because @lock_depth,the same process issues two or more consecutive requests for
               *  BKL would not hang processor.
               */

          !!  In Linux 2.6.11,BKL is implemented via semaphore,semaphore is allows kernel preempt
              inside a critical region.
              thus,if a process is holding a semaphore inside a critical region,it maybe preempted
              by other.
              but in the case,semaphore should not be released to prevent the possible data corruption.
              preempt_schedule_irq() temporarily sets @lock_depth to -1,then schedule() does not
              release BKL,because @lock_depth < 0;preempt_schedule_irq() restores the @lock_depth
              before the preempted process is selected to run but have not been replaced.

        Memory Descriptor Read/Write Semaphore >
          @mm_struct defined in <linux/mm_types.h> has a member named @mmap_sem is type of 
          struct rw_semaphore,when Page Fault occurred,page_fault() get read-semaphore to scans
          the memory descriptors.
          @mmap_sem used to protects the mm_struct when it is shared by several lightweight processes.
          e.g.
            process A shared mm_struct @mm with process B

            process A call to do_mmap() to extends virtual memory.
            if no free memory is available,A would be suspended.
            for extending,write-semaphore have to be holden.

            process B is stay running,it is able to use the shard memory but can not read/write @mm.

        Slab Cache List Semaphore >
          struct kmem_list3 is defined in <mm/slab.c>,a member named @list_lock is type of spin lock
          used to protect concurrently access to this structure.
          struct kmem_cache is defined in <linux/slab_def.h>,it contains a member is named @nodelists
          is an array of struct kmem_list3.

          @kmem_list3 is the slab lists for all objects.when kernel control path wants to alloc a new
          node in a nodelist,it have to acquire the @list_lock for grants an exclusive right to access
          and modify the list.

        Inode Semaphore >
          struct indoes is defined in <linux/fs.h>,which contains some kernel primitives such spin lock,
          mutex,rw semaphore,etc.
          
          a member named @i_lock is type of spinlock_t used to protects the struct inode get accessing
          concurrently to several file-system drivers or syscalls.
          any process wants to modify inode have to acquire this lock at first.

        !!  whenever more than one locks is to request,the changes of dead-lock to occur would
            increase significantly.for reduce dead-lock probability,must acquire the locks in a
            predefined order.
          

Chapter 6 : Timing Measurements
    Linux Kernel must performs two main kinds of timing measurement :
      1>  keeping the current time and data
      2>  maintaining timers(timers notify kernel,user program a certain interval of time has elapsed)

      timing measurements are performed by several hardware circuits based on fixed frequency
      oscillators and counters.

    Clock and Timer Circuits :
      clock circuits :
        keep track of the current time of day and to make precise time measurements.
        the timestamp can be re-synchronized by kernel.

      timer circuits :
        they are programmable circuits,they issue the timer interrupts at a fixed,predefined frequence,
        kernel is able to set the frequence.

      <===IBM-compatible===>


      Real Time Clock(RTC) : (Hardware,often integered into Motherboard of Computer)
       real time clock is independent of the CPU and all other chips.
       even the PC power off it stay tick,because it is energized by a small battery,the COMS RAM
       and RTC are integrated in a single chip.
       RTC issue periodic interrupts on IRQ8 at frequencies raning between 2Hz -- 8192Hz .
       it is programmable to make it works like an alarm clock(active IRQ8 when RTC reaches a specific value).

       Linux use RTC only to derive the time and date,it allows processes to program RTC via "/dev/rtc"
       device file.
       the kernel accesses the RTC through the 0x70 and 0x71 I/O ports.

     Time Stamp Counter(TSC) :
       80x86 microprocessor include a CLK inpt pin,CPU receive a clock signal through it from an external
       oscillator.
       Starting with the Pentium,80x86 microprocessor has a counter that is increased at each clock signal,
       the counter is accessible through the 64-bit Time Stamp Counter (TSC) register.

       /** 
        * the data of TSC is abled to be readed by assembly instruction "rdtsc".
        * everytime a clock signal raised on the CLK input pin,TSC automatically increase.
        */

       Linux can use TSC to get more accurate time measurements than Programmable Interval Timer.
       different PC might have the different clock signal frequency,thus the kernel does not specify
       a specialized clock signal frequency when comiling.

       Linux determine the clock signal frequency when initializing the system,to figure out the
       actual clock signal frequency of a CPU,the function calibrate_tsc() is used to counting the
       number of clock signals that occur in a time interval of approximately 5 milliseconds.

       /* x86_init_ops is defined in <arch/x86/include/asm/x86_init.h> */
       x86_init_ops is a structure which contains some primitives used when boot the system on 
       x86 platform.

       the structure x86_platform_ops contains a member named "calibrate_tsc" is type of 
       unsigned long function with none of parameters.
       @calibrate_tsc would be set to the architecture depending calibrate tsc function
       native_calibrate_tsc() .

       finally,the time constant is produced by properly setting up one of the channels of
       the Programmable Interval Timer.
       /** 
        * 1Hz means the thing appears once in a periodic
        * 100Hz means the thing appears 100 times in a periodic
        * so,we can define the N Hz is the number of clock signal is appear in a second
        * then we know after N times clock signal is past,a second is past
        */

       !! to avoid losing significant digits in the integer divisions,calibrate_tsc()
          returns the duration,in microseconds,of a clock tick multiplied by 2^32 .
          /*  clock tick,it is alike to that the clock hanged on the wall tick once
           *  to tell us a second is past.
           */

     Programmable Interval Timer(PIT) :
       PIT notify the kernel that a specified time interval has elapsed.it issues a special
       interrupt called "timer interrupt" to the kernel after a time interval has elapsed.
       after set up PIT,it continuously issues interrupt at a fixed frequency specified by kernel.

       /*  PIT is also used to drive an audio amplifier connected to the computer's
        *  internal speaker.
        *  IBM-compatiable PC cotains at least a PIT,it implemented by an 8254 CMOS chip using
        *  the 0x40--0x43 ports.
        */

       such hardware interrupt is issued on IRQ0 at a (roughly) 1000Hz(or 100Hz) frequency,
       the time interval approximately equal to 1 millisecond and called a tick.

       <kernel/time/ntp.c>
         /*  tick_nsec - the tick in nanoseconds.
          *  #  it approximately equal to 999.848 nanoseconds.
          *     clock signal frequency of about 1000.15Hz .
          *     this variable may automatically adjusted by kernel when it synchronizes to
          *     an external clock.
          */
         unsigned long tick_nsec;
        
       shorter ticks result in higher resolution timers,that will help to some featrues are
       requie more accurate timer to do something.(ie. I/O multiplexing or multimedia playback ...)
       but shorter ticks means CPU have to traps into Kernel Mode more frequently(timer interrupt).
       /*  the frequency of timer interrupts depends on the hardware architecture.  */

       some macro definitions used by kernel to yields time constant :
         CONFIG_HZ - yields the approximately number of timer interrupts per second.
                     this is defined in kernel config file,and Makefile use this macro 
                     to produces HZ in header <timeconst.h> .
                     default value on x86 platform is 1000

         CLOCK_TICK_RATE - it is the 8254 chip's internal oscillator frequency.
                           in header <arch/x86/include/asm/timex.h>,it is deined with value
                           PIT_TICK_RATE,which is defined in <linux/timex.h> with value
                           1193182ul .

         LATCH - it is used by PIT,defined in <linux/jiffies.h> with value
                 (CLOCK_TICK_RATE + HZ/2) / HZ
    
       on x86 platform,the PIS is model i8253,and some associated macros and functions are defined
       in header <arch/x86/include/asm/i8253.h>,implementations in <arch/x86/kernel/i8253.c> :
         #define PIT_MODE 0x43
         #define PIT_CH0 0x40
         #define PIT_CH2 0x42

         /**
          * setup_pit_timer - register a clock event device for PIT,and set the clock event
          *                   as the global clock event.
          * 
          * #  a static object named @pit_ce is type of struct clock_event_device
          *    defined in <i8253.c>,the definition of struct clock_event_device in
          *    <linux/clockchips.h> .
          *    the static object @pit_ce's @set_mode member is assigned to init_pit_timer() which
          *    is also in <i8253.c> as a static function.
          *    the default mode of PIT device is CLOCK_ENV_FEAT_PERIODIC | CLOCK_ENV_FEAT_ONESHOT .
          *    so it supports two kinds of work modes.
          *    when booting system,the boot CPU have to initialize PIT through communicate to the
          *    device via I/O ports :
          *      case CLOCK_ENV_FEAT_PERIODIC:
          *              outb_pit(0x34, PIT_MODE);           =>  work on periodic mode.
          *              outb_pit(LATCH & 0xff, PIT_CH0);    =>  LSB of LATCH
          *              outb_pit(LATCH >> 8, PIT_CH0);      =>  MSB of LATCH
          *
          *      case CLOCK_ENV_FEAT_ONESHOT:
          *              outb_pit(0x38, PIT_MODE);           =>  work on oneshot mode.
          *
          *      0x30 means PIT UNUSED,in this case,frequency of internal oscillator is set to zero.
          *
          *    outb_pit -> outb_p -> outb
          *                          inline function constructed by BUILDIO macro.
          *    outb has same function as IO instruction "outb" that is write first operand
          *    to the second operand which should be I/O port.
          *    how many length of data can be written is depends on hardware,sometimes,delay to
          *    consecutive operations is required for device to read data from I/O port.
          */
         extern void setup_pit_timer(void);

     CPU Local Timer :
       local APIC provides another time-measuring device: the CPU local timer.

       CPU local timer is similar to PIT but has same difference :
         >  APIC's timer counter is 32-bits,PIT's timer counter is 16-bits,therefore,CPU local
            timer can programmed to issues timer interrupt at very lower frequency.
            (counter stores the number of ticks that must elapsed before issue a timer interrupt)

         >  CPU local timer issues timer interrupt only to its CPU,PIT raise global clock event.

         >  CPU local timer based on the bus clock signal,it can be programmed in such a way to
            decrease the timer counter every 1, 2, 4, 8, 16, 32, 64, 128 bus clock signals.
            PIT make use its own clock signals,can be programmed in a more flexible way.

     High Precision Event Timer(HPET) :
       timer chip developed jointly by Intenl and Microsoft.

       HPET provides a number of hardware timers that can be exploited by the kernel.
       basically,it includes up to eight 32-bit or 64-bit independent counters.
       each counter is driven by its own clock signal,and the frequency at least is 10Hz.
       any counter associated with at most 32 timers and the timer is composed by a
       "comparator" and a "match register".

       comparator used to checks whether the value of counter is matched in the match register,
       if it is,then raise a hardware interrupt.
       /**
        * some timers support to generate a periodic interrupt.
        * for program them,have to write instructions into the I/O address where mapped to 
        * their register,such mapping is constructed during bootstraping phase by BIOS.
        * HPET allows kernel process I/O operation on counter and match register,and set the
        * oneshot mode or periodic mode(work mode depends on timer).
        */

                +-------+
                |counter|  <== 32bit or 64bit
                +-------+
                  | | |
                  | | +------->  timer1 { comparator match-register }
                  | +--------->  timer2
                  +----------->  timer3
                  ...            ...

     ACPI Power Management Timer(ACPI PMT) :
       ACPI PMT is another clock device included in almost all ACPI-based motherboards,and its
       clock signal frequency is fixed at roughly 3.58Hz.
       it actually a simple counter increased at each clock tick,kernel can read its value through
       an I/O port whose address mapping is established by BIOS.

       ACPI PMT is preferable to the TSC if the OS or the BIOS may dynamically lower the frequency
       or voltage of the CPU to save power.
       /**
        * CPU enter the power save mode,TSC would automatically changes its frequency,
        * ACPI PMT would not.
        * however,a HPET is present,it should always be preferred to the other circuits because
        * of its richer architecture.
        */

    The Linux Timekeeping Architecture :
      several time-related activitys that kernel must carry on >
        <  updates the time elapsed since system startup
        <  updates the time and date
        <  time slots checking for the processes on CPUs
        <  updates resource usage statistics
        <  checks whether the interval of time associated with each software timer has eplased

      linux timekeeping architecture is the set of time-related kernel data structures and some functions.
      it depends on the availability of the TSC,of the ACPI PMT,and of the HPET.
      kernel uses two basic timekeeping functions :
        one to keep the current time up-to-date
        another to count the number of nanoseconds that have eplased within the current second
      /**
       * 80x86 based multiprocessor machine have a different timekeeping architecture to uniprocessor
       *   uniprocessor : all time-keeping activities are triggered by interrupts raised by the
       *                  global timer(PIT or HPET)
       *   multiprocessor : all general activities are triggered by the interrupts raised by the global
       *                    timer,while CPU-specific activities are triggered by the interrupts raised
       *                    by the local APIC timer
       *                    general activity - such software timer
       *                    CPU-specific - monitoring the execution time of the currently running process
       *   #  the early SMP system based on 80486 Intel processor did not have local APICs.
       *      on some SMP motherboards,the local timer interrupts are not usable.
       */

      Data Structures of the Timekeeping Architecture :
        the timer object :
          "understanding the linux kernel" has introduced a structure is named timer_opts,which represents
          a timer object.
          but such structure had been removed in patch 
            "Time: i386 Conversion - part 4: Remove Old timer_opts Code"
          at Fri,17 Mar 2006 .
          SO THE Linux 2.6.34.1 HAS NOT SUCH STRUCTURE NOW!

        timer_list :
          Linux 2.6.34.1 has a structure is named @timer_list which is inclued in <linux/timer.h> .

          <linux/timer.h>
            /**
             * timer_list - list is constructed by timers,and each entry represents a timer
             * @entry:      embedded list_head object
             * @expires:    expire jiffies(setting jiffies + timeout)
             * @function:   what to do after expired
             * @data:       argument for @function(deviceID or other meaningful data)
             * @base:       time vector,indicates which vector this timer based
             * @start_site: which instruction started this timer
             * @start_comm: command name
             * @start_pid:  pid of the process started this timer
             */
            struct timer_list {
                    struct list_head entry;
                    unsigned long expires;
                    void (*function)(unsigned long);
                    unsigned long data;
                    struct tvec_base *base;

            #ifdef CONFIG_TIMER_STATS
                    void *start_site;
                    char start_comm[16];
                    int start_pid;
            #endif
                    ...
            };

            /** 
             * structure tvec_base is defined in <kernel/timer.c>.
             * different timer belongs to different time vector.
             * it has a spin lock member used to prevent race condition on it.
             */

          timer_list is one of components for high-resolution timers.
          high-resolution timers is newer timer mechanism(timer wheel has been abandoned).
          timer_list as the component brings the two new benefits :
            >  time ordered enqueuing into a rb-tree
            >  independent of ticks (the processing is based on nanoseconds)

          hrtimer base data structure - ktime_t
            every time value,absolute or relative,is in a special nanoseconds-resolution type
            ktime_t.
            
            <linux/ktime.h>
              union ktime {
                      s64 tv64;
              #if BITS_PER_LONG != 64 && !defined(CONFIG_KTIME_SCALAR)
                      struct {
              # ifdef __BIG_ENDIAN
                              s32 sec, nsec;
              # else 
                              s32 nsec, sec;
              # endif } tv;
              #endif
              };

              typedef union ktime ktime_t;

              #define KTIME_MAX ((s64)~((u64)1 << 63))

              #if (BITS_PER_LONG == 64)
              # define KTIME_SEC_MAX (KTIME_MAX / NSEC_PER_SEC)
              #else
              # define KTIME_SEC_MAX LONG_MAX
              #endif

        jiffies :
          there are two jiffies existed,one is @jiffies_64 is defined in <kernel/timer.c>,
          another is @jiffies which is defined in the linker script.

          <linux/jiffies.h>
            #define __jiffy_data __attritbue__((section(".data")))

            extern u64 __jiffy_data jiffies_64;
            extern unsigned long volatile __jiffy_data jiffies;

            /**
             * initial value for jiffies,the counter will overflow five minutes after the
             * system boot.
             */
            #define INITIAL_JIFFIES ((unsigned long)(unsigned int) (-300 * HZ))

          jiffies is a counter that stores the number of elapsed ticks since the system was started.
          it is increased by one when a timer interrupt occurs,that is,on every tick.

          jiffies would wraparound after data type is overflowed.
          linux handles cleanly the overflow of jiffies through some macros :
            time_after, time_after_eq, time_before, time_before_eq
            /* these macro yield the correct value even if a wraparound occurred. */

          in some cases,the kernel needs the real number of system ticks eplased since the system boot,
          regardless of the overflows of jiffies,so @jiffies_64 saves the ticks eplased,and linker
          equated @jiffies to the LSB 32bits of @jiffies_64.
          /*
           * value of @jiffies_64 is not atomic on 32-bit architecture,
           * therefore,read @jiffies_64 must through function get_jiffies_64().
           * the function acquires @xtime_lock before reading,any writing(updating) to
           * @jiffies_64 must wait until the reading is accomplished.
           * any updating to @jiffies_64 will synchronize to @jiffies(LSB 32bits of @jiffies_64).
           */

        xtime :
          xtime is a variable used to save the current time and date,it is type of struct timespec.
          
          <linux/time.h>
            struct timespec {
                    /* __kernel_time_t is a type definition of "long" */
                    __kernel_time_t tv_sec;  /* seconds */
                    long tv_nsec;            /* nanoseconds */
            };

          @xtime.tv_sec - number of seconds that have eplased since midnight of January 1,1970(UTC)
          @xtime.tv_nsec - number of nanoseconds that have eplased within the last second

          <kernel/time/timekeeping.c>
            /* xtime_lock - @xtime sequence lock */
            __cacheline_aligned_in_smp DEFINE_SEQLOCK(xtime_lock);

            /* xtime - current time and date */
            struct timespec xtime __attribute__ ((aligned (16)));

            /*
             * the function get_jiffies_64() above acquires @xtime_lock reading lock before read from
             * @jiffies_64.
             */

          @xtime variable is usually updated once in a tick,the user programs get the current time and
          date through it,and kernel use it to updating inode timestamps,etc.

          !!  @xtime_lock IS USED TO PROTECT SEVERAL CRITICAL REGIONS OF THE TIMEKEEPING ARCHITECTURE.

      Timekeeping Architecture in Uniprocessor Systems :
        on uniprocessor platform,all time-related activity are triggered by the PIT interrupt on IRQ0.
        some of these activities are executed right after the interrupt raised,the remaining activites
        are carried on by deferrable functions.

        initialization phase :
          during kernel initialization,the function time_init() is used to initializes system timekeeping
          architecture.

          <arch/x86/kernel/time.c>
            /**
             * time_init - initializes TSC and delay the periodic timer init to late x86_late_time_init()
             *  
             * #  @late_time_init is a pointer to a function which receives nothing and returns nothing,
             *    if it is not NULL,the function start_kernel() will call to it.
             */
            void __init time_init(void);

            /*
             * x86_late_time_init - x86 platform timekeeping architecture initializer
             *
             * #  this function calls to x86_init.time.timer_init() to sets up timer,
             *    then calls to tsc_init() to sets up TSC.
             *    generally,the @timer_init of @x86_init points to hpet_time_init().
             */
            static __init void x86_late_time_init(void);

            /**
             * hpet_time_init - set up system timer
             * 
             * # if hpet_enable() returned false,use PIT as a substitution,else,use the HPET.
             *   set up default timer interrupt via setup_default_timer_irq(),it associates
             *   IRQ0 with @irq0 is type of struct irqaction.
             */
            void __init hpet_time_init(void);

            /**
             * irq0 - the interrupt action of interrupts raised on IRQ0
             * @handler: time_interrupt() as the interrupt handler
             * @flags:   disables IRQ0 during interrupt handler is processing |
             *           no irq balancing |
             *           interrupt is used for polling |
             *           timer interrupt
             * @name:    "timer"
             */
            static struct irqaction irq0 = {    
                    .handler = timer_interrupt,
                    .flags = IRQF_DISABLED | IRQF_NOBALANCING | IRQF_IRQPOLL | IRQF_TIMER,
                    .name = "timer
            };

            /**
             * hpet_enable() is defined in <arch/x86/kernel/hpet.c>.
             * if hpet is capable,@boot_hpet_disable must be false,and the @hpet_address is the
             * the memory address mapped to hpet I/O ports.
             * if hpet is enabled,@hpet_legacy_int_enabled must be 1,its value is setup by
             * hpet_enable_legacy_init(),this function is called by hpet driver used to init HPET.
             */

          @wall_to_monotonic is type of struct timespec,it is defined in <kernel/time/timekeeping.c>.
          this variable is what kernel need to add to @xtime to get to monotonic time,or @xtime
          corrected for sub jiffie times.
          its @tv_sec maybe negative,but @tv_nsec always be positive.

          !!  <kernel/time/clocksource.c> defined @curr_clocksource,it is type of struct clocksource,
              and function clocksource_select() sets up it to the best clocksource available in the
              system.
              generally,the variable @timekeeper is type of struct timekeeper and used to represents the
              timekeeper is activied in the system,a member named @clock is point to @curr_clocksource.
              /* static function change_clocksource() used to modify @clock member. */

          @xtime is initialized by function timekeeping_init() which is defined in <kernel/time/timekeeping.c>,
          start_kernel() calls to timekeeping_init() to initializes the clocksource and common timekeeping
          values.
            /* read the persistent clock to get @now,it is specified during system booting. */
            @xtime.tv_sec = now.tv_sec;
            @xtime.tv_nsec = now.tv_nsec;
            ...
            if @boot.tv_sec == 0 && @boot.tv_nsec == 0
            then
                    @boot.tv_sec = @xtime.tv_sec
                    @boot.tv_nsec = @xtime.tv_nsec
            /* initializes @wall_to_monotonic with the boot time */
            set_normalized_timespec(&wall_to_motonotic, -@boot.tv_sec, -@boot.tv_nsec);

            /**
             * macro definition NSEC_PER_SEC = 1000000000L in <linux/time.h>.
             *
             * @nsec = -@boot.tv_nsec, @sec = -@boot.tv_sec
             * loop until @nsec < NSEC_PER_SEC
             *         @nsec -= NSEC_PER_SEC
             *         ++@sec
             * loop until @nsec > 0
             *         @nsec += NSEC_PER_SEC
             *         --@sec
             * finally,setup @wall_to_monotonic
             * 
             * the first loop will be skipped because @nsec < 0,the second loop ensure
             * @wall_to_monotonic.tv_nsec is positive and @wall_to_monotonic.tv_sec is
             * negative.
             * so,use @xtime + @wall_to_monotonic is able to get boot time.
             * but @wall_to_monotonic is no longer the boot time,for get boot time,have to
             * use getboottime(),the real boot time is get through @wall_to_monotonic and
             * @total_sleep_time.
             */
      
        timer_interrupt :
          timer_interrupt() is the ISR for timer interrupt.
          
          <arch/x86/kernel/time.c>
            /**
             * timer_interrupt - common timer interrupt handler
             * @irq:             IRQ line number,it usually is IRQ0
             * @dev_id:          device exclusive data
             * return:           IRQ_HANDLED
             */
            static irqreturn_t timer_interrupt(int irq, void *dev_id);

          timer_interrupt just a common timer interrupt ISR,what it will does is depends on what
          the timer device is selected by kernel.
          generally it does the following steps :
            1>  increase irq_cpustat_t.__nmi_count to update the NMI interrupt counter.
            2>  if @timer_ack is TRUE
                then
                        acquire spin lock @i8259A_lock
                        "outb" 0x0c on PIC_MASTER_OCW3 - it is PIC_MASTER_ISR <- PIC_MASTER_CMD(0x20)
                        "inb" on PIC_MASTER_POLL - it also is PIC_MASTER_ISR
                        release spin lock
                        /* 
                         * these two assembly instructions acknowledge to PIC we have handled the interrupt
                         * AEOI will end it automatically.
                         * 
                         * this IF is optimized out for !IO_APIC AND x86_64
                         * AEOI means "Automatic End Of Interrupt",it is work mode of intel 8259A PIC
                         */
            3>  call to @global_clock_event->event_handler
                if no HPET chip had probed,it generally is PIC
            4>  if MCA_bus is TRUE
                then
                        /* acknowledge irq0 by setting bit 7 in port 0x61 */
                        outb_p(inb_p(0x61) | 0x80, 0x61)
                /* MCA - Microchannel Architecture Bus */
            5>  return

          !!  @event_handler is setup by framework,the struct clock_event_device is described in
              <linux/clockchips.h>.
         
          
          !!  if a timer device is selected by kernel used for periodic tick,then,@event_handler
              of the clock_event_device belongs to such device will be assigned to @tick_handle_periodic.

              function tick_setup_device() is defined in <kernel/time/tick-common.c>,it changes the clock
              event device of a tick device to a newer clock event device.
                if the tick device is used for periodic tick,then call to tick_setup_periodic(),this function
                sets the @event_handler to function @tick_periodic_handler if no broadcast is enabled,
                otherwise,set to @tick_periodic_handler_broadcast.

                if the tick device is used for oneshot tick,then call to tick_setup_oneshot(),this function 
                sets the @event_handler of the newer clock event device to the event handler of current tick
                device,and changes the work mode of the clock devent device to CLOCK_EVT_MODE_ONESHOT.

             !!  IF THE TICK DEVICE HAVE NONE OF CLOCK EVENT DEVICE NOW,THEN CAN NOT SET IT AS AN ONESHOT
                 TICK DEVICE,THUS ITS WORK MODE WILL AUTOMATICALLY SET TO TICKDEV_MODE_PERIODIC.

             !!  CONFIG_NO_HZ => dynamic clock (nohz)
                                 allow to stop periodic clock when system is in idle state

          <kernel/time/tick-common.c>
            /**
             * tick_handle_periodic - the common periodic tick handler
             * @dev:                  the underlying clock event device
             *
             * #  the main works are accomplished by tick_periodic().
             *    it checks whether @tick_do_timer_cpu is equals to current cpu at first
             *      TRUE
             *        acquires @xtime_lock
             *        updates @tick_next_period to keep tracing to next periodict event
             *        call to do_timer(1)
             *        release @xtime_lock
             *    call to update_process_times() to updates CFQ time slots of processes
             *    call to profile_tick(CPU_PROFILING)
             *    return
             */
            void tick_handle_periodic(struct clock_event_device *dev);

          <kernel/timer.c>
            /**
             * do_timer - updates timekeeping architecture associated data
             * @ticks:    tick delta
             * 
             * #  this function add @ticks to @jiffies_64
             *    call to update_wall_time() in order to updates wall time
             *    call to calc_global_load() for re-calculating system global loading
             *    if a time slot of a process had exhausted,scheduler have to pick next
             *    process for running
             */
            void do_timer(unsigned long ticks);
            
      Timekeeping Architecture in Multiprocessor systems :
        multiprocessor system can rely on two different sources of timer interrupts :
          1>  PIT or High Precision Event Timer(hrtimer)
              /* PIT or HPET used for the global timer event */
          2>  CPU local timer
              /* CPU local timer interrupt signals timekeeping activities related to the local CPU. */

        initialization phase :
          linux uses interrupt vector 239(0xef) as the local APIC timer interrupt.

          <arch/x86/kernel/irqinit.c>
            /**
             * smp_intr_init - initializes CPU-to-CPU interrupt gates
             * 
             * #  if no CONFIG_SMP AND no CONFIG_X86_64 OR CONFIG_X86_64_APIC defined
             *    this function does nothing.
             *    otherwise,setup IPI.
             */
            static void __init smp_intr_init(void);

            /**
             * apic_intr_init - initializes APIC related interrupt gates
             *
             * #  if no CONFIG_X86_64 OR CONFIG_X86_64_APIC defined
             *    kernel will not creates some APIC related interrupt gates.
             *    kernel will not creates local CPU APIC timer interrupt gate,
             *    otherwise,create local APIC timer interrupt gate on vector 0xef(LOCAL_TIMER_VECTOR),
             *    the handler is apic_timer_interrupt().
             */
            static void __init apic_intr_init(void);

          apic_timer_interrupt() is a low-level interrupt handler,after acknowledged interrupt,it calls to
          local_apic_timer_interrupt() to deal with the local CPU APIC timer interrupt.
          /**
           * symbol "apic_timer_interrupt" is re-defined to "smp_apic_timer_interrupt" in <arch/x86/kernel/entry_64.S>.
           * and function smp_apic_timer_interrup() is defined in <arch/x86/kernel/apic/apic.c>.
           * local_apic_timer_interrupt() is defined in the same header,which retrieve current CPU-id from SMP
           * environment,next retrieve the per-CPU variable @lapic_events is type of struct clock_event_device,
           * if @event_handler is NULL,then call to lapic_timer_setup() to sets the clock event device work on
           * CLOCK_EVT_MODE_SHUTDOWN,that is disable it;
           * otherwise,call to the @event_handler set by framework.(NMI counter increase)
           */

          function "static void __init calibrate_APIC_clock(void)" is used to calibrate the local APIC timer's
          frequency of booting CPU during a tick(1ms),the exact value is used to program the local APICs.
          /**
           * calibrate_APIC_clock() is called by "void __init setup_boot_APIC_clock(void)",it as an operation
           * inclued by x86_init_ops as the member @setup_percpu_clockev.
           * global variable @x86_cpuinit is type of struct x86_cpuinit_ops,member @setup_percpu_clockev is set
           * to "void __cpuinit setup_secondary_APIC_clock(void)",this function will call to
           * "static void __cpuinit setup_APIC_timer(void)".
           * function setup_APIC_timer() copies the calibrated value from boot CPU and use it to program its
           * local APIC timer.
           */

          !!  setup_boot_APIC_clock() is called by APIC_init_uniprocessor(),which is called by smp_sanity_check().
              start_kernel -> rest_init -> kernel_init -> smp_prepare_cpus -> smp_sanity_check
              -> APIC_init_uniprocessor -> setup_boot_APIC_clock

          !!  @x86_cpuinit is figure in __cpuinitdata,this macro expands to "__section(.cpuinit.data)".
          
          !!  under SMP environment,each cpu has a per-CPU variable named @tick_cpu_device is type of 
              struct tick_device.
              /* uniprocessor environment,the CPU stay has such variable,but only one is existed in the system. */
              in function setup_tick_device(),if the clock event device of tick device is unassigned,
              then function checks whether @tick_do_timer_cpu is equals to TICK_DO_TIMER_BOOT(value is -2)
              /* no CPU took the do_timer update,assign it to current cpu */
                TRUE
                        sets @tick_do_timer_cpu to current cpu  /* the CPU becomes do timer cpu */
                        sets @tick_next_periodic = ktime_get()  /* set next periodic */
                        sets @tick_period = ktime_set(0, NSEC_PER_SEC / HZ) /* set timer tick frequency */
              
              IN FUNCTION tick_periodic(),IF CURRENT CPU IS NOT THE @tick_do_timer_cpu,THEN ONLY UPDATES THE PROCESSES'
              TIMES ON CURRENT CPU AND PROFILE TICK.
              THE CPU IS @tick_do_timer_cpu,THEN CALL TO do_timer(1) TO UPDATES "Timekeeping Architecture" RELATED
              DATA.

              as described above,SMP environment,it is still only one CPU deal with do_timer update,and finally,
              does the same thing as what uniprocessor does.

      Updating the Time and Date :
        function do_timer() updates @jiffies_64,call to function update_wall_time() to updates wall time.
    
        <kernel/time/timekeeping.c>
          /**
           * update_wall_time - uses the current clocksource to increment the wall time
           * 
           * #  function is called from the timer interrupt,and @xtime_lock is required.
           */
          void update_wall_time(void);

          the things update_wall_time() does :
            1>  get current clocksource of @timekeeper.
                /**
                 * struct clocksource is defined in <linux/clocksource.h>,it is hardware
                 * abstraction for a free running counter,provides mostly state-free accessors
                 * to the underlying hardware.
                 */

            2>  calculate offset,it is number of clock cycles in one NTP interval.
                /**
                 * clock cycle - time is not a consecutive line,it is separated short lines 
                 *               linked together,each of them is a clock cycle,means a time
                 *               periodic.
                 *               generally,tick represents such time periodic.
                 *               HZ specified how many tick in a second,in other word,how long
                 *               the separated line is in one second.
                 */

            3>  get shift,it is the shift value of current clocksource.
                /**
                 * for convert clock cycles to nanoseconds,have to calculate
                 *   (clock cycles / FREQUENCY) * NSEC_PER_SEC => nanoseconds
                 *
                 * division is deprecated in kernel,thus for remove division,must specify
                 * multiple and shift to convert the formula above to
                 *   clock cycles * multiple >> shift
                 *
                 * clocksource.mult => multiple AND clocksource.shift => shift
                 */

            4>  use offset to updates shift.
                /**
                 * this is implemented via a while-cycle,and inside the cycle,
                 * function logarithmic_accumulate(offset, shift) is called to
                 * update @xtime.tv_sec,new offset is got from result of the function.
                 */
                then call to timekeeping_adjust(offset) to adjust NTP error.

            5>  store full nanoseconds in @xtime.tv_nsec.
                @xtime.tv_nsec = @timekeeper.xtime_nsec >> @timekeeper.shift + 1
                /**
                 * timekeeper.xtime_nsec stores the clock shifted nano seconds remainder not
                 * stored in @xtime.tv_nsec.
                 */

            6>  use @xtime to update @timekeeper.xtime_nsec and @timekeeper.ntp_error

            7>  call to update_xtime_cache() with the converted nanoseconds.
                /** 
                 * @xtime_cache is cache for @xtime,@xtime.tv_nsec stored the full nanoseconds.
                 *   @xtime_cache = @xtime + converted nanoseconds
                 *
                 * at step 5,@xtime stored the remainded nanoseconds from most recently timer
                 * interrupt.
                 * the converted nanoseconds is calculated by offset -- clock cycle value at
                 * this time(the timer interrupt).
                 */

            8>  call to update_vsyscall(&xtime, timekeeper.clock, timekeeper.mult) to the data
                readonly from vsyscalls,such data is written by timer interrupt or systc.

      Updating Local CPU Statistics :
        updating process times :
          tick_periodic() call to update_process_times(user_mode(get_irq_regs())) to updating
          Local CPU Statistics about to processes running on current CPU.
          
          <kernel/timer.c>
            /**
             * update_process_times - charge one tick to the current process
             * @user_tick:            1 => the user time tick
             *                        0 => the system time tick
             */
            void update_process_times(int user_tick);

            the things that update_process_times() to does :
              1>  fetch @current process on current CPU.

              2>  call to account_process_tick(@current, @user_tick).
                  the function is defined in <kernel/sched.c>,
                  if @user_tick == 1
                          => account_user_time()
                  else if @current != this_rq()->idle || irq_count() != HARDIRQ_OFFSET
                          => account_system_time()
                  else
                          => account_idle_time()

                  account_user_time() is also defined in <kernel/sched.c>,which updates current
                  process's associated times(@utime, @utimescaled, group user time).
                    utime += @cputime               /* via cputime_add() */
                    utimescaled += @cputime_scaled  /* utime - time spent in User Mode */
                    /* cputime is @cpu_one_jiffy,cputime_scaled is cputime_to_scaled(@cpu_one_jiffy) */

                  next add user time to cpu_usage_stat,it converts cputime_t @cputime to 64-bit
                  then checks whether TASK_NICE(@current) > 0,if it is,update cpu_usage_stat.nice;
                  otherwise,update cpu_usage_stat.user.
                  finally call to account_update_integrals(@current) to account for user time used,this
                  function will updates the fields associated to mm member of the task.

                  account_system_time() is also defined in <kernel/sched.c>,which account system cpu time
                  to a process.it is similar to account_system_time(),but it might updates @irq,@softirq,or
                  @system in cpu_usage_stat.
                  in the case that @current->flags & PF_VCPU && irq_count() - @hardirq_offset == 0
                  then just account guest cpu time to a process and return. /* HARDIRQ_OFFSET */
                  /* account_system_time() call to account_update_integrals(),too. */

                  account_idle_time() might updates @iowait or @idle in cpu_usage_stat depends on 
                  this_rq()->nr_iowait > 0.

              3>  call to run_local_timers() to starts hrtimers,then raise a softirq is type of 
                  TIME_SOFTIRQ,before returning,call to softlockup_tick() to checks whether the
                  watchdog thread has hung or not.
                  /* run_local_timers() is called from per-CPU time interrupt of local-CPU on SMP */

              4>  call to rcu_check_callbacks() to checks whether this CPU is in a non-context-switch
                  quiescent state.
                  if no rcu pending on this CPU,do nothing.
                  if @user_tick == 1 OR (current CPU is idle AND rcu active AND not in softirq  AND
                  hardirq_count() <= (1 << HARDIRQ_SHIFT))
                  then note a sched quiescent state and a bottom-half quiescent state.
                  if not in softirq,then note a bottom-half quiescent state only.

                  before return,checks a quiescent state from the current CPU through call to
                  rcu_preempt_check_callbacks(),if @current->rcu_read_lock_nesting == 0,then disable
                  flag RCU_READ_UNLOCK_NEED_QS in @current->rcu_read_unlock_sepcial,next note a preempt
                  quiescent state;otherwise,just enable RCU_READ_UNLOCK_NEED_QS as well.
                  finally,raise a softirq is type of RCU_SOFTIRQ and returns.
                  /* rcu_process_callbacks() handler will deal with this softirq. */
                  /* callbacks of RCU are queued by call_rcu() */

              5>  checks if there are some messages have to be logged in kernel message loop-queue and 
                  print them.(via printk_tick() to checks and prints)

              6>  checks if there are some performance events pending and deal with them.

              7>  call to scheduler_tick() to does scheduling,this will cause run-queue clock
                  updating,cpu-load updating.

                             (for real-time process,task_tick_fair() for non-real-time process)
                  it call to task_tick_rt() updates current task's runtime statistics in the run-queue
                  of this CPU.
                  reduce one to current task's real-time time_slice,if it is greater than 0,then returns;
                  otherwise,set it equals to DEF_TIMESLICE,next checks the @run_list of a real-time 
                  scheduling class which the curr is associated to whether contains another processes,
                  if it is,then requeue current task to the end,and setup its task need resched flag.
                  /**
                   * time_slice is not equal to 0 means the time slot of current is not exhausted.
                   */

                  it call to perf_event_task_tick(rq->curr) to adjust performance event related robin
                  tree.

                  /* on SMP,loading rebalance will be triggered before return. */

              8>  call to run_posix_cpu_timers(@current),this function is a piece of POSIX.1 standard,
                  which checks whether there are some posix timer on current CPU need to be fired.

      Profiling the Kernel Code :
        Linux kernel includes a minimalist code profiler called "readprofile" used to discover where
        the kernel spends its time in Kernel Mode.

        Linux 2.6.34.1 supports to four kinds of profiler :
          <linux/profile.h>
            #define CPU_PROFILING   1
            #define SCHED_PROFILING 2
            #define SLEEP_PROFILING 3
            #define KVM_PROFILING   4

            /* these functions are implemented in <kernel/profile.c> */

            /**
             * profile_setup - setup function for kernel profiler
             * @str:           kernel parameter,it must be form "<type>,<prof_shift>"
             *                 supported forms -
             *                   "schedule,<prof_shift>"
             *                   "sleep,<prof_shift>"
             *                   "kvm,<prof_shift>"
             *                   "<prof_shift>"  =>  cpu profiling
             *                   <prof_shift> is a parameter denotes the size of the code fragments
             *                   to be profiled,the size equals to 2^<prof_shift>.
             *
             * #  this function will sets @prof_on to the macro definitions above to indicates
             *    kernel profiling is on.
             *    the kernel parameter must be the forms :
             *      profile=<type>,<prof_shift> OR profile=<prof_shift>
             */
            int profile_setup(char *str);

            /**
             * profile_init - initialize kernel profiler
             * return:        0 => succeed | profiler off
             *                -ENOMEM => failed
             *
             * #  this function calculates size of the code to profiling,setup corresponding data objects.
             */
            int profile_init(void);

            /**
             * profile_tick - do profiling on a tick
             * @type:         what type of profiler to activate
             *
             * #  if @type == CPU_PROFILING AND @timer_hook != NULL
             *      then call to @timer_hook() to process profiling
             *    else if "in Kernel Mode" AND @prof_cpu_mask != NULL AND "current cpu in prof cpu mask"
             *      then call to @profile_hit(type, (void *)profile_pc(regs)) to process profiling
             */
            void profile_tick(int type);

            !!  tick_periodic() call to profile_tick() with CPU_PROFILING.
                @timer_hook is installed via register_timer_hook() and uninstalled
                via unregister_timer_hook(),it is defined in <kernel/profile.c> is type of 
                  int (*timer_hook)(struct pt_regs *)

      Checking the Watchdogs :
        NMI - nonmaskable interrupt

        Watchdog system :
          it is useful to detect kernel bugs that cause a system freeze.

        Linux kernel supports two kinds of watchdog :
          touch watchdog every tick came
          touch watchdog every NMI occurred

        hard lockup :
          Linux kernel initializes NMI in trap gate,the handler is assembly entry "nmi" defined in
          <arch/x86/kernel/entry_64.S>,it call to do_nmi() to deal with this exception.
          function setup_apic_nmi_watchdog() is defined in <arch/x86/kernel/apic/nmi.c> used to
          setup NMI watchdog during system booting.
          the watchdog is Local APIC version deal with "hard lockup",it checks the value of @irq0_irqs.
          if "this" timer interrupt handler accomplished,@irq0_irqs must increased 1.
          if lapic watchdog detected current value as same as the older,that means the last
          timer interrupt handler did not execute completely,it turn out CPU has some problem.
          /**
           * !!  interrupt can interrupts exception.
           *     when CPU processing a timer interrupt,the local IRQ is disabled until timer interrupt
           *     exited.
           * For activate it,must boot kernel with parameter 'nmi_watchdog' .
           * local APIC is able to product NMI exception,such NMI is regonized by CPU only.
           */

        soft lockup :
          function update_process_times() call to softlockup_tick() to touch watchdog is tick-version.
          it just deal with "soft lockup".in this case,watchdog as a kernel thread named "khungtaskd",
          the "main" function of this thread is watchdog() which defined in <kernel/hung_task.c>.
          the primary works function watchdog() to does are :
            1>  enter a infinite loop.
            2>  enter another while-loop,hang itself until timeout.
                /* task state set to TASK_INTERRUPTIBLE */
            3>  call to check_hung_uninterruptible_tasks() with a timeout value.
                check_hung_uninterruptible_tasks() scan all threads with the specified timeout value,
                if there is a thread the time it hung is greater than @timeout,then report this problem.
                prints CPU registers,kernel stack,etc.
            4>  goto 2> .

     Software Timers and Delay Functions :
       a timer is a software facility that allows functions to be invoked at some future moment,after a given
       time interval has eplased;a timeout denotes a moment at which the time interval associated with a timer
       has eplased.

       Linux introduced two types of timers :
         dynamic timer - used by the Kernel
         interval timer - used by the User Mode processes

       Linux kernel cannot ensure that timer functions will start right at their expiration times,it can only
       ensure that they are executed either at the proper time or after with a delay of up to a few hundreds of
       milliseconds(none real-time strictly)
       /* reason is the checking timer functions usually dealt with by deferrable functions */

       Dynamic Timers :
         dynamically created and destroyed,no limit to the number of currently active timers.

         Data structures for dynamic timers :         
           dynamic timers are linked by structure "struct timer_list".
                                        (introduced in Data Structures of Timekeeping Architecture)

           <kernel/timer.c>
             #define TVN_BITS (CONFIG_BASE_SMALL ? 4 : 6)
             #define TVR_BITS (CONFIG_BASE_SMALL ? 6 : 8)
             #define TVN_SIZE (1 << TVN_BITS)
             #define TVR_SIZE (1 << TVR_BITS)
             #define TVN_MASK (TVN_SIZE - 1)
             #define TVR_MASK (TVR_SIZE - 1)

             /* tvec - structure represents timer vector node */
             struct tvec {
                     struct list_head vec[TVN_SIZE];
             };

             /* tvec_root - structure represents timer vector root */
             struct tvec_root {
                     struct list_head vec[TVR_SIZE];
             };

             /**
              * structures @tvec and @tvec_root used to partitions the expiration values into
              * blocks of ticks,that allows dynamic timers to percolate efficiently from lists
              * with larger expiration values to lists with smaller ones.
              */
              

             /**
              * tvec_base - structure collected all data needed by dynamic timer API
              * @lock:      race condition protector
              * @running_timer: the dynamic timer is handled now by local CPU
              * @timer_jiffies: the earliest expiration time of the dynamic timers yet
              *                 to be checked :
              *                   @it == @jiffies => no backlog of deferrable functions has
              *                                      accumulated
              *                   @it < @jiffies  => lists of dynamic timers that refer to
              *                                      previous ticks must be dealt with
              *                 @it is increased once when run_timer_softirq() handles dynamic timer
              * @next_timer:    next expires time
              * @tv1:           all dynamic timers
              * @tv2:           dynamic timers in next 2^14 - 1 ticks
              * @tv3:           dynamic timers in next 2^20 - 1 ticks
              * @tv4:           dynamic timers in next 2^26 - 1 ticks
              * @tv5:           dynamic timers do not belong to @tv2 -- @tv4
              */
             struct tvec_base {
                     spinlock_t lock;
                     struct timer_list *running_timer;
                     unsigned long timer_jiffies;
                     unsigned long next_timer;
                     
                     struct tvec_root tv1;
                     struct tvec tv2;
                     struct tvec tv3;
                     struct tvec tv4;
                     struct tvec tv5;
             } ____cacheline_aligned;

             !!  each CPU in system has a per-CPU variable @tvec_bases it is a pointer points to
                 an object is type of struct tvec_base.

                 except boot CPU,all another CPUs allocate an object is type of struct tvec_base
                 then use the address setup its per-CPU variable @tvec_bases;
                 /**
                  * allocating and setup is handled by function "static int init_timer_cpu(int cpu);"
                  * defined in <kernel/timer.c>.
                  * in that function,@timer_jiffies and @next_timer are assigned with the value
                  * of @jiffies.(of course,these list_head arraies are initialized,too)
                  */
                 boot CPU's @tvec_bases use a static object is named boot_tvec_bases defined in
                 <kernel/timer.c>.

         a dynamic timer can be created through three methods :
           1>  static global variable in the code
               /* use macro DEFINE_TIMER(_name, _function, _expires, _data) <linux/timer.h> */
           2>  local variable inside a function,in this case,the object is stored in Kernel Mode stack
           3>  dynamically allocated descriptor

         <linux/timer.h>
           /**
            * init_timer - initialize a dynamic timer
            * @timer:      pointer points to an object is type of struct timer_list
            */
           #define init_timer(timer)

           !!  if want a timer is deferrable,must initialize it through init_timer_deferrable().
               in this case,timer_set_deferrable() would open the flag TBASE_DEFERRABLE_FLAG of
               its base.
               <kernel/timer.c> #define TBASE_DEFERRABLE_FLAG (0x1)
               #  all tvec_base are 2 byte aligned,the lower bit of base in timer_list is guaranteed
                  to be zero.
                  (unsigned long)base | TBASE_DEFERRABLE_FLAG => deferrable timer (flag opened)


           /* TIMER_NOT_PINNED - timer is not pinned can be rebalanced to another CPU on SMP */
           #define TIMER_NOT_PINNED 0
           /* TIMER_PINNED - timer is pnned can not be relanced to another CPU on SMP */
           #define TIMER_PINNED     1

           /**
            * add_timer_on - add a timer on a particular cpu
            * @timer:        the timer to add
            * @cpu:          cpu ID to place timer
            *
            * #  this function is implemented in <kernel/timer.c>.
            *    it fetch per-CPU variable @tvec_bases is type of struct tvec_base *.
            *    setup start info(address of instruction,process command, pid).
            *    setup base for @new_timer,and if necessary,updates @tvec_bases->next_timer.
            *                                               ( it := @new_timer->expires)
            *    queue this @new_timer to the tail of the @base,which list to place is depend on
            *    @new_timer->expires - @base->timer_jiffies,use the result to determin time vector.
            *    wake up idle CPUs to make them check condition to determine need to reevaluate 
            *    the timer wheel when nohz is active.
            */
           extern void add_timer_on(struct timer_list *timer, int cpu);

           /* add_timer - no CPU specified version,the dynamic timer may assigned to another CPU */
           extern void add_timer(struct timer_list *timer);

           /**
            * del_timer - deactive a timer
            * @timer:     the timer to deactive
            * return:     0 => delete an inactive timer
            *             1 => delete an active timer
            *
            * #  clean the timer info through timer_stats_timer_clear_start_info().
            *    if @timer is pending,then detach it,and updates @next_timer of its base
            *    if necessary.
            *    return.
            */
           extern int del_timer(struct timer_list *timer);
           /**
            * del_timer_sync() is synchronized version to wait until @timer->@function accomplished.
            * it fetch and lock @timer's base(spin lock).
            * if @timer is running then unlock @base and returns,because it is running on another CPU,
            * there is no way to stop function it running.
            * otherwise,does the same things as what del_timer() does.
            * return: -1 => timer running 
            *          0 => timer is not pending and deactived 
            *          1 => timer is pending and deactived
            */

           /**
            * mod_timer - modify a timer with a new expiration in TIMER_NOT_PINNED state
            * @timer:     the timer to modify
            * @expires:   new expiration
            * return:     1 => succeed
            *             0 => fault
            *
            * #  this function call to an internal function __mod_timer() to process primary works.
            *                                               <kernel/timer.c>
            *    fetch and lock the @timer's base,if this timer is pending,then detach it.
            *    update its @base if necessary.
            *    fetch per-CPU variable @tvec_bases,if @base != @new_base,then reset base for @timer
            *    to @new_base.
            *    update expiration of @timer,and update its base's new_timer member if necessary.
            *    call to internal_add_timer() to queue @timer.
            *
            * !! vector is determined by "expires - timer_jiffies",thus,modification maybe associate
            *    @timer with a new vector.
            */
           extern int mod_timer(struct timer_list *timer, unsigned long expires);

           /* mod_timer_pending - modify a pending timer's timeout */
           extern int mod_timer_pending(struct timer_list *timer, unsigned long expires);

           /* mod_timer_pinned - modify a timer with a new expires in TIMER_PINNED state */
           extern int mod_timer_pinned(struct timer_list *timer, unsigned long expires);

           /* timer_pending - if @timer->entry.next != NULL => pending OTHERWISE not pending */
           static inline int timer_pending(const struct timer_list *timer);

           e.g.
             int kthread_example(void *data)
             {
                     struct timer_list *example_timer = kmalloc(sizeof(struct timer_list), GFP_KERNEL);
                     if (IS_ERR(example_timer)) {
                             printk(KERN_DEBUG "Failed to allocate memory for @example_timer.");
                             return -ENOMEM;
                     }

                     init_timer(example_timer);

                     /**
                      * before timer added to timer list of a CPU,it is safely to assign values for members
                      * of it.
                      * but in the case timer is added and activated,modify jiffies can only through mod_timer().
                      * @function and another properties can not be modified furthuer.
                      */
                     /* if @function is NULL,it must be a bug */
                     example_timer->function = example_timer_fn;
                     example_timer->expires = jiffies + (unsigned long)16;

                     add_timer_on(example_timer, smp_processor_id());
                     ...
                     mod_timer(example_timer, jiffies + (unsigned long)32);
                     ...
                     del_timer(example_timer);
                     ...
                     kfree(example_timer);
                     return 0;
             }

         run_timer_softirq :
           TIMER_SOFTIRQ handler is run_timer_softirq().
           function init_timers() call to open_softirq() to setup this function as a handler for softirq TIMER_SOFTIRQ,
           it also setup per-CPU variable @tvec_bases and CPU timer notifier,such notifier is used for support to
           CPU hotplug(migrate timers on the CPU ready to offline).

           <kernel/timer.c>
             /**
              * run_timer_softirq - handler for softirq TIMER_SOFTIRQ
              * @h:                 softirq action acciated to this handler
              */
             static void run_timer_softirq(struct softirq_action *h);

             !!  run_timer_softirq() call to hrtimer_run_pending() to expire hrtimers,it is called from
                 timer softirq every jiffy.
                 function hrtimer_run_pending() checks whether high-resolve timer is active
                   activated => return
                   not activated => check if high-resolution timer mode is enabled AND nohz mode inactive
                                      TRUE  => checkout to HRT
                                      FALSE => stay work on nohz mode
                 /* clocksource switch happens in the timer interrupt with @xtime_lock is held. */
                 /**
                  * tick for hrtimer works on one-shot mode,the corresponding timer interrupt handler
                  * is hrtimer_handler() which is defined in <kernel/hrtimer.c>
                  */
                 AS DESCRIBED ABOVE,WE HAD KNOWN THAT HRT USE THE SAME TYPE OF SOFTIRQ AS NOHZ TIMER.

             /**
              * __run_timers - internal routine for run_timer_softirq()
              * @base:         the per-CPU variable @tvec_bases fetched
              *                by run_timer_softirq()
              */
             static inline void __run_timers(struct tvec_base *base);

             __run_timers() does :
               1>  acquire lock to local @base
               2>  enter a while-loop,iterating until @timer_jiffies before @jiffies
                                                 older                                   newer
                                                 [ - - - - @timer_jiffies - - - @jiffies ]
                                                 [ - - - (@timer_jiffies @jiffies) - - - ] coincided
                                                 [ - - - - @jiffies - - - @timer_jiffies ]
                                                 after                                   before
               3>  cascade timers -- place the dynamic timers they had not expired to new vectors in
                   the @base(adjust tv2 -- tv5)
               4>  increase @timer_jiffies once
               5>  for each expired dynamic timer,detach it from @base,and set @running_timer points to
                   it to indicate that "this" timer is dealt with,now.
               6>  call to "this" timer's @function with its @data.
               7>  goto 2>

               /* detach_timer() is defined in <kernel/timer.c>,which dequeue a timer from timer_list. */

         An Application of Dynamic Timers - the nanosleep() System Call :
           system call "nanosleep" is defined in <kernel/hrtimer.c> in system call definition method 2.

           <kernel/hrtimer.c>
             SYSCALL_DEFINE2(nanosleep, struct timespec __user *, rqtp, struct timespec *, rmtp);
             =>
               <linux/syscalls.h>
                 SYSCALL_DEFINE2(name, ...) = SYSCALL_DEFINEx(2, _##name, __VA_ARGS__)
                 =>  finally :
                   asmlinkage long sys_nanosleep(struct timespec __user *rqtp, struct timespec __user *rmtp);

             /**
              * sys_nanosleep - system call nanosleep
              * @rqtp:          pointer points to an object is type of struct timespec as the requested time
              *                 interval
              * @rmtp:          remained time interval to save when this syscall is interrupted
              * return:         0 => time interval elapsed completely
              *                 -EFAULT => error,copy to(/from) user failed
              *                 -EINVAL => error,invalid timespec
              *                 -ERESTARTNOHAND => error,restart syscall with no handler
              *                 -ERESTART_RESTARTBLOCK => error,interrupted by signal and restarted
              *                                           in this case,@rmtp saves the remained time interval
              *                                           if syscall restart have forbidden,just saves the
              *                                           remained time interval only,do not to restart syscall
              */

           the syscall sys_nanosleep() creates hrtimer on the stack of the process in Kernel Mode,before 
           function hrtimer_nanosleep() exit,such hrtimer would be destroyed.
                    (sys_nanosleep -> hrtimer_nanosleep)

           the primary sleeping routine is handled by function do_nanosleep().

           /**
            * do_nanosleep - main routine of hrtimer_nanosleep()
            * @t:            hrtimer sleeper created by hrtimer_nanosleep()
            * @mode:         work mode of hrtimer
            * return:        T => time interval elasped completely
            *                F => interrupted
            */
           static int __sched do_nanosleep(struct hrtimer_sleeper *t, enum hrtimer_mode mode);

           what do_nanosleep does :
             1>  set @t->function to hrtimer_wakeup(),
                 set @t->task to @current
             2>  enter do-while loop until hrtimer completed or interrupted by an signal,
             3>  set @current to TASK_INTERRUPTIBLE
             4>  start timer expiring through hrtimer_start_expires(&@t->timer, @mode)
             5>  if timer inactive,set sleeper's @task to NULL
                 this means,the hrtimer had timeout
             6>  if @t.task != NULL,call to schedule() relinquish CPU
             7>  cancel hrtimer and change @mode to HRTIMER_MODE_ABS
             8>  goto 3>
             9>  exit do-while loop,set @current to TASK_RUNNING
             10> return @t->task == NULL

           !!  IF @current IS RE-PICKED AS THE NEXT TASK TO RUN,AND THE SPECIFIED TIME INTERVAL HAVE NOT
               EPLASED,THEN THE do-while LOOP WILL RESTART A HRTIMER AGAIN AND RELINQUISH CPU.
               @expires := @request + @jiffies

       Delay Functions :
         the minimum wait time for dynamic timer is 1-milliseconds,it is useless when kernel need a timer
         to wait a time interval less than 1-milliseconds.(1-microseconds / 1-nanoseconds)
         such case appears often in device driver.

         <arch/x86/include/asm/delay.h>
           /**
            * udelay - delay function used to wait a time interval in microseconds
            * @n:      how many microseconds to wait for
            */
           #define udelay(n)  (__builtin_constant_p(n) ? \
                               ((n) > 20000 ? __bad_udelay() : __const_udelay((n) * 0x10c7ul)) \
                               __udelay(n))

           !!  __udelay => __const_udelay(n * 0x000010c7)

           /**
            * ndelay - delay function used to wait a time interval in nanoseconds
            * @n:      how many nanoseconds to wait for
            */
           #define ndelay(n)  (__builtin_constant_p(n) ? \
                               ((n) > 20000 ? __bad_ndelay() : __const_udelay((n) * 0x5ul)) \
                               : __ndelay(n))

           !!  __ndelay => __const_udelay(n * 0x00005)

           /**
            * __const_udelay - main delay function
            * @xloops:          how many loops to iterates
            */
           extern void __const_udelay(unsigned long xloops);

           /**
            * use_tsc_delay - switch the delay clock source to TSC
            */
           void use_tsc_delay(void);

           !!  kernel must take a way to convert microseconds and nanoseconds to loops,and have to
               calibrate how many loops in a tick.
               the convertion is :
                 32-bit mull
                 =>  eax : @xloops *= 4
                     edx : cpu_data(raw_smp_processor_id()).loops_per_jiffy * (HZ / 4)

                 @loops_per_jiffy is the loops in a tick which is calibrated during system booting through
                 calibrate_delay() defined in <init/delay.c>.
                 the cpu_data() fetch an object is type of struct cpuinfo_x86 is introduced in
                 <arch/x86/include/asm/processor.h>.

               for boot CPU,just set @loops_per_jiffy to its clock frequency as well,but for the rest CPUs,
               must call to calibrate_delay() to determine the value of its @loops_per_jiffy.
               the @loops_per_jiffy can be set through kernel parameter "lpj=<v>" .

           !!  <arch/x86/lib/delay.c>
                 /**
                  * delay_fn - the real delay function setup during system booting
                  * @arg1:     loops to delay
                  */
                 static void (*delay_fn)(unsigned long) = delay_loop;

                 delay_loop() is defined in the same source code file as default delay function.
                 static function delay_tsc() is the delay function based TSC,and use_tsc_delay()
                 switch @delay_fn to delay_tsc().

           !!  HPET or TSC as the delay clock source,one loop corresponds to one CPU cycle -- 
               time interval between two consecutive CPU clock signals.
               PIT  or unspecified,one loop corresponds to the time duration of a single iteration
               of a tight instruction loop.

     System Calls Related to Timing Measurements :
       some system call primitives related to timing measurement >

         SUS :
           time
           gettimeofday
           adjtimex
           /* settimeofday REQUEST _BSD_SOURCE */
           setitimer
           alarm           

         POSIX.1 :
           clock_gettime
           clock_settime
           clock_getres
           timer_create
           timer_gettime
           timer_settime
           timer_getoverrun
           timer_delete
           clock_nanosleep

       !!  Linux System Call routines always return long int.
       SUS System Call :
         <kernel/time.c>
           /**
            * sys_time - system call time to retrieve time in seconds since the Epoch
            * @tloc:     pointer points to an object is type of time_t,
            *            if @tloc is not NULL,then write the time into
            *            *@tloc
            * return:    time in seconds since UTC 1970-01-01 00:00
            *            -EFAULT => failed to write time into @tloc
            */
           SYSCALL_DEFINE1(time, time_t __user *, tloc);

           !!  the system call sys_time() is able to be implemented at user-level using
               sys_gettimeofday(),but this interface is remained for compatible.

           /**
            * sys_gettimeofday - system call gettimeofday to retrieve time of day
            * @tv:               pointer points to an object is type of struct timeval,
            *                    the object is used to save time info
            * @tz:               pointer points to an object is type of struct timezone,
            *                    if @tz is not NULL,then write @sys_tz(system timezone) to @tz
            * return:            0 => succeed
            *                    -EFAULT => tv == NULL OR (tv != NULL AND write time info to @tv failed)
            *                               tz != NULL AND write @sys_tz to @tz failed
            *
            * #  this function call to do_gettimeofday() to fetch current system time,function 
            *    timekeeping_get_ns() would be called by it,and @xtime_lock will be acquired during
            *    read current time from the clock source of @timekeeper.
            */                         
           SYSCALL_DEFINE2(gettimeofday, struct timeval __user *, tv, struct timezone __user *, tz);

           !!  under linux,the second field of struct timezone @tz_dsttime is never been used.

           /**
            * sys_adjtimex - system call adjtimex used to tune kernel clock
            * @txc_p:        pointer points to an object is type of struct timex,
            *                it saves the new parameters used by kernel to tunes clock
            * return:        -EFAULT => failed to copy current values of kernel clock to @txc_p
            *                TIME_OK => succeed
            *                TIME_ERROR => failed to adjust kernel clock
            *                ...
            *                TIME_* => represent clock synchronization status,introduced in <linux/timex.h>
            *
            * 
            * #  this function use @txc_p to tunes kernel clock,if succeed,the paramter
            *    @txc_p save the copy of the current values of kernel clock.
            *    @xtime_lock is write-lock during tuning kernel clock.
            *    real-time-clock (RTC) integered in motherboard also be modified.
            */
           SYSCALL_DEFINE1(adjtimex, struct timex __user *, txc_p);

           !!  structure timex is defined in <linux/timex.h> used to discipline kernel clock oscillator.
           !!  clock synchronization status :
                 TIME_OK     0           =>  synchronized,no leap second
                 TIME_INS    1           =>  insert leap second
                 TIME_DEL    2           =>  delete leap second
                 TIME_OOP    3           =>  leap second in progress
                 TIME_WAIT   4           =>  leap second has occurred
                 TIME_ERROR  5           =>  not synchronized
                 TIME_BAD    TIME_ERROR  =>  bw compat

         <kernel/itimer.c>
           /**
            * sys_setitimer - system call allow User Mode processes to set special timers
            *                 - interval timers(no relating to PIT)
            * @which:         which policy to apply -
            *                   ITIMER_REAL    => actual elapsed time,SIGALRM
            *                   ITIMER_VIRTUAL => time spent by the process in User Mode,SIGVTALRM
            *                   ITIMER_PROF    => time spent by the process both in User Mode and in
            *                                     Kernel Mode,SIGPROF
            * @value:         itimer parameter to set
            * @ovalue:        the older itimer parameter to save
            * return:         0 => succeed
            *                 -EINVAL => invalid itimer parameter
            *                 -EFAULT => failed when copy from user or copy to user
            */
           SYSCALL_DEFINE3(setitimer, int, which, struct itimerval __user *, value,
                           struct itimerval __user *, ovalue);

           !!  structure itimerval is defined in <linux/time.h>
                 struct itimerval {
                         struct timeval it_interval;
                         struct timeval it_value;
                 };

                 @it_interval means the time interval must to be waitted before send an signal
                 @it_value means the times for signals have to be sent

                 /**
                  * everytime after an signal had sent,@it_value been decreased,stop when it becomes to zero.
                  * everytime an signal was sent and @it_value is not equal to zero,reset itimer to
                  * @it_interval for next signal is going to be raised.
                  */

           !!  structure task_struct has member named @signal is type of a pointer for structure signal_struct.
               members {
                 struct hrtimer real_timer            /* high-resolution timer */
                 struct pid *leader_pid               /* process group id */
                 ktime_t signal_struct.it_real_incr   /* time interval in ktime_t,conversion occurred */
               } reserved for ITIMER_REAL for @current task.

           !!  ITIMER_VIRTUAL and ITIMER_PROF are migrated to cpu itimer now,not as the process itimer.
               for these two policies,cpu itimer is set to CPUCLOCK_VIRT and CPUCLOCK_PROF,respectively.
               function set_cpu_itimer() is defined in <kernel/itimer.c>,it operates an array contains two
               elements included in signal_struct has name @it is type of struct cpu_itimer.
               parameter @clockid of set_cpu_itimer() has argument CPUCLOCK_VIRT or CPUCLOCK_PROF.
           
           /**
            * sys_alarm - system call alarm,send called SIGALRM after an especified time interval in seconds 
            *             had been elapsed.
            * @seconds:   time interval in seconds.
            * return:     0 => timer is not activated,that is,time interval elapsed and signal raised
            *             > 0 => remaining time in seconds of a pending timer
            *
            * #  this function call to alarm_setitimer() to set an itimer(ITIMER_REAL) at oneshot mode
            *    with parameter @seconds.
            */
           SYSCALL_DEFINE1(alarm, unsigned int, seconds);

       !!  POSIX.1b API returns 0 means succeed,otherwise,returns error code.
       POSIX.1 System Call : 
         <linux/time.h>
           /* system clocks for POSIX.1b interval timers */
           #define CLOCK_REALTIME            0          /* resolution = 999848 */
                   /**
                    * real-time clock of the system.
                    * essentially,the value of @xtime.
                    */
           #define CLOCK_MONOTONIC           1          /* resolution = 999848 */
                   /**
                    * real-time clock of the system purged of every time warp due to the
                    * synchronization with an external time source.
                    * essentially,it represented by the sum of @xtime and @wall_to_monotonic.
                    */
           #define CLOCK_PROCESS_CPUTIME_ID  2
           #define CLOCK_THREAD_CPUTIME_ID   3
           #define CLOCK_MONOTONIC_RAW       4
           #define CLOCK_REALTIME_COARSE     5
           #define CLOCK_MONOTONIC_COARSE    6

         <linux/posix-timers.h>
           struct k_itimer;        /* POSIX.1b interval timer */

           /* struct k_clock - POSIX timer operations */
           struct k_clock {
                   int res;  /* in nanoseconds */
                   int (*clock_getres)(const clockid_t which_clock, struct timespec *tp);
                   int (*clock_set)(const clockid_t which_clock, struct timespec *tp);
                   int (*clock_get)(const clockid_t which_clock, struct timespec *tp);
                   int (*timer_create)(struct k_itimer *timer);
                   int (*nsleep)(const clockid_t which_clock, int flags, struct timerspec *,
                                  struct timespec __user *);
                   int (*nsleep_restart)(struct restart_block *restart_block);
                   int (*timer_set)(struct k_itimer *timr, int flags,
                                     struct itimerspec *new_setting,
                                     struct itimerspec *old_setting);
                   int (*timer_del)(struct k_itimer *timr);
           #define TIMER_RETRY 1
                   void (*timer_get)(struct k_itimer *timr,
                                     struct itimerspec *cur_setting);
           };

         <kernel/posix-timers.c>
           /* posix_clocks - table for POSIX clocks,MAX_CLOCKS = 16 defined in <linux/time.h> */
           static struct k_clock posix_clocks[MAX_CLOCKS];

           /**
            * CLOCK_DISPATCH - generic caller routine used to dispatch clock from posix_clocks table
            *                  and does @call on it
            * @clock:          which clock
            * @call:           which routine to call
            * @arglist:        arguments enclosed by parentheses
            *
            * #  @clock < 0,negative index was selected in the table,thus call to posix cpu clock routine
            * #  @call == NULL,no such routine is existed,thus call to common routine
            */
           #define CLOCK_DISPATCH(clock, call, arglist)  \
                   ((clock) < 0 ? posix_cpu_##call arglist :  \
                           (posix_clocks[clock].call != NULL  \
                            ? (*posix_clocks[clock].call) arglist : common_##call arglist))

           /**
            * sys_clock_gettime - POSIX.1b API,get time in seconds and in nanoseconds from a clock
            * @which_clock:       indicates which clock to get the time,it is type of 
            *                       clockid_t => __kernel_clockid_t => int
            * @tp:                pointer points to an object is type of struct timespec,it saves
            *                     the time retrieve from @posix_clocks[@which_clock]
            * return:             0 => get time succeed
            *                     -EINVAL => invalid clock id
            *                     -EFAULT => copy to user failed
            */
           SYSCALL_DEFINE2(clock_gettime, const clockid_t, which_clock, struct timespec __user *, tp);

           /* sys_clock_settime - POSIX.1b API,set time in seconds and in nanoseconds for a clock */
           SYSCALL_DEFINE2(clock_settime, const clockid_t, which_clock, const struct timespec __user *, tp);

           /* sys_clock_getres - POSIX.1b API,get @res(timer resolution) from a clock */
           SYSCALL_DEFINE2(clock_getres, const clockid_t, which_clock, struct timespec __user *, tp);

           !!  typedef int __kernel_timer_t;
               typedef __kernel_timer_t timer_t;

           /* sys_timer_create - POSIX.1b API,create and start an interval timer */
           SYSCALL_DEFINE3(timer_create, const clockid_t, which_clock, struct sigevent __user *,
                           timer_event_spec, timer_t __user *, create_timer_id);

           !!  structure sigevent is defined in <asm-generic/siginfo.h>
                 #define SIGEV_SIGNAL     0        /* notify via signal */
                 #define SIGEV_NONE       1        /* other notification: meaningless */
                 #define SIGEV_THREAD     2        /* deliver via thread creation */
                 #define SIGEV_THREAD_ID  4        /* deliver to thread */


                 #ifndef __ARCH_SIGEV_PREAMBLE_SIZE
                 #define __ARCH_SIGEV_PREAMBLE_SIZE  (sizeof(int) * 2 + sizeof(sigval_t))
                 #endif

                 #define SIGEV_MAX_SIZE  64
                 #define SIGEV_PAD_SIZE  ((SIGEV_MAX_SIZE - __ARCH_SIGEV_PREAMBLE_SIZE)  \
                                          / sizeof(int))

                 /* structure for notification from asynchronous routines(aio_read, aio_write,etc.) */
                 typedef struct sigevent {
                         sigval_t sigev_value;
                         int sigev_signo;
                         int sigev_notify;
                         union {
                                 int _pad[SIGEV_PAD_SIZE];
                                 int _tid;
                                 
                                 struct {
                                         void (*_function)(sigval_t);
                                         void *_attribute;        /* really pthread_attr_t */
                                 } _sigev_thread;
                         };
                 } sigevent_t;

           /* sys_timer_gettime - POSIX.1b API,get the remaining time from an interval timer */
           SYSCALL_DEFINE2(timer_gettime, timer_t, timer_id, struct itimerspec __user *, setting);

           /* sys_timer_settime - POSIX.1b API,set an interval timer */
           SYSCALL_DEFINE4(timer_settime, timer_t, timer_id, int, flags, const struct itimerspec __user *,
                           new_setting, struct itimerspec __user *, old_setting);

           /* sys_timer_getoverrun - POSIX.1b API,get the number of overruns of an interval timer */
           SYSCALL_DEFINE1(timer_getoverrun, timer_t, timer_id);

           /* sys_timer_delete - POSIX.1b API,delete an interval timer */
           SYSCALL_DEFINE1(timer_delete, timer_t, timer_id);

           /**
            * sys_clock_nanosleep - POSIX.1b API,suspend process until an especial time interval(in nanoseconds)
            *                       elapsed
            * @which_clock:         clock id
            * @flags:               control flags 
            *                         0 => @rqtp is an interval related to the current value of @which_clock
            *                         TIMER_ABSTIME => @rqtp is a absolute time
            * @rqtp:                request
            * @rmtp:                remained time
            * return:               0 => time elapsed completely
            *                       -EINVAL => invalid time
            *                       -EFAULT => failed when copy to user / copy from user
            */
           SYSCALL_DEFINE4(clock_nanosleep, const clockid_t, which_clock, int, flags,
                           const struct timespec __user *, rqtp, struct timespec __user *, rmtp);


Chapter 7 : 
    Scheduling Policy :
      the set of rules used to determine when and how to select a new process to run is called "Scheduling Policy".

      Linux scheduling is based on time sharing technique :
        several processes run in "time multiplexing",the CPU time is divided into slices,one for each runnable
        process.                                                               /* OR called quantum */
        !!  time sharing technique relies on timer interrupts and is thus transparent to processes.

      Linux scheduling is also based on ranking processes :
        process priority.
        each process is associated with a value that tells the scheduler how appropriate it is to let the
        process run on a CPU.
        !!  linux process priority is dynamic,the scheduler keep track of what processes are doing and adjusts
            their priorities periodically.

      Process classify :
        I/O-Bound
          make heavy use of I/O devices and spend much time waiting for I/O operations to complete
        CPU-Bound
          number-crunching applications that require a lot of CPU time

        alternative three for classifications >
          1>  interactive processes
                interact constantly with their users,and therefore spend a lot of time waiting for
                keypresses and mouse operations.
                the average delay must fall between 50 -- 150 milliseconds.
                /* shell, text editor, graphical application, etc. */

          2>  batch processes
                do not need user interaction,often run in the background.
                scheduler can decreases their priority as well.
                /* language compiler, database searching, scientific computation, etc. */

          3>  real-time processes
                very stringent scheduling requirements.
                never be blocked by lower-priority processes,should have a short guaranteed response
                time with a minimum variance.
                /* video / sound application, robot controller, program collects data from physical sensor, etc. */

          !!  batch process can be I/O-Bound and CPU-Bound.
              interactive process often be I/O-Bound.
              real-time program are explicitly recognized as such by the scheduling algorithm in Linux.

      !!  PROGRAMMER CAN MODIFY THE PROCESS PRIORITY THROUGH SOME SYSTEM-CALL API.
          SCHEDULER TENDS TO FAVOR THE PROCESS IS INTERACTIVE OVER BATCH.

      Process Preemption :
        when a process enter TASK_RUNNING state,kernel checks whether its priority is greater than the @current running
        process,if it is,then scheduler will be invoked,@current would be interrupted and another process is selected to
        run(usually,"this" process").
        when a process's time slice had exhausted(TID_NEED_RESCHED set),scheduler would be called after timer interrupt
        terminated,and @current running process would be preempted by anoter process selected by scheduler.

        !!  A process had preempted is not suspended,it stay in TASK_RUNNING state,but it is no longer uses the CPU.
            Since Linux 2.6,the Kernel is preemptive,that is,a process can be preempted even when it is executing the
            Kernel Code in Kernel Mode.
            /* interrupt can not be preempted but can be interrupted by another */

      How Long Must a Quantum Last ? :
        quantum duration is critical for system performance,it should be neither too long nor too short.

        suppose,quantum duration is 5s :
          batch process 1 run at first,and wait for N seconds to start next execution.
            N = 5s * total(processes - 1)
          and,maybe an interactive process preempt batch process 1.
        suppose,quantum duration is 5ms,and process switch require 5ms :
          process 1 run,and exhausted time slice,scheduler does process switch
            time consumed = 5ms + 5ms => 10ms
          CPU time duration = 1s
            1s = 1000ms
          times for process switch in 1s = 1000ms / 10ms / 2 = 50
          times for a process runs in 1s = 1000ms / 10ms / 2 = 50
          => 50% CPU time had consumed on process switch

        Linux quantum duration rule : choose a duration as long as possible,while keeping good system response time.
                                      /* compromise */

    The Scheduling Algorithm :
      Scheduling Algorithm for earlier verions of Linux :
        at every process switch,kernel scanned the list of runnable processes,computed their priorities,and selected the
        "best" process to run.  /* too costly */

      Scheduling Algorithm of Linux 2.6 :
        /* scale well with the number of runnable processes */
        select the process to run in constant time,independently of the number of runnable processes.
        /* scale well with the number of processors */
        each CPU has its own queue of the runnable processes.
        distinguishing interactive processes and batch processes.

        swapper process (PID 0) :
          executes only when the CPU cannot execute other processes.
          each CPU has its own swapper process.
          scheduler always can find a process to run.
    
        scheduling classes : /* every Linux process always scheduled according to a scheduling class */
          SCHED_FIFO 
            First-In First-Out real-time process
            when the scheduler assigns the CPU to the process,it leaves the process descriptor in its current position in
            the runqueue list.if no other high-priority process is runnable,the process continues to use the CPU as long as
            it wishes,even if other processes that have the same priority are runnable.

          SCHED_RR
            Round Robin real-time process
            when the scheduler assigns the CPU to the process,it puts the process descriptor at the end of the runqueue list.
            this ensure a fair assignment of CPU time to all SCHED_RR processes that have the same priority.

          SCHED_NORMAL
            Conventional time-shared process

          Scheduling of Conventional Processes :
            every conventional process has a static priority,kernel represents the static priority use the number ranging from
            100(highest-priority) -- 139(lowest-priority).
            !!  A NEW PROCESS ALWAYS INHERITS THE STATIC PRIORITY OF ITS PARENT.
                USER CAN CHANGE STATIC PRIORITY OF A PROCESS THROUGH nice() OR setpriority() SYSTEM CALL.BUT A USER CAN ONLY 
                CONTROL THE PRIORITY OF THE PROCESS WHICH IS BELONGS TO "this" USER,UNLESS "this" USER IS root.

            Base time quantum :
              /* FORMULA1 */
                                                    (140 - static priority) * 20 if static priority < 120
              base time quantum(in milliseconds) = 
                                                    (140 - static priority) * 5  if static priority >= 120

              base time quantum is assigned when a process has exhausted its previous time quantum.

              btq     =>  base time quantum
              sp      =>  static priority
              nv      =>  nice value
              idelta  =>  interactive delta
              stt     =>  sleep time threshold
                                            sp      nv      btq         idelta  stt
              highest static priority       100     -20     800ms       -3      299ms
              high static priority          110     -10     600ms       -1      499ms
              default static priority       120       0     100ms       +2      799ms
              low static priority           130     +10      50ms       +4      999ms
              lowest static priority        139     +19       5ms       +6      1199ms

            Dynamic priority and average sleep time :
              every conventional process also has a dynamic priority is a value ranging from 100(high) -- 139(low).
              this property is the number actually looked up by the scheduler when selecting the new process to run.

              empirical formula :
                dynamic priority = max(100, min(static priority - bonus + 5, 139))
                @bonus : [0, 10]
                         related to the average sleep time of the process

              average sleep time :
                average number of nanoseconds that the process spent while sleeping.
                sleeping in TASK_INTERRUPTIBLE state contributes to the average sleep time in a different
                way from sleeping in TASK_UNINTERRUPTIBLE state.
                the average sleep time is decreases while a process is running.

                ast => average sleep time
                b   => bonus
                g   => granularity

                /**
                 * the times that process is sleep greater,the average sleep time is lower.
                 *   total sleep time / times(process sleep) := average sleep time
                 */
                ast                         b           g
                    0 <= ast < 100ms        0           5120
                100ms <= ast < 200ms        1           2560
                200ms <= ast < 300ms        2           1280
                300ms <= ast < 400ms        3            640
                400ms <= ast < 500ms        4            320
                500ms <= ast < 600ms        5            160
                600ms <= ast < 700ms        6             80
                700ms <= ast < 800ms        7             40
                800ms <= ast < 900ms        8             20
                900ms <= ast < 1000ms       9             10
                   1s == ast                10            10

                average sleep time is also used by scheduler to determine whether the process is an
                interactive process or a batch process.
                a process is considered "interactive" if it satisfies 
                  dynamic priority <= 3 * static priority / 4 + 28
                  <=>
                  bonus - 5 >= static priority / 4 - 28
                               /* interactive delta */

            Active and expired processes :
              for prevent process starvation,when a process finishes its time quantum,it can be replaced
              by a lower priority process whose time quantum has not yet been exhausted.

              two disjoint sets of runnable processes :
                Active processes {
                        runnable processes have not yet exhausted their time quantum
                        allowed to run
                }                
            
                Expired processes {
                        runnable processes have exhausted their time quantum
                        forbidden to run until all active processes expire
                }

              an active batch process exhausted its quantum always becomes expired.
              an active interactive process exhausted its quantum usually remains active - scheduler refills
              its time quantum and leaves it in the set of active processes.
              /**
               * if waited_long_time(eldest_expired_process) || higher_priority(expired_process)
               * then
               *         quantum exhausted-interactive process => expired interactive process
               *         #  do not refill quantum
               */

          Scheduling of Real-Time Processes :
            every real-time process associated with a real-time priority,it is a value ranging from
            1(highest priority) -- 99(lowest priority).
            a real-time process inhibits the execution of every lower-priority process while it remains
            runnable.
            real-time processes are always considered "active".

            !!  USER CAN CHANGE REAL-TIME PRIORITY THROUGH SYSTEM CALL sched_setparam() AND sched_setscheduler().

            if same_priority(some_real_time_processes)
            then
                    scheduler : select first_runnable(PerCPU_Fetch(smp_cpu_id(), runqueue))

            conditions that real-time process to be replaced : /* one of the following */
              1>  real_time_priority(@current) < real_time_priority(rtp)
              2>  set_task_state(@current, TASK_INTERRUPTIBLE) || set_task_state(@current, TASK_UNINTERRUPTIBLE)
                  /* blocking */
              3>  task_state(@current) == TASK_STOPPED / TASK_TRACED / EXIT_ZOMBIE / EXIT_DEAD
              4>  relinquish_cpu(smp_cpu_id(), @current) through system call sched_yield()
              5>  scheduling_class(@current) == SCHED_RR && is_quantum_exhausted(@current)

            !!  SYSTEM CALLS nice() AND setpriority() ON A REAL-TIME PROCESS WITH SCHED_RR DO NOT CHANGE THE
                REAL-TIME PRIORITY BUT RATHER THE DURATION OF THE BASE TIME QUANTUM .
                THE BASE TIME QUANTUM OF A REAL-TIME PROCESS WITH SCHED_RR DO NOT DEPEND ON THE REAL-TIME
                PRIORITY,BUT RATHER ON THE STATIC PRIORITY OF IT,ACCORDING TO THE FORMULA1.

      Data Structures Used by the Scheduler :
        The runqueue Data Structure :
          each CPU on the system has a Per-CPU variable is named @runqueues,it is type of struct rq introduced 
          in <kernel/sched.c>.

          <kernel/sched.c>
            /**
             * struct rq - the structure used to represent runqueue
             * @lock:                  runqueue lock
             * @nr_running:            number of runnable processes in the runqueue lists
             * @cpu_load:              CPU load factors based on the average number of processes in the runqueue
             * @in_nohz_recently:      indicate whether timekeeping in nohz mode recently
             * @load:                  load weight
             * @nr_load_updates:       the times that @load updated
             * @nr_switches:           number of process switches performed by the CPU
             * @cfs:                   completely fair scheduling runqueue
             * @rt:                    real time runqueue
             * @leaf_cfs_rq_list:      leaf of robin tree of the cfs rq
             * @leaf_rt_rq_list:       leaf of robin tree of the rt rq
             * @nr_uninterruptible:    number of processes that in TASK_UNINTERRUPTIBLE state in the runqueue
             * @curr:                  current task
             * @idle:                  idle task - @swapper mentioned abovde
             * @next_balance:          the jiffies next balance to occurs
             * @prev_mm:               memory descriptor of previous task
             * @clock:                 sched clock,monotonic per cpu clock
             * @nr_iowait:             number of processes that were previously in the runqueue lists and are now
             *                         waiting for a disk I/O operation to complete
             * @rd:                    the root scheduling domain
             * @sd:                    the base scheduling domain of this CPU
             * @active_balance:        flag set if some process shall be migrated from this runqueue to another
             * @push_cpu:              not used
             * @cpu:                   the cpu this runqueue belongs to
             * @online:                is the @cpu online?
             * @avg_load_per_task:     average load for per task
             * @migration_thread:      the "migration" kernel thread
             * @migration_queue:       list of processes to be removed from the runqueue
             * @hrtick_timer:          high-resolution timer
             * @yld_count:             the times that sys_sched_yield() was called by the task in this runqueue
             * @sched_count:           the times that schedule() was called by the task in this runqueue
             * @sched_goidle:          the times that no task can be selected to run thus selected idle process to
             *                         execute
             * @bkl_count:             the times that big kernel locked
             */
            struct rq {
                    raw_spinlock_t lock;
                    unsigned long nr_running;
                    #define CPU_LOAD_IDX_MAX 5
                    unsigned long cpu_load[CPU_LOAD_IDX_MAX];

            #ifdef CONFIG_NO_HZ
                    unsigned char in_nohz_recently;
            #endif
                    
                    struct load_weight load;
                    unsigned long nr_load_updates;
                    u64 nr_switches;

                    struct cfs_rq cfs;
                    struct rt_rq rt;

            #ifdef CONFIG_FAIR_GROUP_SCHED
                    struct list_head leaf_cfs_rq_list;
            #endif
            #ifdef CONFIG_RT_GROUP_SCHED
                    struct list_head leaf_rt_rq_list;
            #endif

                    unsigned long nr_uninterruptible;

                    struct task_struct *curr, *idle;
                    unsigned long next_balance;
                    struct mm_struct *prev_mm;

                    u64 clock;

                    atomic_t nr_iowait;

            #ifdef CONFIG_SMP
                    struct root_domain *rd;
                    struct sched_domain *sd;
                    ...
                    int active_balance;
                    int push_cpu;
                    int cpu;
                    int online;
                  
                    unsigned long avg_load_per_task;

                    struct task_struct *migration_thread;
                    struct list_head migration_queue;
            #endif

                    ...
            
            #ifdef CONFIG_SCHED_HRTICK
                    ...

                    struct hrtimer hrtick_timer;
            #endif

                    ...

            #ifdef CONFIG_SCHEDSTATS
                    ...
                    unsigned int yld_count;

                    unsigned int sched_switch;
                    unsigned int sched_count;
                    unsigned int sched_goidle;
                    ...
                    unsigned int bkl_count;
            #endif
            };

          because each CPU has its own @runqueues,thus a task only existed just one @runqueues of a CPU,but
          when load balancing occurred,it maybe migrated to another @runqueues of different CPU in the system.

          !!  HAVE TO BE AWARE CLEARLY THAT struct rq IS ASSOCIATED TO THE CPU,NOT TO THE PROCESS.WHEN SELECT
              A CPU FOR PROCESS,THAT IS SELECT A RUNQUEUE OF THE CPU FOR PROCESS,THE PROCESS WILL RESIDE ON IT.
          !!  HAVE TO BE AWARE CLEARLY THE DIFFERENCE BETWEEN 
                @rq->curr(CURRENT RUNNING TASK OF RUNQUEUE)
                @current(THIS THREAD)
                @p(TASK)

        Process Descriptor :
          structure task_struct is defined in <linux/sched.h> used to represents a task.
          each task correspond to a process normally,but it a process contains several threads,every thread
          correspond to a task. (in other words,a task correspond to a thread)

          struct task_struct had been introduced in Chapter 3 Processes.

          the members of struc task_struct that related to Linux scheduler :
            __u32 thread_info->flags         =>  store the TIF_NEED_RESCHED flag,which is set if the scheduler 
                                                 must be invoked  
            __u32 thread_info->cpu           =>  logical number of the CPU owning the runqueue to which the
                                                 runnable process belongs
            volatile long state              =>  process state
            int prio                         =>  dynamic priority
                static_prio                  =>  static priority,converted from USER NICE VALUE
                normal_prio                  =>  normal priority (CFS)
            struct list_head rt.run_list     =>  real-time runnable list where @current is linked into
            unsigned int policy              =>  scheduling class (SCHED_NORMAL, SCHED_RR, SCHED_FIFO)
            cpumask_t cpus_allowed           =>  bit mask of the CPUs that can execute the process
            unsigned int rt.time_slice       =>  real-time ticks left in the time quantum of the process
            u64 se.slice_max                 =>  maximum time slice
            unsigned int rt_priority         =>  real-time priority

            /**
             * @rt is type of struct sched_rt_entity
             * @se is type of struct sched_entity,CFS use it to records process times
             */

          !!  LINUX 2.6.34.1 APPLY CFS ON THE PROCESS SCHEDULING.

          when static function copy_process() defined in <kernel/sched.c> is called for fork a child process,
          it call to dup_task_struct() to make a copy of the parent's task_struct for child process.
          so,@rt and @se will be copied as well,thus child has the same time informations as its parent's.
          !!BUT!!
            copy_process() call to sched_fork(@child) later to performs scheduling entity setup,almost
            all fields would be initialized to 0.of course,@se.slice_max is set to 0,and @rt unchanged.
            copy_process() just assign child to a CPU through sched_fork() but not start it due to child is
            in TASK_WAKING state,and sched_fork() let child inherits the @normal_prio from its parent as the
            @prio(dynamic priority).

        Sched Clock Data :
          sched clock for unstable cpu clocks.
          create a semi stable clock from a mixture of other events :
            TSC
            GTOD
            explicit idle event

          <kernel/sched_clock.c>
            /**
             * sched_clock_data - structure used to record clock data fro scheduler
             * @tick_raw:         raw tick,get from TSC
             * @tick_gtod:        GTOD,gettimeofday relating facility,as base and the unstable clock deltas
             * @clock:            clock value
             */
            struct sched_clock_data {
                    u64 tick_raw;
                    u64 tick_gtod;
                    u64 clock;
            };

        Sched Class :
          schedule class defined a set of primitives to describe how to deal with linux scheduling.
          <linux/sched.h>
            struct sched_class;

          member @next is a const pointer points to an object is also type of struct sched_class,thus all sched_class
          are linked.

          the primitives :
            <specifier>@<type>@<name> => data type describing
            <specifier>@<type>@r      => return type
            task: struct task_struct
            i:    int
            ui:   unsigned int
            cpumask*: struct cpumask *
        
            /**
             * "rq" related parameters and "p" related parameters are pass-by-pointer,
             *  and types can be deduced from their names.
             */

            enqueue_task            @rq @p i@wakeup
            dequeue_task            @rq @p i@sleep
            yield_task              @rq
            check_preempt_curr      @rq @p i@flags
            task@r pick_next_task   @rq
            put_prev_task           @rq @p

          #ifdef CONFIG_SMP
            i@r select_task_rq      @p i@sd_flag i@flags
            pre_schedule            @this_rq @task
            post_schedule           @this_rq
            task_waking             @this_rq @task
            task_woken              @this_rq @task
            set_cpus_allowed        @p const@cpumask*@newmask
            rq_online               @rq
            rq_offline              @rq
          #endif /*CONFIG_SMP */

            set_curr_task           @rq
            task_tick               @rq @p i@queued
            task_fork               @p
            switched_from           @this_rq @task
            switched_to             @this_rq @task i@running
            prio_changed            @this_rq @task i@oldprio i@running
            ui@r get_rr_interval    @rq @task
           
          #ifdef CONFIG_FAIR_GROUP_SCHED
            moved_group             @p i@on_rq
          #endif /* CONFIG_FAIR_GROUP_SCHED */  

          These primitives are setup by scheduler initializing routine.Because Linux 2.6.34.1 introuduced CFS -
          - Completely Fair Scheduler,thus the sched_class in kernel is set to cfs class in normally.

      Functions Used by the Scheduler :
        The scheduler_tick() function :
          <arch/x86/kernel/tsc.c>
            /**
             * native_sched_clock - architecture depended sched_clock(),used to returns current time in nanosec units
             * return:              time data readed from TSC and converted to nanoseconds
             */
            u64 native_sched_clock(void);

          #ifndef CONFIG_PARAVIRT
            unsigned long long
            sched_clock(void) __attribute__((alias("native_sched_clock")));
          #endif

          <kernel/sched_clock.c>
            sched_clock - generic scheduler clock
              => (jiffies - INITIAL_JIFFIES) * (NSEC_PER_SEC / HZ)
            /* architecture can overrides this */

          <linux/sched.h>
            /**
             * scheduler_tick - called from timer code to updates scheduling related data
             */
            extern void scheduler_tick(void);

            what scheduler_tick() does :
              1>  retrieve cpu id,runqueue of local cpu,the task current running of runqueue.
              2>  call to sched_clock_tick() to update scheduler clock.
                  /**
                   * sched_clock_tick() call to sched_clock_local(),it calculate scheduler
                   * clock from TSC, GTOD, Explicit Idle Event .
                   */
              3>  acquire runqueue lock.
              4>  update runqueue clock.
                  /**
                   * call to sched_clock_cpu().in normally,sched_clock_local() will be called,
                   * except current CPU is not a local CPU,then called to sched_clock_remote().
                   */
              5>  update cpu load info of the @runqueue.
                  /**
                   * @rq->cpu_load[].
                   * @rq->calc_load_update is treated as a timer to indicate that 
                   * @this_rq should go to account the number of active processes.
                   * (@calc_load_update after(expired) or equal to(timeout) @jiffies)
                   */
              6>  call to @curr->sched_class->task_tick(@rq, @curr, 0)
                  to update run-time statistics of the @current.
                  if @cfs_rq->nr_running > 1 || !sched_feat(WAKEUP_PREEMPT)
                  then
                          check_preempt_tick(@cfs_rq, @curr)
                          /**
                           * should @current be preempted?
                           * new task with higher priority => TRUE
                           * starving => TRUE
                           */
                  task_tick() of cfs will call entity_tick() on all cfs_rq related
                  to @current->se,that is all @se in the se.@group_node would be
                  updated. (each @se has a member named @cfs_rq is type of struct cfs_rq)
                  /**
                   * @se.sum_exec_time increase and @se.vruntime decrease
                   * @delta get from scheduler clock substract to @se.exec_start
                   * @se.exec_start updated to @clock
                   */
              7>  release runqueue lock.
              8>  call to perf_event_task_tick(@curr) sign a performance event associated
                  with task tick.
              #ifdef CONFIG_SMP
              9>  call to idle_cpu(cpu) to check whether local cpu is idle.
                  @rq->idle_at_tick = 1 => local cpu is idle
                                      0 => local cpu is busy
              10> trigger load balance on "this" runqueue with local cpu.
                  arise SCHED_SOFTIRQ if it is time to do periodic load balancing.
              #endif /* CONFIG_SMP */

            local cpu maybe idle even its runqueue is not empty(CPU hyper-threading technology)

            !!  update_process_times() call to scheduler_tick(),and task_tick_rt() is called
                if @current is a real-time process,otherwise,task_tick_fair() is called in normally.
                and what task_tick_rt() does has introduced in the description what update_process_times()
                does at early Chapter "Timing Measurements".

        The try_to_wake_up() function :
          awake a sleeping or stopped process by setting its state to TASK_RUNNING and inserting it into
          the runqueue of the local CPU.

          <kernel/sched.c>
            /**
             * try_to_wake_up - try to wake up threads are satisfied condition
             * @p:              task to wake up
             * @state:          process state mask
             * @wake_flags:     wake up flags,used to determine whether should do a
             *                  synchronous wakeup
             * return:          1 => woken up
             *                  0 => task already active
             */
            static int try_to_wake_up(struct task_struct *p, unsigned int state,
                                      int wake_flags);

            !!  @current is always on the run-queue unless the actual re-schedule is
                in progress,and in such case,just change process state to
                TASK_RUNNING as well.

            !!  TASK_WAKING state is set by scheduler used to make the process is waking
                here,this guarantees that nobody will actually run it,and a signal or
                other external event cannot wake it up and insert it on the runqueue either.

            what try_to_wake_up() does :
              1>  check scheduler feature whether SYNC_WAKEUPS is disabled,if it is,then
                  clear WF_SYNC in @wake_flags.
              2>  retrieve local cpu via get_cpu().
                  retrieve the runqueue which @p is residing,and lock this_rq,disable
                  interrupt.
                  update runqueue clock.
              3>  check whether @p->state & @state == 1 ?
                  if @p does not satisfy the condition,then do not wake up it.
              4>  check if @p is enqueued into a runqueue,if it is,then goto
                  "out_running"(process is already active,@se on a runqueue).
                  /**
                   * task enqueued into a runqueue,then @task->se.on_rq set to 1.
                   * @task->rq != NULL means this task is residing on @rq,but
                   * it is not guarantee to the @task had been enqueued into @rq.
                   */
              5>  save local CPU id,because on SMP,task maybe migrated to another CPU
                  by load balancing.
              #ifdef CONFIG_SMP
              6>  if task is already active,then goto "out_activate"
                  else
                          fix up the @nr_uninterruptible count of this @rq,
                          if @p is contributes to load,then decrease @rq->nr_uninterruptible
                          /* @p is in TASK_UNINTERRUPTIBLE and do not with flag PF_FREEZING */

                          set @p tp TASK_WAKING
                          call to @p->sched_class->task_waking() if it is not NULL
                          release @rq->lock but local interrupt stay disabled

                          get CPU id of @p through select_task_rq()
                          /**
                           * called with SD_BALANCE_WAKE,schedule balancing take
                           * place be there.
                           * which @rq to select follows these  rules :
                           *   > the CPU is idle - prefer to previously executing CPU
                           *     and to the local CPU.
                           *   > if select previously CPU cause lower workload,then
                           *     select the previous CPU.
                           *   > if the process has been executed recently,then select
                           *     the old CPU.
                           *   > if migrate the process to the local CPU will reduce the
                           *     unbalance between the CPUs,then select local CPU.
                           */
                                  if current CPU id is not equal to older,
                                  must task migrating occurred,set task's cpu to
                                  current CPU.

                          retrieve runqueue of this CPU and lock it,then update
                          its clock.
                          updates schedule statistic @rq->ttwu_count,and if it is
                          necessary,update @rq->ttwu_local @rq->ttwu_wake_remote,
                          too.
              #endif /* CONFIG_SMP */
            "out_activate":
              7>  increase schedule statistics @p->se.nr_wakeups.
                  if WF_SYNC then increase @p->se.nr_wakeups_sync.
                  if task migrating occurred then increase @p->se.nr_wakeups_migrate.
                  if no task migrating occurred then increase @p->se.nr_wakeups_local.
                  else,increase @p->se.nr_wakeups_remote.
              8>  activate task @p on @rq via call to function activate_task().
                  it enqueue @p into @rq and increase @nr_running of @rq.
              9>  check whether current in interrupt context.
                  if it is not,then update average of @current->se.avg_wakeup,
                  and set @se.last_wakeup to @se.sum_exec_runtime.
                  /**
                   * at there,no task_rq_unlock() was called,that is local interrupt
                   * stay disabled.
                   */
            "out_running": /* in this case,return 0 */
              10> check preempt of @p,if any process can preempt @p,then replace
                  it.
                  /**
                   * allow @rq->curr is preempted by @p require to close WAKE_SYNC bit 
                   * in @wake_flags the parameter of try_to_wake_up().
                   * preemption is not occurs immediately,it just mark @rq->curr is
                   * TIF_NEED_RESCHED.
                   * on SMP,function resched_task() will checks whether task is polling,
                   * (the target CPU is not actively polling the status of TIF_NEED_RESCHED
                   *  flag of process)
                   * if it is not,must send an IPI via smp_send_reschedule() to force
                   * rescheduling on the target CPU(@task's CPU).
                   */
              11> set @p to TASK_RUNNING.(@p is runnable,now)
              #ifdef CONFIG_SMP
              12> call to @p->sched_class->task_woken() if it is not NULL.
                  if @rq->idle_stamp != 0,then update @rq->avg_idle.
              #endif /* CONFIG_SMP */
              13> unlock @rq,enable interrupt then put cpu and return.

        The recalc_task_prio() function :
          !!  LINUX 2.6.34.1 NO SUCH FUNCTION.
              EACH struct rq HAS MEMBER NAMED "cfs_rq",THAT IS THE CFS
              SCHEDULING,IF THE SCHEDULER IS CFS POLICY,THEN EVERY
              RUNQUEUE HAS A CFS ENTITY IT ATTACH TO THIS RUNQUEUE.

          because cfs,the priority is related to the weight.to avoid the subversion of
          "niceness" due to uneven distribution of tasks with abnormal "nice" values
          across CPUS,the contribution that each task makes to its runqueue's load
          is weighted according to its scheduling class and "nice" value.
          /* for SCHED_NORMAL,it just a scaled version of the time slice allaction */

          nice levels are multiplicative with a gentle 10% change for every nice 
          level changed.i.e. a CPU bound task goes from nice 0 to nice 1,it will
          get an estimated 10% less CPU time than another task on this CPU.

          the "10% effect" is relative and cumulative :
            from any nice level >
              go up 1 level => -10% CPU usage
              go down 1 level => +10% CPU usage
              /**
               * @p1 -10% and @p +10%,then the relative distance between them is 
               * an estimated 25%.
               * to achieve this,use a multiplier of 1.25 .
               */
          
          <kernel/sched.c>
            static const int prio_to_weight[40] = {
                    88761, 71755, 56483, 46273, 36291, /* [-20, -16] */
                    28154, 23254, 18705, 14949, 11916, /* [-15, -11] */
                     9548,  7620,  6100,  4904,  3906, /* [-10, -6] */
                     3121,  2501,  1991,  1586,  1277, /* [-5, -1] */
                     1024,   820,   655,   526,   423, /* [0, 4] */
                      335,   272,   215,   172,   137, /* [5, 9] */
                      110,    87,    70,    56,    45, /* [10, 14] */
                       36,    29,    23,    18,    15, /* [15, 19] */
            };
          
          struct task_struct.@static_prio is converted from nice value.
          struct task_struct.@normal_prio is based on @static_prio.
          
          <linux/sched.h>
            #define MAX_USER_RT_PRIO 100
            #define MAX_RT_PRIO MAX_USER_RT_PRIO     /* 100 */

            #define MAX_PRIO (MAX_RT_PRIO + 40)      /* 140 */
            #define DEFAULT_PRIO (MAX_RT_PRIO + 20)  /* 120 */

          <kernel/sched.c>
            #define NICE_TO_PRIO(nice) (MAX_RT_PRIO + (nice) + 20)
            #define PRIO_TO_NICE(prio) ((prio) - MAX_RT_PRIO - 20)
            #define TASK_NICE(p) PRIO_TO_NICE(p->static_prio)

            #define USER_PRIO(p) ((p) - MAX_RT_PRIO)
            #define TASK_USER_PRIO(p) USER_PRIO((p)->static_prio)
            #define MAX_USER_PRIO (USER_PRIO(MAX_PRIO))

            /**
             * __normal_prio - return the priority based on static_prio
             * @p:             the task
             * return:         @p->static_prio
             */
            static inline int __normal_prio(struct task_struct *p);

            /**
             * normal_prio - calculate the expected normal priority
             * @p:           the task
             * return:       MAX_RT_PRIO - 1 - @p->rt_prioriy => @p is real-time
             *               __normal_prio(@p) => @p is not real-time
             */
            static inline int normal_prio(struct task_struct *p);

            /**
             * effective_prio - calculate the current priority
             * @p:              the task
             * return:          the current priority
             * # this function set @p->normal_prio to __normal_prio(@p),
             *   then check if the task has RT policy or boosted to RT
             *   priority,if it is not,return @normal_prio,otherwise,
             *   return @prio.in other words,that is keep the priority
             *   if task is RT or boosted to RT prioriry.
             */
            static int effective_prio(struct task_struct *p);

        The schedule() function :
          the linux scheduler enter point.it must to find out a process in rq and assign the CPU
          to it.it is invoked by several kernel routines directly or in a lzay way(deferred).

          <linux/sched.h>
            /* __sched - indicates the following text is reside on section ".sched.text" */
            #define __sched __attribute__((__section__(".sched.text")))

            /**
             * schedule - the linux scheduler enter point
             * # asmlinkage __sched schedule(void) { ... } EXPORT_SYMBOL(schedule);  
            asmlinkage void schedule(void);

          direct invocation :
            called from @current,because the resource it needs is not available now,the process
            would be blocked until the resource is available.
            the following steps @current have to do :
              1>  enqueue itself into an appropriate wait queue.
              2>  set process state to TASK_UNINTERRUPTIBLE or TASK_INTERRUPTIBLE.
              3>  call to schedule().
              4>  check whether resource is available now. /* selected by scheduler */
                  if it is not,goto 2>                     /* continues right after 3> */
                                                           /* scheduler set process to */
                                                           /* TASK_RUNNING */
              5>  dequeue itself from the wait queue.

          lazy invocation :
            invoked by set TIF_NEED_RESCHED flag of @current.
            a check on the value of this flag is always made before resuming the execution of
            a User Mode process,schedule() will definitely be invoked at some time in the
            near future.

            lazy invocation instances :
              >  scheduler_tick(),@current has used up its quantum of CPU.
              >  a process is woken up by try_to_wake_up(),and it has higher priority than
                 @current,then TIF_NEED_RESCHED of @rq->curr is set by resched_task().
              >  when a sched_setscheduler() system call is issued.

          what to do before context switch :
            1>  disable preemption.
                retrieve @rq of this CPU.
            2>  save @rq->curr in a local variable.
                release the locks had holden by @prev.
            3>  call to schedule_debug(@prev) to do time-debugging checks and statistics.
                if kernel control path is not atomic with preempt off currently AND
                   @prev is not in exit state
                   /* do_exit() call to schedule() atomically */
                then
                        scheduling bug occurred
                        call to __schedule_bug(@prev) to print scheduling info
                else
                        increase this_rq()->@sched_count
                        #ifdef CONFIG_SCHEDSTATS
                        AND @prev->lock_depth >= 0
                        then
                                increase this_rq()->@bkl_count
                                increaase @prev->sched_info.bkl_count

            4>  if scheduler feature HRTICK is enabled
                then
                        call to hrtick_clear(@rq) to cancel @rq->hrtick_timer,if it is
                        active
            5>  raw spin lock irq to @rq->lock,update @rq's clock,disable TID_NEED_RESCHED of 
                @prev
            6>  check if @prev is not in TASK_RUNNING and preemption is disabled,
                if it is,then check if @prev in signal pending state.
                if @prev is signal pending,then set @prev to TASK_RUNNING,
                otherwise,deactivate @prev.
                set unsigned long pointer @switch_count to @prev->nivcsw.
            7>  call to pre_schedule(@rq, @prev).
                it is a wrapper of function pre_schedule() of @prev's sched class.
            8>  if @rq no running process is exist,then call to idle_balance().
                idle_balance() is defined in <kernel/sched_fair.c>,which is used to
                pull tasks from other CPUs if the CPU(its parameter) is going to idle.
                /* BUT idle_balance() IS A STATIC FUNCTION WITHOUT EXPORT SYMBOL */
            9>  put @prev task.
                if @prev is TASK_RUNNING,then have to updates the time data related to scheduling
                of it.
                call to sched call put_prev_task() before return.
            10> pick next task through pick_next_task(@rq).
                it select the next task from robin-tree of @rq.
            11> if @prev != @next
                then
                        process sched info switch between @prev and @next
                        sign a performance event about to task sched out
                        increase @rq->nr_switches
                        set @rq->curr to @next,it as the current process of @rq
                        increase *@switch_count /* @prev->nivcsw */

            /**
             * above are the works did by schedule() before context_switch().
             * context_switch() finally would call to architecture depend function switch_to().
             * what the context_switch() does has described in Chapter 3 Processes :: Process Switch.
             */

          what to do after context switch :
            12> after context_switch(),update @cpu and @rq of @cpu.
            13> if @prev == @next,that means no other task in @rq should be selected to run.
                call to raw_spin_unlock_irq(@rq->lock) to release lock.
            14> call to post_schedule(@rq).
                call this function without @rq->lock hold and preempt disabled.
                it checks if @rq->post_schedule is TRUE?
                if it is
                then
                        acquire @rq->lock with interrupt disabled
                        if the function post_schedule() of sched class of @rq->curr is not NULL,
                        then call to it
                        release @rq->lock and enable interrupt
                        set @rq->post_schedule to 0
                /* post_schedule() is correspond to pre_schedule() */
            15> if reacquire_kernel_lock(@current) < 0 => if failed to reacquire kernel locks
                then
                        set @prev := @rq->curr
                        set @switch_count := &@prev->nivcsw
                        goto need_resched_nonpreemptible(3>)
            16> enable preempt without resched.
            17> if need_resched() is TRUE? => @current's thread flag
                => (struct thread_info *)(asm("esp") & ~(THREAD_SIZE - 1)) :: TIF_NEED_RESCHED is on
                   /* process's thread_info object is saved in kernel stack of the process */
                if it is
                then
                        goto need_resched(1>)

      Runqueue Balancing in Multiprocessor Systems :
        three types of multiprocessor machines :
          1>  classic multiprocessor architecture
                all CPUs share a common set of RAM chips.
          2>  hyper-threading
                invented by Intel.
                a hyper-threaded chip is a microprocessor that executes
                several threads of execution at once.
                includes several copies of internal registers,and quickly switches between them.
                deal with another thread when current thread is stalled for memory access.
                Linux considers a hyper-threaded physical CPU as several different logical CPUs.
          3>  NUMA
                CPUs and RAM chips are grouped in local "nodes"(usually,one CPU with several RAM chips).
                CPU access to the RAM chips inside the local node is little or no contention,and fast;
                CPU access to the RAM chips outside to the local node(remote node) is slower.
                /* Memory arbiter is the bottleneck for Classic Multiprocessor Architecture */

        For SMP,a CPU can only run the runnable process inside the runqueue of its owns,that is,a runnable
        process is always stored in exactly one runqueue - no runnable process ever appears in two or more
        runqueues(task bind to CPU,if it remains runnable).
        /* task bind to CPU may induce a severe performance penaly - CPU overloaded */

        runqueue balancing : if necessay,move the processes in a overloaded runqueue to another of other CPU.
                             load balancing algorithm should take into consideration the topology of the CPUs
                             in the system.
                             since Linux 2.6.7,algorithm based on the notion of "scheduling domains".
                             /**
                              * easily tune for all kinds of existing multiprocess architecture,include
                              * multi-core architecture.
                              */

        Scheduling Domains :
          essentially,a scheduling domain is a set of CPUs whose workloads should be kept balanced by the
          kernel.
          scheduling domains are hierarchically organized :
                  the top scheduling domain {
                          span all CPUs in the system

                          child scheduling domain {
                                  span CPU0, CPU1, CPU2

                                  child scheduling domain {
                                          span CPU0
                                  }

                                  child scheduling domain {
                                          span CPU1, CPU2
                                  }

                          }

                          child scheduling domain {
                                  span CPU3
                          }

                          child scheduling domain {
                                  span CPU4, CPU5, CPU6, CPU7

                                  ...
                          }
                          
                          ...
                  }
          !!  base domain => the scheduling domain at the bottom of a scheduling domain
                             it usually span one CPU
                             
          every scheduling domain is partitioned in one or more groups,each of which represents a subset
          of the CPUs of the scheduling domain.workload balancing is always done between groups of a 
          scheduling domain.
          /**
           *
           * @phys_domains @sched_group_phys     group1                group2
           * tsd : { CPU0, CPU1, CPU2, CPU3 } => csd1 : { CPU0, CPU 1} csd2 : { CPU2, CPU3 }
           * @p->CPU0
           * balance @p : move @p to CPU2
           * => if and only if group1.workload > group2.workload
           */          

          <linux/sched.h>
            /* scheduling domain flags */
            #ifdef CONFIG_SMP

            #define SD_LOAD_BALANCE         0X0001  /* do load balancing on this domain */
            #define SD_BALANCE_NEWIDLE      0x0002  /* balance when about to become idle */
            #define SD_BALANCE_EXEC         0x0004  /* balance on exec */
            #define SD_BALANCE_FORK         0x0008  /* balance on fork,clone */
            #define SD_BALANCE_WAKE         0x0010  /* balance on wakeup */
            #define SD_WAKE_AFFINE          0x0020  /* wake task to waking CPU */
            #define SD_PREFER_LOCAL         0x0040  /* prefer to keep tasks local to this domain */
            #define SD_SHARE_CPUPOWER       0x0080  /* domain members share CPU power */
            #define SD_POWERSAVINGS_BALANCE 0x0100  /* balance for power savings */
            #define SD_SHARE_PKG_RESOURCES  0x0200  /* domain members share cpu pkg resources */
            #define SD_SERIALIZE            0x0400  /* only a single load balancing instance */

            #endif

            /* scheduling domain levels */
            enum sched_domain_level {
                    SD_LV_NONE = 0,
                    SD_LV_SIBLING,
                    SD_LV_MC,
                    SD_LV_CPU,
                    SD_LV_NODE,
                    SD_LV_ALLNODES,
                    SD_LV_MAX
            };

            /**
             * sched_group - structure sched_group used to represent a group of
             *               sched_domain instances
             * @next:        next scheduling domain group
             * @cpu_power:   CPU power of this group,SCHED_LOAD_SCALE being max power
             *               for a single CPU
             * @cpumask:     the CPUs this group covers
             *               NOTE : this filed is variable length alike to
             *                      struct sched_domain.@span
             */
            struct sched_group {
                    struct sched_group *next;
                    unsigned int cpu_power;
                    unsigned long cpumask[0];
            };

            /**
             * sched_domain - structure sched_domain represent the scheduling domain used by Linux
             *                load balancing algorithm
             * @parent:       parent of this domain
             * @child:        child of this domain
             * @groups:       groups this domain belong to
             * @min_interval: minimum balance interval in ms
             * @max_interval: maximum balance interval in ms
             * @busy_factor:  less balancing by factor if busy
             * @imbalance_pct: no balance until over watermark
             * @cache_nice_tries: leave cache hot tasks for # tries
             * @busy_idx:     busy index
             * @idle_idx:     idle index
             * @newidle_idx:  new idle index
             * @wake_idx:     wake index
             * @forkexec_idx: fork execution iindex
             * @smt_gain:     SMT Hyper-Threading support - Simulate MultiThreading
             * @flags:        scheduling domain flag
             * @level:        scheduling domain level
             * @last_balance: jiffies for the last balance
             * @balance_interval: balance interval in ms
             * @nr_balance_failed: number of balance failed
             * @last_update:  jiffies for the last scheduling domain info updating
             * @span:         span of all CPUs in this domain
             *                NOTE : this filed is variable length.
             *                       allocated dynamically by attaching extra space to
             *                       the end of the structure,depending on how many CPUs
             *                       the kernel has booted up with.
             *                       # it is also be embedded into static data structures
             *                         at build time.
             */
            struct sched_domain {
                    /* these fields must be setup */
                    struct sched_domain *parent;
                    struct sched_domain *child;
                    struct sched_group *groups;
                    unsigned long min_interval;
                    unsigned long max_interval;
                    unsigned int busy_factor;
                    unsigned int imbalance_pct;
                    unsigned int cache_nice_tries;
                    unsigned int busy_idx;
                    unsigned int idle_idx;
                    unsigned int newidle_idx;
                    unsigned int wake_idx;
                    unsigned int forkexec_idx;
                    unsigned int smt_gain;
                    int flags;
                    enum sched_domain_level level;

                    /* runtime fields */
                    unsigned long last_balance;
                    unsigned int balance_interval;
                    unsigned int nr_balance_failed;

            #ifdef CONFIG_SCHEDSTATS
                    ...
            #endif
            #ifdef CONFIG_SCHED_DEBUG
                    ...
            #endif
                
                    unsigned long span[0];
            };

          <kernel/sched.h>
            /**
             * static_sched_group - structure static_sched_group used to
             *                      define per-CPU variable @sched_group_phys
             * @sg:                 the group @cpus belong to
             * @cpus:               cpu bitmap
             */
            struct static_sched_group {
                    struct sched_group sg;
                    DECLARE_BITMAP(cpus, CONFIG_NR_CPUS);
            };

            /**
             * static_sched_domain - structure static_sched_domain used to
             *                       define per-CPU variable @phys_domains
             * @sd:                  the scheduling domain @span belong to
             * @span:                cpu bitmap
             */
            struct static_sched_domain {
                    struct sched_domain sd;
                    DECLARE_BITMAP(span, CONFIG_NR_CPUS);
            }

            /* phys_domains - represent physical CPUs scheduling domain */
            static DEFINE_PER_CPU(struct static_sched_domain, phys_domains);

            /* sched_group_phys - represent the group of physical CPUs scheduling domain */
            static DEFINE_PER_CPU(struct static_sched_group, sched_group_phys);

          @phys_domains => the top scheduling domain  /* span all physical CPUs */
          @sched_group_phys => the group of the top scheduling domain

          The rebalance_tick() function :
            !!  LINUX 2.6.34.1 HAS NO SUCH FUNCTION WAS DEFINED.
                INSTEAD,WORKLOAD REBALANCING IS TRIGGERED IN FUNCTION scheduler_tick() IF
                NECESSARY WITH CONFIG_SMP DEFINED.
                FUNCTION trigger_load_balance(@rq, @cpu) IS USED TO TRIGGER REBALANCING.
                /* @cpu => smp_processor_id() @rq => cpu_rq(@cpu) */

            /* based on cfs */
            <kernel/sched_fair.c>
              /**
               * trigger_load_balance - trigger softirq SCHED_SOFTIRQ to mark load 
               *                        rebalance
               * @rq:                   this rq
               * @cpu:                  the cpu is processing scheduler_tick()
               *
               * # to check if this @cpu is need to be rebalanced.
               *   if @jiffies time_after_eq @rq->next_balance AND
               *      cpu_rq(@cpu)->sd != NULL
               *   then
               *           raise_softirq(SCHED_SOFTIRQ);
               * ! if defined CONFIG_NO_HZ,the checking will be more complicated.
               *   it is the place where to nominate a new idle load balancing owner,
               *   or decide to stop the periodic load balancing if the whole system
               *   is idle.
               */
              static inline void trigger_load_balance(struct rq *rq, int cpu);

              #ifdef CONFIG_NO_HZ
              /**
               * nohz - idle load balancing recorder when periodic ticking is stopped
               * @load_balancer:        load balancer for nohz mode
               * @cpu_mask:             cpumask for idle CPUs
               * @ilb_grp_nohz_mask:    mask of groups of scheduling domains the CPUs
               *                        inside them appropriate to be nominated as
               *                        @load_balancer
               */
              static struct {
                      atomic_t load_balancer;
                      cpumask_var_t cpu_mask;
                      cpumask_var_t ilb_grp_nohz_mask;
              } nohz ____cacheline_aligned = {
                      .load_balancer = ATOMIC_INIT(-1),
              };
              ...
              #endif

              what trigger_load_balance() to does with CONFIG_NO_HZ :
                1>  if @rq recently in nohz mode AND this cpu is not idle at this tick
                    then
                            set @rq->in_nohz_recently := 0
                            check if @nohz.load_balancer == cpu
                            /* this cpu is nominated to does load balancing for all CPUs */
                                    clear this cpu from @nohz.cpu_mask
                                    set @nohz.load_balancer := -1
                                    /* this cpu is no longer be the load balancer,busy now */
                            check if @nohz.load_balancer == -1
                                    find a new idle load balancing cpu
                                    if @ilb < @nr_cpu_ids /* unfound */
                                            call to resched_cpu(@ilb)
                                            /* set TIF_NEED_RESCHED for cpu_rq(@ilb)->curr */
                2>  if @rq is idle at this tick => this cpu is idle AND
                       @nohz.load_balancer == @cpu AND
                       cpumask_weight(@nohz.cpu_mask) == num_online_cpus()
                    then
                            resched_cpu(@cpu)
                            return
                            /**
                             * this cpu is idle and doing idle load balancing for all the
                             * cpus with ticks stopped.
                             */
                3>  if @rq is idle at this tick AND
                       @nohz.load_balancer != @cpu AND
                       cpumask_test_cpu(@cpu, @nohz.cpu_mask)
                    then
                            just return
                            /**
                             * this cpu is idle and the idle load balancing is done by someone,
                             * no need to raise the SCHED_SOFTIRQ.
                             */

          The run_rebalance_domains() function :
            function run_rebalance_domains() defined in <kernel/sched_fair.c> is the softirq handler
            for SCHED_SOFTIRQ which is registered by function sched_init() defined in <kernel/sched.c>.
            if necessay,trigger_load_balance() may raise such softirq from scheduler_tick().

            <kernel/sched_fair.c>
              /**
               * run_rebalance_domains - ISR for softirq SCHED_SOFTIRQ
               *                         it is used to deal with workload rebalancing
               * @h:                     softirq action
               */
              static void run_rebalance_domains(struct softirq_action *h);

              what run_rebalance_domains() does :
                1>  retrieve local CPU id.
                    use the CPU id to retrieve @runqueues.
                    declare a enum type cpu_idle_type variable named @idle,
                    if @this_rq->idle_at_tick => T
                    then @idle := CPU_IDLE       /* this cpu is idle */
                    else @idle := CPU_NOT_IDLE   /* this cpu is busy */
                                  /* the possible values for such enum type */

                2>  call to auxiliary routine rebalance_domains(@this_cpu, @idle).
                #ifdef CONFIG_NO_HZ
                3> if this cpu is the owner for idle load balancing,then do the
                   balancing on behalf of the other idle cpus whose ticks are stopped.
                   if @this_rq->idle_at_tick AND @nohz.load_balancer == this_cpu
                   then
                           for each cpu in the @nohz.cpu_mask
                                   check if balance cpu is this cpu
                                           T => continue to next cycle
                                   check if need_resched() is true,that is @rq->curr
                                   has flag TIF_NEED_RESCHED
                                           break
                                   call to rebalance_domains(the approriate cpu, CPU_IDLE)
                                   let the @cpu to does the scheduling domain rebalancing
                                   /**
                                    * let the @cpu does load balancing,because it is idle
                                    * now,so scheduler can migrate some tasks in the same
                                    * group of another CPU(runqueues) to this cpu.
                                    */

                                   update local variable @rq through cpu_rq(@balance_cpu)
                                   check if @this_rq->next_balance time after @rq->next_balance
                                   then
                                           update @this_rq->next_balance to @rq->next_balance
                #endif /* CONFIG_NO_HZ */

          The rebalance_domains() function :
            this function checks each scheduling domain to see if it is due to be balanced,
            and initiates a balancing operation if so.

            <kernel/sched_fair.c>
              /**
               * rebalance_domains - auxiliary routine of run_rebalance_domains(),
               *                     does the real rebalancing for scheduling domains
               * @cpu:               the cpu to does rebalancing
               * @idle:              cpu state => CPU_IDLE OR CPU_NOT_IDLE
               * # balancing paramters are set up in arch_init_sched_domains().
               */
              static void rebalance_domains(int cpu, enum cpu_idle_type idle);

              what rebalance_domains() does :
                1>  retrieve the runqueue associated with @cpu.
                    initializes local variable @next_balance to @jiffies + 60 * HZ
                    as the record.
                    initializes local variable @balance to 1.
                    initializes local variable @update_next_balance to 0.
                2>  traverse all domains through macro for_each_domain(the cpu, the scheduling domain).
                    this macro function is defined in <kernel/sched.c>,it traverse up to the top
                    scheduling domain.
                3>  in the cycle of for each domain : (the initial value of @sd is cpu_rq(@cpu)->sd)
                      if @sd->flags & SD_LOAD_BALANCE => F
                      then
                              continue to next cycle
                              /* this scheduling domain does not need load balancing */

                      set local variable @interval to @sd->balance_interval
                      check if @idle != CPU_IDLE is true
                      then
                              set @interval *= @sd->busy_factor
                              /* @cpu is not idle,then set @interval to an empirical value */
                      convert @interval(ms) to jiffies
                      check if @interval is zero,then set @it to 1
                      check if @interval > HZ * NR_CPUS / 10,if it is,set @interval to the result

                      set local variable @need_serialize to @sd->flags & SD_SERIALIZE
                      /* indicator used to tell whether need to lock the balancing spin lock */
                      try to lock spin lock @balancing if @need_serialize is TRUE,if failed to
                      lock,then goto out /* serializing required but failed to acquire lock */

                      check if @jiffies time after or equal to @sd->last_balance + @interval
                      => T    /* it is time to do rebalancing */
                              call to load_balance(@cpu, @rq, @sd, @idle, &@balance)
                              if the return value is TRUE,then update @idle CPU_NOT_IDLE
                              set @sd->last_balance to @jiffies
                              /**
                               * return value is TRUE,means the tasks had pulled over,
                               * so the @cpu is no longer idle,or one of SMT siblings is
                               * not idle.
                               */
                      if @need_serialize is TRUE,then unlock the spin lock
                  out:

                      check if @next_balance time after @sd->last_balance + @interval
                      => T
                              update @next_balance to @sd->last_balance + @interval
                              set @update_next_balance to 1

                      check if @balance is false
                      => T
                              break  /* stop the load balance at this level */
                              /**
                               * @balance is past by address to load_balance(),if @balance
                               * is false now,then there is another CPU in the sched group
                               * which is doing load balancing more actively.
                               */
                      next cycle :
                4>  check if @update_next_balance is T
                    => T
                            set @rq->next_balance to @next_balance
                            /* the next timepoint to does workload balancing for @rq */

          The load_balance() function :
            this function called by rebalance_domains() to check this_cpu to ensure it is 
            balanced within domain.if imbalance have detected,then attempt to move tasks.

            <kernel/sched_fair.c>
              /**
               * load_balance - check if this cpu is balanced,try to move tasks if imbalance
               *                detected
               * @this_cpu:     this cpu to check
               * @this_rq:      runqueue of this cpu
               * @sd:           the domain at current level
               * @idle:         idle state of this cpu
               * @balance:      indicator used to tell caller if there is anoter CPU active
               * return:        the number of tasks they have moved
               */
              static int load_balance(int this_cpu, struct rq *this_rq,
                                      struct sched_domain *sd, enum cpu_idle_type idle,
                                      int *balance);

              what load_balance() does :
                1>  initializes local variables :
                      @all_pinned := 0 => whether the tasks is pinned on a CPU
                      @active_balance := 0 => indicate balancing is active
                      @sd_idle := 0 => indicates the scheduling domain at this level is idle
                      @ld_moved := unspecified => tell how many tasks load_balance() moved
                2>  get per-CPU variable @load_balance_tmpmask is type of cpumask_var_t to
                    @cpus.
                    copy @cpus to @cpu_active_mask(activated cpus in this system).
                                  /* defined in <kernel/cpu.c> with EXPORT_SYMBOL */
                3>  check if this cpu is idle AND
                             @sd->flags set SD_SHARE_CPUPOWER AND
                             @sd->parent unset SD_POWERSAVINGS_BALANCE
                    then
                            @sd_idle := 1 => this scheduling idle is idle now
                                             even there maybe slibing is busy
                            /**
                             * parent sd enabled power savings policy.
                             * idle sibling can pick up load irrespective of busy siblings.
                             * idle sibling percolate up state as CPU_IDLE.
                             */
                4>  increase @sd->lb_count[@idle].
                                  /* load balance statistics */
           redo:
                5>  call to update_shares(@sd),update @sd->last_update and call to walk_tg_tree() with
                    @tg_nop(@down) @tg_shares_up(@up) @sd(@tree),if necessary.
                    /**
                     * walk_tg_tree() => iterate full tree
                     *                   call to @down when first entering a node
                     *                   call to @up when leave it for the final time
                     * tg_shares_up(task group, the data)
                     * =>  re-compute the task group their per cpu shares over the given domain
                     */
                6>  find the busiest group through find_busiest_group(),@balance will pass to it.
                    check if *@balance => false
                    then
                            goto out_balanced /* no busiest group */
                    check if @group => true
                    then
                            increase @sd->ld_nobusyg[@idle]
                            goto out_balanced
                7>  find busiest runqueue through find_busiest_queue().
                    check if @busiest => false
                    then
                            increase @sd->lb_nobusyq[@idle]
                            goto out_balanced
                8>  add @imbalance(set by find_busiest_queue()) to @sd->lb_imbalance[@idle]
                    set @ld_moved to zero ready for moving
                9>  check if @busiest->nr_running > 1
                    then
                            do tasks moving :  /* another task in @busiest */
                                    disable local interrupt
                                    lock @this_rq and @busiest
                                    call to move_tasks(@this_rq, @this_cpu, @busiest,
                                                       @imbalance, @sd, @idle, &@all_pinned)
                                    set @ld_moved to the return value
                                    release locks
                                    enable local interrupt
                            check if @ld_moved => true AND
                                     @this_cpu != smp_processor_id()
                            then
                                    set TIF_NEED_RESCHED for cpu_rq(@this_cpu)->curr
                                    /**
                                     * some other cpu did the load balance.
                                     * task moved,so it is possible to encounter the case
                                     * @this_cpu != smp_processor_id()
                                     * @this_cpu is selected to do load balance,
                                     * but another one have done it.
                                     */
                            check if @all_pinned => true
                            then
                                    /* all tasks on this runqueue were pinned by CPU affinity */
                                    call to cpumask_clear_cpu() to remove the cpu of
                                    @busiest from @cpus
                                    check if @cpus is NULL
                                    then
                                            goto redo
                                            /**
                                             * after removed the cpu of @busiest,we
                                             * encountered no cpu is working for
                                             * loadl balance.
                                             * we need re-do it.
                                             */
                                    else
                                            goto out_balanced
                10> check if @ld_moved => zero
                    then
                            update load balance statistics
                            /**
                             * failed to done load balance,no tasks moved even there
                             * is a busiest runqueue.
                             */
                            if need to active balance for @sd,then acquire @busiest->lock with
                            local interrupt disabled,and check if the @curr task on @busiest
                            cpu cannot be moved to @this_cpu,then release lock and enable
                            local interrupt,set @all_pinned to 1(can not be moved),finally
                            goto out_one_pinned.

                            if @busiest do not active balance,then set @busiest->active_balance
                            to 1(some task of @busiest need to be moved),and push @this_cpu to
                            @busiest->push_cpu,finally,set @active_balance to 1.

                            release @busiest->lock and enable local interrupt.
                            check if @active_balance => true
                            then
                                    wake up @busiest->migration_thread
                                    /**
                                     * the @migration_thread walks the chain of the scheduling domain,
                                     * from the base domain of the @busiest to the top domain,looking
                                     * for an idle CPU.if found,invoke move_tasks() to move one
                                     * process into the idle runqueue.
                                     */
                            set @sd->nr_balance_failed := @sd->cache_nice_tries + 1
                    else
                            set @sd->nr_balance_failed to 0
                            /* @ld_moved > 0 => task moved */
                11> check if @active_balance => false
                    then
                            set @sd->balance_interval to @sd->min_interval
                            /**
                             * time interval for @sd to balance
                             */
                    else
                            check if @sd->balance_interval < @sd->max_interval
                            then
                                    set @sd->balance_interval *= 2
                12> check if @ld_moved => zero AND
                             @sd_idle => zero AND
                             @sd->flags set @sd_SHARE_CPUPOWER AND
                             @sd'parent does not set SD_POWERSAVINGS_BALANCE
                    then
                             /**
                              * no task moved,and this sd is not idle
                              * domain members share power and parent does not
                              * enable power savings
                              */
                             set @ld_moved to -1
                13> goto out
    out_balanced:
                14> increase @sd->ld_balanced[@idle]
                    set @sd->nr_balance_failed to zero
                    /* load is balanced */
  out_one_pinned:
                15> tune up the balancing interval
                    check if @sd_idle => false AND
                             @sd->flags set SD_SHARE_CPUPOWER AND
                             @sd'parent does not set SD_POWERSAVINGS_BALANCE
                    then
                            set @ld_moved to -1   /* members share power */
                    else
                            set @ld_moved to zero /* move tasks failed */
             out:
                16> check if @ld_moved => true /* negative number also is T */
                    then
                            call to update_shares(@sd) to update @last_update,
                            if necessary
                    return @ld_moved
                           /**
                            * -1 => cpu power sharing
                            * 0  => failed to move tasks
                            * positive => moved tasks
                            */

          The move_tasks() function :
            the function move tasks from a busiest runqueue to another lower-load runqueue
            within scheduling domains.

            <kernel/sched_fair.c>
              /**
               * move_tasks - migrate tasks
               * @this_rq:    this rq of this cpu
               * @this_cpu:   this cpu is the cpu nomimated to does load balancing
               * @busiest:    the busiest runqueue in the same scheduling domain
               * @max_load_move:  the maximum number of tasks allowed to be migrated
               * @sd:         the scheduling domain of current level
               * @idle:       state of this cpu
               * @all_pinned: indicator used to tell whether all tasks pinned by CPU
               *              affinity
               * return:      if total load moved is greater than zero
               * # this function call to the schedule class associated load balance
               *   routine,for cfs,it is load_balance_fair().
               */
              static int move_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,
                                    unsigned long max_load_move, struct sched_domain *sd,
                                    enum cpu_idle_type idle, int *all_pinned);

    System Calls Related to Scheduling

