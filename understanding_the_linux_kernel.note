                                                Understanding the Linux Kernel (Third Edition)
Linux kernel version : 2.6.34.1

Chapter 1 : Introducation
    Linux joins System V Release 4 (SVR4),developed by AT&T.
    other UNIX OS or UNIX-like OS :
      4.4 BSD
      Digital UNIX
      AIX
      HP-UX
      Solaris
      Mac OS X
      ...

      (opensource)
      FreeBSD
      NetBSD
      OpenBSD
      Linux
      ...

    The common attributes between Linux and well-known commercial Unix kernels :
      monolithic kernel
      compiled and statically linked traditional Unix kernels
      (of course support module for dynamically load and unload)
      kernel threading
      multithreaded application support(lightweight process LWP supporting)
      preemptive kernel
      multiprocessor support(SMP)
      filesystem

      streams(but Linux has no analogue to the STREAMS I/O subsystem introduced in SVR4)  

    The advantages of Linux :
      Linux is cost-free
      Linux is fully customizable in all its components
      Linux runs on low-end,inexpensive hardware platforms
      Linux is powerful
      Linux developers are excellent programmers
      The linux kernel can be very small and compact
      Linux is highly compatible with many common operating systems
      Linux is well supported

    Basic Operating System Concepts :
      the operating system must fulfill two main objectives :
        >  interact with the hardware components,servicing all low-level programmable elements
           included in the hardware platform.
        >  provide an execution environment to the applications that run on the computer system.
    
      modern OS does not allow user program interact with hardware directly and forbid them to
      access arbitrary memory locations.
      in particular,the hardware introduces at least two different execution modes for the CPU :
        a nonprivileged mode for user programs
        a privileged mode for the kernel
        /*  UNIX calls these
         *  User Mode and Kernel Mode
         */

    Multiuser Systems :
      A multiuser system is a computer that is able to concurrently and independently execute several
      applications belonging to two or more users.

      Concurrently :
        applications can be activated at the same time and contend for various resources such as CPU,memory,
        hard disks,and so on.
      Independently :
        each application can perform its task with no concern for what the applications of the other uses are
        doing.

      Multiuser operating systems must include several features :
        >  An authentication mechanism for verifying the user's identify
        >  A protection mechanism against buggy user programs that could block other applications running 
           in the system
        >  A protection mechanism against malicious user programs that could interfere with or spy on the 
           activity of other users
        >  An accounting mechanism that limits the amount of resource units assigned to each user

      To ensure safe protection mechanisms,operating systems must use the hardware protection associated 
      with the CPU privileged mode.
      Unix is a multiuser system that enforces the hardware protection of system resources.

    Users and Groups :
      in a multiuser system,each user has a private space on the machine,the operating system must ensure that
      the private portion of a user space is visible only to its owner.Unix-like system use UserID or UID as
      user identifier,it is a unique number.
      to selectively share material with other users,each user is a member of one or more user groups,which 
      are identified by a unique number called a user group ID.each file is associated with exactly one group.
      any Unix-like operating system has a special user called root or superuser,the root user can do almost
      everything,because the operating system does not apply the usual protection mechanism to it.

    Processes :
      A process can be defined either as "an instance of a program in execution" or  as the "execution context"
      of a running program.
      in traditional operating systems,a process executes a single sequence of instructions in an address space;
      the address space is the set of memory addresses that the process is allowed to reference.

      multiple processes environment :
        in modern operating system allow processes with multiple execution flows,that is,multiple sequences
        of instructions executed in the same address space.
        system that allow concurent active processes are said to be multiprogramming or multiprocessing.

      multiuser systems must enforce an multiple processes environment,but some multiprocessing operating
      systems are not multiuser.

      it is important to distinguish programs from processes;several processes can execute the same program
      concurently,while the same process can execute several programs sequentially.

      scheduler is an important system compoent in multiprocessing system,which process should be hold the 
      CPU to execute is determined by scheduler.
    
      preemptable :
        some operating systems allow only nonpreemptable processes,which means that the scheduler is invoked
        only when a process voluntarily relinquishes the CPU.but process of a multiuser system must be
        preemptable.

        UNIX is a multiuer and multiprocessing operating system with  preemptable processes.

        UNIX-like operating systems adopt a process/kernel model.each process has the illusion that it is the
        only process on the machine,and it has exclusive access to the operating system services.

    Kernel Architecture :
      monolithic kernel,each kernel layer is integrated into the whole kernel program and runs in Kernel Mode
      on behalf of the current process.

      microkernel operating systems demand a very small set of functions from the kernel,generally including a 
      few synchronization primitives,a simple scheduler,and an interprocess communication mechanism.
      several system processes that run on top of the microkernel implement other operating system layer functions,
      like memory allocators,device drivers,and system call handlers.

      comparison between monolithic kernel and microkernel :
        1>  microkernel is slower than monolithic kernel,because kernel layer communication has a cost.
        2>  microkernel force the system programmers to adopt a modularized approach,that means microkernel
            is very modularized.every relativly kernel layer is independent program that must interact with
            the other layers through well-defined and clean software interfaces.
        3>  microkernel can be easily ported to other architectures fairly easily.all hardware-dependent components
            are generally encapsulated in the microkernel code.
        4>  microkernel tend to make better use of random access memory than monolithic kernel,system processes
            that are not implementing needed functionalities might be swapped out or destroyed.

        UNIX and UNIX-like operating system almost satisfy all advantages of microkernel.

      the main advantages of using modules include :
        >  a modularized approach.
        >  platform independence
        >  frugal main memory usage
        >  no performance penalty

    An Overview of Unix Filesystem :
      the UNIX operating system design is centered on its filesystem,which has several interesting characteristics.

        Files - A Unix file is an information container structured as a sequence of bytes;the kernel does not
                interpret the contents of a file.

        Hard and Soft Links - A filename included in a directory is called a file hard link,or more simply,a link.
                              Soft links also called symbolic links,symbolic links are short files that contain an
                              arbitrary pathname of another file.

        Hard links limitations :
          it is not possible to create hard links for directories.
          links can be created only among files included in the same filesystem.

        File Types - File type represent what the file is.
          Unix files may have one of the following types :
            Regular file
            Directory
            Symbolic link
            Block-oriented device file
            Character-oriented device file
            Pipe and named pipe
            Socket

        File Descriptor and Inode - A file descriptor is a number greater than or equal to zero,the number
                                    associared with a file data structure which represent the file opened by
                                    process.
                                    All information needed by the filesystem to handle a file is included in a 
                                    data structure called an inode.each file has its own inode,which the filesystem
                                    uses to identify the file.

        Access Rights and File Mode - there are three types of access rights :
          read
          write
          execute
          /*  access rights occupied nine bits  */
                      
        Three additonal flags :
          suid(Set User ID)
          sgid(Set Group ID)
          sticky - an executable file with the sticky flag set corresponds to request to
                   the kernel to keep the program in memory after its execution terminates.

        File Mode is a mode mixed Access Rights and Additonal flags.

        File-Handling System Calls -  kernel provide some primitives to user mode to help user interact with actual
                                      file stored in block device.
                                      opening a file - open
                                      accessing an opened file - read write lseek ...
                                      closing a file - close
                                      renaming and deleting a file - rename unlink link ...
                       
    An Overview of Unix Kernels :
      Unix kernels provide an execution environment in which applications may run.
      
      The Process/Kernel Mode - Actually,some CPUs can have more than two execution states,but all standard Unix
                                kernels use only Kernel Mode and User Mode.
        A program usually executes in User Mode and switches to Kernel Mode only when requesting
        a service provided by the kernel.when the kernel has satisfied the program's request,it
        puts the program back in User Mode.the way is system calls.
        after sets up parameters of syscall,then executes the hardware-dependent CPU instruction to
        switch from User Mode to Kernel Mode.
        Unix systems include a few privileged processes called kernel threads with the following characteristics :
          >  they run in Kernel Mode in the kernel address space
          >  they do not interact with users,and thus do not require terminal devices
          >  they are usually created during system startup and remain alive until the system
             is shut down.
        Unix kernels do much more than handle system calls;in fact,kernel routines can be activated in several ways :
          >  A process invokes a system call
          >  the CPU executing the process signals an exception,which is an unusual condition such as
             an invalid instruction.the kernel handles the exception on behalf of the process that 
             caused it.
          >  A peripheral device issues an interrupt signal to the CPU to notify it of an event such
             as a request for attention,a status change,of the completion of an I/O operation.
             each interrupt signal is dealt by a kernel program called an interrupt handler.
          >  A kernel thread is executed.because it runs in Kernel Mode,the corresponding program
             must be considered part of the kernel.

      Process Implementation - To let the kernel manage processes,each process is represented by a process descriptor that
                               includes information about the current state of the process.
                               that include :
                                 >  the program counter(PC) and stack pointer(SP) registers
                                 >  the general purpose registers
                                 >  the floating point registers
                                 >  the processor control registers(Processor Status Word) containing information about
                                    the CPU state
                                 >  the memory management registers used to keep track of the RAM accessed by the process

      When a process is scheduled,then kernel use the former stored information to recover process status,
      and set IP to the next instruction.(it also known as PC,process counter,but IP is a instruction register)

      Reentrant Kernels - All Unix kernels are reentrant.this means that several processes may be executing in Kernel Mode at
                          the same time.of course,on uniprocessor systems,only one process can progress,but many can be blocked
                          in Kernel Mode when waiting for the CPU or the completion of some I/O operation.
        reentrant functions : the functions they modify only local variables and do not alter global data structures.
                              (reentrant functions,that is how some real-time kernels are implemented)
                              but kernel can include nonreentrant functions and use locking mechanisms to ensure that only one process
                              can execute a nonreentrant function at a time.

              generial kernel control path :
                Run process -> Timer interrupt -> Deal with interrupt -> Schedule tasks -> Run process -> ...

              when one of the following events occurs,the CPU will enter a new path to do something :
                >  a process executing in User Mode invokdes a system call.if the service would not be satisfied,
                   then scheduler pick next process to run,the former process enter sleeping until condition is
                   satisifed or wake up by a signal.
                   (system calls is triggered via soft interrupt,on x86,it is "int 0x80")
                >  the CPU detects an exception.CPU must starts the execution of a suitable procedure.
                   after the procedure terminates,CPU return to the path before the exception is detected.
                >  a hardware interrupt occurs,and the interrupts enabled.
                   CPU must starts processing another kernel control path to handle the interrupt.
                >  an interrupt occurs while the CPU is running with kernel preemption enabled,and a higher priority
                   process is runnable.(lower intterupt could be preempted by higher interrupt)

              three states of CPU current be :
                >  running a process in User Mode                   (process context)
                >  running an exception or a system call handler    (process context)
                >  running an interrupt handler                     (interrupt context)

      Process Address Space - Each process runs in its private address space.A process running in User Mode refers to private stack,
                              data,and code areas.when running in Kernel Mode,the process addresses the kernel data and code areas
                              and uses another private stack.
                              reentrant kernel,each kernel control path refers to its own private kernel stack.
                              sometimes,kernel shares process address space to another same program,certainly that is code area.
                              but process also can initiativly shares its address space to another different program.(IPC)

      Synchronization and Critical Regions - reentrant kernel requires the use of synchronization.
                                             if a kernel control path is suspended while acting on a kernel data structure,no other
                                             kernel control path should be allowed to act on the same data structure unless it has
                                             been reset to a consistent state.
                                             critical region :
                                               any section of code that should be finished by each process that begins it before
                                               another process can enter it.

      Kernel preemption disabling - A synchronization mechanism applicable to preemptive kernel consists of disabling kernel 
                                    preemption before entering a critical region and reenabling it right after levaing the region.
                                    nonpreemptability is not enough for multiprocessor systems,because two kernel control paths running
                                    on different CPUs can concurrently access the same data structure.

      Interrupt disabling - Another synchronization mechanism for uniprocessor systems consists of disabling all hardware interrupts
                            before entering a critical region and reenabling them right after leaving it.
                            if critical region is too large,this will down system efficiency;moreover,on a multiprocessor system,
                            disabling interrupts on the local CPU is not sufficient.

      Semaphores - A semaphore is simply a counter associated with a data structure;it is checked by all kernel threads before they
                   try to access the data structure.
                   each semaphore may be viewed as an object composed of :
                     an integer variable
                     a list of waiting processes
                     two atomic method : down() and up()

                     down() decrease semaphore value,up() increase semaphore value.if its value is less than 0,process have to wait on
                     it until semaphore is available.

      Spin locks - some kernel data structures should be protected from being concurrently accessed by kernel control paths that run
                   on different CPUs.in this case,if the time required to update the data structure is short,a semaphore could be very
                   inefficient.
                   a spink lock is very similar to semaphore,but it has no process list;when a process finds the lock closed by another
                   process,it "spins" around repeatedly,executing a tight instruction loop until the lock becomes open.
                   of course,spin locks are useless in a uniprocessor environment.

      Avoiding deadlocks - the simplest case of deadlock occurs when process p1 gains access to data structure a and process p2 gains
                           access to b,but p1 then waits for b and p2 waits for a.
                           Several operating systems,including Linux,avoid this problem by requesting locks in a predefined order.

    Signals and Interprocess Communication :
      Unix signals provide a mechanism for notifying processes of system events.
      there are two kinds of system events :
        Asynchronous notifications
        Synchronous notifications

      POSIX standard defines about 20 different signals,2 of which are user-definable and may be used as a primitive mechanism for
      communication and synchronization among processes in UserMode.

      process may ignore the signal or asynchronously execute a specific procedure(signal handler),kernel provides the default
      signal handler for every signal.
      the default actions are :
        terminate the process
        write the execution context and the contents of the address space in a file(coredump) and terminate the process
        ignore the signal
        suspend the process
        resume the process's execution,if it was stopped

      SystemV IPC(AT&T's Unix System V) - semaphores, message queues, shared memory
      POSIX standard also defined some IPCs - posix semaphores, posix message queues, posix shared memory

    Process Management : 
      Unix makes a neat distinction between the process and the program it is executing.
      process is the program which is loadded into memory,it contains the resources all the program needs.
      the program it is executing,that means it had been loadded and the process is TASK_RUNNING.
      a parent process is such process it has been called fork() syscall,fork() would create a new process and its resources
      is duplicated to parent process,the process is child process.

      Copy-On-Write - this approach defers page duplication until the last moment(i.e., until the parent or the child is
                      required to write into a page).
                      the naive implementation of fork() was quite time consuming.

      Zombie process - a process would exited if it called _exit(),and kernel send SIGCHLD to its parent.
                       the zombie process is such process it had been exited but its status had not been checked,generally,
                       this work should be completed by its parent.
                       when the process exited,some resources are still effect and saved in its process address space,until
                       parent calls wait() systemcall to wait the process,that would destroy all resources.
                       if parent exited before wait its child,then kernel process init will adopts childs,that procedure calls
                       wait() periodically on its child processes.

      Process groups and login sessions - modern Unix operating systems introduce the notion of process groups to represent a 
                                          "job" abstraction.
                                          a job,it might be combined by several processes,and they are in the same process group,
                                          the group leader is the process whose PID is equal to the group ID.
                                          a new process would be inserted into its parent's group initially.
                                          normally,a job is treated as a single entity.
                                          moder Unix kernels also introduce login sessions.
                                          informally,a login session contains all processes that are descendants of the process that
                                          has started a working session on a specific terminal--usually,the first command shell process
                                          created for the user.
                                          all processes in a process group must be in the same login session,a login session may have
                                          several process groups active simultaneously;one of these process groups is always in the
                                          foreground,which means that it has access to the terminal.others are in the background.

    Memory Management :
      Virtual memory -  all recent Unix systems provide a useful abstraction called virtual memory.
                        virtual memory acts as a logical layer between the application memory requests and the hardware
            Memory Management Unit.(MMU)
            its puposes and advantages :
              several processes can be executed concurrently.
              it is possible to run applications whose memory needs are larget than the available physical memory.
              processes can execute a program whose code is only partially loaded in memory.
              each process is allowed to access a subset of the available physical memory.
              processes can share a single memory image of a library or program.
              programs can be relocatable--that is,they can be placed anywhere in physical memory.
              programmers can write machine-independent code,because they do not need to be concerned about physical
              memory organization.
            the main ingredient of a virtual memory subsystem is the notion of virtual address space.

      Random access memory usage - all Unix operating systems clearly distinguish between two portions of the
                                   random access memory(RAM).a few megabytes are dedicated to storing the kernel
                                   image.the remaining portion of RAM is usually handled by the virtual memory
                                   system and is used in three possible ways :
                                     to satisfy kernel requests for buffers,descriptors,and other dynamic kernel data structures.
                                     to satisfy process requests for generic memory areas and for memory mapping of files.
                                     to get better performance from disks and other buffered devices by means of caches.

      Kernel Memory Allocator - the kernel Memory Allocator(KMA) is a subsystem that tries to satisfy the requests fof memory areas
                                from all parts of the system.
                                a good KMA should have the following features :
                                  it must be fast.actually,this is the most crucial attribute,because it is invoked by all kernel
                                  subsystems.
                                  it should minimize the amount of wasted memory.
                                  it should try to reduce the memory fragmentation problem.
                                  it should be able to cooperate with the other memory management subsystems to borrow and release
                                  page frames from them.
                                  all recent Unix operating systems adopt a memory allocation strategy called demand pagine.
                                  with demand paging,a process can start program execution with none of its pages in physical memory.

      Caching - a good part of the available physical memory is used as cache for hard disks and other block devices.this is because
                hard drives are very slow.
                data read previously from disk and no longer used by any process continue to stay in RAM,and defer writing to disk as
                long as possible.
                the sync() system call forces disk synchronization by writing all of the "dirty" buffers into disk.to avoid data loss,
                all operating systems take care to periodically write dirty buffers back to disk.

    Device Drivers :
      the kernel interacts with I/O devices by means of device drivers.device drivers are included in the kernel and consist of
      data structures and functions that control one or more devices.
      each driver interacts with the remaining part of the kernel through a specific interface,this approach has the following
      advantages :
        >  device-specific code can be encapsulated in a specific module.
        >  vendors can add new devices without knowing ther kernel source code,only the interface specifications must be known.
        >  the kernel deals with all devices in a uniform way and accesses them through the same interface.
        >  it is possible to write a device driver as a module that can be dynamically loaded in the kernel without requiring
           the system to be rebooted.it is also possible to dynamically unload a module that is no longer needed,therefore
           minimizing the size of the kernel image strored in RAM.


/*  END OF CHAPTER1  */


Chapter 2 : Memory Addressing
    Memory Address :
      the three kinds of addresses on 80x86 microprocessors >
        1>  logical address
            included in the machine language instructions to specify the address of an operand or of an instruction.
            this type of address embodies the well-known 80x86 segmented architecture.
            each logical address consists of a segment and an offset that denotes the distance from the start of
            the segment to the actual address.

        2>  linear address(also known as virtual address)
            a signle 32-bit unsigned integer that can be used to address up to 4GB.
            linear addresses are usually represented in hexadecimal notation,their values range from
            0x00000000 -- 0xffffffff

        3>  physical address
            used to address memory cells in memory chips.
            they correspond to the electrical signals sent along the address pins of the microprocessor to the memory bus.
            they are represented as 32-bit or 36-bit unsigned integers.

      MMU Memory Management Unit,it transforms a logical address into a linear address by means of a hardware circuit called
      a segmentation unit.
      Page Unit,it transforms the linear address into a physical address.

      In multiprocessor systems,RAM chips may be accessed concurrently(CPUs share same memory).
      Memory Arbiter is inserted between the bus and every RAM chip,it is used to ensure serially operations perform.
      (if the RAM chip is free or it is busy servicing a request by another CPU(in this case,it delay the CPU's request))
      even uniprocessor systems use memory arbiters,because they include specialized processors called DMA(Direct Memory Access)
      controllers that operate concurrently with the CPU.
      (GPU might use DMA zone)

      /*  Memory Arbiter is unvisible to program  */

    Segmentation in Hardware :
      (80386 model)
      Intel microprocessors perform address translation in two different ways called "real mode" and "protected mode".

      Segment selectors and Segmentation registers :
        a logical address consists of two parts : a segment identifier and an offset that specifies the relative address
        within the segment.

        Logical Address : (Segment Identifier , Offset)
        segment identifier : a 16-bit field called the Segment Selector.
        offset : a 32-bit field associated the segment identifier(Segment Selector).

        Segment Selector { index (15-3), TI (2), RPL (1-0) }
        /*  TI : Table Indicator
         *  RPL : Requestor Privilege Level
         */

        processor provides six segmentation registers for retrieve segment selector quickly :
          cs, ss, ds, es, fs, gs
          /*  the only purpose is to hold Segment Selectors  */
        program could reuse some segmentation registers,but have to store its contents in memory,and then
        restore it later.

        cs : code segment register,which points to a segment containing program instructions.
        ss : stack segment register,which points to a segment containing the current program stack.
        ds : data segment register,which points to a segment containing global and static data.
        es,fs,gs : these are extra segment registers available for far pointer addressing like video
                   memory and such.
            
        the remaining three segmentation registers are general purpose and may refer to arbitrary data segments.

        cs register has another important function :
          it includes a 2-bit field that specifies the Current Privilege Level(CPL) of the CPU.
          value 0 denotes the highest privilege level,value 3 denotes the lowest one.
          (Linux only use 0(kernel mode RING0) and 3(user mode RING3))

          Segment Descriptors : 
            Global Descriptor Table(GDT)
            Local Descriptor Table(LDT)

        each segment is represented by an 8-byte segment descriptor,they are stored either in GDT or LDT.
        program is allowed to have its own LDT,if it have to stores some segments besides those stored in GDT.

        gdtr : gdtr control register,it contains the address and size of the GDT in main memory.
        ldtr : ldtr control register,it contains the address and size of the current LDT in main memory.

    Format of Segment Descriptor : (every 8-byte 64-bit)
      Fields >
        Base :      contains the linear address of the first byte of the segment.
        G :         granularity flags,if it is cleared(0),the segment size is expressed in bytes;otherwise,it is
                    expressed in multiples of 4096 bytes.
        Limit :     holds the offset of the last memory cell in the segment,thus binding the segment length.
        S :         system flag,if it is cleared(0),the segment is a system segment that stores critical data 
                    structures such as the Local Descriptor Table;otherwise it is a normal code or data segment.
        Type :      Characterizes the segment type and its access rights.
        DPL :       Descriptor Privilege Level,used to restrict accesses to the segment.it represents the minimal
                    CPU privilege level requested for accessing the segment.
        P :         Segment-Present flag,it is equal to 0 if the segment is not stored currently in main memory.
                    Linux always sets this flag (bit 47) to 1,because it never swaps out whole segments to disk.
        D or B :    called D or B depending on whether the segment contains code or data.
                    its meaning is slightly different in the two cases,but it is basically set(equal to 1) if the
                    addresses used as segment offsets are 32 bits long,and it is cleared if they are 16 bits long.
        AVL :       may be used by the operating system,but it is ignored by Linux.

      Composing :
        0-15 : LIMIT (0-15)
        16-31 : BASE (0-15)
        32-39 : BASE (16-23)
        40-43 : TYPE
        44 : S
        45-46 : DPL
        47 : P
        48-51 : LIMIT (16-19)
        52 : AVL
        53 : none
        54 : D or B
        55 : G
        56-63 : BASE (24-31)

    The Segment Descriptors widely used in Linux :
      Code Segment Descriptor -
        it may be included either GDT or LDT,the descriptor has the S flag set.
      Data Segment Descriptor - 
        data segment.included either GDT or LDT,has S flag set.
        stack segments are implemented by means of generic data segments.
      Task State Segment Descriptor(TSSD) -
        task state segment,it is a segment used to save the contents of the processor registers;it can appear
        only in the GDT.The corresponding Type field has the value 11 or 9,depending on whether the corresponding
        process is currently executing on a CPU.has no S flag.
      Local Descriptor Table Descriptor(LDTD) -
        a segment containing an LDT;it can appear only in the GDT.
        the corresponding Type field has the value 2.has no S flag.

    Fast Access to Segment Descriptors :
      80x86 processor provides an additional nonprogrammable register for each of the six programmable segmentation registers.
      each nonprogrammable register contains the 8-byte segment descriptor specified by the segment selector contained in the
      corresponding segmentation register.
      every time a segment selector is loaded in a segmentation register,the corresponding segment descriptor is loaded from
      memory into the matching nonprogrammable CPU register.

        segs1 --> cs  &&  segd1 --> unprogrammable(cs)

      CPU only need to access GDT or LDT is the time to change the contents of the segmentation registers!

      segment selector fields :
        index - identifies the segment descriptor entry contained in the GDT or in the LDT.
        TI    - table indicator,specifies whether the segment descriptor is inclued in the GDT(TI = 0) or
                in the LDT(TI = 1).
        RPL   - requestor privilege level,specifies the current privilege level of the CPU when the corresponding
                segment selector is loaded in to the cs register;it also may be used to selectively weaken the
                processor privilege level when accessing data segments.

      segment descriptor index = GDT + segment selector index field * 8 (every segment descriptor occupy 8-byte)

      ! the first entry in GDT always set to 0,this ensures that logical address with a null segment selector will be considered
        invalid,thus causing a processor exception.
      ! maximum of number of segment descriptors in GDT is 8191(2^13 - 1).

    Segmentation Unit :
      segmentation unit handle address translation.
      translate logical address to linear address : (in : read, out : write)
      /*  logical addres : (composed by) segment identifer, offset  */

        if register changed,then in segment register
        else in nonprogrammable register
        ->
        if TI == 1,  in LDT(ldtr)
        else TI == 0,  in GDT(gdtr)
        ->
        in index
        ->
        segment descriptor = index * 8 + GDT
        ->
        in segment descriptor
        ->
        in BASE field
        ->
        linear address = BASE + offset (LIMIT determine the length of the segment)

    Segmentation in Linux :
      in fact,segmentation and paging do same work,linear address to physical space.
      linux prefers paging to segmentation for these reasons :
        >  memory management is simpler when all process use the same segment register values,
           that is,when they share the same set of linear addresses.
        >  one of the design objectives of Linux is portability to a wide range of architectures;
           RISC architectures in particular have limited support for segmentation.
        /*  linux 2.6 use segmentation only when required by the 80x86 architecture.  */

      Linux processes running in User Mode use the same pair of segments to address instructions and data.
        user code segment AND user data segment (Segment Selector : __USER_CS, __USER_DS /* macros */)
      Linux processes running in Kernel Mode use the same pair of segments to address instructions and data.
        kernel code segment AND kernel data segment (Segment Selector : __KERNEL_CS, __KERNEL_DS /* macros */)

      CS means cs register, DS means ds register.

      all processes either in User Mode or in Kernel Mode,may use the same logical address.
      (linear address associated such segment start at 0,and reach the addressing limit of (2^32 - 1))

      Linux,logical address coincide with linear address.(so all segments start at 0x00000000)

      about RPL :
        when CPL of cs is changed,ds and ss have to correspondingly updated.
        e.g.
          CPL = 3, ds -> user data segment, ss -> user stack inside user data segment
          CPL = 0, ds -> kernel data segment, ss -> kernel stack inside kernel data segment

        ! When switching from User Mode to Kernel Mode,Linux always makes sure that the 
          ss register contains the Segment Selector of the kernel data segment.
          
      when saving a pointer to an instruction or to a data structure,the kernel does not need to store the
      Segment Selector component of the logical address,because the ss register contains the current 
      Segment Selector.
      /*  a function,it has a stack frame.and ss contains the Segment Selector used to get Segment Descriptor
       *  of the stack.so a pointer just contains the offset for the Segment Descriptor(in unprogrammable register).
       *  in the case for instruction is same,but use the cs register.
       */

    The Linux GDT :
      uniprocessor : one GDT
      multiprocessor : every processor one GDT

      all GDTs are stored in the "cpu_gdt_table" array,while the addresses and sizes of the GDTs are stored in the 
      "cpu_gdt_descr" array.(2.6.34.1,use struct "desc_struct" to represent segment descriptor)

      each GDT includes 18 segment descriptors and 14 null,unused,or reserved entries.unused entries are inserted
      on purpose so that Segment Descriptors usually accessed together are kept in the same 32-byte line of hardware
      cache.
      the 18 segment descriptors included in each GDT point to the following segments :
        >  four user and kernel code and data segments
        >  a task state segment(TSS),different for each processor in the system.
           the linear address space corresponding to a TSS is a small subset of the linear address space corresponding
           to the kernel data segment.
           TSS stored in the "init_tss" array.
           in particualr :
             BASE -> index in init_tss
             G = 0 ; if LIMIT = 0xeb (because TSS is 236 byte)
             TYPE = 9 || 11
             DPL = 0
        >  a segment including the default Local Descriptor Table(LDT),usually shared by all processes.
        >  three Thread-Local Storage(TLS) segments :
             multithreaded applications could make used of up to TLSs containing data local to each thread.
            syscall set_thread_area() and get_thread_area() create and release a TLS segment for the
            executing process.
        >  three segments related to Advanced Power Management(APM) :
             the BIOS code makes use of segments,so when the Linux APM driver invokes BIOS functions to get
             or set the status of APM devices,it may use custom code and data segments.
        >  five segments related to Plug and Play(PnP) BIOS services.
           Linux PnP driver invokes BIOS functions to detect the resources used by PnP devices.
           (custom data and data segments).
        >  a special TSS used by the kernel to handle "Double fault" exceptions.

      each processor has its own TSS.
      a few entries in the GDT may depend on the process that the CPU is executing(i.e. TLS).
      in some cases a processor may temporarily modify an entry in its copy of the GDT(i.e. invoke APM's BIOS procedure).

    The Linux LDTs :
      most Linux User Mode applications do not make use of a Local Descriptor Table.
      thus,kernel defines a default LDT to be shared by most processes,it is stored in the "default_ldt" array.
      default_ldt includes five entries,and two of them are used by kernel effectively :
        a call gate for iBCS executables;
        a call gate for Solaris/x86 executables;

      /*  Call gate is a mechanism provided by 80x86 to change CPU privilege while invoking a predefined function */

      process may require to set up their own LDT,via syscall modify_ldt().(i.e. Wine)
      any custom LDT created by modify_ldt() also requires its own segment.
      if the CPU is executing a process which has a custom LDT,then the LDT entry in CPU GDT also be changed accordingly.

      /*  User Mode applications also may allocate new segments by means of modify_ldt()  */
      /*  kernel never use these segments and do not keep track of the corresponding Segment Descriptor  */

    Paging in Hardware :
      the paging unit translates linear address into physical ones,it must to check the requested access type against the
      access rights of the linear address.if the memory access is not valid,it generates a Page Fault Exception.

      linear addresses are grouped in fixed-length intervals called page :
        contiguous linear addresses within a page are mapped into contiguous physical addresses.
        in this way,kernel can specify the physical address and the access rights of a page instead
        of those of all the linear addresses included in it.

      page unit thinks of all RAM as partitioned into fixed-length page frames(or physical pages),each page frame contains a page.
      a page frame is a constituent of main memory,and hence it is a storage area.
      
      ! the data structures that map linear to physical addresses are called page tables,they are stored in memory and kernel will
        initializes them before enabling page unit.        

      ! start with 80386,80x86 processors :
          if cr0.PG = 1,enable page;
          if cr0.PG = 0,linear addresses are interpreted as physical addresses;

    regular paging :
      start with 80386,a page handles 4kB memory.

    32bit linear address : Directory(10bit), Table(10bit), Offset(12bit)
                           (most significant) (middle)     (least significant)

    translation of linear address(based on type of translation table) :
      the physical address of the Page Directory in use is stored in a control register named cr3.
      get Page Directory from cr3 ->
      use Directory field to determine the Page Table associated with the linear address in Page Directory ->
      use Table field to determine the Page Frame in the Directory ->
      use Offset field to determine the relative position within the Page Frame

      cr3 { Page Directory address }
      Page Directory {
        ...
        Page Table
        Page Table {    /*  10bit Directory as index  */
          ...
          Page Frame
          Page Frame {      /*  10bit Table as index  */
            address
            address  /*  12bit Offset as index for addresses in a page frame  */
            ...
          }
        }
      }

      physical address = retrievePageFrame(retrievePageTable(cr3, linear.Directory), linear.Table) + Offset

      /*  12bit Offset,each page consists of 4kB of data.  */
      /*  10bit Directory,so Page Directory include 1024 entries;10bit Table,so Page Table include 1024 entries,
       *  so a Page Directory can address up to 1024x1024x4096 = 2^32 memory cells.
       */

    The entries of Page Directories and Page Tables have the same structure,each entry includes the following fields :
      Present flag :
        it = 1,the referred-to page(or Page Table) is contained in main memory;
        it = 0,the referred-to page(or Page Table) is not contained in main memory,the remainder bits may be used by OS for
        its own purpose;
        it = 0 and access physical address via page unit,page unit will stores the linear address in cr2,and generates 
        exception 14 : Page Fault

      Field containing the 20 most significant bits of a page frame physical address :
        a page frame has a 4-kB capacity,so its physical address must be multiple of 4096,so the 12 least significant
        bits are always equal to 0.
        it is Page Directory,the page frame contains a Page Table;
        it it Page Table,the page frame contains a page of data;

      Accessed flag :
        page unit set each time accesses the corresponding page frame.(page unit never reset it,OS have to do this)
        OS may be use it when selecting pages to be swapped out.
        
      Dirty flag :
        Page Table entry only,it is set each time a write operation is performed on the page frame.
        page unit never reset it,OS have to do this.

      Read/Write flag :
        contains the access right of the page or of the Page Table. /* Read/Write OR Read Only */
        
      User/Supervisor flag :
        contains the privilege level required to access the page or Page Table.

      PCD and PWT flags :
        controls the way the page or Page Table is handled by the hardware cache.
        /* PWT -> writethrough PCD -> cache disable */

      Page Size flag :
        Page Directory entry only,if it = 1,then entry refers to a 2MB- or 4MB-long page frame.
                                                                   /*  PAE  */

      Global flag :
        Page Table entry only,this flag was introduced in the Pentium Pro to prevent frequently used pages from being
        flushed from the TLB cache.it works only if the Page Global Enable(PGE) flag of register cr4 is set.
        
      extended paging :
        starting with the pentium model,80x86 microprocessors introduce extended paging.
        extended paging allows page frames to be 4MB.in this case,Page Table is no longer need,thus save memory and 
        preserve TLB entries.
        extended paging : Directory(10bit), Offset(22bit)

    Page Directory entries for extended paging are the same as for normal paging,except that :
      >  the Page Size flag must be set.
      >  Only the 10 most significant bits of the 20-bit physical address field are significant.

    extended paging coexists with regular paging;it is enabled by setting the PSE flag of the cr4 processor register.

    Hardware Protection Scheme :
        only two privilege levels are associated with pages and Page Tables,it is indicated by User/Supervisor field.
        when User/Supervisor == 0,the page can be addressed only when the CPL is less than 3.
        when User/Supervisor == 1,the page can always be addressed.

        segmentation has three types of access rights,but only two types of access rights are associated with pages.
        if Read/Write == 0,the corresponding Page Table or page can only be read;otherwise it can be read and written.
        (Read Page Directory to get Page Table,read Page Table to get page frame)

    An Example of Regular Paging :
        a process is allowed to access linear addresses from 0x20000000 - 0x2003ffff.
        Directory field is 0010000000 for all addresses,
        Table field contain values in 0000000000 - 0000111111 (0 - 63)
        Offset field contain values in 000000000000 - 111111111111

        Directory is 0x80 or 128 decimal,so the 129th entry in Page Directory will be selected,
        the 129th entry contains the physical address of the Page Table of current process.
        Table is 0 - 63,so all the remaining 1024 - 64(960) entries are filled with zeros,so
        only the first Page to 63th Page in Page Table is valid.
        Finally,use Offset to access Page Frame.

        Suppose that the process needs to read the byte at linear address 0x20021406,this address is handled by the paging
        unit as follows :
          1>  use 0x80 to select the 129th entry in Page Directory.
          2>  the Table field 0x21 is used to select entry 0x21 of the Page Table.
          3>  Finally,the Offset field 0x406 is used to select the byte at offset 0x406 in the desired page frame.

        if process accessed a linear address which is outside to its linear address space,because the address is not in its
        address space,thus the address's Present flag is 0,cleared,Page Unit will issues an exception.
        all addresses are not valid for process in its linear address space,Present flag of each address will be cleared.

    The Physical Address Externsion (PAE) Paging Mechanism :
        normal CPU only supports 4GB RAM (from 80386 to the Pentium),which is used 32-bit physical addresses.
        in pratice,due to the linear address space requirements of User Mode processes,the kernel cannot directly address
        more than 1GB of RAM.
        and server needs more than 4GB RAM.
        Intel introduced a mechanism called Physical Address Extension(PAE)<36-bit physical address> on Pentium Pro processor.

        PAE is activated by setting the Physical Address Externsion flag in the cr4 control register.
        if PAE is open,then the Page Size flag in page directory entry will set to 1.

    PAE paging mechanism :
      >:
      the 64 GB of RAM are split into 2^24 distinct page frames,and the physical address field of Page Table entries has
      been expanded from 20 to 24 bits.
      the Page Table entry size has been doubled from 32bits to 64 bits,as a result,a 4-kB PAE Page Table includes 512
      entries instead of 1024.

      >:
      a new level of Page Table called the Page Directory Pointer Table(PDPT) consisting of four 64-bit entries has
      been introduced.

      >:
      the cr3 control register contains a 27-bit Page Directory Pointer Table base address field.because PDPTs are stored
      in the first 4GB of RAM and aligned to a multiple of 32 bytes,27 bits are sufficient to represent the base address
      of such tables.

      >:
      when mapping linear addresses to 4kB pages,the 32 bits of a linear address are interpreted in the following way :
        cr3 Points to a PDPT
        
        bits 31-30
          point to 1 of 4 possible entries in PDPT

        bits 29-21
          point to 1 of 512 possible entries in Page Directory

        bits 20-12
          point to 1 of 512 possible entries in Page Table

        bits 11-0
          Offset of 4-kB page

        linear-address : { PDPT Index(31-30), Page Directory Index(29-21), Page Table Index(20-12), Offset(11-0) }

      >:
      when mapping linear addresses to 2-MB pages(PS flag open),the 32 bits of a linear address are interpreted in
      the following way :
        cr3 Points to a PDPT

        bits 31-30
          point to 1 of 4 possible entries in PDPT

        bits 29-21
          point to 1 of 512 possible entries in Page Directory

        bits 20-0
          Offset of 2-MB page

        linear-address : { PDPT Index(31-30), Page Directory Index(29-21), Offset(20-0) }

        once cr3 is set,it is possible to address up to 4 GB of RAM,if we want to address more RAM,we will have to put 
        a new value in cr3 or change the content of the PDPT.
        PAE only extend physical address,User Mode processes are still address 4 GB linear address space(still 32 bits).
        
      Paging for 64-bit Architectures : 
        Paging levels in some 64-bit architectures >
        Platform name         Page size        address bit      paging levels       linear address splitting
        alpha                 8kB              43               3                   10 + 10 + 10 + 13
        ia64                  4kB              39               3                   9 + 9 + 9 + 12
        ppc64                 4kB              41               3                   10 + 10 + 9 + 12
        sh64                  4kB              41               3                   10 + 10 + 9 + 12
        x86_64                4kB              48               4                   9 + 9 + 9 + 9 + 12

        two levels paging :
          Page Directory  <level 1>
          Page Table      <level 2>

          cr3 get Page Directory address.
          PD[Directory] -> Page Table
          PT[Table] -> Page Frame
          PF[Offset] -> page of data

    Hardware Cache : 
        Hardware cache memories were introduced to reduce the speed mismatch between CPU and RAM.
        they are based on the well-known locality principle,which holds both for programs and data structures.
        it makes sense to introduce a smaller and faster memory that contains the most recently used code and data.
        for this purpose,a new unit called the "line" was introduced into the 80x86 architecture.

        DRAM : dynamic RAM
        SRAM : static RAM,it is more faster than DRAM and it is on-chip

        the cache is subdivided into subsets of lines.
        there are some different strategy to determine how to store cache :
          1>  the cache can be direct mapped,in which case a line in main memory is always stored at the exact
              same location in the cache.
          2>  the cache is fully associative,meaning that any line in memory can be stored at any location in the cache.
          3>  degree N-way set associative,where any line of main memory can be stored in any one of N lines of the cache.

                          DRAM Main Memory
                               |
                               v
        CPU { SRAM cache -> Cache controller <- Paging unit }

        SRAM stores the actual lines of memory.(it is the cache memory)
        Cache controller stores an array of entries,every entry is the line of the cache memory.
        Each entry includes a tag and a few flags that describe the status of the cache line,the tag consists of some
        bits that allow the cache controller to recognize the memory location currently mapped by the line.

        The bits of the memory's physical address : { TAG, SUBSET INDEX, OFFSET }

        when accessing a RAM memory cell,the CPU extracts the subset index from the physical address and compares the tags
        of all lines in the subset with the high-order bits of the physical address.
        if a line with the same tag as the high-order bits of the address is found,the CPU has a cache hit;otherwise,it
        has a cache miss.

          subset_index := SUBSET_INDEX(physical_address)
          tag := TAG(physical_address)

          hardware_cache_subset := get_subset(subset_index)
          line := try_hit_cache_line(hardware_cache_subset, tag)

          if is_hit(line)
                  if is_expired(line)
                          update_cache_line(&line, physical_address, hardware_cache_subset)
                  return get_cache(line, OFFSET(physical_address))

          line := make_cache_line(fetch_from_RAM(physical_address))
          new_cache_entry := make_cache_entry(tag, hardware_cache_subset, line)

          insert_hardware_cache_entry(new_cache_entry, hardware_cache_subset)
          map_cache_line(line)
          

        when a cache hit occurs,the cache controller behaves differently,depending on the access type :
          READ >
            controller selects the data from the cache line and transfers it into a CPU register.

          WRITE >
            write-through :
              write data into both cache line and mapped RAM.

            write-back :
              just write data into cache line,the controller updates RAM only when the CPU executes an instruction
              requiring a flush of cache entries or then a FLUSH hardware signal occurs.

        when a cache miss occurs,the cache line is written to memory,if necessary,and the correct line is fetched from
        RAM into the cache entry.

        Multiprocessor system :
          there must have an additional hardware circuitry to synchronize the cache contents.
          whenever a CPU modifies its hardware cache,it must check whether the same data is contained in the other
          hardware cache;if so,it must notify the other CPU to update it with the proper value.(cache snooping)

          new model have more cache,L1-cache,L2-cache,L3-cache,etc.
          linux ignore hardware details,and assumes there is a single cache.

          the CD flag of the cr0 control register is used to enable or diable the cache circuitry.
          the NW flag,in the same register,specifies whether the write-through or the write-back strategy is used for
          the caches.

          some processors allow OS associate a different cache management policy with each page frame,that is PCD flag
          in Page Directory and Page Table;and PWT(Page Write-Through),which specifies whether the write-back or the 
          write-through strategy must be applied while writing data into the page frame.
          (Linux default clear these flags)

   Translation Lookaside Buffers(TLB) :
     80x86 processors include another cache called Translation Lookaside Buffers(TLB) to speed up linear address
     translation.
     when a linear address is used for the first time,the corresponding physical address is computed through slow
     accesses to the Page Tables in RAM.the physical address is then stored in a TLB entry for further accessing.
        
     in a multiprocessor system,each CPU has its own TLB,called the local TLB of the CPU.contrary to the hardware
     cache,these local TLB need not to be synchronized,because processes running on the existing CPUs may associate
     the same linear address with different physical ones.

     when the cr3 control register of a CPU is modified,the hardware automatically invalidates all entries of the 
     local TLB,because a new set of page tables is in use and the TLBs are pointing to old data.

   Paging in Linux :
     before 2.6.11,linux paging model has three level,
     starting with 2.6.11,linux paging model has four level.
     (linux adopts a common paging model that fits both 32-bit and 64-bit architectures.)

     paging level :
       Page Global Directory
       Page Upper Directory
       Page Middle Directory
       Page Table

       PGD { PUDs }
       PUD { PMDs }
       PMD { PTs }
       PT -> Page Frame (a page frame)
    
       cr3 -> Address of PGD

     on 32-bit architecture(no PAE),PUD and PMD fields will be zero.(code still same,so it is work on 64-bit)
     but kernel keeps a position for the PUD and PMD by setting the number of entries in them to 1 and mapping
     these two entries into the proper entry of the PGD.

     on 32-bit architecture(PAE),PUD is eliminated.
     PGD -> 80x86's Page Directory Pointer Table
     PMD -> 80x86's Page Directory
     PT  -> 80x86's Page Table

     on 64-bit architecture,three or four levels of paging are used depending on the linear address bit splitting
     performed by the hardware.

     Linux's handling of processes relies heavily on paging :
       linear address to physical address,design >
         < assign a different physical address space to each process,ensuring an efficient protection against
           addressing errors.
         < distinguish pages(groups of data) from page frames(physical ddress in main memory),this allows the same
           page to be stored in a page frame,then saved to disk and later reloaded in a different page frame,
           this is "the basic ingredient" of the virtual memory mechanism.

     The Linear Address Fields :
       the macros simplify Page Table handling >
         PAGE_SHIFT
           specifies the length in bits of the Offset field; /* usually is defined to 12 */
           this macro is used by PAGE_SIZE to return the size of the page,finally,
           the PAGE_MASK macro yields the value 0xfffff000 and is used to mask all the bits of the Offset field.
           /**
            * PAGE_SIZE := 1UL << PAGE_SHIFT => 00000000000000000001000000000000 -> 2^12 = 4096
            * PAGE_MASK := ~(PAGE_SIZE - 1)  => PAGE_SIZE - 1 => 00000000000000000000111111111111 -> 4095
            *                                   ~(PAGE_SIZE - 1) => 11111111111111111111000000000000
            *                                                       |--------||--------||----------|
            *                                                       |         |         |
            *                                                       |         |         +--> Offset 12 bit
            *                                                       |         +--> Table 10 bit
            *                                                       +--> Directory 10 bit
            */

         PMD_SHIFT
           the total length in bits of the Offset and Table fields of a linear address;
           the logarithm of the size of the area a Page Middle Directory entry can map.
           the PMD_MASK macro is used to mask all the bits of the Offset and Table fields.
           PAE -> off
             PMD_SHIFT = 22 (12 Offset + 10 Table)
             PMD_SIZE = 2^22 (4MB)
             PMD_MASK = 0xffc00000

           PAE -> on
             PMD_SHIFT = 21 (12 Offset + 9 Table)
             PMD_SIZE = 2^21 (2MB)
             PMD_MASK = 0xffe00000
   
           !  large pages do not make use of the last level of page tables,thus LARGE_PAGE_SIZE which yields
              the size of a large page,is equal to PMD_SIZE(2PMD_SHIFT)
              LARGE_PAGE_MASK is used to mask all the bits of the Offset and Table fields in a large page address,
              is equal to PMD_MASK.
   
         PUD_SHIFT
           determines the logarithm of the size of the area a Page Upper Directory entry can map.
           PUD_SIZE macro computes the size of the area mapped by a single entry of the Page Global Directory,
           PUD_MASK macro is used to mask all the bits of the Offset,Table,Middle Air,and Upper Air fields.
   
         PGDIR_SHIFT
           determines the logarithm of the size of the area that a Page Global Directory entry can map.
           PGDIR_SIZE macro computes the size of the area mapped by a single entry of the Page Global Directory,
           the PGDIR_MASK macro is used to mask all the bits of the Offset,Table,Middle Air,and Upper Air fields.
           PAE -> off
             PGDIR_SHIFT = 22 (the same value yielded by PMD_SHIFT and by PUD_SHIFT)
             PGDIR_SIZE = 2^22 (4MB)
             PGDIR_MASK = 0xffc00000
   
           PAE -> on
             PGDIR_SHIFT = 30 (12 Offset + 9 Table + 9 Middle Air)
             PGDIR_SIZE = 2^30 (1GB)
             PGDIR_MASK = 0xc0000000
   
         PTRS_PER_PTE PTRS_PER_PMD PTRS_PER_PUD PTRS_PER_PGD
           compute the number of entries in the Page Table,Page Middle Directory,Page Upper Directory,Page Global Directory.
           they yield the values 1024, 1, 1, 1024, respectively; (PAE disabled)
           they yield the values 512, 512, 1, 4, respectively; (PAE enabled)

      Page Table Handling :
        pte_t -> Page Table entry
                 # for 2level paging,the union contains
                     pteval_t pte
                     pteval_t pte_low
                 # for 3level paging,the structure contains
                   two members : struct { unsigned long pte_low, pte_high };
                                 pteval_t pte;
                 # for 64 bit,the structure contains only one member :
                     pteval_t pte
                     
        pmd_t -> Page Middle Directory entry
                 # struct pmd_t { pmdval_t pmd; };
        pud_t -> Page Upper Directory entry
                 # struct pud_t { pudval_t pud; };
        pgd_t -> Page Global Directory entry
                 # struct pgd_t { pgdval_t pgd; };
        pgprot_t -> the protection flags associated with a single entry        
                    (64-bit PAE on,32-bit PAE off)
                    # struct pgprot_t { pgprotval_t pgprot; };
        
        ! for 3level paging,*val_t is u64
          for 2level paging,*val_t is unsigned long

        conversion macros :
          pte_t __pte(unsigned int)
          pmd_t __pmd(unsigned int)
          pud_t __pud(unsigned int)
          pgd_t __pgd(unsigned int)
          pgprot_t __pgprot(unsigned int)
    
          unsigned int pte_val(pte_t)
          unsigned int pmd_val(pmd_t)
          unsigned int pud_val(pud_t)
          unsigned int pgd_val(pgd_t)
          /*  reverse casting  */
    
        macros for RW a page table entry operations :
          unsigned pte_none(pte_t)
          unsigned pmd_none(pmd_t)
          unsigned pud_none(pud_t)
          unsigned pgd_none(pgd_t)
          /*  return 0 if @arg == 1,return 1 if @arg == 0  */
    
          pte_clear(mm, addr, ptep)
          pmd_clear(pmd)
          pud_clear(pud)
          pgd_clear(pgd)
          /*  clear an entry of the corresponding page table  */
          /*  forbidding a process to use the linear addresses mapped by the page table entry  */
    
          static inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr, pte_t *ptep);
          /*  clears a Page Table entry and returns the previous value  */
    
          set_pte(ptep, pte)
          set_pmd(pmdp, pmd)
          set_pud(pudp, pud)
          set_pgd(pgdp, pgd)
          /*  write a given value into a page table entry  */
    
          set_pte_atomic(ptep, pte)
          /*  atomic operation  */
          /*  if PAE on,it also ensures that the 64-bit value is written atomically  */

          set_pte_at(mm, addr, ptep, pte) => native_set_pte_at(mm, addr, ptep, pte)
                                               => native_set_pte(ptep, pte)                            
          /* set pte at a specified position - @ptep */
    
          static inline int pte_same(pte_t, pte_t);
          /*  returns 1 if @arg1 == @arg2,specify @arg1.privileges == @arg2.privileges,otherwise retuns 0  */
    
          static inline int pmd_large(pmd_t pte);
          /*  returns 1 if the Page Middle Directory entry refers to a large page(2MB or 4MB),0 otherwise  */
    
          static inline int pmd_bad(pmd_t pmd);
          /*  it is used by functions to check Page Middle Directory entries passed as input parameters.
           *  returns 1 if the entry points to a bad Page Table :
           *    Present flag cleared
           *    Read/Write flag cleared
           *    Accessed or Dirty cleared
           *    0 otherwise.
           */
    
          static inline int pud_bad(pud_t pud);
          static inline int pgd_bad(pgd_t pgd);
          /*  these macros always yield 0.  */
    
          no pte_bad() macro is defined,because no Present,no Read/Write,no Accessed or no Dirty is legal for
          Page Table entry.
    
          pte_present(pte_t)
          /*  returns 1 if Present == 1 or Page Size == 1,0 otherwise  */
    
          Page Size flag in Page Table entries has no meaning for the paging unit of the microprocessor,
          but kernel marks Present = 0 and Page Size = 1 for the pages present in main memory but without read,
          write,or execute privileges.
          any access to such pages will triggers a Page Fault exception because Present is cleared,but kernel
          is able to detect that the Page Fault exception is not due to a missing page by checking the value of
          Page Size!
    
          pmd_present(pmd_t pmd)
          /*  returns 1 if the Present == 1,0 otherwise  */
    
          pud_present(pud_t pud)
          pgd_present(pgd_t pgd)
          /*  same as above  */
    
        query or setting functions for Page Table entry's value of any of the flags :
          Page flag reading :
            pte_user()        /*  User/Supervisor  */
            pte_read()        /*  User/Supervisor,pages on the 80x86 processor cannot be protected against reading  */
            pte_write()       /*  Read/Write  */
            pte_exec()        /*  User/Supervisor,pages on the 80x86 processor cannot be protected against code execution  */
            pte_dirty()       /*  Dirty  */
            pte_young()       /*  Accessed -- _PAGE_ACCESSED */
            pte_file()        /*  Dirty,when the Present is cleared and the Dirty flag is set,the page belongs to a
                               *  non-linear disk file mapping.
                               *  pte_flags(@pte) & _PAGE_FILE
                               *                    ^_PAGE_BIT_DIRTY => 6
                               */
            pte_flags()       /*  retrieve flags of the pte
                               *  native_pte_vla(@pte) & PTE_FLAGS_MASK
                               *  ^pte.pte               ^~PTE_PFN_MASK => PHYSICAL_PAGE_MASK
                               */
    
          Page flag setting :
            mk_pte_huge()               /*  Page Size and Present  */
            pte_wrprotect()             /*  Read/Write */
            pte_rdprotect()             /*  User/Supervisor  */
            pte_exprotect()             /*  User/Supervisor  */
            pte_mkwrite()               /*  Read/Write  */
            pte_mkread()                /*  User/Supervisor  */
            pte_mkexec()                /*  User/Supervisor  */
            pte_mkclean()               /*  Dirty  */
            pte_mkdirty()               /*  Dirty  */
            pte_mkold()                 /*  Accessed  */
            pte_mkyoung()               /*  Accessed  */
            pte_modify(p, v)            /*  Sets all access rights in a Page Table entry @p to a specified value @v  */
            ptep_set_wrprotect()        /*  pointer version  */
            ptep_set_access_flags()     /*  if the Dirty == 1,sets the page's access rights to a specified value and
                                         *  invokes flush_tlb_page()
                                         */
            ptep_mkdirty()              /*  pointer version  */
            ptep_test_and_clear_dirty() /*  like pte_mkclen() but acts on a pointer to a Page Table entry and returns
                                         *  the old value of the flag
                                         */
            ptep_test_and_clear_young() /*  like pte_mkold() but acts on a pointer to a Page Table entry and returns 
                                         *  the old value of the flag
                                         */
    
        macros acting on Page Table entries :
          pgd_index(addr)     
          /*  yields the index of the entry in Page Global Directory that maps the linear address @addr  */
    
          pgd_offset(mm, addr)
          /*  yields the linear address of the entry in a Page Global Directory that corresponds to the 
           *  address @addr,the Page Global Directory is found through a pointer within the memory descriptor
           */
    
          pgd_offset_k(addr)
          /*  yields the linear address of the entry in the master Kernel Page Global Directory that corresponds
           *  to the address @addr
           */
    
          pgd_page(pgd)
          /*  yields the page descriptor address of the page frame containing the Page Upper Directory referred to
           *  by the Page Global Directory entry @pgd,in a two or three-level paging system,this macro is
           *  equivalent to pud_page() applied to the folded Page Upper Directory entry
           */
    
          pud_offset(pgd, addr)
          /*  yields the linear address of the entry in a Page Upper Directory that corresponds to @addr,in a 
           *  two- or three-level paging system,this macro yields @pgd,the address of a Page Global Directory entry
           */
    
          pud_page(pud)
          /*  yields the linear address of the Page Middle Directory referred to by the Page Upper Directory entry @pud,
           *  in a two- or three-level paging system,this macro is equivalent to pmd_page() applied to the foled
           *  Page Middle Directory entry
           */
    
          pmd_index(addr)
          /*  yields the index of the entry in the Page Middle Directory that maps the linear address @addr  */
          
          pmd_offset(pud, addr)
          /*  yields the address of the entry in a Page Middle Directory that corresponds to @addr,
           *  in a two- or three-level paging system,it yields @pud,the address of a Page Global Directory entry
           */
    
          pmd_page(pmd)
          /*  yields the page descriptor address of the Page Table referred to by the Page Middle Directory entry @pmd,
           *  in a two- or three-level paging system,@pmd is actually an entry of a Page Global Directory
           */
    
          mk_pte(p, prot)
          /*  use a page descriptor @p and a group of access rights @prot to builds the corresponding Page Table entry  */
          /*  expanded to : pfn_pte(page_to_pfn(@p), (@prot)) => combine these two parameters to setup @pte of a temporary
           *                                                     pte_t object,and return this temporary object by value
           *                                                     (physical page and memory protection flag in @pte)
           *  page_to_pfn => __page_to_pfn
           *  __page_to_pfn(@page) : defined in <asm-generic/pgtable.h>
           *                         support 3 memory models(flat, discontiguous, sparse)
           *                         all of these models are make use of @page the pointer convert to page frame number
           *  __pfn_to_page(@pfn)  : do the reverse to __page_to_pfn()
           *  pfn_to_page => __pfn_to_page 
           */
    
          pte_index(@addr)
          /*  yields the index of the entry in the Page Table that maps the linear address @addr  */
    
          pte_offset_kernel(dir, addr)
          /*  yields the linear address of the Page Table that corresponds to the linear address @addr mapped by the 
           *  Page Middle Directory @dir,used only on the master kernel page tables
           */
    
          pte_offset_map(dir, addr)
          /*  yields the linear address of the entry in the Page Table that corresponds to the linear address @addr,
           *  if the Page Table is kept in high memory,the kernel establishes a temporary kernel mapping,to be released
           *  by means of pte_unmap.
           *  the macros pte_offset_map_nested() and pte_unmap_nested() are identical,but they use a different temporary
           *  kernel mapping.
           */
    
          pte_page(x)
          /*  returns the page descriptor address of the page referenced by the Page Table entry @x  */
    
          pte_to_pgoff(pte)
          /*  extracts from the content @pte of a Page Table entry the file offset corresponding to a page belonging to
           *  a non-linear file memory mapping
           *  <linux/types.h> #define pgoff_t unsigned long
           *  # type of an index into the page cache
           */
    
          pgoff_to_pte(offset)
          /*  sets up the content of a Page Table entry for a page belonging to a non-linear file memory mapping  */
    
        page allocation functions :
          pgd_alloc(mm)
          /*  allocates a new Page Global Directory;if PAE is on,it also allocates the three children Page Middle Directories
           *  that map the User Mode linear addresses.the argument @mm is ignored on the 80x86 architecture.
           */
    
          pgd_free(pgd)
          /*  releases the Page Global Directory at address @pgd;if PAE is on,it also releases the three Page Middle
           *  Directories that map the User Mode linear addresses
           */
    
          pud_alloc(mm, pgd, addr)
          /*  in a two- or three-level paging system,this function does nothing;it simply returns the linear address of
           *  the Page Global Directory entry pgd
           */
    
          pud_free(x)
          /*  in a two- or three-level paging system,this macro does nothing  */
    
          pmd_alloc(mm, pud, addr)
          /*  defined so generic three-level paging systems can allocate a new Page Middle Directory for the linear address
           *  @addr;if PAE is off,the function simply returns the @pud -- that is,the address of the entry in the
           *  Page Global Directory;if PAE is on,the function returns the linear address of the Page Middle Directory entry
           *  that maps the linear address @addr,the argument cw is ignored
           */
    
          pmd_free(x)
          /*  does nothing,because Page Middle Directories are allocated and deallocated together with their parent
           *  Page Global Directory.
           */
    
          pte_alloc_map(mm, pmd, addr)
          /*  returns the address of the Page Table entry corresponding to @addr,if the Page Middle Directory entry is null,
           *  the function allocates a new Page Table by invoking pte_alloc_one().
           *  if a new Page Table is allocated,the entry corresponding to @addr is initialized and the User/Supervisor is set,
           *  if the Page Table is kept in high memory,the kernel establishes a temporary kernel mapping,to be released by
           *  pte_unmap().
           *  # pte_unmap() is defined in <arch/x86/include/asm/pgtable_32.h>
           *    the macro function is expanded to kunmap_atomic(@pte, __KM_PTE)
           *    macro __KM_PTE is defined in the same file,and it is expanded to
           *      KM_NMI_PTE / KM_IRQ_PTE / KM_PTE0
           *      |            |            |            
           *      |            |            +--> neither NMI nor IRQ
           *      |            +--> in_nmi() is FALSE and in_irq() is TRUE
           *      +--> in_nmi() is TRUE
           */
    
          pte_alloc_kernel(mm, pmd, addr)
          /*  if @pmd associated with the address @addr is null,the function allocates a new Page Table,it then returns the 
           *  linear address of the Page Table entry associated with @addr,used only for master kernel page tables
           */
    
          pte_free(pte)
          /*  releases the Page Table associated with the @pte page descriptor pointer  */
          
          pte_free_kernel(pte)
          /*  equivalent to pte_free(),but used for master kernel page tables  */
    
          clear_page_range(mmu, start, end)
          /*  clears the contents of the page tables of a process from linear address @start to @end by iteratively
           *  releasing its Page Tables and clearing the Page Middle Directory entries
           */
    
          because the Page Table that is supposed to contain it might not exist,in such cases,it is necessary to allocate
          a new page frame,fill it with zeros,and add the entry.
          if PAE on,the kernel uses three-level paging,when the kernel creates a new Page Global Directory,it also allocates
          the four corresponding Page Middle Directories;these are freed only when the parent Page Global Directory is released.
          when two or three-level paging is used,the Page Upper Directory entry is always mapped as a single entry within the
          Page Global Directory.
    
      
      Physical Memory Layout : 
        kernel must build a physical addresses map that specifies which physical address ranges are usable by the kernel and 
        which are unavailable(hardware device I/O shared memory or BIOS data).

      Reserved page frames :
        those falling in the unavailable physical address ranges
        those containing the kernel's code and initialized data structures
        (such page frames can never be dynamically assigned or swapped to disk)

        /*  general rule,linux kernel is installed in RAM starting from the physical address 0x00100000,
         *  from the second megabyte.
         *  the total number of page frames required depends on how the kernel is configured.
         */    

        why kernel loaded starting with the second megabyte?
        :  the PC architecture has several peculiarities that must be taken into account.
           PF0 (BIOS data,system hardware configuration detected during POST(Power-On Self Test)).
           0x000a0000 -- 0x000fffff are usually reserved to BIOS routines and to map the internal memory of ISA graphics
           cards.
           additional page frames within the first megabyte may be reserved by specific computer models.

    in the early stage of the boot sequence,the kernel queries the BIOS and learns the size of the physical memory,
    later,kernel executes the machine_specific_memory_setup() routine,which builds the physical address map.
    (such functions is named default_machine_specific_memory_setup() in arch/x86/kernel/e820.c)

    /*  kernel builds this table on the basis of the BIOS list,if this is available,otherwise the kernel builds the table
     *  following the conservative default setup:
     *    all page frames with numbers from 0x9f(LOWMEMSIZE()) to 0x100(HIGH_MEMORY) are marked as reserved.
     */

    /*  POST stage,BIOS writes information about the system hardware devices into the proper page frames,
     *  and initialization stage,kernel will copies such data into the suitable kernel data structures,
     *  then consider these page frames usable.
     *  BIOS may not provide information for some physical address ranges,in such case,linux kernel assumes
     *  such ranges are not usable.
     */

     after machine_specific_memory_setup(),the function setup_memory() will be invoked,it analyzes the table
     of physical memory regions and initializes a few variables that describe the kernel's physical memory
     layout.
       the vairables :  (thest variables are declared in *.c files under the directory mm/)
         num_physpages      --  page frame number of the highest usable page frame
         totalram_pages     --  total number of usable page frames
         min_low_pfn        --  page frame number of the first usable page frame after the kernel image in RAM
         max_pfn            --  page frame number of the last usable page frame
         max_low_pfn        --  page frame number of the last page frame directly mapped by the kernel(low memory)
         totalhigh_pages    --  total number of page frames not directly mapped by the kernel(high memory)
         highstart_pfn      --  page frame number of the first page frame not directly mapped by the kernel
         highend_pfn        --  page frame number of the last page frame not directly mapped by the kernel

     /*  under x86,function setup_arch()<kernel/setup.c> invokes setup_memory_map()<kernel/e820.c>,this function
      *  will copies the e820 data structure object from &e820 to &e820_saved,and setup_arch() will updates 
      *  some of the variables,another will be updated by the functions in the files under mm/ .
      */
         
     /*  linux kernel prefers to skip the first megabyte of RAM to ensure it can never be loaded into groups of
      *  noncontiguous page frames.
      *  generally,kernel keeps initialized data structures right after kernel code,and the uninitialized data
      *  structures follows it.
      */

    Process Page Tables :
      two parts of the linear address space of a process :
        1>  linear address 0x00000000 -- 0xbfffffff can be addressed either User Mode or Kernel Mode.
        2>  linear address 0xc0000000 -- 0xffffffff can only be addressed in Kernel Mode.

    macro PAGE_OFFSET yields value 0xc0000000,this is the offset in the linear address space of a process
    where the kernel lives.
    (in some cases,the process is running in Kernel Mode maybe need to access the linear address space of 
     User Mode for retrieve or store data.)

    !  the content of the first entries of the Page Global Directory that map linear address lower than
       0xc0000000(768 entries with PAE disabled,3 entries with PAE enabled),depends on the specific process.

    Kernel Page Tables :
      the kernel maintains a set of page tables for its own use,rooted at a so-called master kernel Page Global
      Directory.
      after these page tables were initialized,they will never be directly accessed by any process or kernel thread.
      (the highest entries of the master kernel Page Global Directory are the reference model for the corresponding
      entries of the Page Global Directories of every regular process in the system.)

    kernel initializes these page tables with two-phase activity :
      1>  the kernel creates a limited address space including the kernel's code and data segments,the initial
          Page Tables,and 128KB for some dynamic data structures.
          the minimal address space is just large enough to install the kernel in RAM and to initialize its core
          data structures.
      2>  the kernel takes advantage of all of the existing RAM and sets up the page tables properly.

    (right after kernel image loaded,CPU is stil running in real mode,thus,paging is not enabled)

    Provisional kernel Page Tables :
      A provisional Page Global Directory is initialized statically during kernel compilation,while the provisonal
      Page Tables are initialized by startup_32() assembly function defined in <arch/x86/kernel/head_32.S>.

      the Provisional Page Global Directory is contained in swapper_pg_dir variable,the Provisional Page Tables
      are stored starting from pg0.pg0 is the Page Frame which number is 0,it is the first Page Frame,right after
      the end of the kernel's uninitialized data segments(symbol _end).

      suppose all the limited address space fit in the first 8MB of RAM.there,kernel just required two Page Tables,
      but each Page Table is points to a Page Frame,and Page Frame holds one Page,each Page Frame consisting with
      a Page size is 4KB(1024(entries) * 2(Page Tables) * 4KB(Page Size) = 8MB).(at this time,PAE is off)

      for easily addressed both in real mode and protected mode to the 8MB,kernel must create a mapping from
      both the linera addresses [0x00000000 -- 0x007fffff] and the linera addresses [0xc0000000 -- 0xc07fffff]
      into the physical address [0x00000000 -- 0x007fffff].
        linear [0x00000000 -- 0x007fffff] -> [0x00000000 -- 0x007fffff] physical
        linear [0xc0000000 -- 0xc07fffff] -> [0x00000000 -- 0x007fffff] physical

      kernel create the desired mapping by filling all the swapper_pag_dir entries(1024) with zeroes,except for
      entries 0, 1, 0x300(768), 0x301(769) . 
      entries 0x300 and 0x301 will span all linear addresses between [0xc000000 -- 0xc07fffff].
      /*  head_32.S defined swapper_pg_dir :
       *    ENTRY(swapper_pg_dir)
       *    .fill 1024,4,0    #  .fill repear,size(byte),value
       */

      initialization :
        > the address field of entries 0 and 0x300 is set to the physical address of pg0,while the address field
          of entries 1 and 0x301 is set to the physical address of the page frame following pg0(it is pg1).
        > the Present, Read/Write, User/Supervisor flags are set in all four entries(on -> 1).
        > the Accessed, Dirty, PCD, PWD, and Page Size flags are cleared in all four entries(off -> 0).

      start_32() copies the address of swapper_pg_dir to cr3,and enables paging unit(PG flag of the cr0).
      /*  A part of code  */
        movl     $swapper_pg_dir-0xc0000000,%eax    # 0xc0000000 => PAGE_OFFSET,3 GiB linear address
        movl     $eax,%cr3            #  set he page table pointer
        movl     %cr0,%eax            #  get value of cr0
        orl      $0x80000000,%eax     #  open PG
        movl     %eax,%cr0            #  write back

        #  0xc0000000 is the __PAGE_OFFSET
        #  addresses in [0x00000000, 0xc00000000) is used for User Mode.

    Final Kernel Page Table :
      Final Kernel Page Table when RAM size is less than 896MB >
        the final mapping provided by the kernel page tables must transform linear addresses starting
        from 0xc0000000 into physical addresses starting from 0.

        <arch/x86/include/asm/page.h>
        #define __pa(x)  __phys_addr((unsigned long)(x))
        /*  convert an linear address starting from PAGE_OFFSET to the corresponding physical address  */

        #define __va(x)  ((void *)((unsigned long)(x) + PAGE_OFFSET))
        /*  convert a physical address to the corresponding linear address starting from PAGE_OFFSET  */

        the master kernel Page Global Directory is still stored in swapper_pg_dir,it is initialized by
        the paging_init() which is declared in <arch/x86/include/asm/pgtable_32.h>,and defined in
        <arch/x86/mm/init_32.c> with the prototype "void __init paging_init(void);" .
        it executes these works :
          invoke pagetable_init() to set up the Page Table entries properly;
          writes the physical address of swapper_pg_dir in the cr3;
          if the CPU supports PAE and if the kernel is compiled with PAE support,sets the PAE flag in
          the cr4;
          invokes __flush_tlb_all() to invalidate all TLB entries;

        /*  body of it  */
        void __init paging_init(void)
        {
                pagetable_init();
                __flush_tlb_all();
                kmap_init();
                sparse_init();
                zone_sizes_init();
        }

        /*  this routines also unmaps the page at virtual kernel address 0,so
         *  that we can trap those pesky NULL-reference erros in the kernel.
         */

        the actions performed by pagetable_init() depend on both the amount of RAM  present and on the CPU
        model.(suppose less than 896MB)
        /*  the highest 128MB of linear address are left available for several kinds of mapping,the kernel
         *  address space left for mapping the RAM is thus 1GB - 128MB = 896MB
         */

        the identity mapping of the first megabytes of physical memory(8MB,the supposed size) built by startup_32()
        is required to complete the initialization phase of the kernel,when this mapping is no longer necessary,
        the kernel clears the corresponding page table entries by invoking the zap_low_mappings() .

      Final Kernel Page Table when RAM size is between 896MB and 4096MB >
        in this case,the RAM can not be mapped entirely into the kernel linear address space.
        Linux does a mapping to map a RAM window of 896MB into the kernel linear address space,if a program
        need to address other parts of the existing RAM,some other linear address interval must be mapped to
        the required RAM,this implies changing the value of some page table entries.
        (that is the remaining RAM(4096-896) is left unmapped status and handled by dynamic remapping)

      Final Kernel Page Table when RAM size is more than 4096MB >
        in this case,CPU supports PAE,and kernel is compiled with PAE support.
        even PAE is on,linear address is till 32-bit,but physical address is 36-bit.
        the main difference with the previous case is that,at this time,three-level paging model is used.
        (pgd -> pmd -> pte)
        /*  but kernel still directly maps 896MB RAM window  */

        the setups :
          > kernel initializes the first three entries(pgd_index(PAGE_OFFSET) = 3) in the Page Global Directory
            corresponding to the user linear address space with the address of an empty page(empty_zero_page).
          > kernel sets the fourth entry with the address of a Page Middle Directory(pmd) allocated by
            invoking alloc_bootmem_low_pages().
          > kernel sets the first 448 entries(896MB / 2MB(PAGE_SIZE)) in the Page Middle Directory(PAE on,512 entries)
            are filled with the physical address of the first 896MB of RAM.
            /*  phys_addr = 0x00000000;
             *  for(j = 0; j < PTRS_PER_PMD && phys_addr < max_low_pfn * PAGE_SIZE; ++j) {
             *          ...
             *          phys_addr += PTRS_PER_PTE * PAGE_SIZE;  /*  PAE on,512  */
             *  }
             */
          > kernel copies the fourth Page Global Directory entry into the first entry,so as to mirror the mapping
            of the low physical memory in the first 896MB of the linear address space.

            /*  this mapping is required in order to complete the initialization of SMP  */
            (if these page table entries is no longer neccesary,kernel will call zap_low_mappings() to clears them.)

      
      Fix-Mapped Linear Address :
        the initial part of the fourth gigabyte of kernel linear address maps the physical memory of the system,
        however,at least 128MB of linear addresses are always left available because the kernel uses them to implement
        noncontiguous memory allocation and fix-mapped linear addresses.
        (noncontiguous memory allocation is just a special way to dynamically allocate and release pages of memory)

        basically,a fix-mapped linear address is a constant linear address like 0xffffc000 whose corresponding physical
        address does not have to be the linear address minus 0xc0000000,but rather a physical address set in an arbitrary
        way.thus,each fix-mapped linear address maps one page frame of the physical memory.
        fix-mapped linear address is similiar to the linear address that map the first 896MB conceptually,but it can map
        any physical address,while the mapping established by the linear address in the initial portion of the fourth
        gigabyte is linear.

        each fix-mapped linear address is represented by a small integer index defined in the enum fixed_addresses 
        data structure :
          <arch/x86/include/asm/fixmap.h>
          enum fixed_addresses {
                  FIX_HOLE,
                  FIX_VSYSCALL,
                  FIX_APIC_BASE,
                  FIX_IO_APIC_BASE_0,
                  [...]
                  __end_of_fixed_addresses
          };

          fix-mapped linear addresses are placed at the end of the fourth gigabyte of linear addresses,
          fix_to_virt() function computes the constant linear address starting from the index.
          <arch/x86/include/asm/fixmap.h>
            /*  fix_to_virt - computes the constant linear address starting from the index in fixed_addresses.
             *  @idx : the index.
             *  return - the constant linear address.
             *  #  if idx is not in fixed_addresses,then an error "undefined symbol __this_fixmap_does_not_exist"
             *     will be reported by compiler.
             */
            static __always_inline unsigned long fix_to_virt(const unsigned int idx);

            /*  set_fixmap - associates a fix-mapped address with a physical address.
             *  @idx : the index in fixed_address.
             *  @phys : the physical address have to be associated.
             */
            #define set_fixmap(idx, phys)  __set_fixmap(idx, phys, PAGE_KERNEL)

            /*  set_fixmap_nocache - nocache version.
             *    this function will sets PCD flag of the Page Table entry,thus disabling the hardware cache when
             *    accessing the data in the page frame.
             */
            #define set_fixmap_nocache(idx, phys)  __set_fixmap(idx, phys, PAGE_KERNEL_NOCACHE)

            /*  clear_fixmap - clear fix-mapped,removes the linking between a fix-mapped linear address
             *                 and the physical address.
             *  @idx:          the index of member in fixed_addresses.
             */
            #define clear_fixmap(idx)  __set_fixmap(idx, 0, __pgprot(0))

    
      Handling the Hardware Cache and the TLB :
        Handling the hardware cache >
          macro L1_CACHE_BYTES yields the size of a cache line in bytes.
          <arch/x86/include/asm/cache.h>
            #define L1_CACHE_BYTES  (1 << L1_CACHE_SHIFT)

          to optimize the cahce hit rate,the kernel considers the architecture in making the following decisions :
            1>  the most frequently used fields of a data structure are placed at the low offset within the 
                data structure,so they can be cached in the same line.
            2>  when allocating a large set of data structures,the kernel tries to store each of them in memory
                in such a way that all cache lines are used uniformly.

                #  80x86 microprocessors does cache synchronization automatically,linux need not to care about anymore.
                   the kernel does provide,however,cache flushing interfaces for processors that does not synchronize caches.

        Handling the TLB >
          TLB is used keep records about mapping between linear addresses and physical addresses,
          so processors can not synchronize their own TLB cache automatically.(kernel determines if the mapping is invalid)
    
          the methods linux provides for TLB synchronization :
            <arch/x86/include/asm/tlbflush.h>
              /*  flush_tlb_all - flushes all TLB entries even refer to global pages.
               *  #  typically used when changing the kernel page table entries.
               */
              #define flush_tlb_all()  __flush_tlb_all()
    
              /*  flush_tlb_kernel_range - flushes all TLB entries in a given range of linear addresses,
               *                           even refer to global pages.
               *  @start : range start.
               *  @end   : range end.
               */
              static inline void flush_tlb_kernel_range(unsigned long start, unsigned long end);
    
              /*  flush_tlb - flushes all TLB entries of the non-global pages owned by the current process.
               *              (current mm struct TLBs)
               */
              #define flush_tlb()  __flush_tlb()
    
              /*  flush_tlb_mm - flushes all TLB entries of the non-global pages owned by a given process.
               *  @mm : a pointer points to a mm_struct object.
               *  #  typically used when forking a new process.
               */
              static inline void flush_tlb_mm(struct mm_struct *mm);
    
              /*  flush_tlb_range - flushes the TLB entries corresponding to a linear address interval of a given process.
               *  @vma : the virtual memory area of current process.
               *  @start : range start.
               *  @end : range end.
               *  #  typically releasing a linear address interval of a process.
               */
              static inline void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);
    
              /*  flush_tlb_pgtables - flushes the TLBs of a given contiguous subset of page tables of a given process.  */
              flush_tlb_pgtables
              !  some architecure does not offer such function,i.e. x86.
              #  80x86 architecure nothing has to be done when a page table is unlinked from its parent table.
    
              /*  flush_tlb_page - flushes the TLB of a single Page Table entry of a given process.
               *  @vma : the virtual memory area of the process.
               *  @addr : specified address.
               *  #  typically used processing a Page Fault.
               */
              static inline void flush_tlb_page(struct vm_area_struct *vma, unsigned long addr);
    
              every microprocessor usually offers a far more restricted set of TLB-invalidating assembly language instructions,
              Intel microprocessors offers only two TLB-invalidating techniques :
                >  all Pentium models automatically flush the TLBs relative to non-global pages when a value is loaded into cr3.
                >  in Pentium Pro and later models,the "invlpg" assembly language instruction invalidates a single TLB mapping a
                   given linear address.

      Linux macros that exploit such hardware technique :
        <arch/x86/include/asm/tlbflush.h>
          /*  __flush_tlb - rewrites cr3 register back into itself.  */
          #define __flush_tlb()  __native_flush_tlb()

          /*  __flush_tlb_global - disables global pages by clearing the PGE flag of cr4,
           *                       rewrites cr3 register back into itself,and sets again the PGE flag.
           */
          #define __flush_tlb_global()  __native_flush_tlb_global()

          /*  __flush_tlb_single - executes "invlpg" assembly language instruction with parameter @addr.  */
          #define __flush_tlb_single(addr)  __native_flush_tlb_single(addr)

        #  for SMP : a processor sends a interprocessor interrupt to other to forces synchronizing.

      kernel avoids TLB flushes :
        >  when performing a process switch between two regular processes that use the same set of page tables.
        >  when performing a process switch between a regular process and a kernel thread.

        #  when the kernel assigns a page frame to a User Mode process and stores its physical address into Page Table
           entry,it must flush any local TLB entry that refers to the corresponding linear address,on SMP,synchronization
           have to be done between CPUs.

      Lazy TLB mode : (kernel use this strategy to avoid useless TLB flushing in SMP)
        the basica idea is :
          if several CPUs are using same page tables and a TLB entry must be flushed on all of them,then TLB flushing
          may,in some cases,be delayed on CPUs running kernel threads.

        when some CPUs start running a kernel thread,the kernel sets it into lazy TLB mode.
        each CPU in lazy TLB mode does not flush the corresponding entries;however,the CPU remembers that its current
        process is running on a set of page tables whose TLBs for the User Mode addresses are invalid.
        as soon as the CPU in lazy TLB mode switches to a regular process with a different set of page tables,the
        hardware automatically flushes the TLBs,and the kernel sets the CPU back in non-lazy TLB mode.
        but,if a CPU in lazy TLB mode switches to a regular process that owns the same set of page tables used by the
        previously running kernel thread,then any deferred TLB invalidation must be effectively applied by the kernel.

        data structures associated to lazy TLB mode :
          @cpu_tlbstate variable is a static array of @NR_CPUS structures consisting of an @active_mm filed pointing to the
          memory descriptor of the current process,and a @state flag that can assume only two values : TLBSTATE_OK | TLBSTATE_LAZY
          each memory descriptor includes a @cpu_vm_mask field that stores the indices of the CPUs that should receive
          Interprocessor Interrupts related to TLB flushing.(it is meaningful just for current process is executing)
          (the CPU has relative to the active memory,in default,indices of all CPUs of the system will be stored into
           @cpu_vm_mask,including the CPU is lazy TLB mode)

        if a CPU recevied a Interprocess interrupt related to TLB flushing,kernel have to checks @state field of its
        @cpu_tlbstate element is equal to TLBSTATE_LAZY,in this case,the kernel refuses to invalidate the TLBs and
        removes the CPU index from the @cpu_vm_mask filed of the memory descriptor.
        two consequences :
          >  as long as the CPU remains in lazy TLB mode,it will note receive other Interprocessor Interrupts related to
             TLB flushing.

          >  if the CPU switches to another process that is using the same set of page tables as the kernel thread that
             is being replaced,the kernel invokes __flush_tlb() to invalidate all non-global TLBs of the CPU.


/*  END OF CHAPTER2  */


Chapter 3 : Processes
    processes are often called "tasks" or "threads" in the Linux source code.

    Processes,Lightweight Processes,and Threads :
      a process is an intance of a program in execution!
      kernel's point of view :
        the purpose of a process is to act as an entity to which system resources are allocated.

      multithreaded applications :
        user programs having many relatively independent execution flows sharing a large portion of the application data
        structures.
      kernel's point of view :
        a multithreaded application was just a normal process.

      "Linux uses lightweight processes to offer better support for multithreaded applications."
      Basically,two lightweight processes may share some resources,if they were associated.

      NPTL - Native POSIX Thread Library
      NGPT - Next Generation POSIX Threading Package

      thread groups :
        in Linux,a thread group is basically a set of lightweight processes that implement a multithreaded application
        and act as a whole with regards to some system calls such as getpid(),kill(),and _exit().

    Process Descriptor :
      To manage processes,the kernel must have a clear picture of what each process is doing.
      <linux/sched.h>
        struct task_struct;
        /*  task_struct - linux process descriptor whose fields contain all the information related to a single process.  */
        /*  some important members :
         *    volatile long state;
         *    struct thread_info *thread_info;
         *    struct mm_struct *mm, *active_mm;
         *    struct tty_struct *tty;
         *    struct fs_struct *fs;
         *    struct files_struct *files;
         *    struct signal_struct *signal;
         *    struct completion *vfork_done;
         *
         *    # @mm: memory descriptor owned by the process
         *      @active_mm: memory descriptor used by the process when it is in execution
         *      @vfork_done: completion object for vfork() system call
         *      @fs: process's filesystem info
         */
        !!  the routine task_lock() lock up task_struct.@alloc_lock,task_unlock() does the reverse.
                                                        /* concurrent protection of mm */

    Process State :
      the @state field of the process descriptor describes what is currently happening to the process.
      it consists of an array of flags,each of which describes a possible process state.
      process states :
        TASK_RUNNING
          -  running on CPU or wating to be executed.
        TASK_INTERRUPTIBLE
          -  suspended(sleeping) until some condition becomes true.
        TASK_UNINTERRUPTIBLE
          -  like TASK_INTERRUPTIBLE,except that delivering a signal to the sleeping process leaves its state unchanged.
        TASK_STOPPED
          -  execution has been stopped.
        TASK_TRACED
          -  execution has been stopped by a debugger.
        EXIT_ZOMBIE (@state, @exit_state)
          -  execution has been terminated,but the parent process has not yet issued a wait4() or waitpid() system call to
             return information about the dead process.(status have to be reported)
        EXIT_DEAD   (@state, @exit_state)
          -  the final state : the process is being removed by the system because the parent process has just issued a wait4()
             or waitpid() system call for it.(status had been reported)

      <linux/sched.h>
        /*  set_task_state - set the task's state.
         *  @tsk : a pointer points to the task_struct.
         *  @state_value : state value.
         */
        #define set_task_state(tsk, state_value)  set_mb((tsk)->state, (state_value))

        /*  set_current_state - set currently executing task's state.
         *  @state_value : state value.
         */
        #define set_current_state(state_value)    set_mb(current->state, (state_value))

        /*  set_mb - enable memory barrier before set value,disable it later.  */


    Identifying a Process :
      general rule : each execution context that can be independently scheduled must have its own process descriptor,
                     even lightweight processes.
      Process ID : Unix-like operating systems allow users to identify processes by means of a number called PID.
                  (task_struct.pid)
                  /*  by default,the maximum PID number is 32767(PID_MAX_DEFAULT - 1)[32-bit].
                   *                                       4194303[64-bit]
                   *  for change the maximum PID number dynamically,can write a new value into
                   *  /proc/sys/kernel/pid_max
                   */
                  the kernel uses the pidmap_array bitmap to manages PID numbers,which denotes which are the PIDs currently assigned
                  and which are the free ones.
                  /*  32-bit,a page frame contains 32768 bits,so such structure is stored in a single page.
                   *  64-bit,kernel can adds the additional pages to the bitmap,if the PID number is too large.
                   */

                  one process one Process ID !

      POSIX 1003.1c : all threads of a multithreaded application must have the same PID.
      thread groups : the identifier shared by the threads is the PID of the thread group leader,that is,the PID of the
                      first lightweight process in the group.
                      (task_struct.tgid)[getpid() retrieves the value of this field]

    Process descriptors handling :
      Processes are dynamic entities,Linux packs two different data structures in a single per-process memory area :
        > thread_info
        > the Kernel Mode process stack

      ! about thread_info structure :
          the structure is defined in <arch/x86/include/asm/thread_info.h>
          members :
          struct task_struct *task            # main task
          struct exec_domain *exec_domain     # execution domain
          __u32 flags                         # low level flags
          __u32 status                        # thread synchronous flags
          __u32 cpu                           # current cpu
          int preempt_count                   # preempt counter
          mm_segment_t addr_limit             # address limit
                                              # the boundary of address that this thread is access ok
                                              # include the boundary itself
          struct restart_block restart_block  # restart block
          void __user *sysenter_return        # sysenter return address
          # CONFIG_X86_32
          unsigned long previous_esp          # esp of the previous stack in
                                              # case of nested (IRQ) stacks
          __u8 supervisor_stack[0]
          # end CONFIG_X86_32
          int uaccess_err


          INIT_THREAD_INFO(tsk) - initialize thread info
            @task = &@tsk 
            @exec_domain = &@default_exec_domain
            @preempt_count = INIT_PREEMPT_COUNT
            @addr_limit = KERNEL_DS
            @restart_block = { @fn = do_no_restart_syscall }
            # other members are initialized to _zero_


      the length of this memory area is usually 8kB(2 pages),kernel stores the 8kB memory area in two consecutive page
      frames with the first page frame aligned to a multiple of 2^13.(turn out to be a problem when littel dynamic memory
      is available,but kernel could to be configured that use 4kB memory area to stores the stack and thread_info)

      the thread_info structure resides at the begining of the memory area,and the stack grows downward from the end(the end
      of the memory area) !
      [  thread_info.task -> task_struct ; task_struct.thread_info -> thread_info  ]
      /*  Linux task_struct is defined in <linux/sched.h>,it contains a field named "stack" which type is void* ,
       *  kernel use this field to assocaited task_struct with thread_info object.
       *  macro "#define task_thread_info(task) ((struct thread_info *)(task)->stack)" is used to retrieve the
       *  thread_info object which associated with current process.
       *  @current's thread_info => asm("esp") & ~(THREAD_SIZE - 1)
       */

      /**
       *           +---------+  -->  high address
       *           |         |
       *           | process |
       *           | kernel  |
       *           | STACK   |
       *           |         |
       *           +---------+  -->  esp /* C Compiler prefer to use ebp instead of esp(convention) */
       *           |         |           /* so "push $0x1" => "movb $0x1, -2(%ebp)" */
       *           | process |           /* and the memory unit (%esp) saves caller's %ip when invocation occurred */
       *           | thread  |           /* when process switch happens,kernel checkout to */
       *           | info    |           /* the segment contains the process's kernel stack */
       *           |         |           /* BE AWARE,PROCESS SWITCH IS NOT FUNCTION CALLING! */
       *           +---------+  -->  low address
       */

       /*  <linux/sched.h>
        *    union thread_union {
        *            struct thread_info thread_info;
        *            unsigned long stack[THREAD_SIZE / sizeof(long)];
        *    };  /*  task_struct.stack will points to a thread_union object.  */
        *        /*  <arch/x86/include/asm/page_32_types.h>:  #define THREAD_SIZE (PAGE_SIZE << THREAD_ORDER)  */
        */

       /*  because the thread_info structure is 52B long,the kernel stack can expand up to 8140B  */

      the kernel uses the alloc_thread_info and free_thread_info macros to allocate and release the memory area storing a
      thread_info structure and a kernel stack :
      <kernel/fork.c>
        /*  alloc_thread_info - sets thread_info field in struct task_struct.
         *  @tsk : the process descriptor.
         *  return - NULL or the address of the thread_info object former allocated.
         */
        static inline thread_info *alloc_thread_info(struct task_struct *tsk);

        /*  free_thread_info - release the thread_info object.
         *  @ti : the pointer points to a thread_info structure object.
         */
        static inline void free_thread_info(struct thread_info *ti);

    Identifying the current process :
      the kernel can easily obtain the address of the thread_info structure of the process currently running on a 
      CPU from the value of the esp register.
      8kB thread_union :
        the kernel masks out the 13 least significant bits of esp to obtain the base address of the thread_info structure.
      4kB thread_union :
        the kernel masks out the 12 least significant bits of esp to obtain the base address of the thread_info structure.

      <arch/x86/include/asm/thread_info.h>
        /*  current_thread_info - retrieve the address of the thread_info object which is owns to the running process.  */
        static inline struct thread_info *current_thread_info(void)
        {
                return (struct thread_info *)(current_stack_pointer & ~(THREAD_SIZE - 1));
        }

        the kernel can easily obtain the address of the task_struct structure of the process currently running on a CPU from
        the macro "current".
        /*  this is can be simply achieved by thread_info.task  */
        <arch/x86/include/asm/current.h>
          /*  get_current - retrieve the address of a task_struct object which is owns to the current running process.  */
          static __always_inline struct task_struct *get_current(void);    
          #define curren get_current()

        advantage of storing the process descriptor with the stack emerges on multiprocessor systems :
          the correct current process for each hardware processor can be derived just by checking the stack.

          /*  earlier versions of Linux did not store the kernel stack and the process descriptor together,instead,they
           *  were forced to introduce a global static variable called current to identify the process descriptor of the
           *  running process.On multiprocessor systems,it was necessary to define current as an array -- one element for
           *  each available CPU.
           *  #  Linux 2.6 introduced percpu data,macro "current" will retrieves the percpu data for running task.
           */

    Doubly linked lists :
      kernel data structure.
      <linux/list.h>
        struct list_head {
                struct list_head *next, *prev;
        };    
        some methods :
      <linux/list.h>
        list_add(n, p)        inserts @n to @p's next.
        list_add_tail(n, p)   inserts @n to the tail of the list represented by @p.
        list_del(p)           deletes an element pointed to by @p.
        list_empty(p)         checks if the list specified by the address @p of its head is empty.
        list_entry(p, t, m)   returns the address of the data structure of type @t in which the list_head field
                              that has the name @m and the address @p is included.
        list_for_each(p, h)   scans the elements of the list specified by the address @h of the head;in each
                              iteration,a pointer to the list_head structure of the list element is returned in @p.
        list_for_each_entry(p, h, m)
                              similar to list_for_each,but returns the address of the data structure embedding the list_head
                              structure rather than the address of the list_head structure itself.
                              /**
                               * p = list_entry(h->next, typeof(*p), m)
                               */

        usage :
          struct kobject {
                  ...
                  struct list_head list;
                  ...
          };

          struct kobject kobj;
          INIT_LIST_HEAD(&kobj.list);

          struct kobject *new = kmalloc(sizeof(struct kobject), GFP_KERNEL);
          if (new) {
                  list_add(&new->list, &kobj.list);
          }

          ...
          
          list_for_each_entry(new, &kobj.list, list) {
                  printk(KERN_DEBUG "%p", new);
                  ...
          }

          ...

          struct kobject *temp = NULL;
          list_for_each_entry_safe(new, temp, &kobj.list, list) {
                  list_del(&new->list);
              kfree(new);
          }

          ...


      another doubly linked list : hlist.
        <linux/list.h>
          struct hlist_node {
                  struct hlist_node *next, **prev;
          };
          struct hlist_head {
                  struct hlist_node *first;
          };

          methods are similar to list_head's all defined in <linux/list.h>

        <linux/klist.h>
          /* some generic list helpers,extending struct list_head a bit */

          /**
           * klist_node - node of klist,usually embedded in a container
           * @n_klist:    the klist where this node on
           * @n_node:     doubly linked list for adjacent klist_node objects
           * @n_ref:      refcounter of this node
           */
          struct klist_node {
                  void *n_klist;
                  struct list_head n_node;
                  struct kref n_ref;
          };

          /**
           * klist - klist object
           * @k_lock:    protection
           * @k_list:    list implementation
           * @get:       routine used to get the container which includes
           *             current klist_node(get refcount)
           * @put:       routine used to put the container which includes
           *             current klist_node(put refcount)
           */
          struct klist {
                  spinlock_t k_lock;
                  struct list_head k_list;
                  void (*get)(struct klist_node *);
                  void (*put)(struct klist_node *);
          } __attribute__ ((aligned(4)));

          /**
           * klist_iter - klist iterator
           * @i_klist:    the klist
           * @i_cur:      current klist node
           */
          struct klist_iter {
                  struct klist *i_klist;
                  struct klist_node *i_cur;
          };

          some methods(not complete) :
            #define KLIST_INIT(_name, _get, _put)
            #define DEFINE_KLIST(_name, _get _put) \
                    struct klist _name = KLIST_INIT(_name, _get, _put)

            void klist_init(struct klist *k, void (*get)(struct klist_node *),
                            void (*put)(struct klist_node *));

            void klist_add_tail(struct klist_node *n, struct klist *k);
            ...

            /* just put reference counter @n_ref,do not the real unlink */
            void klist_del(struct klist_node *n);
            /**
             * real unlink, may sleep - TASK_UNINTERRUPTIBLE
             * the method @release of kref structure is set to klist_release().
             * if the kref of the node @n becomes _zero_,then kref_put() should be
             * called by kernel control path,then klist_release() will be called to
             * delete @n and wake up the process called klist_remove() waiting for
             * deleting
             */
            void klist_remove(struct klist_node *n);

            /* is @n attached to a klist object? */
            int klist_node_attached(struct klist_node *n);
            
            void klist_iter_init(struct klist *k, struct klist_iter *i);
            /* position specified version */
            void klist_iter_init_node(struct klist *k, struct klist_iter *i,
                                      struct klist_node *n);
            void klist_iter_exit(struct klist_iter *i);
            struct klist_node *klist_next(struct klist_iter *i);

          klist is widely used in Linux Device Driver Model.


    The process list :
      a list that links together all existing process descriptors.(task_struct.tasks)
      the head of the process list is the init_task task_struct descriptor.

      useful macros :
        SET_LINKS and REMOVE_LINKS macros are used to insert and to remove a process descriptor in the process 
        list.(but did not find such macros were defined in Linux 2.6)

      <linux/sched.h>
        #define for_each_process(p)  \
                for (p = &init_task; (p = next_task(p)) != &init_task)
        /*  @p must be the pointer which is type of task_strcut  */

      The lists of TASK_RUNNING processes :
        only the TASK_RUNNING process is could to be runned on a CPU.

      #  earlier version :
           all TASK_RUNNING processes are putting into runqueue,and scheduler have to scans the whole list in order
           to select the "best" runnable process.(too costly to maintain the list ordered according to process priorities)
    
      Linux 2.6 :
        the aim is to allow the scheduler to select the best runnable process in constant time,independently of the number
        of runnable processes.

      principle :
        split the runqueue in many lists of runnable processes,one list per process priority.
        each task_struct has a field run_list is type of list_head.if the process priority is equal to k,the
        run_list field links the process descriptor into the list of runnable processes having priority k.
        on SMP,each CPU has its own runqueue.

        #  to make scheduler operations more efficient,the runqueue list has been split into 140(0 -- 139) different lists.
           kernel must preserve a lot of data for every runqueue in the system,however,the main data structures of a runqueue
           are the lists of process descriptors belonging to the runqueue;all these lists are implemented by a single 
           prio_array_t data structure.(Linux 2.6 no such data structure)
           /*
            *  prio_array_t {
            *          int nr_active;                /*  the number of process descriptors linked into the lists  */
            *          unsigned long bitmap[5];      /*  priority bitmap:each flag is set if and only if the correspoding
            *                                            priority list is not empty.  */
            *          struct list_head queue[140];  /*  the 140 heads of the priority lists  */
            *  };
            */

      task_struct.prio stored the dynamic priority of the process.
      task_struct.array is a pointer to the prio_array_t data structure of its current runqueue.(Linux 2.6.34.1 no such field)

      <linux/sched.h>
        void (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup, bool head);
        void (*dequeue_task) (struct rq *rq, struct task_struct *p, int sleep);

        /*  struct sched_class members.
         *  enqueue_task - inserts @p to @rq,called when a task enters a runnable state.
         *  dequeue_task - removes @p from @rq,called when a task is no longer runnable.
         */

    Relationships Among Processes :
      processes created by a program have a parent/child relationship.
      and the children created by a program have sibling relationships.

      task_struct.real_parent; /*  type task_struct*  */
      task_struct.parent;      /*  type task_struct*  */
      task_struct.children;    /*  type list_head     */
      task_struct.sibling;     /*  type list_head     */

      real_parent : the process which has been created @this,or @init process.
      parent : current parent,this is the process that must be signaled when the child process terminates.
               as usual,real_parent == parent,but it may occasionally differ,such as when another process issues a
               ptrace() system call requesting that it be allowed to monitor @this process.
      children : @this process's children.
      sibling : @this process's siblings.

      task_struct.group_leader;  /*  type task_struct,thread group leader  */

      #  a process can be a leader of a process group or of a login session,it can be a leader of a thread group,and
         it can also trace the execution of other processes.

      (task_struct.signal)->leader_pid;   /*  PID of the group leader in the thread group  */
      task_struct.pid                     /*  Process ID  */
      task_struct.tgid;                   /*  PID of the thread group leader  */
      tasK_struct.sessionid;              /*  ID of session associated now  */
      task_struct.ptrace_children;        /*  the head of a list containing all children being traced by a debugger  */
                                          /*  Linux 2.6 no such field  */
      task_struct.ptraced;                /*  a list of tasks this task is using ptrace on  */
                                          /*  Linux 2.6 no ptrace_list field  */

    The pidhash table and chained lists :
      kernel must be able to derive the process descriptor pointer corresponding to a PID.
      scanning the process list sequentially and checking the @pid fields of the process descriptors is feasible but rather
      inefficient.
      four hash tables are used to speed up such operation :
        /*  the reason for multiple hash tables :
         *    the process descriptor includes fields that represent different types of PID.
         *    each type of PID requires its own hash table.
         */

        task_struct.pids[PIDTYPE_MAX];    /*  type pid_link  */

        [type]         [field]  [introduce]
        PIDTYPE_PID    pid      PID of the process
        PIDTYPE_TGID   tgid     PID of thread group leader process  /*  Linux 2.6 no such PIDTYPE,but has this field  */
        PIDTYPE_PGID   pgrp     PID of the group leader process    
        PIDTYPE_SID    session  PID of the session leader process    
        /*  Linux 2.6 does not exist @pgrp and @session fields in task_struct,there is a field named 'pids' which type
         *  is array of struct pid_link and represents the four type pid hashlist.
         */
        /*  Linux 2.6,for retrieve group leader's PID,use function task_pgrp(struct task_struct *) ,
         *  for retrieve session leader's PID,use function task_session(struct task_struct *) .
         */

        <linux/pid.h>
          enum pid_type {
                PIDTYPE_PID,
                PIDTYPE_PGID,
                PIDTYPE_SID,
                PIDTYPE_MAX
          };  /*  Linux 2.6 only has three types of pid.  */

        task_struct.pid;  /*  this field has type pid_t,such type from typedef __kernel_pid_t,
                           *  and __kernel_pid_t is type of int.
                           */
        task_struct.pids; /*  an array of pid_link.
                           *  struct pid_link {
                           *          struct hlist_node node;
                           *          struct pid *pid;
                           *  };
                           */

        these four hash tables are dynamically allocated during the kernel initialization phase,the size of 
        a single hash table depends on the amount of available RAM.

      Linux 2.6 use find_pid_ns() and find_vpid() to get the pid object.
      <linux/pid.h>
        /*  find_pid_ns - find the pid in a specified pid_namespace.
         *  @nr : pid value.
         *  @second-arg : a pointer points to the pid_namespace which is used to find the pid.
         *  return - struct pid *,or NULL.
         */
        extern struct pid *find_pid_ns(int nr, struct pid_namespace *);

        /*  find_vpid - find the pid in current pid_namespace.  */
        extern struct pid *find_vpid(int nr);

        #define pid_hashfn(x)  hash_long((unsigned long)x, pidhash_shift)
        /*  Linux 2.6 : defined in <kernel/pid.c>
         *  #define pid_hashfn(nr, ns)  hash_long((unsigned long)nr + (unsigned long)ns, pidhash_shift)
         *  this macro is used to transform PID into a table index.
         *  @pidhash_shift stores the length in bits of a table index.
         */

        /*  hash_long() based on a multiplication of the index by a suitable large number.
         *  the magic constant is 0x9e370001(2654404609).
         *  unsigned long hash = val * 0x9e370001;
         *  0x9e370001 is a prime near to (2^32) * ((square_root(5) - 1) / 2) that can also
         *  easily multiplied by additions and bit shifts,because it is equal to 2^31 + 2^29 - 2^25 + 2^22 - 2^19 - 2^16 + 1
         */

      Linux uses chaining to handle colliding PIDs;each table entry is the head of a doubly linked list of colliding
      process descriptors.(as usual,the number of processes in the system is far below 32768)

      the data structures used in the PID hash tables are quite sophisticated,because they must keep track of the 
      relationships between the processes.
        >  if kernel wants to retrieve all processes in a specified thread group,it must finds out all the processes
           each tgid field == @tgid_value.
           but use @tgid_value to find a process just returns one process descritpor,the thread group leader.
           so kernel have to maintains a list of processes for each thread group!

           the fields of the pid structure :
             int nr;  /*  PID number  */
             struct hlist_node pid_chain;    /*  hash chain list  */
             struct list_head pid_list;    /*  the head of the per-PID list  */

           Linux 2.6 use another definition of pid structure :
             <linux/pid.h>
               struct upid {
                       int nr;
                       struct pid_namespace *ns;
                       struct hlist_node pid_chain;
               };  /*  this structure represented the Identifier for the pid structure.  */
               struct pid {
                       atomic_t count;
                       unsigned int level;
                       struct hlist_head tasks[PIDTYPE_MAX];  /*  tasks that use this pid  */
                       struct rcu_head rcu;
                       struct upid numbers[1];
               };

      pid handling functions :
        <linux/pid.h>
          /*  do_each_pid_task - do-while loop head.  */
          #define do_each_pid_task(pid, type, task)

          /*  @pid : the pid structure pointer.
           *  @type : pid type.
           *  @task : task_struct pointer used to iterate all tasks in the same pid.
           */

          #define while_each_pid_task(pid, type, task)
          /*  while_each_pid_task - do-while loop end.  */

          /*  Linux 2.6 no such functions  */
          #define find_task_by_pid_type(type, nr)
          #define find_task_by_pid(nr)

          /*  Linux 2.6 has these :
           *    <linux/sched.h>
           *      extern struct task_struct *find_task_by_vpid(pid_t nr);
           *      extern struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns);
           *      /*  use pid value or pid value with namespace to find the corresponding task.  */
           */

          /*  attach_pid - attach task with pid.
           *  @task : the task pointer.
           *  @type : pid type.
           *  @pid : the pid attach to.
           */
          extern void attach_pid(struct task_struct *task, enum pid_type type, struct pid *pid);

          /*  detach_pid - detach task with pid.
           *  @task : the task pointer.
           *  @second-arg : the pid type.
           *  #  @task will attach to NULL pid structure.
           *     and if pid_link[PIDTYPE].pid->tasks[ALL] is empty,then free_pid(pid).
           */
          extern void detach_pid(struct task_struct *task, enum pid_type);

        <linux/sched.h>
          /*  next_thread - retrieve the next thread.
           *  @p : the task in the specified thread group.
           *  return - next task pointer in the thread group or @p.
           */
          static inline struct task_struct *next_thread(const struct task_struct *p);

    How Processes Are Organized :
      runqueue lists group all processes in a TASK_RUNNING state.

      processes in a TASK_STOPPED,EXIT_ZOMBIE,EXIT_DEAD state are not linked in specific lists.
      there is no need to group processes in any of these three states.

      processes in a TASK_INTERRUPTIBLE,TASK_UNINTERRUPTIBLE state are subdivided into many classes,
      each of which corresponds to a specific event.in this case,the process state does not provide enough
      information to retrieve the process quickly,so it is necessary to introduce additional lists of processes,
      there are called wait queues.

      Wait queues :
        wait queues used in kernel particularly for interrupt handling,process synchronization,timing.

        conditional waits on events :
          a process wishing to wait for a specific event places itself in the proper wait queue and
          relinquishes control.
          therefore,a wait queue represents a set of sleeping processes,which are woken up by the kernel
          when some condition becomes true.

        <linux/wait.h>
          typedef struct __wait_queue wait_queue_t;
          typedef int (*wait_queue_func_t)(wait_queue_t *wait, unsigned mode, int flags, void *key);
          struct __wait_queue {
                  unsigned int flags;
                  #define WQ_FLAG_EXCLUSIVE  0x01
                  void *private;            /*  private data,generally stored task_struct address */
                  wait_queue_func_t func;
                  struct list_head task_list;
          };

          struct __wait_queue_head {
                  spinlock_t lock;
                  struct list_head task_list;
          };
          typedef struct __wait_queue_head wait_queue_head_t;

          wait_queue_head_t {
                                            wait_queue_head_t.@task_list
                                                        |
                                                        V
                  ... -> wait_queue_t.task_list - > task_list -> wait_queue_t.task_list {
                          ... -> wait_queue_t.task_list -> wait_queue_t.task_list -> wait_queue_t.task_list -> ...
                  };
          };

        there are two kinds of sleeping processes :
          exclusive processes - wait_queue_t.flags == 1 are selectively woken up by the kernel
          nonexlusive processes - wait_queue_t.flags == 0 are always woken up by the kernel when the event occurs

          #  two kinds to prevent race for a resource accessing just allow one process on it.
             a process waiting for a resource that can be granted to just one process at a time is a typical
             exclusive process.
             processes waiting for an event that may concern any of them are nonexclusive.

      Handling wait queues :
        <linux/wait.h>
          !  use these two macro to initialize a wait queue head object which is declared statically or dynamically.

          /*  DECLARE_WAIT_QUEUE_HEAD - declare a wait_queue_head_t object statically.  */        
          #define DECLARE_WAIT_QUEUE_HEAD(name)  \
                  wait_queue_head_t name = __WAIT_QUEUE_HEAD_INITIALIZER(name);

          /**
           *  __init_waitqueue_head - initialize a wait_queue_head_t object which is dynamically allocated.
           *  @q:                   waitqueue head
           *  @key:                 lock correctness validator
           *  # this rountine initialize @lock member of wait_queue_head_t with @key,and then
           *    initialize the doubly linked list @task_list member
           */ 
          extern void __init_waitqueue_head(wait_queue_head_t *q, struct lock_class_key *key);

          #define init_waitqueue_head(q)                     \
                  do {                                       \
                      static struct lock_class_key __key;    \
                      __init_waitqueue_head((q), &__key);    \
                  } while (0)


          !  use these two functions to initialize a wait queue entry with @task or with @wake_up_func.

          /*  init_waitqueue_entry - initialize a wait_queue element with task @p.
           *  @q : the target to be initialized.
           *  @p : task pointer.
           *  #  @q->flags = 0;
           *     @q->private = @p;
           *     @q->func = default_wake_function;    /*  for nonexclusive process  */
           */
          static inline void init_waitqueue_entry(wait_queue_t *q, struct task_struct *p);

          /*  init_waitqueue_func_entry - initialize @q with @func.
           *  @q : the wait_queue_t object's address.
           *  @func : the wake up function.
           *  #  @q->flags = 0;
           *     @q->private = NULL;
           *     @q->func = func;
           */
          static inline void init_waitqueue_func_entry(wait_queue_t *q, wait_queue_func_t func);


          !  use these two macros to put "current" into a wait queue and automatically remove it later.

          /*  DEFINE_WAIT_FUNC - declare a wait_queue_t object @name and initialize it with @function,
           *                     this object's private field will be initialized to "current".
           */
          #define DEFINE_WAIT_FUNC(name, function)
          /*  DEFINE_WAIT - put "current" process into wait queue and automatically remove it at the time
           *                it is woken up.
           *  #  autoremove_wake_function() will invokes default_wake_function() at first,then remove this
           *     element from the wait queue.
           */
          #define DEFINE_WAIT(name)  DEFINE_WAIT_FUNC(name, autoremove_wake_function)


          !  use these three functions to complete INSERT | INSERT INTO EXCLUSIVE | REMOVE operations.

          /*  add_wait_queue - insert @wait to @q.
           *  @q : the head.
           *  @wait : the element.
           *  #  for nonexclusive processes.
           */
          extern void add_wait_queue(wait_queue_head_t *q, wait_queue_t *wait);

          /*  add_wait_queue_exclusive - insert @wait to @q.
           *  #  for exclusive processes.
           */
          extern void add_wait_queue_exclusive(wait_queue_head_t *q, wait_queue_t *wait);

          /*  remove_wait_queue - remove @wait from @q.
           *  #  this function does not use the @q parameter,but it as an identifier.
           */
          extern void remove_wait_queue(wait_queue_head_t *q, wait_queue_t *wait);

          !  use this function to check if the wait queue is active.

          /*  waitqueue_active - check if @q is activing(it is not empty) now.  */
          static inline int waitqueue_active(wait_queue_head_t @q);

      Wait bit queue :
        a wait queue used to waiting for a specified bit is set.

        <linux/wait.h>
          /**
           * wait_bit_key - key of wait bit queue
           * @flags:        address of flags that will be tested
           * @bit_nr:       number of bit that will be tesed in @flags
           */
          struct wait_bit_key {
                  void *flags;
                  int bit_nr;
          };

          /**
           * wait_bit_queue - wait queue for wait bit
           * @key:            wait bit queue key
           * @wait:           wait queue
           */
          struct wait_bit_queue {
                  struct wait_bit_key key;
                  wait_queue_t wait;
          };

          ! wait bit usually is handled by __wait_on_bit_lock(),which do checking in
            a do-while cycle,stop condition is test_and_set_bit(@key.bit_nr, @key.flags),
            that is wait until the bit is cleared,but return with it have been set.
            # unlock version is __wait_on_bit(),the stop condition of do-while cycle is
              test_bit(@key.bit_nr, @key.flags) returned FALSE OR @action() returned TRUE

      Process wishing to wait :
        <linux/wait.h>
          /*  sleep_on - let "current" sleep on @q and it will enter TASK_UNINTERRUPTIBLE state.  */
          extern void sleep_on(wait_queue_head_t *q);
          /*  interruptible_sleep_on - "current" will enter TASK_INTERRUPTIBLE state.  */
          extern void interruptible_sleep_on(wait_queue_head_t *q);

          /*  sleep_on_timeout - timer version,@timeout is the maximum time to sleep.
           *                     "current" will enter TASK_UNINTERRUPTIBLE state.
           *                     if "current" is woken up before timer expire,then
           *                     the left time will be returned.
           */
          extern long sleep_on_timeout(wait_queue_head_t *q, signed long timeout);
          /*  interruptible_sleep_on - TASK_INTERRUPTIBLE version.  */
          extern long interruptible_sleep_on_timeout(wait_queue_head_t *q, signed long timeout);

          !  the sleep_on()-like functions cannot be used in the common situation where one has to
             test a condition and atomatically put the process to sleep when the condition is not
             verified.
             they are a well-known source of race conditions,their use is discouraged.
             #  sleep_on()-like functions were defined in <kernel/sched.c>.
                for sleep_on(),it calls to sleep_on_common(),that function sets "current"'s state to
                TASK_UNINTERRUPTIBLE,initializes a wait entry with "current",and insert it into @q,
                calls schedule_timeout() with MAX_SCHEDULE_TIMEOUT left current process sleeping.

          /*  prepare_to_wait - does the prepare works for "current" is going to wait.
           *  @q : the wait queue head.
           *  @wait : the wait entry.
           *  @state : the state "current" will be.
           *  #  this function will set "current"'s state to @state at first,then insert @wait
           *     into @q.
           *  #  this version is defined for nonexclusive process.
           */
          void prepare_to_wait(wait_queue_head_t *q, wait_queue_t *wait, int state);

          /*  prepare_to_wait - exclusive version.  */
          void prepare_to_wait_exclusive(wait_queue_head_t *q, wait_queue_t *wait, int state);

          /*  finish_wait - finish "current" waiting.
           *  @q : the wait queue head.
           *  @wait : the wait entry.
           *  #  this function will removes @wait from @q,and sets "current"'s state to
           *     TASK_RUNNING(this will happens before removing).
           */
          void finish_wait(wait_queue_head_t *q, wait_queue_t *wait);

          Usage for prepare_to_wait_*() and finish_wait() :
            #define __wait_event(wq, condition)                                  \
            do {                                                                 \
                    DEFINE_WAIT(__wait);                                         \
                                                                                 \
                    for (;;) {                                                   \
                            prepare_to_wait(wq, &__wait, TASK_UNINTERRUPTIBLE);  \
                            if (condition)                                       \
                                    break;                                       \
                            schedule();                                          \
                    }                                                            \
                    finish_wait(wq, &__wait);                                    \
            } while (0)
          

          /*  wait_event - "current" waiting for @condition gets TRUE.
           *  @wq : the wait queue head pointer.
           *  @condition : the condition is a C expression.
           *  #  "current" will enter TASK_UNINTERRUPTIBLE state.
           *     @condition will be checked each time @wq is woken up.
           *  #  function wake_up() is used to wake a wait_queue up.
           */          
          #define wait_event(wq, condition)     \
          do {                                  \
                  if (condition)                \
                          break;                \
                  __wait_event(wq, condition);  \
          } while (0)

          /*  wait_event_timeout - timer version.
           *                       if "current" is woken up before timer expired,
           *                       the left time will be returned.
           */
          #define wait_event_timeout(wq, condition, timeout)
          
          /*  wait_event_interruptible - TASK_INTERRUPTIBLE version.
           *  return - interrupted,returns -ERESTARTSYS;
           *           condition got TRUE,returns 0.
           */
          #define wait_event_interruptible(wq, condition)


      Kernel wake up the waiting processes :
        <linux/wait.h>
          /*  __wake_up* - the main procedure to wake up processes.
           *               defined in <kernel/sched.c>
           */        
          void __wake_up(wait_queue_head_t *q, unsigned int mode, int nr, void *key);
          void __wake_up_locked(wait_queue_head_t *q, unsigned int mode);
          void __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr);

          /*  TASK_UNINTERRUPTIBLE processes.  */
          #define wake_up(x)  __wake_up(x, TASK_NORMAL, 1, NULL)
          #define wake_up_nr(x, nr)  __wake_up(x, TASK_NORMAL, nr, NULL)
          #define wake_up_all(x)  __wake_up(x, TASK_NORMAL, 0, NULL)
          #define wake_up_locked(x)  __wake_up_locked((x), TASK_NORMAL)

          /*  TASK_INTERRUPTIBLE processes.  */
          #define wake_up_interruptible(x)  __wake_up(x, TASK_INTERRUPTIBLE, 1, NULL)
          #define wake_up_interruptible_nr(x, nr)  __wake_up(x, TASK_INTERRUPTIBLE, nr, NULL)
          #define wake_up_interruptible_all(x)  __wake_up(x, TASK_INTERRUPTIBLE, 0, NULL)
          #define wake_up_interruptible_sync(x)  __wake_up_sync((x), TASK_INTERRUPTIBLE, 1)

          !  all macros wakeup all nonexclusive processes.
             that is the parameter @nr to __wake_up*() functions is 0,just wake up everything;
            (includes all nonexclusive processes and all exclusive processes)
             if @nr == small + venumber,then wake up all nonexclusive processes and one
             exclusive process.(that is what wake_up() to do)
             e.g. (code)
               if (curr->func(curr, mode, wake_flags, key) &&
                       (flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
                           break;
                                                  /*  negative number is also TRUE  */

          !  TASK_NORMAL is used to wake up TASK_UNINTERRUPTIBLE processes;
             TASK_INTERRUPTIBLE is used to wake up TASK_INTERRUPTIBLE processes.
          !  '_nr' suffix macros are used to wake up exclusive processes with the given @nr.
             @nr => @nr_exclusive.
          !  '_all' suffix macros will wake up all exclusive processes.
          !  '_sync' suffix macro check whether the priority of any of the woken processes is
             higher than that of the processes currently running in the systems and incoke
             schedule() if necessary.(these checks is not made by this macro)
          !  '_locked' suffix macro requires the wait_queue_head_t.lock has been held.
          
      
    Process Resource Limits :
      each process has an associated set of resource limits,which specify the amount of system resources
      it can use.

      <linux/sched.h>
        struct signal_struct {
                ...
                struct rlimit rlim[RLIM_NLIMITS];
                ...
        };

        task_struct.signal->rlim;

      <linux/resource.h>
        struct rlimit {
                unsigned long rlim_cur;  /*  the current resource limit  */
                unsigned long rlim_max;  /*  the maximum resource limit  */
        };

      !  only the superuser(or,more precisely,a user who has the CAP_SYS_RESOURCE capability) can increase the 
         @rlim_max or set the @rlim_cur to a value greater than the corresponding @rlim_max field.

      Kernel use the @index of @rlim member to identify the type of resource-limit.
      Toltal number of resource-limits in Linux 2.6 is 16,they are defined in <include/asm-generic/resource.h>.

      #define RLIMIT_CPU  0            /*  if cpu time exceeds,kernel will send SIGXCPU to the process,  */
        /*  CPU time in sec  */        /*  then,if the process does not terminate,SIGKILL will be sent.  */
      #define RLIMIT_FSIZE  1          /*  if process wish to enlarge filesize greater than this limit,  */
        /*  maximum filesize  */       /*  kernel will send SIGXFSZ  signal.  */
      #define RLIMIT_DATA  2           /*  heap size in bytes  */
        /*  max data size  */
      #define RLIMIT_STACK  3
        /*  max stack size  */
      #define RLIMIT_CORE  4           /*  if limit == 0,kernel does not creates core dump file  */
        /*  max core file size  */

      #ifndef ...
      #define RLIMIT_RSS  5
        /*  max resident set size  */
      #define RLIMIT_NPROC  6
        /*  max number of processes  */
      #define RLIMIT_NOFILE  7
        /*  max number of open files  */
      #define RLIMIT_MEMLOCK  8        /*  size of nonswappable memory in bytes  */
        /*  max locked-in-memory address space  */
      #define RLIMIR_AS  9             /*  kernel checks this limit when malloc() was called  */
        /*  address space limit  */
      #define RLIMIT_LOCKS  10
        /*  maximum file locks held  */
      #define RLIMIT_SIGPENDING  11
        /*  max number of pending signals  */
      #define RLIMIT_MSGQUEUE  12
        /*  maximum bytes in POSIX mqueues  */
      #define RLIMIT_NICE  13
        /*  max nice prio allowed to raise to  0-39 for nice level 19 .. -20  */
      #define RLIMIT_RTPRIO  14
        /*  maximum realtime priority  */
      #define RLIMIT_RTTIME  15
        /*  timeout for RT tasks in us  */
      #endif 

      #define RLIMIT_NLIMITS  16

      /*  resource limit value  */
      #ifndef RLIM_INFINITY
      #define RLIM_INFINITY  (~0UL)
        /*  no user limit is imposed on the corresponding resource  */
      #endif


    Process Switch :
      kernel suspends the executing of the process that is running on the CPU and resume it later.
      kernel suspends a process and pick up another process to be executing.

      these different names all refers to the process switch : process switch, task switch, context switch

      !  Process Switch occurs only in Kernel Mode.

      Hardware Context :
        the set of data that must be loaded into the registers before the process resumes its execution on the
        CPU is called the "hardware context".it is the subset of the process execution context,which includes all
        information needed for the process execution.

        Linux,a part of hardware context is stored in process descriptor,while the remaining part is saved in the
        Kernel Mode stack.

        !  because process switches occur quite often,it is important to minimize the time spent in saving and 
           loading hardware contexts.

        /*  Old versions of Linux took advantage of the hardware support offered by the 80x86 architecture and
         *  performed a process switch through a "far jmp" instruction to the selector of the 
         *  Task State Segment Descriptor of the next process.
         */

        Linux 2.6 uses software to perform a process switch for the following reasons :
          >  Step-by-step switching performed through a sequence of "mov" instructions allows better control
             over the validity of the data being loaded.
             (it is possible to check the values of the ds and es segmentation registers)

          >  the amount of time required by the old approach and the new approach is about the same.
             However,it is not possible to optimize a hardware context switch,while there might be room
             for improving the current switching code.

      Linux stores the contents of all registers used by a process in User Mode into the Kernel Mode
      stack before performing process switching.(includes ss esp .etc)

      Task State Segment :
        80x86 architecture includes a specific segment type called Task State Segment(TSS) to store hardware
        contexts.Linux does not use hardware context switching,but it is still set up TSS for each CPU in the
        system.
        the reasons :
          >  when an 80x86 CPU switches from User Mode to Kernel Mode,it fetches the address of the Kernel Mode
             stack from the TSS.
          >  when a User Mode process attempt to access an I/O port by means of an "in" or "out" instruction,
             the CPU may need to access an I/O Permission Bitmap stored in the TSS to verify whether the process
             is allowed to address the port.

        #  a process executes an "in" or "out" I/O instruction in User Mode :
           the control unit performs the following operations >
             1>  checks the 2-bit IOPL field in the "eflags" register.
                 IOPL == 3, executes I/O instructions,
                 Otherwise, performs the next check.
             2>  accesses the "tr" register to determine the current TSS,and thus the proper
                 I/O Permission Bitmap.
             3>  checks the bit of the I/O Permission Bitmap corresponding to the I/O port
                 specified in the I/O instruction.
                 cleared => instruction is executed,
                 setted  => raises a "General protection" exception.

             # system call iopl() and ioperm() grant a process the privilege to access I/O ports,
               but they can be invoked only by root user.

        the tss_struct structure describes the format of the TSS :
          <arch/x86/include/asm/processor.h>
            struct tss_struct {
                    struct x86_hw_tss x86_tss;
                    /*  extra 1 is there because CPU will access an
                     *  additional byte,and the extra byte must be all 1 bits.
                     */
                     unsigned long io_bitmap[IO_BITMAP_LONGS + 1];
                     unsigned long stack[64];  /*  another 0x100 bytes for emergency kernel stack  */
            } ____cachingline_aligned;

        the init_tss array stores one TSS for each CPU on the system :
          <arch/x86/include/asm/processor.h>
            DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss);
            /*  the invoked macro is defined in <linux/percpu-defs.h>  */
        !  at each process switch,the kernel updates some fields of the TSS so that the corresponding
           CPU's control unit may safely retrieve the information it needs.
           (TSS reflects the privilege of the current process on the CPU,but there is no need to maintain
            TSSs for processes when they're not running.)

        TSS <=> TSSD (Task State Segment Descriptor)
        /*  in the Intel's original design,each process in the system should refer to its own TSS;
         *  the second least significant bit of the Type field is called the Busy bit;
         *  it is set to 1 if the process is being executed by a CPU,and to 0 otherwise.
         *  in Linux design,there is just one TSS for each CPU,so the Busy bit is always set to 1.
         */

        TSSDs stored in GDT,whose base address is stored in the gdtr register of each CPU.
          the gdtr register of each CPU contains the TSSD Selector of the corresponding TSS.
          the register also includes two hidden,nonprogrammable fields :
            the Base field of the TSSD
            the Limit field of the TSSD
      
      The thread field :
        at every process switch,the hardware context of the process being replaced must be saved in
        somewhere,but it can not be stored in TSS,because in Linux Design,one TSS for one CPU.

        /*  thread - task_struct member which type is thread_struct  */
        task_struct.thread;     /*  CPU-specific state of this task  */

        <arch/x86/include/asm/processor.h>
          struct thread_struct {
                  ...
          };
          /*  this structure represents the CPU-specific state for a task,
           *  Linux saves hardware context in such object whenever the process is being switched out.
           *  this structure contains fields for most of the CPU registers,except the general-purpose
           *  registers such as eax,ebx, etc.,which are stored in the Kernel Mode stack.
           */


      Performing the Process Switch :
        A process switch may occur at just one well-defined point :
          the schedule() function.(<include/linux/sched.h>, <kernel/sched.c>)
        
        every process switch consists of two steps :
          1>  switching the Page Global Directory to install a new address space.
          2>  switching the Kernel Mode stack and the hardware context,which provides all the
              information needed by the kernel to execute the new process,including the CPU registers.

        @prev -> the process been replaced.
        @next -> the process being activated.

        The switch_to macro :
          <include/asm-generic/system.h>
            /*  switch_to - switch previous process to next process.
             *  @prev : the previous process,it is often got by "current".
             *  @next : the next process switch to.
             *  @last : it is an output parameter that specifies a memory location
             *          in which the macro writes the descriptor address of process C.
             *          (this is done after A resumes its execution)
             *  #  before the process switching,the macro saves in the eax CPU register
             *     the content of the variable identified by the first input parameter @prev,
             *     after the process switching,when A has resumed its execution,the macro
             *     writes the content of the eax CPU register in the memory location of A
             *     identified by the third output parameter @last.
             *  #  this function will call to the architecture based __switch_to()
             *     function to accomplishes the primary works,that function is
             *     defined in <arch/"model"/asm/process_(32 | 64).h>
             */
            #define switch_to(prev, next, last)                 \
                  do {                                          \
                      ((last) = __switch_to((prev), (next)));   \
              } while (0)

          !  there is another switch_to() function is defined in <arch/x86/include/asm/system.h>,
             which has introduced the detail for how process switching be executed.
             the switch_to() in <asm/system.h> is the architecture based switch_to() function,
             not the asm-generic version.
             for x86,this switch_to() will be called by context_switch() function which is defined
             in <kernel/sched.c>.(because <linux/sched.h> includes <asm/systemd.h>)
             the primary assembly :
               pushfl                 #  save eflags
               pushl %%ebp            #  save base stack pointer
               movl %%esp, %[prev_sp] #  save  stack pointer
               movl %[next_sp], %%esp #  restore stack pointer
               movl $1f, %[prev_ip]   #  save ip(the resume point of current)
               pushl %[next_ip]       #  push ip into stack for ret instruction
               __switch_canary        #  macro defined in <asm/system.h>
               jmp __switch_to        #  jump to __switch_to() function
               1:                     #  symbol
               popl %%ebp             #  restore base stack pointer
               popfl                  #  restore eflags


          figure :
            switch_to(A, B, A) {
              A {
                      prev = A
                      next = B
                      eax = A
                      last = A
              }

              B {
                      prev = B
                      next = other
                      eax = A    /*  this register will be updated after function invocation completed
                                  *  or process switching was occurred.
                                  */
                      last = A
              }
            }
            /*  there just one kernel existed,so the code for context switching is same between
             *  processes.
             */
            switch_to(C, A, C) {
              C {
                      prev = C
                      next = A
                      eax = C
                      last = C
              }

              A {
                      prev = A
                      next = other
                      eax = C    /*  this register will be updated after function invocation completed
                                  *  or process switching was occurred.
                                  */
                      last = C
              }
            }

        !  the process has been switched at the time that @next->thread.esp was loadded into
           esp register.
           (this operation is take affect when arch_end_context_switch(@next) was called)
           the kernel stack of previous process is saved in @prev->thread.esp.
           (this operation is accomplished when arch_start_context_switch(@prev) was called)
       

        The __switch_to() function :
          <arch/x86/include/asm/system.h>
          <arch/x86/kernel/process_(32 | 64).c>

            /*  __switch_to - does the bulk of the process switch started by the switch_to() macro.
             *  @prev_p : previous task pointer.
             *  @next_p : next task pointer.
             *  return -  @prev_p.
             *            switch_to() macro will replaces esp register(ebp was not replaced,it was pushed
             *            into the stack of @prev and popped up later) before jmp to __switch_to().
             *            but the arguments of __switch_to() are stored in CPU generic-purpose registers
             *            that is eax(@prev_p) and edx(@next_p),finally,it returns the value in eax register,
             *            so it is @prev_p.
             *  #  __attribute__(regparm(3)) should be attached to __switch_to(),but it did not detect such
             *     GCC attribute is used.
             *     regparm(number) : this attribute just take affect only if x86-32 targers,it tell compiler
             *                       that,store the parameter from number one to @number in CPU registers
             *                       eax,edx,ecx(so the maximum @number is 3) to instead store them on stack.
             */
            __notrace_funcgraph struct task_struct *
            __switch_to(struct task_struct *prev_p, struct task_struct *next_p);

            the works this function does :
              /*  @prev_p -> previous task pointer.
               *  @prev   -> previous task's thread structure.
               *  @next_p -> next task pointer.
               *  @next   -> next task's thread structure.
               */
              1>  get local CPU id and local tss.
              2>  check if the task @next_p is used math function,if it used and
                  @next_p->fpu_counter > 5,then set @preload_fpu to true.
              3>  call to "__unlazy_fpu(prev_p);" this macro optionally save the contents of the FPU,MMX,XMM of
                  @prev_p.
              4>  if @preload_fpu is T => prefetch(@next->xstate) .
              5>  call to "load_sp0(@tss, @next);",load @next->esp0 into @tss->esp0.
                  any future privilege level change from User Mode to Kernel Mode raised by a sysenter assembly
                  instruction will copy this address in the esp register.
              6>  call to "lazy_save_gs(@prev->gs);",this macro is defined through savesegment(gs, (v)),it stores
                  gs register in the @prev->gs.
              7>  call to "load_TLS(@next, @cpu);",loads in the Global Descriptor Table of the local CPU the 
                  Thread-Local Storage segments used by the @next_p process,the Segment Selectors are stored in
                  @next->tls_array member.
              8>  restore IOPL if needed.
                  in normal use,the flags restore in the switch assembly will handle this,but if the kernel
                  is running virtualized at a non-zero CPL,the popf will not restore flags,so it must be done
                  in a separate step.
              9>  handle debug registers and/or IO bitmaps.
                  if this is necessary,call to "__switch_to_xtra(@prev_p, @next_p, @tss);".
              10> if @preload_fpu is T => execute clts instruction.
              11> call to "arch_end_context_switch(@next_p);" to ensure context switching has been completed.
              12> if @preload_fpu is T => call to "__math_stat_restore();" restore math registers.
              13> restore gs if needed(if (@prev->gs || @next->gs) lazy_load_gs(@next->gs); ).
              14> update percpu data via "percpu_write(current_task, @next_p);").
              15> return @prev_p.


        Saving and Loading the FPU,MMX,XMM registers :
          from Intel 80486DX,the arithmetic floating-point unit(FPU) has been integrated into the CPU.the name
          'mathematical coprocessor' continues to be used in memory of the days when floating-point computations
          were executed by an expensive special-purpose chip.
          ESCAPE instructions(for compatible with older models) are instructions with a prefix byte ranging
          between 0xd8 -- 0xdf,these instructions act on the set of floating-point registers included in the CPU.
          !  if a process is using ESCAPE instructions,the contents of the FPU registers belong to its hardware
             context,so they are should be saved and restored later.

          MMX : new instructions were introduced on Pentium models,supposed to speed up the execution of multimedia
                applications.
          MMX instructions act on the FPU registers.
          disadvantage : programmers can not mix FPU instructions and MMX instructions.
          advantage    : for OS designer,save the FPU state is to save MMX state.
          MMX introduced SIMD(single-instruction multiple-data) pipeline inside the processor.

          !  Pentium III model extends that SIMD capability :
               it introduces the SSE extensions(Streaming SIMD Extensions),which adds facilities for handling
               floating-point values caontained in eight 128-bit registers called the XMM registers.
               (XMM0 -- XMM7)
               XMM registers do not overlap the FPU and MMX registers.(it is able to mix SSE and FPU/MMX)
          !  Pentium 4 model introduces yet another feature : SSE2 Extensions.
               which is basically an extension of SSE supporting higher-precision floating-point values,
               it uses the same set of XMM registers as SSE.

          80x86 model do not save the FPU,MMX,XMM registers in the TSS automatically,but it enables kernel to do
          that if necessary.

          cr0.TS flag : TS(Task-Switching)
            which obeys the following rules :
              >  every time a hardware context switch is performed,the TS flag is set.
              >  every time an ESCAPE,MMX,SSE,SSE2 instruction is executed when the TS flag is set,
                 the control unit raises a "Device not available" exception.

          figure :
            A is using mathematical coprocessor;
            switch occurs from A to B,cr0.TS = 1,saves floating-point registers to A.tss;
            B is not using mathmetical coprocessor => kernel do not need to restore floating-point registers;
            B try to use mathmetical coprocessor -> cr0.TS has been setted -> "Device not available" exception;
            kernel handle the exception => restore the floating-point registers from B.tss;

          FPU,MMX,XMM structures :
            <arch/x86/include/asm/processor.h>
              struct i387_fsave_struct;    /*  FPU,MMX state  */
              struct i387_fxsave_struct;   /*  SSE,SSE2 state */
              struct i387_soft_struct;     /*  older compatibility  */
                                           /*  it is used for the older CPU model which is no
                                            *  mathmetical coprocessor.
                                            */
              union thread_xstate {
                struct i387_fsave_struct fsave;
                struct i387_fxsave_struct fxsave;
                struct i387_soft_struct soft;
                struct xsave_struct xsave;
              };  /*  thread_struct.xstate (pointer type)  */

            the process descriptor includes two additional flags :
            task_struct.thread_info.status { TS_USEDFPU }
            task_struct.flags { PF_USED_MATH }

            TS_USEDFPU : it specifies whether the process used the FPU,MMX,XMM registers in the current execution run.
            PF_USED_MATH : it specifies whether the contents of the thread_struct.xstate are significant,the flag is
                           cleared in two cases :
                             1>  when the process starts executing a new program by invoking an execve() system call,
                                 because the control will never return to the former program,the data currently stored
                                 in thread_struct.xstate is never used again.
                             2>  when a process that was executing a program in User Mode starts executing a signal
                                 handler procedure,because signal handlers are asynchronous with respect to the program
                                 execution flow,the floating-point registers could be meaningless to the signal handler.
                                 however,the kernel saves the floating-point registers in thread_struct.xstate before
                                 starting the handler and restores them after the handler terminates.(that is the signal
                                 handler is allowed to use floating-point features)

          Saving the FPU registers :
            <arch/x86/include/asm/i387.h>
              /*  __unlazy_fpu - save fpu state.
               *  @tsk : the target,which often is @prev in __switch_to().
               *  return - none.
               *  #  this function checks if TS_USEDFPU flag of @tsk's thread_info.status is set,
               *     TS_USEDFPU == 1,then call to __save_init_fpu(@tsk),and call to stts().
               *     TS_USEDFPU == 0,then @tsk->fpu_counter = 0.
               */
              static inline void __unlazy_fpu(struct task_struct *tsk);

              /*  __save_init_fpu - save fpu state and then initializes them.
               *  @tsk : the target.
               *  return - none.
               *  #  this function maybe call to either xsave(@tsk) or fxsave(@tsk),that is determined by
               *     task_thread_info(@tsk)->status & TS_XSAVE.
               *     then call to clear_fpu_state(@tsk) to initializes fpu state to fixed values,
               *     and clear TS_USEDFPU in the @tsk's thread_info.status.
               */
              static inline void __save_init_fpu(struct task_struct *tsk);

            <arch/x86/include/asm/system.h>
              /*  stts - a macro sets cr0.TS flag.  */
              #define stts() write_cr0(read_cr0() | X86_CR0_TS)

          Loading the FPU registers :
            the contents of the floating-point registers are not restored right after the @next process
            resumes execution.(but __switch_to() restored it if @preload_fpu is TRUE)
            however,the TS flag of cr0 has been set by __unlazy_fpu(),thus,the first time the @next process
            tries to execute an ESCAPE,MMX,SSE/SSE2 instruction will traps an exception,then handler calls to
            math_state_restore() to restore the contents.

            <arch/x86/include/asm/i387.h>
              /*  math_state_resotre - exception handler to deal with use mathematical coprocessor when TS == 1.
               *  #  this function checks @task is used math at first(PF_USED_MATH flag),
               *     if it is not,then call to init_fpu() to initializes FPU(PF_USED_MATH set to 1) before
               *     next execution;
               *     call to clts() to clear cr0.TS,then invoke __math_state_restore() to do primary works.
               *     (if TS == 1,execute ESCAPE,MMX,SSE/SSE2 instruction will traps exception)
               */
              extern asmlinkage void math_state_restore(void);

              /*  __math_state_restore - restores fpu for @tsk and set TS_USEDFPU.
               *  #  @tsk = thread->task;  =>
               *     @thread = current_thread_info();
               *     call to restore_fpu_checking(@tsk),(this function execute "fxrstor" instruction)
               *     set TS_USEDFPU,
               *     @tsk->fpu_counter++.
               */
              extern void __math_state_restore(void);

          Using the FPU,MMX,and SSE/SSE2 units in Kernel Mode :
            !  IF IT IS NOT NECESSARY,DO NOT USE x87 IN KERNEL MODE.
            use x87 in Kernel Mode is more expensive than User Mode.
          
            if kernel use FPU,it should avoid interfering with any computation carried on by the current 
            User Mode process.

            <arch/x86/include/asm/i387.h>
              /*  kernel_fpu_begin - kernel ready to use FPU.
               *  #  disable preempt,
               *     checks if current thread is used FPU(TS_USEDFPU),then save the state,
               *     if it is not,just clts().
               */
              static inline void kernel_fpu_begin(void);

              /*  kernel_fpu_end - kernel end use FPU.
               *  #  set cr0.TS by stts(),
               *     enable preempt.
               */
              static inline void kernel_fpu_end(void);

            because the FPU state of User Mode process has been saved,so math_state_restore() will be called
            later when the process tries to execute an ESCAPE,MMX,SSE/SSE2 instruction.

            !  the kernel uses FPU only in a few places,typically when moving or clearing large memory areas
               or when computing checksum functions.


    Creating Processes :
      Traditional Unix systems treat all processes in the same way :
        resources owned by the parent process are duplicated in the child process.
        !  this approach makes process creation very slow and inefficient.

      Modern Unix kernels solve this problem by introducing three different mechanisms :
        1>  Copy-On-Write
        2>  Lightweight processes allow both the parent and the child to share many perprocess kernel data
            structures,such as the paging tables,the open file tables,and the signal dispositions.
        3>  the vfork() system call creates a process that shares the memory address space of its parent,
            to prevent the parent from overwriting data needed by the child,the parent's execution is blocked
            until the child exits or executes a new program.

      The clone(),fork(),and vfork() System Calls :
        Lightweight processes are created in Linux by using a function named clone().
        <linux/sched.h>
          /*  clone - Linux system call,create a lightweight process.
           *  @fn : the function to be executed.
           *  @child_stack : User Mode stack pointer was assigned to esp register.
           *  @flags : flags,the low byte specifies the signal number to be sent to the parent process when
           *           child terminates,the SIGCHLD signal is generally selected;
           *           remaining three bytes encode a group of clone flags.
           *  @arg : arguments to @fn.
           *  @ptid : address of a User Mode variable of the parent process that will hold the PID of the 
           *          new lightweight process.meaningful only if the CLONE_PARENT_SETTID flag is set.
           *  @newtls : address of a data structure that defines a Thread Local Storage segment for the new
           *            lightweight process,meaningful only if the CLONET_SETTLS flag is set.
           *  @ctid : address of a User Mode variable of the new lightweight process that will hold the PID
           *          of such process,meaningful only if the CLONE_CHILD_SETTID flag is set.
           *  return - lighetweight process's thread ID will be returned,if succeed.
           *           Otherwise,-1 will be returned,and no lightweight process has been created.
           *         error value will be set appropriately.
           */
          int clone(int (*fn)(void *), void *child_stack, int flags, void *arg,
                    /*  pid_t *ptid, void *newtls, pid_t *ctid  */ ...);
          /*  System-Call  */

          clone flags :
            CLONE_VM - shares the memory descriptor and all Page Tables.
            CLONE_FS - shares the table that identifies the root directory and the current working directory,
                       as well as the value of the bitmask used to mask the initial file permissions of a new
                       file.
            CLONE_FILES - shares the table that identifies the open files.
            CLONE_SIGHAND - shares the tables that identify the signal handlers and the blocked and pending signals.
                            if this flag is true,the CLONE_VM flag must also be set.
            CLONE_PTRACE - if traced,the parent wants the child to be traced too,furthermore,the debugger may want
                           to trace the child on its own,in this case,the kernel forces the flag to 1.
            CLONE_VFORK - set when the system call issued is a vfork().
            CLONE_PARENT - set the parent of the child to the parent of the calling process.
            CLONE_THREAD - inserts the child to the same thread group of the parent,and forces the child to share
                           the signal descriptor of the parent,the child's tgid and group_leader fields are set
                           accordingly.if the flag is true,the CLONE_SIGHAND flag must also be set.
            CLONE_NEWNS - set if the clone needs its own namespace,that is,its own view of the mounted filesystems;
                          it is not possible to specify both CLONE_NEWNS and CLONE_FS.
            CLONE_SYSVSEM - shares the SystemV IPC undoable semaphore operations.
            CLONE_SETTLS - creates a new Thread Local Storage segment for the lightweight process.
            CLONE_PARENT_SETTID - writes the PID of the child into the User Mode variable of the parent pointed to
                                  by the @ptid parameter.
            CLONE_CHILD_CLEARTID - when set,the kernel sets up a mechanism to be triggered when process will exit or
                                   when it will start executing a new program.in these cases,the kernel will clear the
                                   User Mode variable pointed to by the @ctid parameter and will awaken any process
                                   waiting for this event.
            CLONE_DETACHED - a legacy flag ignored by the kernel.
            CLONE_UNTRACED - set by the kernel to overried the value of the CLONE_PTRACE flag.
            CLONE_CHILD_SETTID - writes the PID of the child into the User Mode Variable of the child pointed by the
                                 @ctid parameter.
            CLONE_STOPPED - forces the child to start in the TASK_STOPPED state.

          !  the clone flags were defined in kernel have more values,these values were used by kernel routine.

          !  the clone() in C library is actually a wrapper of the sys_clone() defined in <arch/x86/kernel/process.c>.
             and sys_clone() will call to do_fork() function.
             sys_clone() does not have the @fn and @arg parameters,in fact,the wrapper function saves the pointer @fn
             into the child's stack position corresponding to the return address of the wrapper function itself;
             the pointer @arg is saved on the child'stack right below @fn.

          !  sys_fork() similar to sys_clone(),but flag only the SIGCHLD was set and all clone flags are cleared,
             stack pointer is point to the parent's stack,remaining arguments set to NULL.

          !  sys_vfork() similar to sys_clone(),but flag is set to CLONE_VFORK | CLONE_VM | SIGCHLD combination,
             stack pointer is point to the parent's stack,remaining arguments set to NULL.


      The do_fork() function :
        <linux/sched.h> <kernel/fork.c>
          /*  do_fork - the main fork routine.
           *  @clone_flags : same as the @flags parameter of clone().
           *  @stack_start : the stack's start point,same as the @child_stack parameter of clone().
           *  @regs : pointer to the values of the general purpose registers saved into the Kernel Mode stack
           *          when switching from User Mode to Kernel Mode.
           *  @stack_size : unused parameter,always set to 0.
           *  @parent_tidptr : same as the @ptid parameter of the clone().
           *  @child_tidptr : same as the @ctid parameter of the clone().
           *  return - pid of the forked process returned on successful,otherwise returns error code.
           *           return from fork is different between parent and child!(but do_fork() always called 
           *           by kernel in parent's context)
           */
          extern long do_fork(unsigned long clone_flags, unsigned long stack_start, struct pt_regs *regs,
                              unsigned long stack_size, int __user *parent_tidptr, int __user *child_tidptr);

        the primary works that do_fork() do :
          /*  do_fork() call to copy_process() to finish process copying,and copy_process() call to copy_thread()
           *  to finish thread copying.
           */
          1>  allocates a new PID for the child by looking in the @pidmap_array bitmap.

          2>  checks the ptrace field of the parent(current->ptrace) :
              ptrace != 0 => the parent process is being traced by another process,thus do_fork() checks whether
              the debugger wants to trace the child on its own(independently of the value of the CLONE_PTRACE
              flag specified by the parent),in this case,if the child is not a kernel thread(CLONE_UNTRACED
              flag cleared),the function sets the CLONE_PTRACE flag.

          3>  invokes copy_process() to make a copy of the process descriptor.

          4>  if either the CLONE_STOPPED flag is set or the child process must be traced,that is,the PT_PTRACED flag
              is set in @p->ptrace,do_fork() set the state of the child to TASK_STOPPED and adds a pending SIGSTOP
              signal to it.
        
          5>  if the CLONE_STOPPED flag is not set,it invokes the wake_up_new_task() function,which performs the following
              operations :
                >  adjusts the scheduling parameters of both the parent and the child.

                >  if the child will run on the same CPU as the parent,and parent and child do not share the same set of
                   page tables(CLONE_VM flag cleared),it then forces the child to run before the parent by inserting it 
                   into the parent's runqueue right before the parent.(if child flushes its address space and executes a new
                   program right after the forking,this way will prevent unnecessary cost for COPY-ON-WRITE)
                   otherwise(child run on the same CPU as parent,or CLONE_VM flag set),it inserts the child in the last
                   position of the parent's runqueue.

          6>  CLONE_STOPPED == 1 => put @child -> TASK_STOPPED.

          7>  if the parent process is being traced,it stores the PID of the child in the current->ptrace_message,
              and invokes ptrace_notify().(this function was called by ptrace_event()<linux/ptrace.h>,and ptrace_event() was
              called by tracehook_report_clone_complete()<linux/tracehook.h>)
              ptrace_notify() stops the current process and sends a SIGCHLD signal to its parent,the "grandparent" of the child
              is the debugger that is tracing the parent;the SIGCHLD signal notifies the debugger that current has forked a child,
              whose PID can be retrieved by looking into the  current->ptrace_message field.
              /*  Linux 2.6,ptrace_notify() was called in ptrace_event() with ((event << 8) | SIGTRAP),
               *  a siginfo_t structure was allocated in ptrace_notify() and @info.si_signo set to SIGTRAP,ptrace_notify() then
               *  call to ptrace_stop(),which set current->last_siginfo = @info,and put current into TASK_TRACED(let debugger run).
               *  I DID NOT FIND OUT THE SIGCHLD WAS SENT TO THE @current's PARENT FROM THE SOURCE CODE!
               *  ptrace_notify() JUST CALLED WITH THE PARAMETER 100000101 BY ptrace_event()(SIGTRAP == 5,PTRACE_EVENT_FORK == 1).
               */

          8>  if the CLONE_VFORK flag is specified,it inserts the parent process in a wait queue and suspends it until the child
              releases its memory address space.

          9>  terminates by returning the PID of the child.


      The copy_process() function :
        the copy_process() function sets up the process descriptor and any other kernel data structure required for a child's
        execution.
        <kernel/fork.c>
          /*  copy_process - create a new process as a copy of the old one,but does not actually start it yet.
           *  @clone_flag:   clone flags same as do_fork().
           *  @stack_start:  same as do_fork() @stack_start.
           *  @regs:         same as do_fork() @regs.
           *  @stack_size:   same as do_fork() @stack_size,it is unused.
           *  @child_tidptr: same as do_fork() @child_tidptr.
           *  @pid:          a pointer points to an object which type is struct pid,this parameter is used
           *                 to save child's pid.
           *  @trace:        for ptrace mechanism.
           *  return -       the task_struct pointer of the child process,ERR_PTR would be returned if 
           *                 any error occurred.
           */
          static struct task_struct *copy_process(unsigned long clone_flag, unsigned long stack_start,
                                                  struct pt_regs *regs, unsigned long stack_size, 
                                                  int __user *child_tidptr, struct pid *pid, int trace);

        the primary works of copy_process() :
          1>  checks whether the flags passed in the clone_flags parameter are compatible.
              in particular,it returns an error code in the following cases :
                <  CLONE_NEWNS == 1 AND CLONE_FS == 1
                <  CLONE_THREAD == 1 AND CLONE_SIGHAND == 0
                <  CLONE_SIGHAND == 1 AND CLONE_VM == 0
        
          2>  performs any additional security checks by invoking security_task_create(),this function call to
              security_ops->task_create().

          3>  invokes dup_task_struct() to get the process descriptor for the child.
              this function performs the following actions :
                <  invokes __unlazy_fpu() on the current process to save,if necessary.
                   later,dup_task_struct() will copy these values in the thread_info structure of the child.
                <  executes the alloc_task_struct() macro to get a process descriptor for the new process,and
                   stores its address in the @tsk local variable.
                <  executes the alloc_thread_info macro to get a free memory area to store the thread_info structure
                   and the Kernel Mode stack of the new process,and saves its address in the @ti local variable.
                <  copies the contents of the @current's process descriptor into @tsk,then sets @tsk->thread_info
                   to @ti.
                <  copies the contents of the @current's thread_info descriptor into @ti,then sets @ti->task to @tsk.
                <  sets the usage counter of the new process descriptor(@tsk->usage) to 2 to specify that the process
                   descriptor is in use and that the corresponding process is alive.
                <  returns the process descriptor pointer of the new process(@tsk).

          4>  checks whether the value stored in @current->signal->rlim[RLIMIT_NPROC].rlim_cur is smaller than the
              current number of process owned by the user.
              if it is not true,an error code is returned,unless 'capable(CAP_SYS_ADMIN) OR capable(CAP_SYS_RESOURCE) OR
              @p->real_cred->user-> == INIT_USER)'.
              this function gets the current number of processes owned by the user from a per-user data structure named
              user_struct,this data structure can be found through a pointer in the user field of the process descriptor.
              (task_struct->real_cred->user->processes)

          5>  increase the usage counter of the user_struct structure(user_struct.__count) and the counter of the process
              owned by the user(user_struct.processes).

          6>  checks that the number of processes in the system does not exceed the value of the max_threads variable.
              /*  write a new value into /proc/sys/kernel/threads-max file is able to change this limit dynamically.  */

          7>  if the kernel functions implementing the execution domain and the executable format of the new process
              are included in the kernel modules,it increase their usage counters.

          8>  sets a few crucial fields related to the process state :
                <  @tsk->lock_depth = -1         (no lock hold)
                <  @tsk->did_exec = 0            (no exec did)
                <  @tsk->flags = @current->flags & ~PF_SUPERPRIV | PF_FORKNOEXEC

          9>  @tsk->pid = pid_nr(pid)  /*  @pid is allocated by alloc_pid()  */
        
          10> if @clone_flags & CLONE_PARENT_SETTID => *@parent_tidptr = @tsk->pid
              this work is did by do_fork() through put_user()(Linux 2.6)

          11> initializes the list_head data structures and the spin locks included in the child's process descriptor,
              and sets up several other fields related to pending signals,timers,and time statistics.

          12> invokes copy_semundo(),copy_files(),copy_fs(),copy_sighand(),copy_signal(),copy_mm(),copy_namespace()
              to create new data structures and copy into them the values of the corresponding parent process
              data structures unless specified differently by the @clone_flags.

          13> invokes copy_thread() to initialize the Kernel Mode stack of the child process with the values contained
              in the CPU registers when the clone() system call was issued.(these values have been saved in the Kernel
              Mode stack of parent)
              However,the function forces the value 0 into the field corresponding to the eax register(child process
              fork() return or clone() system-call).
              @thread.esp = the base address of the child's Kernel Mode stack
              @thread.eip = address of ret_from_fork()  /*  an assembly function  */
              if parent makes use of an I/O Permission Bitmap,the child gets a copy of such bitmap.
              if @clone_flags & CLONE_SETTLS => child gets the TLS segment specified by the User Mode data structure
              pointed to by the @tls parameter of the clone() system-call.

          14> if @clone_flags & CLONE_CHILD_SETTID => @tsk->set_child_tid = @child_tidptr
              if @clone_flags & CLONE_CHLID_CLEARTID => @tsk->clear_child_tid = @child_tidptr
              these flags specify that the value of the variable pointed to by @child_tidptr in the User Mode address
              space of the child has to be changed,although the actual write operations will be done later.

          15> turns off the TIF_SYSCALL_TRACE flag in the thread_info structure of the child.(ret_from_fork() will not
              notify the debugging process about the system-call termination)
              (system-call tracing is controled by PTRACE_SYSCALL flag in @tsk->ptrace)

          16> @tsk->exit_signal = (@clone_flags & CLONE_THREAD) ? -1 : (@clone_flags & CSIGNAL)
              only the death of the last member of a thread group(usually,the thread group leader) causes a signal
              notifying the parent of the thread group leader.

          17> invokes sched_fork() to complete the initialization of the scheduler data structure of the new process.
              the function also sets the state of the new process to TASK_RUNNING and sets the preempt_count field
              of the thread_info structure to 1,thus disabling kernel preemption.
              Moreover,in order to keep process scheduling fair,the function shares the remaining timeslice of the 
              parent between the parent and the child.

          18> sets the field in the thread_info structure of @tsk to the number of the local CPU returned by
              smp_processor_id().

          19> initializes the fields that specify the parenthood relationships.
              if @clone_flags & (CLONE_PARENT | CLONE_THREAD) =>
                @tsk->real_parent = @tsk->parent = @current->real_parent
              else
                @tsk->real_parent = @tsk->parent = @current
              /*  ! I DID NOT FIND OUT THE CODE SETS @TSK->PARENT = @CURRENT->REAL_PAREN,
               *    DUP_TASK_STRUCT() DID @TSK = @CURRENT.
               */

          20> if the child does not need to be traced(@clone_flags & ~CLONE_PTRACE),it sets the @tsk->ptrace to 0.

          21> executes the SET_LINKS macro to insert the new process descriptor in the process list.

          22> if the child must be traced(PT_PTRACED),it sets @tsk->parent to @current->parent and inserts the child
              into the trace list of the debugger.
              this work is accomplished by tracehook_finish_clone().

          23> invokes attach_pid() to insert the PID of the new process descriptor in the @tsk->pids[PIDTYPE_PID] hash table.

          24> if the child is a thread group leader(@clone_flags & ~CLONE_THREAD) :
                <  @tsk->tgid = @tsk->pid
                <  @tsk->group_leader = @tsk
                <  invokes three times attach_pid() to insert the child in the PID hash tables of type
                   PIDTYPE_TGID,PIDTYPE_PGID,PIDTYPE_SID.
                   /*  !  I DID NOT FIND OUT CODE THAT ATTACHED PIDTYPE_TGID IN COPY_PROCESS(),BECAUSE LINUX 2.6
                    *     NO SUCH PIDTYPE HAD BEEN DEFINED.
                    */

          25> otherwise,if the child belongs to the thread group of its parent(@clone_flags & CLONE_THREAD) :
                <  @tsk->tgid = @current->tgid
                <  @tsk->group_leader = @current->group_leader
                <  invokes attach_pid() to insert the child in the PIDTYPE_TGID hash table
                   (more specifically,in the per-PID list of the @current->group_leader process)

          26> a new process has now been added to the set of processes : ++nr_threads

          27> ++total_forks to keep trace of the number of forked processes.

          28> terminated by returning the child's process descriptor pointer(@tsk).
    

    Kernel Threads :
      What is the Kernel Thread :
        the system process running only in Kernel Mode and deal with some critical tasks such flash cache,swap pages,serving
        network connections,modern operating systems delegate their functions to Kernel threads.
        such Kernel Thread are not encumbered with the unnecessary User Mode context.

      Differ to regular process :
        >  Kernel threads run only in Kernel Mode,while regular processes run alternatively in Kernel Mode and in User Mode.
        >  Because kernel threads run only in Kernel Mode,they use only linear addresses greater than PAGE_OFFSET(as above,
           PAGE_OFFSET is 0xc0000000).
           Regular processes,on the other hand,use all four gigabytes of linear addresses,in either User Mode or Kernel Mode.

      Creating a kernel thread :
        <arch/x86/kernel/process.c>
          /*  kernel_thread - this is the kernel thread create function depends on architecture.
           *  @fn:            the function to execute.
           *  @arg:           the argument of @fn.
           *  @flags:         the clone_flags,because this function call to do_fork().
           *  return -        returns what the do_fork() return.
           */
          int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags) EXPORT_SYMBOL(kernel_thread);

          /*  !  this is the artecture depending function,so when developer developing kernel at the part it is not
           *     depend on architecture should use function kthread_create() to instead invocation of kernel_thread().
           */

        <linux/kthread.h>
          /*  kthread_create - kthread helper used to create a kernel thread,the kernel thread will be stopped,
           *                   use wake_up_process() to start it.
           *  @threadfn:       the function to execute.
           *  @data:           argument of @threadfn.
           *  @namefmt:        const char pointer point to a printf-style string.
           *  @additional:     parameters used to resolve @namefmt.
           *  return -         task_struct pointer points to the kthread or ERR_PTR(-ENOMEM).
           */
          struct task_struct *kthread_create(int (*threadfn)(void *data), void *data, const char namefmt[], ...);

          !  @threadfn can either call do_exit() directly if it is a standalone thread for which noone will call
             kthread_stop(),or return when 'kthread_should_stop()' is true(means kthread_stop() has been called)
             its return value should be a zero value or a negative error code,kthread_stop() will returns what
             @threadfn returned.

          /*  kthread_run - macro used to create a kernel thread and start it.  */
          #define kthread_run(threadfn, data, namefmt, ...)

        kernel_thread() call to do_fork() with (@clone_flags | CLONE_VM | CLONE_UNTRACED),avoids duplicate page tables
        and ensure no process is able to trace the new kernel thread(even if the calling process is being traced).
        @regs to do_fork() is the initial value of CPU registers for a new thread(copy_thread() function sets the
        CPU registers with this initial value for new thread,too)
        /*  kernel_thread() builds up this stack area so that :
         *    >  ebx and edx will be set by copy_thread() to the values of the parameters @fn and @arg,respectively.
         *    >  eip wii be set to the address of the following assembly language fragment :
         *         movl  %edx,%eax
         *         pushl %edx
         *         call  %ebx
         *         pushl %eax
         *         call  do_exit
         *         #  Linux 2.6,regs.ip = (unsigned long)kernel_thread_helper,so kernel_thread_helper() will run before
         *            @fn.
         */

    Process 0 :
      ancestor of all process called "process 0",the "idle process",or,for historical reasons,the "swapper process",is a 
      kernel thread created from scratch during the initialization phase of Linux.
      the statically allocated data structured used by process 0 :
        >  a process descriptor stored in the init_task variable,which is initialized by the INIT_TASK macro.
        >  a thread_info descriptor and a Kernel Mode stack stored in the init_thread_union variable and initialized
           by the INIT_THREAD_INFO macro.
        >  the following tables,which the process descriptor points to :
             init_mm             (INIT_MM)       /*  initialization macro  */
             init_fs             (INIT_FS)
             init_files          (INIT_FILES)
             init_signals        (INIT_SIGNALS)
             init_sighand        (INIT_SIGHAND)
        >  the master kernel Page Global Directory stored in swapper_pg_dir.

      <linux/start_kernel.h> <init/main.c>
        /*  start_kernel - architecture indepening kernel starting routine.
         *  #  on x86_64 platform,this function is called by x86_64_start_kernel_reservations()<arch/x86/kernel/head64.c>.
         */
        extern asmlinkage void __init start_kernel(void);

        start_kernel() initializes all the data structures needed by the kernel,then call to rest_init(),which create 
        a kernel thread named "kernel_init" via kernel_thread(),@clone_flags = CLONE_FS | CLONE_SIGHAND.
        "kthreadadd" kernel thread also created by rest_init(),which deal with new kthread add into system,finally,
        rest_init() enable preempt without resched,next call to schedule() let the process 1 get CPU.
        if rest_init() get CPU later by scheduler assigned,it just call to cpu_idle() without preempt enabled,
        cpu_idle() is a function depends on architecture,on x86 platform,it defined in <kernel/process_64.c>.

        /*  process 0 selected by scheduler only when there are no other processes in the TASK_RUNNING state.
         *  and for SMP,there is a process 0 for each CPU.
         */

        kernel_init() defined in <init/main.c> as a static function,which initializes SMP and SMP Scheduler,does some
        basic works,then open /dev/console(sys_dup(0) called twice for set up STDOUT and STDERR),finally call to
        init_post()<init/main.c>.
        init_post() execute call to run_init_process()<init/main.c> on one of @ramdisk_execute_command,@execute_command,"/sbin/init",
        "/etc/init","/bin/init","/bin/sh",at least one of these commands must be succeed,otherwise,kernel panic.
        run_init_process() call to kernel_execve()<arch/x86/kernel/sys_i386_32.c>,which do a system call(execve) from kernel
        instead of calling sys_execve.

        !  console maybe open six "tty"s(on my computer,this is default).

    Process 1 :
      Has descripted above.
      as a result,the init kernel thread becomes a regular process(kernel_execve() called) having its own per-process
      kernel data structure.
      the process started by run_init_process() will stays alive until the system is shut down.

    Other kernel Threads :
      Linux uses many other kernel threads,some of them are created in the initialization phase and run until shutdown;
      others are created "on demand",when the kernel must execute a task that is better performed in its own execution
      context.
      >
        keventd(also called events)
          executes the functions in the @keventd_wq workqueue.
        
        kapmd
          handles the events related to the Advanced Power Management(APM).
          /*  the newer OS use APIC to manages computer power.  */

        kswapd
          reclaims memory.

        pdflush
          flushes "dirty" buffers to disk to reclaim memory.
          
        kblockd
          executes the function in the @kblockd_workqueue workqueue,essentially,it periodically activates the block
          device drivers.

        ksoftirqd
          runs the tasklets(bottom of interrupt);there is one of these kernel threads for each CPU in the system.
          /*  for example,get informations through the shell command :
           *    ps ax | grep -E '\[.*\]' | grep ksoftirqd
           */

    Destroying Processes :
      process "die" is the process terminates executing the code suppose it run.
      when this occurs,the kernel must be notified so that it can release the resources owned by the process.
      The usual way for a process to terminate is to invoke the exit() C library function.
      this function can be called explicitly by programmer or be called implicitly by compiler.

      Alternaitively,the kernel may force a whole thread group to die,this typically occurs when a process in the
      group has received a signal that it cannot handle or ignore or when an unrecoverable CPU exception has been
      raised in Kernel Mode while the kernel was running on behalf of the process.

      Process Termination :
        In Linux 2.6 there are two system calls that terminate a User Mode application :
          >  <linux/syscalls.h> /*  Kernel code tree  */
               asmlinkage long sys_exit_group(int error_code);
             <linux/unistd.h>   /*  C Library encapsulate  */
               void exit_group(int status);

             the function terminates a full thread group,that is,a whole multithreaded application.
             the main kernel function that implements this system call is called do_group_exit().
             and this system call should be called by exit().

          >  <linux/syscalls.h>
               asmlinkage long sys_exit(int error_code);
             <linux/unistd.h>
               void _exit(int status);
             <stdlib.h>
               void exit(int status);

             the _exit() system call,while terminates a single process,regardless of any other process in the
             thread group of the victim.
             the main kernel function that implements this system call is called do_exit().
             for example,this system call invoked by the pthread_exit() function of the Linux POSIX Thread Library.

        The do_group_exit() function :
          <linux/sched.h> <kernel/exit.c>
            /*  do_group_exit - terminates all threads belong to the thread group of "current".
             *  @exit_code:     the exit code will pass to do_exit(),its value maybe changed in some case.
             *                  the value is passed by exit_group() system call,or an error code supplied by the kernel.
             *                  (abnormal termination)
             *  #  before call to do_exit(),it will sets SIGNAL_GROUP_EXIT up in signal_struct.flags.(if no this flag had
             *     been setted up before)
             */    
            extern NORET_TYPE do_group_exit(int exit_code);

            The function executes the following operations :
              1>  checks whether the SIGNAL_GROUP_EXI flag of current->signal->flags has been setted up.
                  which means that the kernel already started an exit procedure for this thread group,in this case,
                    exit_code = current->signal->group_exit_code;
                    goto invoke do_exit(exit_code);
              
              2>  if signal_group_sig(current->signal) == F
                    sets SIGNAL_GROUP_EXIT flag
                    current->signal->group_exit_code = exit_code

              3>  invokes the zap_other_threads() function to kill the other processes in the thread group of current,
                  if any.
                  in order to do this,the function scans the per-PID list in the PIDTYPE_TGID hash table corresponding
                  to current->tgid,for each process in the list different from current,it sends a SIGKILL signal to it.
                  as a result,all such processes will eventually execute the do_exit() function.
                  /*  because Linux 2.6 no PIDTYPE_TGID was existed,so this function call to next_thread() to retrieve
                   *  the next thread in the thread group.
                   */

              4>  invokes the do_exit() function passing to it the process termination code(@exit_code).
                  do_exit() terminates the process and call to schedule().  /*  no return  */

        The do_exit() function :
          <linux/kernel.h> <kernel/exit.c>
            /*  do_exit - terminates the current task.
             *  @error_code:  the exit code return to User Mode or an error code reported by kernel.
             *  #  do_exit() do not return,and @tsk->exit_code will be setted up with the value @error_code /*  code  */.
             *     (schedule() will be called,so this routine no return,and process switching will be executed by scheduler.)
             */
            NORET_TYPE do_exit(long error_code /*  long code  */) ATTRIB_NORET;

            /*  The C Library function exit() is call to this function via sys_exit() system call  */

            the essential actions are executed as the following :
              1>  sets the PF_EXITING flag in the @flag field of the process descriptor to indicate that the process is being
                  eliminated.
                  if PF_EXITING have being setted up in @flags of current when do_exit() is executing,do_exit() will fix
                  recursive fault and sets up PF_EXITDONE in @flags,then put process state to TASK_UNINTERRUPTIBLE.
                  (in this case,reboot is needed)

              2>  removes,if necessary,the process descriptor from a dynamic timer queue via the del_timer_sync() function.

              3>  detaches from the process descriptor the data structures related to paging,semaphores,filesystem,open file
                  descriptors,namespaces,and I/O Permission Bitmap,respectivly,with the exit_mm(),exit_sem(),exit_files(),
                  exit_fs(),exit_namespace(),and exit_thread() functions.
                  these functions also remove each of these data structures if no other processes are sharing them.

              4>  if the kernel functions implementing the execution domain and the executable format of the process being killed
                  are included in kernel modules,the function decreases their usage counters.
              
              5>  sets the @exit_code of the process descriptor to the process termination code.

              6>  invokes the exit_notify() function to perform the following operations :
                    a>  updates the parenthood relationships of both the parent process and the child processes.
                        all child processes created by the terminating process become children of another process in the
                        same thread group,if any is running,or otherwise of the "init" process.

                    b>  checks whether the @exit_signal process descriptor field of the process being terminated is different
                        from -1,and whether the process is the last member of its thread group.
                        in this case,the function sends a signal(usually SIGCHLD) to the parent of the process being terminated
                        to notify the parent about a child's death.
        
                    c>  otherwise,if the @exit_signal field is equal to -1 or the thread group includes other processes,the
                        function sends a SIGCHLD signal to the parent only if the process is being traced.
                        (informed of the death of the lightweight process)

                    d>  if the @exit_signal process descriptor field is equal to -1 and the process is not being traced,
                        it sets the @exit_state field of the process descriptor to EXIT_DEAD,and invokes release_task()
                        to reclaim the memory of the remaining process data structures and to decrease the usage counter
                        of the process descriptor.
                        the usage counter becomes equal to 1,so that the process descriptor itself is not released right
                        away.

                    e>  otherwise,if the @exit_signal process descriptor field is not equal to -1 or the process is being
                        traced,it sets the @exit_state field to EXIT_ZOMBIE.
                
                    f>  sets the PF_DEAD flag in the @flags field of the process descriptor.

              7>  invokes the schedule() function to select a new process to run.
                  because a process in an EXIT_ZOMBIE state is ignored by the scheduler,the process stops executing right after
                  the switch_to macro in schedule() is invoked.
                  the scheduler will check the PF_DEAD flag and will decrease the usage counter in the descriptor of the
                  zombie process being replaced to denote the fact that the process is no longer alive.
              
    Process Removal :
      The Unix operating system allows a process to query the kernel to obtain the PID of its parent process or the execution
      state of any of its children.
      Unix kernels are not allowed to discard data included in a process descriptor field right after the process terminates.
      They are allowed to do so only after the parent process has issued a wait()-like system call that refers to the 
      terminated process.

      EXIT_ZOMBIE : although the process is technically dead,its descriptor must be saved until the parent process is notified.
      
      If parent dead before children,then system forcing all orphan processes to become children of the init process.
      init process issues wait()-like system call period.

      The release_task() function detaches the last data structures from the descriptor of a zombie process,it is applied on a
      zombie process in two possible ways :
        1>  by the do_exit() function if the parent is not interested in receiving signals from the child(SIGCHLD).
        2>  by the wait4() or waitpid() system calls after a signal has been sent to the parent.

        #   the default action was taken by a process for the signal SIGCHLD is ignores it,but this signal is able to be handled.

        FOR 1> : the memory reclaiming will be done by the scheduler.
        FOR 2> : the memory reclaiming will be progressed immediately.
                 /*  libc wait system call wrapper call to system call wait4(),which is defined in <kernel/exit.c>,
                  *  wait4() call to do_wait() complete the primary recycle routine.

        <linux/sched.h>
          /*  release_task - recycle the resources have holden by @p.
           *  @p:            the task to be released.
           */
          extern void release_task(struct task_struct *p);

          This function executes the following steps :
            1>  decreases the number of processes belonging to the user owner of the terminated process.
                (in user_struct)

            2>  if the process is being traced,the function removes it from the debugger's ptrace_children list and
                assigns the process back to its original parent.

            3>  invokes __exit_signal() to cancel any pending signal and to release the signal_struct descriptor of the 
                process.
                if the descriptor is no longer used by other lightweight processes,the function also removes this data
                structure.
                Moreover,the function invokes exit_itimers() to detach any POSIX interval timer from the process.

            4>  invokes __exit_sighand() to get rid of the signal handlers.

            5>  invokes __unhash_process(),which in turn :
                  a>  decreases by 1 the nr_threads variable.
                  b>  invokes detach_pid() twice to remove the process descriptor from the pidhash hash tables of type
                      PIDTYPE_PID and PIDTYPE_TGID.
                  c>  if the process is a thread group leader,invokes again detach_pid() twice to remove the process
                      descriptor from the PIDTYPE_PGID and PIDTYPE_SID hash tables.
                  d>  use the REMOVE_LINKS macro to unlink the process descriptor from the process list.

                  /*  __unhash_process() is called by __exit_signal(),and timer detaching also have done by it.  */
                  /*  Linux 2.6 have no PIDTYPE_TGID,so __unhash_proces() call to detach_pid() thrice.  */

            6>  if the process is not a thread group leader,the leader is a zombie,and the process is the last member
                of the thread group,the function sends a signal to the parent of the leader to notify it of the death
                of the process.(via do_notify_patent())

            7>  invokes the sched_exit() function to adjust the timeslice of the parent process.
                (similar to copy_process() where sched_fork() was called)

            8>  invokes pus_task_struct() to decrease the process descriptor's usage counter(via call_rcu());
                if the counter becomes zero,the function drops any remaining reference to the process :
                  a>  decreases the usage counter(@__count field) of the user_struct data structure of the user that owns
                      the process,and release that data structure if the usage counter becomes zero.
                  b>  releases the process descriptor and the memory area used to contain the thread_info descriptor and
                      the Kernel Mode stack.
                

/*  END OF CHAPTER3  */
            

Chapter 4 : Interrupts and Exceptions
    An interrupt is usually defined as an event that alters the sequence of instructions executed by a processor,
    such events correspond to electrical signals generated by hardware circuits both inside and outside the CPU chip.

    The type of interrupts :
      Synchronous interrupts : 
        produced by the CPU control unit while executing instructions and are called synchronous because the control
        unit issues them only after terminating the execution of an instruction.(int $0x80)
      
      Asynchronous interrupts :
        generated by other hardware devices at arbitrary times with respect to the CPU clock signals.

    #  Intel microprocessor manuals designate synchronous and asynchronous interrupts as exceptions and interrupts,
       respectively.

    The Role of Interrupt Signals :
      interrupt signals provide a way to divert the processor to code outside the normal flow of control.
      when an interrupt signal arrives,the CPU must stop what it is currently doing and switch to a new activity;
      it does this by saving the current value of the program counter in the Kernel Mode stack and by placing an
      address related to the interrupt type into the program counter.

      difference between interrupt handling and process switching :
        the code executed by an interrupt or by an exception handler is not a process.

      interrupt handling is one of the most sensitive tasks performed by the kernel,because it must satisfy the following
      constraints :
        >  interrupts can come anytime,when the kernel may want to finish something else it was trying to do.the kernel's
           goal is therefore to get the interrupt out of the way as soon as possible and defer as much processing as it can.
           so,an interrupt is divided into a critical urgent part that the kernel executes right away and a deferrable part
           that is left for later.(Linux,interrupt top half part and bottom half part)

        >  an interrupt maybe come when kernel is handling another interrupt.this should be allowed as much as possible,
           because it keeps the I/O devices busy.
           the interrupt handlers must be coded so that the corresponding kernel control paths can be executed in a nested
           manner.when the last kernel control path terminates,the kernel must be able to resume execution of the interrupted
           process or switch to another process if the interrupt signal has caused a rescheduling activity.

        >  although,the kernel may accept a new interrupt signal while handling a previous one,some critical regions exist
           inside the kernel code where interrupts must be disabled,such critical regions must be limited as much as possible,
           because according to the previous requirement,the kernel,and particularly the interrupt handlers,should run most of
           the time with the interrupts enabled.

    Interrupts and Exceptions :
      the Intel documentation classifies interrupts and exceptions as follows :
        Interrupts >
          Maskable interrupts :
            all Interrupt Requests(IRQs) issued by I/O devices give rise to maskable interrupts.
            a maskable interrupt can be in two states :
              masked or unmasked
                a masked interrupt is ignored by the control unit as long as it remains masked

          Nonmaskable interrupts :
            only a few critical events(such hardware failures) give rise to nonmaskable interrupts.
            nonmaskable interrupts are always recognized by the CPU.

        Exceptions >
          Processor-detected exceptions :
            Generated when the CPU detects an anomalous condition while executing an instruction.
            there are further divided into three groups,depending on the value of the eip register
            that is saved on the Kernel Mode stack when the CPU control unit raises the exception.
            
            Faults :
              can generally be corrected;once corrected,the program is allowed to restart with no loss of continuity.
              the saved value of eip is the address of the instruction that caused the fault,and hence that instruction
              can be resumed when the exception handler terminates.

            Traps :
              reported immediately following the execution of the trapping instruction;after the kernel returns control
              to the program,it is allowed to continue its execution with no loss of continuity.
              the saved value of eip is the address of the instruction that should be executed after the one that caused
              the trap.
              a trap is triggered only when there is no need to reexecute the instruction that terminated.the main use of 
              traps is for debugging purpose.the role of the interrupt signal in this case is to notify the debugger that
              a specific instruction has been executed.(such breakpoint feature)

            Aborts :
              a serious error occurred;the control unit is in trouble,and it may be unable to store in the eip register
              the precise location of the instruction causing the exection.
              aborts are used to report severe errors,such as hardware failures and invalid or inconsistent values in
              system tables.
              the interrupt signal sent by the control unit is an emergency signal used to switch control to the corresponding
              abort exception handler.this handler has no choice but to force the affected process to terminate.

        Programmed exceptions >
          occur at the request of the programmer.they are triggered by 'int' or 'int3' instructions;the 'into'(check for overflow)
          and 'bound'(check on address bound) instructions also give rise to a programmed exception when the condition they are
          checking is not true.
          programmed exceptions are handled by the control unit as traps;they are often called "software interrupts",such
          exceptions have two common uses:
            to implement system calls
            to notify a debugger of a specific event.

        #  each interrupt or exception is identified by a number ranging from 0 -- 255,Intel calls this 8-bit unsigned number
           a vector.the vectors of nonmaskable interrupts and exceptions are fixed,while those of maskable interrupts can be
           altered by programming the Interrupt Controller.

    IRQs and Interrupts :
      (PCI,Peripheral Component Interconnect)

      each hardware device controller capable of issuing interrupt requests usually has a single output line designated as
      the Interrupt ReQuest(IRQ) line(more sophisticated devices use several IRQ lines,i.e. PCI card).
      all existing IRQ lines are connected to the input pins of a hardware circuit called the Programmable Interrupt Controller.
      the actions that Programmable Interrupt Controller takes :
        1>  Monitors the IRQ lines,checking for raised signals.If two or more IRQ lines are raised,selects the one having the
            lower pin number.
        2>  If a raised signal occurs on an IRQ line :
              >  Converts the raised signal received into a corresponding vector.(IRQ vector)
              >  Stores the vector in an Interrupt Controller I/O port,thus allowing the CPU to read it via the data bus.
              >  Sends a raised signal to the processor INTR pin--that is,issues an interrupt.
              >  Waits until the CPU acknowledges the interrupt signal by writing into one of the Programmable Interrupt Controllers
                 (PIC) I/O ports;when this occurs,clears the INTR line.
        3>  Goes back to step 1.

      IRQ lines start from 0,therefore,the first IRQ line is usually denoted as IRQ0.
      Intel's default vector associated with IRQn is n+32,but such mapping can be modified by issuing suitable I/O instructions
      to the Interrupt Controller ports!(i.e. IRQ7 <=> vector 81)

      IRQ lines is selectively disabled/enabled via PIC,then that IRQ line will no longer issues interrupts.
      Disable interrupts are not lost;the PIC sends them to the CPU as soon as they are enabled again.

      !  Selectively enabling/disabling of IRQs is not the same as global masking/unmasking of maskable interrupts.
         eflags.IF flag controls that CPU whether ignores the maskable interrupt.

      !  Traditional PICs are implemented by connecting "in cascade" two 8259A-style external chips,each chip can handle up to
         eight different IRQ input lines.because the INT output line of the slave PIC is connected to the IRQ2 pin of the master
         PIC,the number of available IRQ lines is limited to 15.
         e.g.
           A{ IRQ0 ... IRQ7 }-(INT output line)-->{B.IRQ2 B{ IRQ0 IRQ1 IRQ3 ... IRQ7 }}-(INT output line)-->CPU INTR pin

    The Advanced Programmable Interrupt Controller(APIC) :
      INT output line straightforward connect to INTR pin of the CPU is only valid on a single CPU platform.
      Being able to deliver interrupts to each CPU in the system is crucial for fully exploiting the parallelism of the SMP
      architecture.

      I/O Advanced Programmable Interrupt Controller(I/O APIC) introduced from Intel Pentium III.
      80x86 microprocessors include a local APIC which has 32-bit registers,an internal clock,a local timer device,and two
      additional IRQ lines LINT0,LINT1 reserved for APIC interrupts.
      All local APICs are connected to an external I/O APIC,giving rise to a multi-APIC system.
      scheme :
        CPU0.local APIC{local IRQs : LINT0, LINT1}          CPU1.local APIC{local IRQs : LINT0, LINT1}  ...
                |                                                   |
            -------------Interrupt Controller Communication(ICC) bus----------
                                        |
                                    I/O APIC
                                        |
                                    external IRQs(IRQs from hardware)

      The I/O APIC consists of a set of 24 IRQ lines,a 24-entry Interrupt Redirection Table,programmable registers,and a 
      message unit for sending and receiving APIC messages over the APIC bus.
      interrupt priority is not related to pin number(8259A relating to pin number) :
        each entry in the Redirection Table can be individually programmed to indicate the interrupt vector and priority,
        the destination processor,and how the processor is selected.
        the information in the Redirection Table is used to translate each external IRQ signal into a message to one or
        more local APIC units via the APIC bus.

      external IRQs distributing :
        static distribution >
          the IRQ signal is delivered to the local APICs listed in the corresponding Redirection Table entry,the interrupt
          is delivered to one specific CPU,to a subset of CPUs,or to all CPUs at once(broadcast mode).

        dynamic distribution >
          the IRQ signal is delivered to the local APIC of the processor that is executing the process with the lowest
          priority.

          #  every local APIC has a programmable task priority register(TPR),which is used to compute the priority of the
             currently running process.(it is modified when process switch occurred,kernel modifies it)

          if two or more CPUs share the lowest priority,the load is distributed between them using a technique called
          "arbitration" :
            each CPU is assigned a different arbitration priority ranging from 0(lowest) to 15(highest) in the arbitration
            priority register of the local APIC.
            each time an interrupt is delivered to a CPU,its corresponding arbitration priority is automatically set to 0,
            while the arbitration priority of any other CPU is increased.when the arbitration priority register becomes
            greater than 15,it is set to the previous arbitration priority of the winning CPU increased by 1.
                previous_priority := WINNING_CPU.priority
                if ++(this.priority) > 15
                then
                    this.priority := ++previous_priority
          
          !  Pentium 4 local APIC does not have an arbitration priority register,the mechanism is hidden in the bus
             arbitration circuitry.

      the multi-APIC system allows CPUs to generate "interprocessor interrupts" :
        when a CPU wishes to send an interrupt to another CPU,it stores the interrupt vector and the identifier of the
        target's local APIC in the Interrupt Command Register(ICR) of its own local APIC,a message is then sent via the
        APIC bus to the target's local APIC,which therefore issues a corresponding interrupt to its own CPU.

        !  IPIs are actively used by Linux to exchange messages among CPUs.

      multi-APIC for uniprocessor system :
        it include an I/O APIC chip,which may be configured in two distinct ways :
          1>  as a standard 8259A-style external PIC connected to the CPU.
              the local APIC is disabled and the two LINT0 and LINT1 local IRQ lines
              are configured,respectively,as the INTR and NMI pins.
          2>  as a standard external I/O APIC.
              the local APIC is enabled,and all external interrupts are received through the I/O APIC.

    Exceptions :
      80x86 microprocessor introduced 20 different exceptions,kernel must provide a dedicated exception handler for each
      exception type.
      sometimes,a hardware error code is generated by CPU and stored in Kernel Mode stack before start the exception handler.

      About exceptions :
        0  -  Divide error (fault) : integer division by 0
        1  -  Debug (trap or fault) : Raised when the eflags.TF == 1 or when the address of an instruction or operand falls
                                      within the range of an active debug register.
        2  -  Not used : reserved for nonmaskable interrupts(NMI pin).
        3  -  Breakpoint (trap) : caused by an "int3(breakpoint)" instruction.
        4  -  Overflow (trap) : an "into" instruction has been executed while the eflags.OF == 1.
        5  -  Bounds check (fault) : a "bound" instruction is executed with the operand outside of the valid address bounds.
        6  -  Invalid opcode (fault) : CPU execution unit has detected an invalid opcode.
        7  -  Device not avaiable (fault) : an ESCAPE,MMX,or SSE/SSE2 instruction has been executed with the cr0.TS == 1.
        8  -  Double fault (abort) : raised when processor failed to handle exceptions serially.
                                     normally,when the CPU detects an exception while trying to call the handler for a prior
                                     exception,the two exceptions can be handled serially.
        9  -  Coprocessor segment overrun (abort) : problems with the external mathmeatical coprocessor.
        10 -  Invalid TSS (fault) : CPU has attempted a context switch to a process having an invalid Task State Segment.
        11 -  Segment not present (fault) : a reference was made to a segment not present in memory.
        12 -  Stack segment fault (fault) : the instruction attempted to exceed the stack segment limit,or the segment 
                                            identified by "ss" is not present in memory.
        13 -  General protection (fault) : one of the protection rules in the protected mode of the 80x86 has been violated.
        14 -  Page Fault (fault) : the addressed page is not present in memory,the corresponding Page Table entry is null,or
                                   a violation of the paging protection mechanism has occurred.
        15 -  Reserved by Intel
        16 -  Floating-point error (fault) : floating-point unit integrated into the CPU chip has signaled an error condition,
                                             such as numeric overflow or division by 0.
        17 -  Alignment check (fault) : the address of an operand is not correctly aligned.
        18 -  Machine check (abort) : a machine-check mechanism has detected a CPU or bus error.
        19 -  SIMD floating point exception (fault) : the SSE or SSE2 unit integrated in the CPU chip has signaled an error 
                                                      condition on a floating-point operation.

        #  value 20 -- 31 reserved for future development.

      Signals and Exception handlers on Linux :
        0       Divide error                divide_error()                  SIGFPE
        1       Debug                       debug()                         SIGTRAP
        2       NMI                         nmi()                           None
        3       Breakpoint                  int3()                          SIGTRAP
        4       Overflow                    overflow()                      SIGSEGV
        5       Bounds check                bounds()                        SIGSEGV
        6       Invalid opcode              invalid_op()                    SIGILL
        7       Device not available        device_not_available()          None
        8       Double fault                doublefault_fn()                None
        9       Coprocessor segment overrun coprocessor_segment_overrun()   SIGFPE
        10      Invalid TSS                 invalid_TSS()                   SIGSEGV
        11      Segment not present         segment_not_present()           SIGBUS
        12      Stack segment fault         stack_segment()                 SIGBUS
        13      General protection          general_protection()            SIGSEGV
        14      Page Fault                  page_fault()                    SIGSEGV
        15      Intel-reserved              None                            None
        16      Floating-point error        coprocessor_error()             SIGFPE
        17      Alignment check             alignment_check()               SIGBUS
        18      Machine check               machine_check()                 None
        19      SIMD floating point         simd_coprocessor_error()        SIGFPE

        #  Linux exceptions relating code is defined in <arch/x86/include/asm/traps.h>,and implemented in
           <arch/x86/kernel/traps.c>.

    Interrupt Descriptor Table :
      a system table called Interrupt Descriptor Table(IDT) associates each interrupt or exception vector with the address of 
      the corresponding interrupt or exception handler.it must be properly initialized before kernel enables interrupt.
      
      IDT is similar to GDT and LDT,each entry is a 8-byte descriptor,thus,a maximum of 256 * 8 = 2048 bytes are required to
      store IDT.

      idtr register allows the IDT to be located anywhere in memory :
        it specifies both the IDT base linear address and its limit(maximum length).
        this register must be initialized before enabling interrupts by using the "lidt" assembly language instruction.

      Three types of descriptors may included by IDT :
        1>  Task gate
              includes the TSS selector of the process that must replace the current one when an interrupt signal occurs.
              [0, 15] : RESERVED
              [16, 31] : TSS SEGMENT SELECTOR
              [32, 39] : RESERVED
              40 : 1
              41 : 0
              42 : 1
              43 : 0
              44 : 0
              [45, 46] : DPL
              47 : P
              [48, 63] : RESERVED

        2>  Interrupt gate
              includes the segment selector and the offset inside the segment of an interrupt or exception handler.
              while transferring control to the proper segment,the proprocessor clears the eflags.IF flag,thus disabling
              further maskable interrupts.
              [0, 15] : OFFSET (0--15)
              [16, 31] : SEGMENT SELECTOR
              [32, 36] : RESERVED
              37 : 0
              38 : 0
              39 : 0
              40 : 0
              41 : 1
              42 : 1
              43 : 1
              44 : 0
              [45, 46] : DPL
              47 : P
              [48, 63] : OFFSET(16--31)

        3>  Trap gate
              similar to an interrupt gate,except that while transferring control to the proper segment,the processor 
              does not modify the IF flag.
              [0, 15] : OFFSET(0--15)
              [16, 31] : SEGMENT SELECTOR
              [32, 36] : RESERVED
              37 : 0
              38 : 0
              39 : 0
              40 : 1
              41 : 1
              42 : 1
              43 : 1
              44 : 0
              [45, 46] : DPL
              47 : P
              [48, 63] : OFFSET(16--31)

        /**
         * bits 32--34 represents an offset into the IST,Interrupt Stack Table which is
         * stored in Task State Segment.
         * if all three bits are set to _zero_,the Interrupt Stack Table is not used
         */

        #  In particular,the value of the Type field encoded in the bits 40-43 identifies the descriptor type.
        #  Linux use interrupt gate to handle interrupts,and use trap gate to handle exceptions.

    Hardware Handling of Interrupts and Exceptions :
      /*  Suppose Kernel has been initialized,and CPU running on protected mode  */

      Before CPU executes the next instruction,it checks if an interrupt or an exception occurred while it executed the
      previous instruction.if it is,then CPU control unit does the following :
        1>  determines the vector i(0 <= i <= 255) associated with the interrupt or the exception.(Send by PIC)
        2>  reads the i_th entry in IDT through idtr.
        3>  gets the base address of the GDT from the gdtr register and looks in the GDT to read the Segment Descriptor
            identified by the selector in the IDT entry.this descriptor specifies the base address of the segment that
            includes the interrupt or exception handler.
        4>  makes sure the interrupt was issued by an authorized source.
            first
              compares the CPL which is stored in the two least significant bits of the cs register with the DPL of the
              Segment Descriptor included in the GDT.
              if cs.CPL < GDT.Segd.DPL
                    Raises a "General protection" exception     /*  interrupt handler cannot have a lower privilege than  */
                                                                /*  the program that caused the interrupt.  */
            second (for programmed exceptions)
              makes a further security check :
                compares the CPL with the DPL of the gate descriptor included in the IDT.
              if cs.CPL > IDT.entry.DPL
                    Raises a "General protection" exception
              
              !  prevent access by user applications to specific trap of interrupt gates.
        5>  checks whether a change of privilege level is taking place -- if CPL is different from the selected
            Segment Descriptor's DPL.(selected Segd,but cs.CPL has changed)
            if cs.CPL != GDT.Segd.DPL
              start using the stack that is associated with the new privilege level
              {
                a>  reads the tr register to access the TSS segment of the running process.
                b>  loads the ss and esp registers with the proper values for the stack segment and stack pointer
                    associated with the new privilege level.these values are found in the TSS.
                c>  in the new stack,it saves the previous values of ss and esp,which define the logical address of the
                    stack associated with the old privilege level.
                    /**
                     * if start using Kernel Mode stack,the context of ss and esp in User Mode will be saved on the
                     * Kernel Mode stack.
                     */
              }
        6>  if a fault has occurred,it loads cs and eip with the logical address of the instruction that caused the 
            exception so that is can be executed agian.
        7>  saves the contents of eflags,cs,and eip on the stack.(if cs.CPL != GDT.Segd.DPL -> Kernel Mode stack)
        8>  if the exception carries a hardware error code,it saves it on the stack.
        9>  loads cs and eip,respectively,with the Segment Selector and the Offset fields of the Gate Descriptor stroed
            in the i_th entry of the IDT.these values define the logical address of the first instruction of the 
            interrupt or exception handler.

      After the interrupt or exception is processed,the corresponding handler must relinquish control to the interrupted
      process by using the "iret" instruction.
      "iret" forces the CPU control unit to does :
        1>  load the cs,eip,and eflags registers with the values saved on the stack.
            if a hardware error code has been pushed in the stack on top of the eip contents,it must be popped
            before executing iret.
        2>  check whether the CPL(cs.CPL) of the handler is equal to the value contained in the two least significant bits of
            cs(the content of cs is saved on the stack).
            if cs.CPL == stack.cs.CPL
              the interrupted process was running at the same privilege level as the handler,
              "iret" concludes execution.
            else
              load the ss and esp registers from the stack and return to the stack associated with the old privilege level.
              examine the contents of the ds,es,fs,and gs segment registers :
                if any of them contains a selector that refers to a Segment Descriptor whose DPL values is lower than CPL,
                clear the corresponding segment register.
                /*  The CPU control unit does this to forbid User Mode programs that run with a CPL equal to 3 from using
                 *  segment registers previously used by Kernel routines(with a DPL equal to 0).
                 */

    Nested Execution of Exception and Interrupt Handlers :
      Kernel control paths may be arbitrarily nested;an interrupt handler may be interrupted by another interrupt handler.
      the last instructions of a kernel control path that is taking care of an interrupt do not always put the current
      process back into User Mode :
        if the level of nesting is greater than 1,these instructions will put into execution the kernel control path that
        was interrupted last,and the CPU will continue to run in Kernel Mode.

      For kernel control path is able to nested,an interrupt handler must never block !
      all the data needed to resume a nested kernel control path is stored in the Kernel Mode stack,which is tightly bound
      to the current process.(interrupt handler shares stack to interrupted process)

      An interrupt handler may preempt both other interrupt handlers and exception handlers,Conversely,an exception handler
      never preempts an interrupt handler !

      !  THE ONLY EXCEPTION THAT CAN BE TRIGGEED IN KERNEL MODE IS "Page Fault".
         /*  Page Fault exception have vector 14
          *  the coressponding exception handler page_fault() is inserted to IDT by set_intr_gate()
          *  page_fault() as an entry included in <entry_32.S>,the primary function do_page_fault() is
          *  defined in <arch/x86/mm/fault.c>.
          */

      !  Interrupt handlers never perform operations that can induce page faults,and thus,potentially,a process switch.
      !  In contrast to exceptions,interrupts issued by I/O devices do not refer to data structures specific to the 
         current process.

      Linux interleaves kernel control paths for two major reasons :
        1>  to improve the throughput of programmable interrupt controllers and device controllers.
            /*  device controller issues a signal on an IRQ line,
             *  PIC transforms it into an external interrupt,
             *  both PIC and device controller wait for CPU send an acknowledgment.
             *  interleaves kernel control paths can let kernel send the acknowledgment while it is 
             *  handling the previous interrupt.
             */
        2>  to implement an interrupt model without priority levels.
            /*  no longer some predefined levels between interrupts are necessary.  */

      !  On multiprocessor systems,several kernel control paths may execute concurrently,moreover,a kernel control
         path associated with an exception may start executing on a CPU and, due to a process switch,migrate to
         another CPU.

    Initializing the Interrupt Descriptor Table :
      before kernel enables interrupts,it must load the initial address of IDT into idtr register and initializes all
      entries of IDT.
      "int" instruction allows a User Mode process to issue an interrupt signal that has an arbitrary vector ranging
      from 0 to 255,kernel must prevent illegal interrupts and exceptions simulated by User Mode process via "int".
      /*  set DPL of gate descriptor up to 0,cs.CPL == 3 for User Mode process.  */

      !  in a few cases,a User Mode process must be able to issue a programmed exception,for this,must set the
         corresponding gate descriptor's DPL to 3.(that is as high as possible)

      Interrupt,Trap,and System Gates :
        Linux uses a slightly different breakdown and terminology from Intel when classifying the interrupt
        descriptors included in the Interrupt Descriptor Table :
          >  Interrupt gate
               an Intel interrupt gate that cannot be accessed by a User Mode process.(DPL = 0)
               all Linux interrupt handlers are activated by means of Interrupt gates,and all are
               restricted to Kernel Mode.

          >  System gate
               an Intel trap gate that can be accessed by a User Mode process.(DPL = 3)
               the three Linux exception handlers associated with the vectors 4,5,128 are activated by means of
               System gates,so the three assembly language instructions "into","bound",and "int $0x80" can be
               issued in User Mode.

          >  System interrupt gate
               an Intel interrupt gate that can be accessed by a User Mode process.(DPL = 3)
               the exception handler associated with the vector 3 is activated by means of a System interrupt gate,
               so the assembly language instruction "int3" can be issued in User Mode.
               !!  MUST BE AWARE THAT,KERNEL SYSTEM CALL MACRO SYSCALL_DEFINE<n>() IS USED TO WRAPPER A SYSTEM CALL
                   AND THE SYSTEM CALL "sys_<name>()"'S RETURN VALUE MAY DIFFERENCE TO "libc" WRAPPER RETURNS.
                   ANYWAY,THE USER SPACE PROGRAM JUST TO KNOWS WHAT "libc" RETURNS AS WELL.
                   /* IT IS DEPRECATE TO INVOKE SYSTEM CALL FROM KERNEL */

          >  Trap gate
               an Intel trap gate that cannot be accessed by a User Mode process.(DPL = 0)
               most Linux exception handlers are activated by means of Trap gates.

          >  Task gate
               an Intel task gate that cannot be accessed by a User Mode process.(DPL = 0)
               the Linux handler for the "Double fault" exception is activated by means of a Task gate.

      Architecture-dependent functions(just a little) were used to manipulate IDT :
        the gate entity is defined in <arch/x86/include/asm/desc_defs.h> !
        <arch/x86/include/asm/desc.h>
          /*  set_intr_gate - insert an Interrupt gate entry into IDT.
           *  @n:             index of IDT.
           *  @addr:          address of interrupt handler.
           */
          static inline void set_intr_gate(unsigned int n, void *addr);

          /*  set_system_trap_gate - insert a System trap gate entry into IDT.
           *  @n:                    index of IDT.
           *  @addr:                 trap handler address.
           */
          static inline void set_system_trap_gate(unsigned int n, void *addr);

          /*  set_system_intr_gate - insert a System interrupt gate entry into IDT.
          static inline void set_system_intr_gate(unsigned int n, void *addr);

          /*  set_trap_gate - insert a Trap gate entry into IDT.
          static inline void set_trap_gate(unsigned int n, void *addr);

          /*  set_task_gate - insert a Task gate entry into IDT.
           *  @n:             index of IDT.
           *  @gdt_entry:     the index of TSS which is contained in GDT and the function
           *                  to be activated is inside this TSS.
           *                  TSSD can appear only in GDT,when this gate is called,use the
           *                  index to pick the corresponding TSSD from GDT,and access to the TSS.
           *                  the Segment Selector inside Task gate stores the index value.
           *  #  I FOUND OUT Linux 2.6 ONLY USE TASK GATE TO DEAL WITH "Double fault" EXCEPTION,
           *     AND "GDT_ENTRY_DOUBLEFAULT_TSS" IS DEFINED WITH VALUE 31.
           *     OBJECT "doublefault_tss" IS DEFINED IN <arch/x86/kernel/doublefault_32.c>,IP
           *     REGISTER IS POINT TO "doublefault_fn"(DEFINED IN SAME FILE).
           *     THE GATE IS PLACED IN IDT WITH INDEX 8,AND THE SEGMENT DESCRIPTOR IS PLACED IN THE
           *     32nd ENTRY IN GDT.
           */
          static inline void set_task_gate(unsigned int n, unsigned int gdt_entry);

        #  all gate can be accessed only in Kernel Mode is attached __KERNEL_CS as the 
           Segment Selector.

      Preliminary Initialization of the IDT :
        IDT is initialized and used by the BIOS routines while the computer still operates in Real Mode.
        However,the IDT is moved to another area of RAM and initialized a second time after Linux take over
        control.(because Linux does not use any BIOS routine)

        IDT is stored in the @idt_table table,which includes 256 entries,the 6-byte @idt_descr variable stores
        both the size of the IDT and its address used in the system initialization phase when the kernel
        sets up the idtr register with the lidt assembly language instruction.

        assembly language function setup_idt() is used to initializes @idt_table,the 256 entries are filled with
        ignore_int and the corresponding interrupt gate.
        which function is defined in <arch/x86/kernel/head_32.S> :
          setup_idt:
                lea ignore_int,%edx                 #  load address of ignore_int into edx
                movl $(__KERNEL_CS << 16),%eax      #  kernel code segment
                movw %dx,%ax                        #  move the content in dx to ax
                movw $0x8E00,%dx                    #  interrupt gate - dpl = 0,present

                lea idt_table,%edi                  #  load address of idt_table into edi
                mov $256,%ecx                       #  set up counter

          rp_sidt:                                  #  repeat setup idt
                movl %eax,(%edi)                    #  the 4-byte content from 0--31
                movl %edx,4(%edi)                   #  the 4-byte content from 32--63
                addl $8,%edi                        #  iterating
                dec %ecx
                jne rp_sidt

          ...

          ignore_int:
                cld
                iret 

        ignore_int() :
          a null handler.
          #ifdef  CONFIG_PRINTK
            ignore_int saves the content of some registers in the stack ->
            invokes printk() to print an "Unknown interrupt" system message ->
            restores the register contents from the stack ->
            executes an "iret" instruction to restart the interrupted program.

          !  the ignore_int() handler should never be executed.if it is executed,that denotes either a 
             hardware problem(such an I/O devices is issuing unforeseen interrupts) or a kernel problem
             (such an interrupt or exception is not being handled properly).

        kernel replaces some of the null handlers with meaningful trap and interrupt handlers at the second
        phase to set up IDT.

        !  once this is done,the IDT includes a specialized interrupt,trap,or system gate for each different
           exception issued by the control unit and for each IRQ recognized by the interrupt controller.

    Exception Handling :
      most exceptions issued by the CPU are interpreted by Linux as error conditions.
      for example,CPU raises a "Divide error" exception,and the corresponding exception handler sends
      a SIGFPE signal to the process caused this exception,then it takes the necessary steps to recover or abort.

      Linux exploits CPU exceptions to manage hardware resources more efficiently :
        >  Linux use "Device not available" exception and cr0.TS flag give rise to load the floating point registers
           of the CPU with new values.
        >  Linux use "Page Fault" exception to defer allocating new page frames to the process until the last
           possible moment.(that is,schduler will take a place in the case)
           !  "Page Fault" exception handler is very complex,because this exception may or may not denote an
              error condition.

      Standard structure of steps that exception handler takes :
        1>  save the contents of most registers in the Kernel Mode stack(coded in assembly language)
        2>  handle the exception by means of a high-level C function.
        3>  exit from the handler by means of the ret_from_exception() function.

      trap_init() :
        Linux use this function to setup IDT entries that refer to nonmaskable interrupts and exceptions.

        <arch/x86/kernel/traps.c>
          /*  trap_init -  the x86 architecture specific trap initialization routine.
           *  #  this function is call to set_trap_gate(),set_intr_gate(),set_system_gate(),
           *     set_system_intr_gate(),set_task_gate() .etc to accomplishes trap initialization.
           *     finally,call to x86_init_irqs.trap_init() to finishes the architecture dependent
           *     trap initializing works.
           */
          void __init trap_init(void);

          !  "Double fault" exception handler is set up by cpu_init() function and which is called by trap_init().
             the "Double fault" exception is handled by means of a task gate instead of a trap or system gate,
             because it denotes a serious kernel misbehavior.thus,the exception handler that tries to print out the
             register values does not trust the current value of the esp register.
             when this exception occurred,CPU executes the doublefault_fn() exception handler on its own private stack
             (specified by TSS)

      Saving the Registers for the Exception Handler : 
        each exception handler starts with the following assembly language instructions :
          @handler_name:
                pushl $0                    #  pad stack with null value,if control unit is not insert a hardware error code
                                            #  on the stack automatically when the exception occurs.
                pushl $do_handler_name      #  address of high-level C function
                jmp error_code              #  the assembly code of "error_code" label will call to the function "do_##name()".

        /*  defined in <arch/x86/kernel/entry_32.S>  */
        /*  entry definition in <entry_64.S> is diff to <entry_32.S>,
         *  the assembly macro "errorentry"/"paranoidentry"/"zeroentry" are used to define 
         *  exception entry points.
         */
        
        the assembly language fragment labeled as "error_code" is the same for all exception handlers except the one for 
        the "Device not available" exception.(Linux 2.6,it is same as others)
        the code performs the following steps : (these are the default actions in Linux 2.6 for all exception handlers)
          1>  saves the registers that might be used by the high-level C function on the stack.
          2>  execute "cld".
          3>  copied the hardware error code saved in the stack at location esp+36 in edx.
              stores the value -1 in the same stack location,this value is used to separate 0x80 exceptions
              from other exceptions.
          4>  loads edi with the address of the high-level "do_##name()" C function saved in the stack at the
              location esp+32,writes the contents of es in that stack location.
          5>  loads in the eax register the current top location of the Kernel Mode stack.
              this address identifies the memory cell containing the last register value saved in step1.
          6>  loads the user data Segment Selector into the ds and es registers.
          7>  invokes the high-level C function whose address is now stored in edi.

        #  the invoked function receives its arguments from the eax and edx registers rather than from the stack.
        #  all "do_##name()" high-level handlers are call to do_trap() function,which is defined in 
           <arch/x86/kernel/traps.c>

      Entering and Leaving the Exception Handler :
        Kernel enters an exception handler through IDT,and then the correpsonding assembly function
        is going to invoke "do_##name()" the high-level C function,it finally call to do_trap() function.

        do_trap() :
          <arch/x86/kernel/traps.c>
            /*  do_trap - generic exception handling interface.
             *  @trapnr:  number of excpetion.
             *  @signr:   the signal must send to the process which cause the exception.
             *  @str:     information.
             *  @regs:    registers contents.
             *  @error_code:  error code.
             *  @info:    signal information.
             *  #  @regs is passed by the exception handler which is in assembly language.
             *  #  DO_ERROR() and DO_ERROR_INFO() macros are used to define "do_##name()" the
             *     high-level C functions.
             */
            static void __kprobes do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
                                          long error_code, siginfo_t *info);
  
          do_trap() will does :
            1>  retrieve current process's task_struct object.
            2>  @tsk->thread.error_code = @error_code;
                @tsk->thread.trap_no = @trapnr;
                if !@info
                      force_sig(@signr, @tsk);
                else
                      force_sig_info(@signr, @info, @tsk);
                the signal will be handled either in User Mode by the process's own signal handler or
                in Kernel Mode(default action).
            3>  it is always checks whether the exception occurred in User Mode or in Kernel Mode.
                in the Kernel Mode,have to checks whether it was due to an invalid argument passed to
                a system call.(different system call deal with invalid argument is difference)
                any other exception raised in Kernel Mode is due to a kernel bug,this denote kernel
                is misbehaving,then call to die() function to prevent data corruption and prints the 
                contents of all CPU registers on the console(this dump is called kernel oops),
                the current process have to be terminated by calling do_exit().
  
          #  when the C function that implements the exception handling terminates,the code performs a
             "jmp" instruction to the ret_from_exception() function.(return to the interrupted control path)
               
    Interrupt Handling :
      Interrupt handling is different to Exception Handling,so it would make no sense to send a Unix signal to the 
      current process.

      Interrupt handling depends on the type of interrupt :
        I/O interrupts :
          An I/O device requires attenton;the corresponding interrupt handler must query the device to determine the
          proper course of action.

        Timer interrupts :
          some timer,either a local APIC timer or an external timer,has issued an interrupt;this kind of interrupt
          tells the kernel that a fixed-time interval has elapsed.

        Interprocessor interrupts :
          A CPU issued an interrupt to another CPU of a multiprocessor system.

      I/O Interrupt Handling :
        in general,an I/O interrupt handler must be flexible enough to service several devices at the same time.
        some devices might share a vector and a IRQ line.(vector 43 for USB port and Sound Card)

        Interrupt handler flexibility is achieved in two distinct ways :
          1>  IRQ sharing
                the interrupt handler executes several interrupt service routines(ISRs).
                each ISR is a function related to a single device sharing the IRQ line.
                it is not possible to know in advance which particular device issued the IRQ,each ISR is
                executed to verify whether its device needs attention;if so,the ISR performs all the operations
                that need to be executed when the device raises an interrupt.

          2>  IRQ dynamica allocation
                An IRQ line is associated with a device driver at the last possible moment.
                that is IRQ line is allocated on demand(the time need to access the device).
                in this way,the same IRQ vector may be used by several hardware devices even if they cannot
                share the IRQ line;of course,the hardware devices cannot be used at the same time.

        not all actions to be performed when an interrupt occurs have the same urgency.
        when an interrupt is handling,the signals on the corresponding IRQ line are temporarily ignored,
        the interrupted process stay in the TASK_RUNNING state,a system freeze is possible to occur.

        Long noncritical operations should be deferred,blocking procedure must to be prevented.

        Actions for interrupt on Linux:
          1>  Critical
                actions such as acknowledging an interrupt to the PIC,reprogramming the PIC or the device controller,
                or updating data structures accessed by both the device and the processor.
                such actions must be performed as soon as possible(these can be executed quickly and are critical).
                critical actions are executed within the interrupt handler immediately,with maskable interrupts disabled.

          2>  Noncritical
                actions such as updating data structures that are accessed only by the processor.
                these actions can also finish quickly,so they are executed by the interrupt handler immediately,with
                the interrupts enabled.

          3>  Noncritical deferrable
                actions such as copying a buffer's contents into the address space of a process.
                these may be delayed for a long time interval without affecting the kernel operations;
                the interested process will just keep waiting for the data.

        Four basic actions all I/O interrupt handlers do :
          1>  save the IRQ value and the register's contents on the Kernel Mode stack.
          2>  send an acknowledgment to the PIC that is servicing the IRQ line,thus allowing it to issue
              further interrupts.
          3>  execute the interrupt service routines(ISRs) associated with all the devices that share the IRQ.
          4>  terminate by jumping to the ret_from_intr() address. 

      Interrupt vectors :
        physical IRQs may be assigned any vector in the range 32-238.however,Linux uses vector 128 to implement
        system calls.

        #  IBM-compatible PC architecture requires that some devices be statically connected to specific IRQ lines.
           in particular :
             interval timer device --> IRQ 0 line
             slave 8259A PIC --> IRQ 2 line
             external mathematical coprocessor --> IRQ 13 line
             an I/O device can be connected to a limited number of IRQ lines.

        interrupt vectors in Linux :
          vector range                      use
          [0, 19](0x0 - 0x13)               nonmaskable interrupts and exceptions
          [20, 31](0x14 - 0x1f)             intel-reserved
          [32, 127](0x20 - 0x7f)            external interrupts (IRQs)
          [128](0x80)                       programmed exception for system calls
          [129, 238](0x81 - 0xee)           external interrupts (IRQs)
          [239](0Xef)                       local APIC timer interrupt
          [240](0xf0)                       local APIC thermal interrupt
          [241, 250](0xf1 - 0xfa)           reserved by Linux for future use
          [251, 253](0xfb - 0xfd)           interprocessor interrupts
          [254](0xfe)                       local APIC error interrupt
          [255](0xff)                       local APIC spurious interrupt

        three ways to select a line for an IRQ-configurable device :
          1>  by setting hardware jumpers
          2>  by a utility program shipped with the device and executed when installing it.
              such a program may either ask the user to select an available IRQ number or probe the
              system to determine an available number by itself.
          3>  by a hardware protocol executed at system startup.
              peripheral devices declare which interrupt lines they are ready to use;the final values
              are then negotiated to reduce conflicts as much as possible.
              once this is done,each interrupt handler can read the assigned IRQ by using a function
              that accesses some I/O ports of the device.

        !  kernel must discover which I/O device corresponding to the IRQ number before enabling interrupts.

      IRQ data structures :
        <linux/irq.h>
          typedef (*irq_flow_handler_t)(unsigned int irq, struct irq_desc *desc);

          /*  struct irq_desc - the irq descriptor structure represents an interrupt.
           *  @irq:             interrupt number for this descriptor.
           *  @handler_irq:     high-level irq-events handler [if NULL, __do_IRQ()].
           *  @action:          the irq action chain.
           *  @status:          status information.
           */
          struct irq_desc {
                unsigned int irq;
                ...
                irq_flow_handler_t handler_irq;
                ...
                struct irqaction *action;
                unsigned int status;
                ...                
          };
        <kernel/irq/handle.c>
          /*  irq_desc - the irq_desc structure array used to establishes relationship
           *             between IRQ vector and IRQ descriptor.
           *  @NR_IRQS:  number of IRQs in the system,architecture dependent,
           *             if undefined,then it is defined with 64.
           *  #  the @handler_irq field for per-irq_desc is setup to "handle_bad_irq",
           *     and @status field is setup to "IRQ_DISABLED".
           */
          struct irq_desc irq_desc[NR_IRQS];

          /*  Linux 2.6 NO "irq_desc_t" DEFINITION.  */

        An interrupt is "unexpected" if it is not handled by the kernel,that is,either if there is
        no ISR associated with the IRQ line,or if no ISR associated with the line recognizes the interrupt
        as raised by its own hardware device.
        in this case,kernel checks the IRQ line number which raised "unexpected" interrupt and disable the line.
        (prevent a faulty hardware device keeps raising an interrupt)
        /*  because the IRQ line can be shared among several devices,the kernel does not disable the line as soon as
         *  it detects a single unhandled interrupt,rather,the kernel stores in the @irq_count and @irqs_unhandled fields
         *  of the "irq_desc" structure the total number of interrupts and the number of unexpected interrupts,respectively.
         *  #  kernel checks these fields,if the value exceed the threshold,kernel disable the IRQ line.
         */

        sample graph :
          +------+  +-----------+
          |(dev1)|  |  (dev2)   |
          | IRQ0 |  | IRQ0 IRQ1 |  (external devices' IRQ lines,physical circuit)
          +------+  +-----------+
             |            |
             |      =============
             |         |
             V         V
           +---+     +---+
           |PIC|     |PIC|         (hardware Programmable Interrupt Controller)
           +---+     +---+
             |         |           (INT output line)
          ====I/O APIC======
                 |
                 V
          ====ICC===========
            |         |
          +------+  +------+
          | CPU1 |  | CPU2 |
          |lAPIC |  |lAPIC |    (local APIC)
          +------+  +------+
            |         |
          ==================
            |         |   
            |         |   
            V         V
           +--+      +--+
           |37|      |82|       (dev1.IRQ0 <=> vector 82, dev2.IRQ0 <=> vector 82, dev2.IRQ1 <=> vector 37)
           +--+      +--+
            |         |
          +-------+  +-------+
          |irqd_37|  |irqd_82|
          +-------+  +-------+
            |         |
          +----+      |
          |ISR9|     =========  (ISR9 handles vector = 37 from dev2)
          +----+      |     |
                      |     |
                   +----+  +----+
                   |ISR4|  |IRS7|  (ISR4 handles vector = 82 from dev1, ISR7 handles vector = 82 from dev2)
                   +----+  +----+

          #  the mapping between IRQ line number and the vector is able to be altered via issue an
             I/O instruction to PIC.
          #  the vector assignable in the scope [32, 238] is because some nonmaskable interrupts and exceptions
             have the fixed vector.
        
        Status of an IRQ line :
          IRQ_INPROGRESS                a handler for the IRQ is being executed
          IRQ_DISABLED                  the IRQ line has been deliberately disbled by a device driver
          IRQ_PENDING                   an IRQ has occurred on the line,its occurrence has been acknowledged to the PIC,
                                        but it has not yet been serviced by the kernel
          IRQ_REPLAY                    the IRQ line has been disabled but the previous IRQ occurrence has not yet been
                                        acknowledged to the PIC
          IRQ_AUTODETECT                the kernel is using the IRQ line while performing a hardware device probe
          IRQ_WAITING                   the kernel is using the IRQ line while performing a hardware device probe,
                                        moreover,the corresponding interrupt has not been raised
          IRQ_LEVEL                     not used on the 80x86 architecture(IRQ level triggered)
          IRQ_MASKED                    not used(IRQ masked - should not be seen again)
          IRQ_PER_CPU                   not used on the 80x86 architectur(IRQ is per-cpu)
                                        /*  if an interrupt request is per CPU type,then disable it is only
                                         *  disabled on local cpu,another were not affected.
                                         */

          @depth field and IRQ_DISABLED specify whether the IRQ line is enabled or disabled.
          <linux/interrupt.h>
            /*  disable_irq - disable IRQ line,wait until all interrupt handlers for IRQ_irq
             *                have completed before running.
             *  @irq:         the IRQ line number.
             */
            extern void disable_irq(unsigned int irq);

            /*  disable_irq_nosync - asynchronous version.  */
            extern void disable_irq_nosync(unsigned int irq);

            #  these two functions are let @depth increase,if @depth == 0,the functions disable theIRQ line
               and set its IRQ_DISABLED flag(this is give rise before @depth increase). 

            /*  enable_irq - enable a IRQ line.
             *  #  this function decrease @depth,if @depth == 0,then enable the IRQ line and clean IRQ_DISABLED flag.
             */
            extern void enable_irq(unsigned int irq);

          @status of each element in irq_desc array is initialized to IRQ_DISABLED during system initializing,this is
          happens in "init_IRQ()" it is defined in <arch/x86/kernel/irqinit.c>(this function also initializes IDT).

        Linux supports several PIC,so kernel designer have used object-oriented approach to encapsulate PIC object.
        In older kernel,it is named "hw_interrupt_type",but in Linux 2.6,it is named "irq_chip".
        <linux/irq.h>
          /*  struct irq_chip - hardware interrup chip descriptor.
           *  @name:            name for /proc/interrupts .
           *  @startup:         start up the interrupt.
           *  @shutdown:        shut down the interrupt.
           *  @enable:          enable the interrupt.
           *  @disable:         disable the interrupt.
           *  @ack:             start of a new interrupt.
           *  ...
           *  @typename:        obsoleted by name,kept as migration helper. 
           */
          struct irq_chip {
                const char *name;
                unsigned int (*startup)(unsigned int irq);
                void (*shutdown)(unsigned int irq);
                void (*enable)(unsigned int irq);
                void (*disable)(unsigned int irq);
                void (*ack)(unsigned int irq);
                ...
                const char *typename;
          };  /*  each irq_desc.chip is points to a irq_chip object.  */
          /*  if @startup is nullptr,then @startup = @enable,
           *  if @shutdown is nullptr,then @shutdown = @disable
           */

        Multiple devices can share a single IRQ,so kernel maintains "irqaction" structure for a
        specific hardware device and a specific interrupt.
        <linux/interrupt.h>
          typedef irqreturn_t (*irq_handler_t)(int, void *);

          /*  struct irqaction - per interrupt action descriptor.
           *  @handler:          interrupt handler function.
           *  @flags:            flags(IRQF_*).
           *  @name:             name of the device.
           *  @dev_id:           cookie to identify the device.
           *  @next:             next irqaction for shared interrupts.
           *  @irq:              interrupt number.
           *  @dir:              pointer to the /proc/irq/NN/name entry.
           *  @thread_fn:        interrupt handler function for threaded interrupts.
           *  @thread:           thread pointer for threaded interrupts.
           *  @thread_flags:     flags related to @thread.
           *  #  Linux use this descriptor to represents an ISR.
           *     the devices sharing the single IRQ line,every interrupt handler
           *     for the device is encapsulated in an "irqaction" object.
           */
          struct irqaction {
                irq_handler_t handler;
                unsigned long flags;
                const char *name;
                void *dev_id;
                struct irqaction *next;
                int irq;
                struct proc_dir_entry *dir;
                irq_handler_t thread_fn;
                struct task_struct *thread;
                unsigned long thread_flags;
          };

          some values for irqaction.flags :
            IRQF_DISABLED           keep irqs disabled when calling the action handler
            IRQF_SHARED             allow sharing the irq among several devices
            IRQF_SAMPLE_RANDOM      irq is used to feed the random generator

        Kernel use a structure named "irq_cpustat_t" to keep track of what each CPU is currently doing,
        and an array named "irq_stat" save "irq_cpustat_t" structure object the size is consistent to NR_CPUS.
        <arch/x86/include/asm/hardirq.h>
          /*  struct irq_cpustat_t - structure used by kernel to keep track of CPU in the system.
           *  @__softirq_pending:    whether there is a softirq is pending.
           *  @__nmi_count:          number of occurrences of NMI interrupts.
           *  @irq0_irqs:            nmi watchdog.
           *  @apci_timer_irqs:      number of occurrences of local APIC timer interrupts.
           *  ...
           */
          typedef struct {
                unsigned int __softirq_pending;  /*  per-CPU 32-bit mask describing the pending softirqs  */
                unsigned int __nmi_count;
                unsigned int irq0_irqs;
                unsigned int apic_timer_irqs;  /*  #ifdef CONFIG_X86_LOCAL_APIC  */
                ...
          } ____cacheline_aligned irq_cpustat_t;

        <kernel/softirq.c>  <linux/irq_cpustat.h>
        #ifndef __ARCH_IRQ_STAT
          /*  irq_stat - irq_cpustat_t array for per-cpu.  */
          irq_cpustat_t irq_stat[NR_CPUS] ____cacheline_aligned;
          EXPORT_SYMBOL(irq_stat);
        #endif

      IRQ distribution in multiprocessor systems :
        when a hardware device raises an IRQ signal,the multi-APIC system selects one of the CPUs
        and delivers the signal to the corresponding local APIC,which in turn interrupts its CPU,no other
        CPUs are notified of the event.

        during system bootstrap,the booting CPU executes the "static void __init setup_IO_APIC_irqs(void)"
        which is defined in <arch/x86/kernel/apic/io_apic.c>,this function initializes the I/O APIC chip,
        the Interrupt Redirection Table of the chip is filled(24-entry).
        
        during system bootstrap,after I/O APIC chip had initialized,all CPUs execute the
        "void __cpu_init setup_local_APIC(void)" which is defined in <arch/x86/kernel/apic/apic.c>,
        this function initializes the local APICs.the task priority register(TPR) of each chip is 
        initialized to a fixed value,the CPU is willing to handle every kind of IRQ signal,regardless of 
        its priority,and the linux kernel never modifies the value after its initialization.

        #  all of these works are done by the hardware.
           but in some cases,the IRQs is distributed in a unfair way to the CPUs by hardware.
           Linux 2.6 makes use of a special kernel thread called kirqd to correct,if necessary,the 
           automatic assignment of IRQs to CPUs.

        <arch/x86/kernel/apic/io_apic.c>
          /*  set_ioapic_affinity_irq - set affinity for a specific irq.
           *  @irq:                     irq vector.
           *  @mask:                    the CPUs that can receive the IRQ.
           *  #  system administrator is able to change the irq affinity through write a 
           *     new CPU bitmap mask into the "/proc/irq/@vector/smp_affinity" file.
           */
          static int set_ioapic_affinity_irq(unsigned int irq, const struct cpumask *mask);

        #  kirqd periodically executes the "do_irq_balance()" function,which keep track of the
           number of interrupt occurrences received by every CPU.
           if imbalance has detected,then "move" IRQ from one CPU to another least loaded CPU,
           or rotates all IRQs among all existing CPUs.

      Multiple Kernel Mode stacks :
        task_struct.stack -> thread_union := { thread_info, stack }(4kB or 8kB)
        task_struct.thread_info => points to the thread_info object associated to current task_struct

        if the size of the thread_union union is 8kB,the Kernel Mode stack of the current process
        is used for every type of kernel control path :
          exceptions, interrupts, deferrable functions

        if the size of the thread_union union is 4kB,the kernel makes use of three types of Kernel Mode
        stacks :
          >  the exception stack is used when handling exceptions(including system call).
             different process has different thread_union,and the stack is contained in it,thus
             kernel makes use of a different exception stack for each process in the system.

          >  the hard IRQ stack is used when handling interrupts.
             there is one hard IRQ stack for each CPU in the system,and each stack is contained in a
             single page frame.

          >  the soft IRQ stack is used when handling deferrable functions.
             there is one soft IRQ stack for each CPU in the system,and each stack is contained in a 
             single page frame.

        #  the per-cpu data @hardirq_stack is type of "union irq_ctx",the union is defined in
           <arch/x86/kernel/irq_32.c>
             union irq_ctx {
                    struct thread_info tinfo;               /*  higher address  */
                    u32 stack[THREAD_SIZE / sizeof(u32)];   /*  lower address  */
             } __attribute__((aligned(PAGE_SIZE)));
           and per-cpu data @softirq_stack also is type of "union irq_ctx".
           @hardirq_stack is used for hard IRQ,and @softirq is used for deferrable functions.
           stack grows towards lower address.

      Saving the registers for the interrupt handler :
        saving registers is the first task of the interrupt handler.
        the address of the interrupt handler for IRQn is initially stored in the @interrupt[n] entry and
        then copied into the interrupt gate included in the proper IDT entry.

        the @interrupt array is build through assembly language instructions and which is defined in
        <arch/x86/kernel/entry_32.S>.
        the array contains NR_IRQS elements(Linux 2.6,NR_VECTORS = 256,FIRST_EXTERNAL_VECTOR = 32,so NR_IRQS = 256 - 32).
        the element at index @n in the array stores the address of the following two assembly language instructions :
          pushl $n-256
          jmp common_interrupt

          /*  Linux 2.6 constructs such entries via a more complex way :
           *  <linux/linkage.h>
           *    #define ENTRY(name)  \
           *        .global name;
           *        ALIGN;
           *        name:
           *
           *    #ifndef END
           *    #define END(name)    \
           *        .size name, .-name
           *    #endif
           *
           *    ----------------------
           *
           *  <arch/x86/kernel/entry_32.S>
           *    .section .init.rodata,"a"
           *    ENTRY(interrupt)
           *    .text
           *        ...             #  now in .init.text subsection
           *    ENTRY(irq_entries_start)
           *      ...
           *      vector=FIRST_EXTERNAL_VECTOR      #  vector = FIRST_EXTERNAL_VECTOR = 32
           *      .rept (NR_VECTORS-FIRST_EXTERNAL_VECTOR+6)/7  #  int(result) = 32
           *        .balign 32      #  position counter forwards to next address @p | 32
           *        .rept 7         #  repeat sequence of code line between next ".endr" 7 times
           *          ...
           *    1:    pushl $(~vector+0x80)     #  vector number
           *          .if ((vector-FIRST_EXTERNAL_VECTOR)%7) <> 6   #  if (vector-32)%7 != 6 ?
           *            jmp 2f      #  '2' is a label,and suffix 'f' means 'f'orward search '2'
           *                        #  immediate value 2 => $2
           *                        #  address 2 => 0x02
           *                        #  near jump
           *          .endif
           *          .previous     #  continues processing of the previous section.
           *            .long 1b    #  'b'ackward search '1','1' is a label.
           *                        #  now in .init.rodata subsection
           *          .text         #  .text subsection
           *            vector=vector+1     #  update vector for next entry
           *                                #  now in .init.text subsection
           *          ...
           *        .endr
           *    2:      jmp common_interrupt    #  jump to common_interrupt program
           *      .endr
           *   END(irq_entries_start)
           *   .previous            #  now in .init.text subsection
           *   END(interrupt)
           *   #  pack 7 stubs into a single 32-byte chunk,and the pointer points to the handler
           *      for the chunk is attached right after it("jmp common_interrupt").
           *   #  .init.rodata has an array :
           *        [index0] := label 1  (an address of an opcode)  #  now @vector = 32
           *        [index1] := label 1  #  the opcode been compiled and @vector is updated.    
           *        ...                  #  the total number of elements in this array is 224.
           *                             #  elements in vector range 0--31 has been initialized by
           *                             #  trap_init().
           *   #  .init.text :
           *        irq_entries_start:  #  a symbol
           *          pushl $(~vector+0x80)     #  the first stub in the 32-byte chunk
           *          jmp 2f                    #  the first stub in the 32-byte chunk
           *          ...                       #  repeat 5 times
           *          pushl $(~vector+0x80)     #  the seventh stub in the 32-byte chunk
           *          jmp common_interrupt      #  the secenth stub in the 32-byte chunk
           *                                    #  ! AND 2f IS THE ADDRESS OF THIS OPCODE
           *        #  all 224 stubs.the exception special stubs had been initialized by trap_init()
           #        #  only external interrupts are placed there.
           */

        Linux use positive number to identify a system call,so the negative number is used to identify
        an interrupt.

        common_interrupt:
          addl $-0x80,(%esp)        # adjust interrupt vector to [-256, -1]
          SAVE_ALL
          TRACE_IRQS_OFF
          movl %esp,%eax
          call do_IRQ               # the corresponding ISR will be called by do_IRQ()
          jmp ret_from_intr
        ENDPROC(common_interrupt)   #  a macro function is defined in <linux/linkage.h>,
                                    #  it will have expended to 
                                    #    .type common_interrupt,@function
                                    #    END(common_interrupt) =>
                                    #      .size common_interrupt,.-common_interrupt
          SAVE_ALL :
            an assembly language macro.
            saves the contents of all registers that the interrupt handler maybe use on the Kernel Mode stack.
            /* through "push" instruction */

            ... # save all registers' contents on Kernel Mode stack 

            movl $(__USER_DS),%edx  #  user data segment
            movl %edx,%ds
            movl %edx,%es
            movl $(__KERNEL_PERCPU),%edx
            movl %edx,%fs
            SET_KERNEL_GS %edx      #  movl $(__KERNEL_STACK_CANARY),%edx
                                    #  movl %edx,%gs
                                    #  __KERNEL_STACK_CANARY is defined in <arch/x86/include/asm/segment.h>,
                                    #  it is equal to GDT_ENTRY_STACK_CANARY * 8

            !  eflags,cs,eip,ss,and esp would not be saved,they are already saved automatically by the
               CPU control unit.

          after SAVE_ALL accomplished,the stack be like :
            [contents of registers]     <=  esp
            [interrupt vector]
            [former contents]
            /*  [eip] if @common_interrupt is executed through "call" instruction,but it is not  */
            ...

      do_IRQ() function :
        <arch/x86/include/asm/irq.h> <arch/x86/kernel/irq.c>
          /*  do_IRQ - do interrupt request,@common_interrupt call to this function.
           *  @regs:   pointer points to pt_regs object,the structure saveing the
           *           contents of registers.
           *  return:  always return 1.
           *  #  because this function is called by @common_interrupt,thus,the content of eax
           *     is the content of esp,which is pointing the saved contents of registers by
           *     @SAVE_ALL.
           *     the top element on the stack is content of ebx,and the first member in pt_regs
           *     is named @bx with type of unsigned long.
           *     the member @orig_ax saved the vector of current irq,because its content is the 
           *     content of esp before @SAVE_ALL had called.
           *  #  do_IRQ() receives the argument through eax register
           */
          extern unsigned int __irq_entry do_IRQ(struct pt_regs *regs);

        the high-level IRQ handler entry point :
          1>  save the old registers in @old_regs.

          2>  get @vector from @orig_ax.

          3>  get @irq from a per-cpu data which is an array named @vector_irq,the elements in it
              is the @irq,and index is @vector.
              @vector_irq is initialized by "__setup_vector_irq()",it use a "for" cycle to initializes
              this array for a specific cpu.
              macro "for_each_irq_desc" is defined in <linux/irqnr.h>.
              @irq is start from 0 to NR_IRQS,but @vector maybe not,because the value of @vector as
              an index for @vector_irq is got from desc->chip_data,which is type of void pointer,and
              it will points to the device PIC cfg data.(through PIC,the vector is mutable)
              #  the vector is able to be used by devices is in the range [32, 238].(local APIC enabled)

          4>  call to "irq_enter()" start tracing.
              a counter representing the number of nested interrupt handles was increased.
              the counter is stored in the @preempt_count filed of the thread_info structure of 
              current process.

          5>  call to "handle_irq(@irq, @regs)" which is defined in <arch/x86/kernel/irq_32.c>.

          6>  if function "handle_irq()" was failed to retrieve irq descriptor via @irq,then returns
              "false",in this case,"ack_APIC_irq()" was called to acknowledge the @irq(it is not
              be handled,because no irq descriptor was find out).
              error message is printed.
          
          7>  call to "irq_exit()" stop tracing.
              decrease the interrupt counter and checks whether deferrable kernel functions are waiting
              to be executed.

          8>  restore registers.

          9>  return control to @common_interrupt,and "ret_from_intr()" will be called.

          #  if "handle_irq()" is succed to handles,the @irq must been acknowledged.
          #  "handle_irq()" checks whether on an interrupt stack now,if current stack is a IRQ stack,
             then keep using this IRQ stack,and call to desc->handle_irq(@irq, @desc);
             otherwise,switch to the IRQ stack for current CPU and then exchange ebx and esp,call to
             desc->handle_irq(@irq, @desc),exchange ebx and esp again after "handle_irq()" is completed.
             
             /*  it is on an interrupt stack now,that is,a hardirq handler is interrupted.  */
             /*  the current task is stored in @irqctx->tinfo.task
              *  the previous stack is stored in @irqctx->tinfo.previous_esp
              *  and the softirq bits in @preempt_count was copied,thus,the softirq checks work in
              *  the hardirq context
              */

          #  there is some difference for x86_64 platform about "handle_irq()" function.
             on x86_64,"handle_irq()" will calls to "generic_handle_irq_desc()" function which
             is defined in <linux/irq.h> as a static inline function.
             that function checks whether desc->handle_irq is NULL,if it is,then call to "__do_IRQ()"
             function;otherwise call to desc->handle_irq.
          #  the function "__do_IRQ()" is the original all in one high-level IRQ handler.
             but this function is deprecated.
             this function is existed only CONFIG_GENERIC_HARDIRQS_NO__DO_IRQ is off.                

        /*  The actions to take while desc->handle_irq is NULL :
         *    x86_32:  do nothing except to acknowledge PIC.
         *    x86_64:  call to __do_IRQ().
         */

      __do_IRQ() function : (Effectively with CONFIG_GENERIC_HARDIRQS_NO__DO_IRQ off)
        <linux/irq.h> <kernel/irq/handle.c>
          #ifndef CONFIG_GENERIC_HARDIRQS_NO__DO_IRQ
          /*  __do_IRQ - the generic hardware interrupt request handler.
           *  @irq:      IRQ number.
           *  return:    always return 1.
           */
          extern unsigned int __do_IRQ(unsigned int irq);
          #endif

        the works "__do_IRQ()" does as the following :
          1>  retrieve the corresponding interrupt descriptor via @irq.and increase the cpu_usage_stat.irq of current
              CPU via call to kstat_incr_irqs_this_cpu()<linux/kernel_stat.h>.

          2>  if this @irq is IRQ_PER_CPU and current CPU has not disabled it,then handles it through
              "handle_IRQ_event()"<kernel/irq/handle.c>.
              acknowledge maybe occurs before handle if desc->chip->ack is not NULL,otherwise it occurs after
              handle via desc->chip->end.
              finally return 1.

          3>  if this @irq is not IRQ_PER_CPU,that means the interrupt is global.
              then ready to handles it.

          4>  acquires spin lock of interrupt descriptor at first,
              if desc->chip->ack is not NULL,acknowledge it.
              clear IRQ_REPLAY | IRQ_WAITING in the @desc->status.
                IRQ_REPLAY is when Linux resends an IRQ that was dropped earlier.
                IRQ_WAITING is used by probe to mark irqs that are being tested.
              set IRQ_PENDING,ready to handle.

          5>  checks whether IRQ_DISABLED | IRQ_INPROGRESS is setted.
              if it is,then do not handle this @irq;
              otherwise,get ISR,clear IRQ_PENDING and set IRQ_INPROGRESS,that is,the @irq is ready to be
              serviced.

          6>  enter a loop to process ISR.
              before "handle_IRQ_event()" is called with the ISR,spin lock of the interrupt descriptor must be
              released.
              this allows the second @irq is occurred while in handler or in "do_IRQ()",so the second @irq will
              be serviced after the previous @irq is handled completely.
              BUT THIS LOOP ONLY HANDLE THE SECOND INSTANCE,NEITHER THE THIRD,NOR THE FOURTH...
              after "handle_IRQ_event()" returned,the kernel control path acquire the spin lock and check if
              IRQ_PENDING is setted,if it is,that means the SECOND came in,then continue to handles it.(clear
              IRQ_PENDING again)
              if there is no more @irq came in,loop will ends.

          7>  call to desc->chip->end to end an interrupt request.
              release spin lock.
              return 1.

        #  "handle_IRQ_event()" is run with local cpu irq disabled,that is "__do_IRQ()" is run with local cpu irq
           disabled.
           the CPU control unit automatically clears eflags.IF because the interrupt handler is invoked through an
           IDT's interrupt gate.(IF indicates if CPU receives maskable interrupt)
           but after "handle_IRQ_event()" finished,the local cpu irq will be enabled again.

        #  why not service the third or the fourth?
           because the acknowledge is deferred before __do_IRQ() exit.(no ack() was called while ISR running)
           so the local APIC of current CPU would not accept further interrupts.(I/O APIC)
           although further occurrences of this type of interrupt may be accepted by other CPU.
        #  everytime "handle_IRQ_event()" returned,the function "note_interrupt()" is called if @noirqdebug is false.
           that function updates some counter in the interrupt descriptor,and if the bad irq exceeded the threshold
           on this IRQ line,the kernel control path will disable the IRQ line.
           /*  <kernel/irq/spurious.c>  */

      Reviving a lost interrupt :
        an interrupt lost :
          CPU0 received an @irq,on IRQ56 =>
          CPU1 disabled IRQ56 before CPU0 acknowledge to the PIC the @irq from =>
          CPU0 call to "do_IRQ()",but it finds IRQ_DISABLED is setted =>
          "do_IRQ()" returned with nothing have done except acknowledge the @irq =>
          the interrupt request @irq lost.

        <linux/irq.h> <kernel/irq/manage.c>
          /*  enable_irq - enable an IRQ line which corresponding to the @irq.
           *  @irq:        IRQ number.
           *  #  this function call to __enable_irq(irq_to_desc(irq), irq, false) with held
           *     chip_bus_lock and raw_spin_lock_irqsave,and release them before exit.
           *  #  if desc is NULL,then nothing to do.
           */
          extern void enable_irq(unsigned int irq);  /*  exported  */

          /*  __enable_irq - internal routine for enable an irq.
           *  @desc:         interrupt descriptor.
           *  @irq:          irq number.
           *  @resume:       bool value indicates whether resume from suspended before enable.
           */
          void __enable_irq(struct irq_desc *desc, unsigned int irq, bool resume);  /*  unexported  */

          #  before enable,the @irq can not been suspended,and the @irq is only able to be enabled 
             when desc->depth == 1.
             if desc->depth > 1 => decrease it
             else if desc->depth < 1 => error
             if desc->depth == 1 => function "check_irq_resend()" enable @irq.

        <linux/irq.h> <kernel/irq/resend.c>
          /*  check_irq_resend - enable @irq,if any lost interrupt was found out,
           *                     revive it.
           *  @desc:             interrupt descriptor.
           *  @irq:              irq number.
           */
          void check_irq_resend(struct irq_desc *desc, unsigned int irq);

          the function detects that an interrupt was lost by checking the value of the IRQ_PENDING flag.
          because IRQ_PENDING is always cleared when leaving the interrupt handler,if the IRQ line is
          disabled but IRQ_PENDING is set,then an interrupt occurrence has been acknowledged but not
          yet serviced.
          thus,it call to desc->chip->enable to enables @irq.
          next,clear IRQ_PENDING and set IRQ_REPLAY,put @irq to the bits @irqs_resend,and schedule
          a tasklet which is named @resend_tasklet.@resend_tasklet is associated to a worker "resend_irqs()"
          which is static defined in "resend.c".
          the function "resend_irqs()" enter a loop until the bitmap @irqs_resend is null,for each @irq
          in the bitmap,it disable local irq and call to irq_to_desc(irq)->handle_irq,after returned
          from handler,it enable local irq then enter next cycle.

          #  IRQ_REPLAY : IRQ has been replayed but not acked yet.
             "__do_IRQ()" always clear it before handling,and it also clear IRQ_PENDING before exit.
          #  tasklet is a kind of softirq,and softirq is interprocessor interrupt.
          #  "check_irq_resend()" resend irqs must met the condition that desc->chip->retrigger is NULL or
             desc->chip->retrigger(irq) is false.
             retrigger is the hardware relative primitive for resend an IRQ to CPU.
             /*  subsection - PIC :
              *    Disable interrups are not lost;the PIC sends them to the CPU as soon as they are
              *    enabled again.
              */

      Interrupt Service Routines :
        the actions relate to an interrupt the handler have to takes named ISRs,and these actions are executed
        by the function "handle_IRQ_event()".ISR just specific to one type of device.

        <linux/irqreturn.h>
          enum irqreturn {
                IRQ_NONE,           /*  interrupt was not from this device  */
                IRQ_HANDLED,        /*  interrupt was handled by this device  */
                IRQ_WAKE_THREAD     /*  handler requests to wake the handler thread  */
          };
          typedef enum irqreturn irqreturn_t;

          /*  IRQ_RETVAL - if the interrupt is handled  */
          #define IRQ_RETVAL(X)  ((x) != IRQ_NONE)

        <linux/irq.h>
          typedef void (*irq_flow_handler_t)(unsgned int irq, struct irq_desc *desc);
          
          /*  handle_IRQ_event - the primary worker deal with a specific interrupt.
           *                     this function call to the actions associated to the IRQ line,
           *                     until one of ISR handled it.
           *  @irq:              IRQ number.
           *  @action:           ISRs.
           *  return:            a value is type of enum irqreturn.
           */
          extern irqreturn_t handle_IRQ_event(unsigned int irq, struct irqaction *action);

          the works "handle_IRQ_event()" does :
            1>  if IRQ_DISABLED in @action is not set,then call to "local_irq_enable_in_hardirq()" to
                enable @irq.thus,new interrupt is able to be raised on this IRQ line.
                ("sti" assembly instruction.and if an action for an interrupt must disabled maskable
                 interrupts before start handling,that is the action is critical.) 
            
            2>  enter a loop,iterator is @action,until @action is NULL.

            3>  start trace irq handling through "trace_irq_handler_entry(@irq, @action)",
                call to @action->handler(@irq, @action->dev_id),
                stop trace irq handling through "trace_irq_handler_exit(@irq, @action, ret)".

            4>  switch-cases :
                  ret = IRQ_WAKE_THREAD >
                    ret := IRQ_HANDLED
                    wake_up_process(action->thread)  /*  action->thread_fn must be not NULL,
                                                      *  IRQF_DIED is not set in @action->thread_flags.
                                                      */

                  ret = IRQ_HANDLED >
                    local var @status |= action->flags

                  default >
                    none

            5>
                local var @retval |= ret         /*  this variable will be return value and initialized to
                                                  *  IRQ_NONE at first.
                                                  *  there is the only place in the function @retval is changed.
                                                  */
                update @action to next ISR

            6>  exit loop

            7>  if IRQF_SAMPLE_RANDOM is set in @status,@irq will be used to feed random number generator.

            8>  call to "local_irq_disable()" to restore the state of local IRQ line,if it was not diabled
                early,this function does nothing.("cli" assembly instruction)
                return @retval.

          #  Attention :
                value of IRQ_NONE is zero.
                and,even there is an ISR handled current interrupt in the loop,the loop is still continues
                unitl @action is NULL.
                thus,if the interrupt is serviced,just @retval is not able to reveal how many ISRs was called.

          #  register informations had stored on the stack by @SAVE_ALL assembly macro function,and "do_IRQ()"
             writes the address about the top of stack that is where esp points to into a percpu data which
             is named @irq_regs.
             so call to ISR do not need to transmit register contents.

          #  suppose,ISR1 requests IRQ line is disabled,but ISR2 requests IRQ line is enabled,then ISR2 will
             be called with IRQ line disabled,because that loop do not check whether local IRQ line have to
             be enabled or disabled.
                  
      Dynamic Allocation of IRQ Lines :
        except to reserved vectors for specific devices,the remaining ones are dynamically handled.
        there is,a way in which the same IRQ line can be used by several hardware devices even if they do
        not allow IRQ sharing.
        !  THE TRICK IS TO SERIALIZE THE ACTIVATION OF THE HARDWARE DEVICES SO THAT JUST ONE OWNS THE
           IRQ LINE AT A TIME.

        <linux/interrupt.h>
        typedef irqreturn_t (*irq_handler_t)(int, void *);

        #ifdef CONFIG_GENERIC_HARDIRQS
          extern int __must_check request_threaded_irq(unsigned int irq, irq_handler_t handler, 
                                                       irq_handler_t thread_fn, unsigned long flags,
                                                       const char *name, void *dev);
          static inline int __must_check
          request_irq(unsigned int irq, irq_handler_t handler, unsigned long flags, const char *name, void *dev);
        #else
          extern int __must_check request_irq(unsigned int irq, irq_handler_t handler, unsigned long flags,
                                              const char *name, void *dev);
          extern int __must_check request_threaded_irq(unsigned int irq, irq_handler_t handler, 
                                                       irq_handler_t thread_fn, unsigned long flags,
                                                       const char *name, void *dev);
        #endif
        /*  request_irq - request to register an ISR on an IRQ line.
         *  @irq:         IRQ number.
         *  @handler:     handler.
         *  @flags:       flags.
         *  @name:        device specified information used in "/proc/irq" and "/proc/interrupts".
         *  @dev:         device specified data.
         *  return:       0 or error code.
         */ 

        /*  request_threaded_irq - request to register an ISR with threaded environment.
         *  @irq:                  IRQ number.
         *  @handler:              handler.
         *  @thread_fn:            thread function.
         *  @flags:                irqaction flags.
         *  @name:                 device specified information.
         *  @dev:                  device specified data.
         *  return:                0 or error code.
         */

        #  to insert an irqaction descriptor in the proper list,the kernel invokes the "setup_irq()" function,
           passing to it the parameters @irq IRQ number and an address about the new irqaction object.
           if there is device used @irq previously,then check whether IRQF_SHARED is set.if it is not setup,
           then register ISR on @irq will be refused.
           @new_irqaction is added into irq_desc[@irq]->action list.
           !  if no other device is sharing the same IRQ,the function("setup_irq()") clears the
              IRQ_DISABLED,IRQ_AUTODETECT,IRQ_WAITING,IRQ_INPROGRESS flags in the interrupt descriptor 
              corresponding to this @new_irqaction.

        !  IN THE CASE THAT CONFIG_GENERIC_HARDIRQS IS NOT DEFINED,FUNCTION "request_irq()" IS UNDEFINED,BUT
           "request_threaded_irq()" IS DEFINED AND EXPORTED.
           IN THE CASE THAT CONFIG_GENERIC_HARDIRQS IS DEFINED,FUNCTION "request_irq()" JUST A WRAPPER WHICH
           CALL TO "request_threaded_irq()" WITH NULL @thread_fn.

        flags for ISR :
          IRQF_DISABLED         keep irqs disabled when calling the action handler
          IRQF_SAMPLE_RANDOM    feed random number generator
          IRQF_SHARED           allow sharing the irq among several devices
          IRQF_PROBE_SHARED     set by callers when they expect sharing mismatches to occur
          IRQF_TIMER            mark this interrupt as timer interrupt
          IRQF_PERCPU           interrupt is percpu
          IRQF_NOBALANCING      exclude this interrupt from irq balancing
          IRQF_IRQPOLL          interrupt is used for polling
                                (only the interrupt that is registered first in an shared interrupt is
                                 considered for performance reasons)
          IRQF_ONESHOT          interrupt is not reenabled after the hardirq handler finished.
                                used by threaded interrupts which need to keep the irq line disabled
                                until the threaded handler has been run

          /*  free_irq - free a registered IRQ line.
           *  @irq:      the IRQ line.
           *  @dev:      identifier of the ISR in action list.(irq_desc[@irq]->action)
           *  #  if the ISR is the last interrupt service routine in the action list,
           *     then disable the IRQ line.(shut it down)
           */
          extern void free_irq(unsigned int irq, void *dev);

        !  DO NOT CALL TO "request_irq()" IN AN INTERRUPT CONTEXT,BECAUSE OF IT MIGHT ENTER SLEEP.

    Interprocessor Interrupt Handling :
      Interprocessor interrupts allow a CPU to send interrupt signals to any other CPU in the system.
      IPI is delivered not through an IRQ line,but directly as a message on the bus that connects the
      local APIC of all CPUs.

      Linux,IPI on multiprocessor system : (three kinds)
        CALL_FUNCTION_VECTOR(vector 0xfb)
          sent to all CPUs but the sender,forcing those CPUs to run a function passed by the sender.
          the corresponding interrupt handler is named "call_function_interrupt()".
          usually,this interrupt is sent to all CPUs except the CPU executing the calling function by
          means of the "smp_call_function()" facility function.
          /*  such function maybe stop the CPUs or force CPUs to set the contents of the
           *  Memory Type Range Registers(MTRRs).
           *  MTTRs : additional registers to easily customize cache operations.
           *          Linux may use them to disable the hardware cache for the address mapping the 
           *          frame buffer of a PCI/AGP graphic card while maintaining the "write combining"
           *          mode of operation : the paging unit combines write transfers into large chunks
           *          before copying them into the frame buffer.
           */

        RESCHEDULE_VECTOR(vector 0xfc)
          when a CPU receives this type of interrupt,the corresponding handler - named "reschedule_interrupt()" -
          limits itself to acknowledging the interrupt.
          rescheduling is done automatically when returning from the interrupt.
        
        INVALIDATE_TLB_VECTOR(vector 0xfd)
          send to all CPUs but the sender,forcing them to invalidate their Translation Lookaside Buffers.
          the corresponding handler,named "invalidate_interrupt()",flushes some TLB entries of the processor.

        /*  The assembly language macro "BUILD_INTERRUPT3(name, nr, fn)" is defined in
         *  <arch/x86/kernel/entry_32.S>,which is similar to "common_interrupt".(vector is still a negative number)
         *  The C language macro "BUILD_INTERRUPT(name, nr)" is defined in <arch/x86/kernel/entry_32.S>,which
         *  is used to define an interprocessor interrupt,actually,it is "BUILD_INTERRUPT3(name, nr, smp_##name)".
         *    e.g.  
         *      <arch/x86/include/asm/entry_arch.h>
         *      BUILD_INTERRUPT(call_function_interrupt, CALL_FUNCTION_VECTOR)
         *      BUILD_INTERRUPT(irq_move_cleanup_interrupt, IRQ_MOVE_CLEANUP_VECTOR)
         *      BUILD_INTERRUPT3(invalidate_interrupt0, INVALIDATE_TLB_VECTOR_START + 0)
         */

        source file <arch/x86/kernel/smp.c> defined some interprocessor interrupt handlers :
          e.g.
            /*  smp_reschedule_interrupt - handler for the IPI with vector RESCHEDULE_VECTOR.
             *  @regs:                     registers' contents saved on the stack via "SAVE_ALL".
             *                             @regs is the esp points to the top of stack.
             *  #  this function does nothing but acknowledge IPI,
             *     rescheduling is automatically done when returning from interrupt.
             */
            void smp_reschedule_interrupt(struct pt_regs *regs);
        
            /*  smp_call_function_interrupt - handler for the IPI with vector CALL_FUNCTION_VECTOR.
             *  @regs:                        registers' contents saved by "SAVE_ALL",@regs is the 
             *                                esp points to the top of stack.
             *  #  this function actually call to "generic_call_function_interrupt()" after it
             *     acknowledged the IPI.
             *     function "generic_call_function_interrupt()" is defined in <kernel/smp.c>.
             */
            void smp_call_function_interrupt(struct pt_regs *regs);
            
            /*  figure about smp_call_function_interrupt :
             *  smp_call_function_interrupt =>
             *  generic_smp_call_function_interrupt {
             *          traverse call_function list (defined in <kernel/smp.c>)
             *          for each entry is type of call_single_data (defined in <linux/smp.h>)
             *          retrieve its container is type of call_function_data (defined in <kernel/smp.c>)
             *          call to @data->csd.func(@data->csd.info)
             *          if @data->refs == 0
             *              delete current entry from list
             *      ####   
             *              struct call_function_data {
             *                struct call_single_data csd;
             *                atomic_t                refs;
             *                cpumask_var_t           cpumask;
             *              };  /*  one @csd corresponding to one @cfd  */
             *
             *              call_function.queue -> csd1 -> csd2 -> ... -> call_function.queue
             *                                      |       |
             *                                      V       V
             *                                     cfd     cfd
             *  }
             */


            !!  the following IPIs are existed only CONFIG_SMP is defined :
                  call_function_interrupt       =>  CALL_FUNCTION_VECTOR
                  reschedule_interrupt          =>  RESCHEDULE_VECTOR
                  irq_move_cleanup_interrupt    =>  IRQ_MOVE_CLEANUP_VECTOR
                  reboot_interrupt              =>  REBOOT_VECTOR
                  invalidate_interrupt{0..7}    =>  INVALIDATE_TLB_VECTOR_START + {0..7}

                  /*  the corresponding "smp_##name()" functions are defined either <arch/x86/kernel/smp.c>
                   *  or <kernel/smp.c>.
                   *  TLB relative function is defined in <arch/x86/mm/tlb.c>.
                   */

      IPI operating functions :
        <arch/x86/include/asm/apic.h>
          /*  struct apic - the generic APIC sub-arch data struct.in other word,the object is type of
           *                struct apic represents the I/O APIC.
           *  @send_IPI_mask:           send IPI with @vector to the CPUs determined by @mask.
           *  @send_IPI_allbutself:     send IPI with @vector to all CPUs except sender.
           *  @send_IPI_all:            send IPI with @vector to all CPUs include sender.
           *  @send_IPI_self:           send IPI with @vector to @self.
           */
          struct apic {
                ...
                void (*send_IPI_mask)(const struct cpumask *mask, int vector);
                ...
                void (*send_IPI_allbutself)(int vector);
                void (*send_IPI_all)(int vector);
                void (*send_IPI_self)(int vector);
                ...
          };

          extern struct apic *apic;     /*  global apic object pointer  */

          /*  the corresponding function pointers will be initialized by kernel across to the CPU model.
           *  different CPU has different apic object.
           */

          /*  Linux 2.6 DOES NOT CONTAIN THE WRAPPERS FOR THESE FUNCTIONS,IN ORDER TO SEND IPI,HAVE TO USE THE
           *  GLOBAL OBJECT @apic IS TYPE OF "struct apic *",FOR EXAMPLE,"apic->send_IPI_all(CALL_FUNCTION_VECTOR)".
           *  Linux SUPPORTS TO SEVERAL TYPES OF APIC,SO THE MEMBERS OF THE OBJECT MAYBE DIFFERENT.
           *  x86 DEFAULT APIC    : apic_default       <arch/x86/kernel/apic/probe_32.c>
           *  x86_64 DEFAULT APIC : apic_flat          <arch/x86/kernel/apic/apic_flat_64.c>
           */

          /*  call function interrupt outer interface :
           *  <kernel/smp.c>,
           *    smp_call_function - run a function on all other CPUs.
           *    @func:              the function to be executed.
           *    @info:              private data of @func.
           *    @wait:              T => wait until other CPUs completed function executing
           *                        F => does not waiting
           *    return:             always 0.
           *    #  can not call this function with disabled interrupts or from a hardirq handler or
           *      from a bottom half handler.
           *    int smp_call_function(void (*func)(void *), void *info, int wait);
           */

    Softirqs and Tasklets :
      non-critical tasks can be deferred for a long period of time,if necessary.
      ISRs are serialized,and often there should be no occurrence of an interrupt until the corresponding
      interrupt handler has terminated.
      deferrable tasks can execute with all interrupt enabled.

      !  Softirqs and tasklets cannot be interleaved on a givent CPU.

      Linux 2.6 non-urgent interruptible kernel functions :
        /*  deferrable functions  */
        /*  Intel manual called programmed exception as "softirq"  */

        softirqs
          statically allocated
          can run concurrently on several CPUs,even if they are of the same type(reentrant,the data
          structures that softirqs hold must be protected by spin lock)

        tasklets
          dynamically allocated
          the same type of tasklet cannot be executed by two CPUs at the same time(however,tasklets of
          different types can be executed concurrently on several CPUs)

        /*  softirqs and tasklets are strictly correlated,because tasklets are implemented on top of 
         *  softirqs.
         */

      Interrupt context :
        the kernel is currently executing either an interrupt handler or a deferrable function.

      The general operations can be performed on deferrable functions :
        Initialization
          defines a new deferrable function
          this is usually done when the kernel initializes iteself or a module is loaded

        Activation
          marks a deferrable function as "pending"
          it is to be run the next time the kernel schedules a round of executions of deferrable functions
          activation can be done at any time(even while handling interrupts)

        Masking
          selectively disables a deferrable function 
          it will not be executed by the kernel even if activated
        
        Execution
          executes a pending deferrable function together with all other pending deferrable functions of
          the same type
          execution is performed at well-specified times

        /*  a deferrable function that has been activated by a given CPU must be executed on the same CPU  */

      Softirq :
        ten kinds of softirqs > <linux/interrupt.h>
          HI_SOFTIRQ                handles high priority tasklets
          TIMER_SOFTIRQ             tasklets related to timer interrupts
          NET_TX_SOFTIRQ            transmits packets to network cards
          NET_RX_SOFTIRQ            recevies packets from network cards
          BLOCK_SOFTIRQ             block device softirq
          BLOCK_IOPOLL_SOFTIRQ      block device I/O poll softirq
          TASKLET_SOFTIRQ           handles regular tasklets
          SCHED_SOFTIRQ             scheduler softirq
          HRTIMER_SOFTIRQ           high-resolution timer
          RCU_SOFTIRQ               rcu lock

          NR_SOFTIRQS               /*  end of enumberated types  */
                                    /*  HI_SOFTIRQ = 0  */

        /*  Before Linux 2.6,the Bottom Half mechanism BH is used to handle non-critical works,
         *  the total number of BHs is 32.
         *  Since Linux 2.6,BH had abandoned,softirqs as a intead,and the maximum number of softirqs
         *  also is 32.all device drivers which used BH should convert it to tasklet.
         */

        data structures used for softirqs >
          <linux/interrupt.h>
            /*  struct softirq_action - softirq action wrapper.
             *  @action:                the deferrable function will be executed.
             */
            struct softirq_action {
                    void (*action)(struct softirq_action *);
                    /*  Linux 2.6 DID NOT DEFINE SUCH FIELD WHICH USED TO SAVE THE ARGUMENT OF
                     *  THE @action DEFERRABLE FUNCTION.
                     */
            };

          <kernel/softirq.c>
            /*  softirq_vec - an array contains all types of softirqs.
             *                the enumberated types defined in <linux/interrupt.h>
             *                as the vector for this array.
             */
            static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;

          thread_info.preempt_count :
            [0, 7]        preemption counter      /*  how many times preemption is explicitly disabled,max depth 256  */
            [8, 15]       softirq counter         /*  how many levels deep the disabling of deferrable functions is  */
            [16, 27]      hardirq counter         /*  specifies the number of nested interrupt handlers on the local CPU  */
            [28]          PREEMPT_ACTIVE flag     /*  allow preemption(0) or refuse preemption(1)
                                                   *  when this flag is set,that means there is a preemption in progress,
                                                   *  schedule() routine will be called at next
                                                   */
          
            /*  some bits in @preempt_count are used to keep track of kernel preemption and of nesting of kernel
             *  control paths.
             *  softirq counter = 0 => deferrable functions are enabled
             *  hardirq counter => irq_enter() increase it and irq_exit() decrease it
             *  macro PREEMPT_ACTIVE is the mask of PREEMPT_ACTIVE flag of preempt_count,which is defined in
             *  <linux/hardirq.h>
             */
          
            useful macro function to determine where the kernel path is now :
              <linux/hardirq.h>
                #define in_irq()  (hardirq_count())       /*  Is kernel running interrupt handler?  */
                #define in_softirq()  (softirq_count())   /*  Is kernel running deferrable function?  */
                #define in_interrupt()  (irq_count())     /*  Is kernel running interrupt handler or deferrable function?  */
  
                /*  if kernel is not make use of Kernel Mode stack
                 *    preempt_count get from current.thread_info
                 *  else
                 *    preempt_count get from irq_ctx union assocaited with the local CPU
                 */

            irq_cpustat_t.__softirq_pending :
              the field @__softirq_pending is a per-CPU 32-bit mask describing the pending softirqs.
              
              <linux/irq_cpustat.h>
                #define local_softirq_pending()  \
                        __IRQ_STAT(smp_processor_id(), __softirq_pending)

        Handling softirqs :
          linux softirq has defined two primitives for operating softirq mechanism.
            OPEN        -       install a softirq deferrable function.
            RAISE       -       raise a installed softirq.
                                /*  structure softirq_action is not chained,so there is only
                                 *  one deferrable function for one type of softirq at the
                                 *  same time.
                                 */

          <linux/interrupt.h>
            /*  open_softirq - install a softirq deferrable function @action at the
             *                 position @softirq_vec[@nr].
             *  @nr:           softirq vector.
             *  @action:       deferrable function.
             */
            extern void open_softirq(int nr, void (*action)(struct softirq_action *));

            /*  raise_softirq - raise a softirq have vector @nr.
             *  @nr:            softirq vector.
             *  #  this function will save the local IRQ flags(eflags.IF) and then disable
             *     local IRQ(through assembly language instruction "cli").
             *     call to "raise_softirq_irqoff()" which must run with irqs disabled,
             *     next,"__raise_softirq_irqoff()" will be called,which set the bit associated 
             *     to @nr in the pending softirqs on current CPU.check if current context
             *     is not interrupt context,if it is,wake up ksoftirqd.
             *     before "raise_softirq()" returns,"local_irq_restore()" is called for restore
             *     interrupt flags to the previously saved.
             */
            extern void raise_softirq(unsigned int nr);
            extern void raise_softirq_irqoff(unsigned int nr);

          checks for active(pending) softirqs should be performed periodically,but without inducing too much
          overhead,they are performed in a few points of the kernel code :
            >  when the kernel invokes the "local_bh_enable()" function to enable softirqs on the local CPU.
               /*  bh => bottom half,had removed from Linux kernel  */
            >  when the "do_IRQ()" function finishes handling an I/O interrupt and invokes the "irq_exit()" macro.
            >  if the system uses an I/O APIC,when the "smp_apic_timer_interrupt()" function finishes handling
               a local timer interrupt.
            >  in multiprocessor systems,when a CPU finishes handling a function triggered by a CALL_FUNCTION_VECTOR
               interprocessor interrupt(IPI).
            >  when one of the special "ksoftirqd/n" kernel threads is awakended.
               /*  on SMP,each CPU in the system has a kernel thread named "ksoftirqd" which deal with the
                *  softirq handling.
                *  Linux kernel make a balance between softirqs and User Mode processes through the task scheduler.
                */

          the do_softirq() function :
            <linux/interrupt.h> 
              /*  do_softirq - function deal with soft interrupt.
               *               defined in <kernel/softirq.c>
               */
              asmlinkage void do_softirq(void);

              the following actions this function will takes:
                1>  check whether in an interrupt context now,if it is,then return.
                    deferrable function can not be executed in an interrupt context.
                2>  call to local_irq_save(@flags) to saves the contents of eflags register to local
                    variable @flags.
                    /*  function local_irq_save() will disable local interrupt.  */
                3>  call to local_softirq_pending() to get the pending softirqs and saves them in
                    local variable @pending.
                4>  if there is a softirq pending,then call to __do_softirq(),which will does the
                    primary works.
                    /*  local interrupt still be disabled at the time control enter __do_softirq().  */
                5>  call to local_irq_restore(@flags) to restores the status of eflags.
                    /*  function local_irq_restore() restores eflags.if eflags.IF flag was not setted previously,
                     *  it will be unsetted after local_irq_restore() finished.
                     */

          the __do_softirq() function :
            <linux/interrupt.h>
              /*  __do_softirq - the main routine for softirq handling.
               *                 it is defined in <kernel/softirq.c>
               */
              asmlinkage void __do_softirq(void);

              /*  GCC feature:
               *    __builtin_return_address - GCC built-in function.
               *                               it returns the return address(eip) of the current function,or
               *                               of one of its callers.
               *    @level:                    it is number of frames to scan up the call stack.0 yields
               *                               the return address of the current function,and so forth.
               *    void *__builtin_return_address(unsigned int level);
               */

              the following actions this function will takes:
                1>  retrieve pending local softirqs.
                2>  account system vtime via account_system_vtime(current).
                3>  call to __local_bh_disable((unsigned long)__builtin_return_address(0)) to updates preempt_count()
                    with SOFTIRQ_OFFSET.
                    /*  preempt_count() => current_thread_info()->preempt_count
                     *  @ip parameter of __local_bh_disable() is used to tracing kernel control path.
                     *  before updating,eflags.IF will be cleared,and eflags will be restored after updated.
                     */
                4>  set_softirq_pending(0),clean the softirq pending bitmap.
                    enable local irq.
                    /*  do_softirq() disabled local interrupt before __do_softirq() was called.  */
                5>  let local variable @h points to @softirq_vec array.
                6>  traverse @pending until @pending == 0 (bits traversing).
                7>  if (@pending & 1)
                      prepare account and tracing;
                      call to h->action(h);
                    h++;
                    pending >>= 1;
                8>  disable local irq.
                9>  check again if new softirq was raised.
                10> if @pending != 0 and @max_restart != 0
                      restart to 4>
                    if @pending != 0 and @max_restart == 0
                      can not restart,wake up ksoftirqd
                11> stop tracing,update system vtime,call to __local_bh_enable().
                12> return control to do_softirq().

                /*  before action 4> the function lockdep_softirq_enter() is called,current->softirq_context++,
                 *  before action 12> the function lockdep_softirq_exit() is called,current->softirq_context++.
                 *  these two functions are defined in <linux/irqflags.h>
                 */

        The ksoftirqd kernel threads :
          the kernel ksoftirqd is represent a solution for a critical trade-off problem.
          the detail of it is defined in <kernel/softirq.c>.

          !  softirq functions may reactive themselves,in fact,both the networking softirqs and the
             tasklet softirqs do this,moreover,external events,such as packet flooding on a network card
             may activate softirqs at very high frequence.

          <kernel/softirq.c>
            /*  per-cpu data for per-cpu kernel thread [ksoftirqd]  */
            static DEFINE_PER_CPU(struct task_struct *, ksoftirqd);

            /*  wakeup_softirqd - wakeup the ksoftirqd kernel thread of local CPU.
             *  #  no indefinitely loop in this function to avoid userspace starvation.
             *     do_softirq() call to this function to deal with the new coming softirqs.
             */
            void wakeup_softirqd(void);

            /*  run_ksoftirqd - the main function to run for the kernel thread [ksoftirqd].
             *  @__bind_cpu:    the cpu associated to the [ksoftirqd],if the cpu is offline,
             *                  the corresponding [ksoftirqd] should to die.
             *  #  the main works this function does:
             *       >  set task state to TASK_INTERRUPTIBLE.
             *       >  enter a loop until kthread_should_stop() returns true.
             *       >  in the loop,set task state to TASK_RUNNING,disable kernel preempt and check local softirqs
             *          via local_softirq_pending().
             *       >  if any local softirqs are pending,call to do_softirq(),then enable preempt and call to
             *          cond_resched(),this function will call to schedule().(flag TIF_NEED_RESCHED of the current 
             *          thread_info set)
             *       >  if none of local softirqs are pending,then enable preempt and set task state to TASK_INTERRUPTIBLE,
             *          next call to schedule() to yield CPU time.
             */
            static int run_ksoftirqd(void *__bind_cpu);

            /*  spawn_ksoftirqd - init function which spawn the [ksoftirqd] kernel thread 
             *                    for current cpu.
             *  #  this function will be called at the time that is kernel initialization.
             *     it will call to cpu_callback()<kernel/softirq.c> to creates a kernel thread
             *     which is about to runs run_ksoftirqd() function.
             *     it call to cpu_callback() twice,the second of invocation is to wakeup [ksoftirqd].
             */
            static __init spawn_ksoftirqd(void) early_initcall(spawn_ksoftirqd);

          the potential for a continuous high-volume flow of softirqs creates a problem that is solved by
          introducing kernel threads.without them,developers are essentially faced with two alternative
          strategies:
            1>  ignoring new softirqs that occur while do_softirq() is running.
                the new coming softirqs will be deferred to next timer interrupt,this is can not be 
                acceptable for network system.
            2>  continuously rechecking for pending softirqs.
                the do_softirq() function could keep checking the pending softirqs and would terminate
                only when none of them is pending.
                userspace stravation will occur.(if a high-frequence flow of packets is received by a network card,
                or a softirq function keeps activating itself)

            !   [ksoftirqd] has low priority,the userspace programs will have a change to run.

        Tasklets :
          tasklets are the preferred way to implement deferrable functions in I/O drivers.
          tasklet is implemented through HI_SOFTIRQ or TASKLET_SOFTIRQ,several tasklets may be associated
          with the same softirq,each tasklet carrying its own function.
          do_softirq() deal with tasklet is type of HI_SOFTIRQ before the tasklet is type of TASKLET_SOFTIRQ.

          <linux/interrupt.h>
            /*  tasklet_struct - the structure represents a kernel tasklet.
             *  @next:           pointer to next tasklet.
             *  @state:          status of this tasklet.
             *                   value :
             *                     TASKLET_STATE_SCHED - pending
             *                     TASKLET_STATE_RUN   - running
             *  @count:          lock counter.
             *  @func:           function pointer to the function this tasklet carrying.
             *  @date:           date to the @func.
             */
            struct tasklet_struct {
                    struct tasklet_struct *next;
                    unsigned long state;
                    atomic_t count;
                    void (*func)(unsigned long);
                    unsigned long data;
            };

          <kernel/softirq.c>
            /*  tasklet_head - the structure represents a tasklet link-list.
             *  @head:         the head.
             *  @tail:         the tail(pointer to pointer).
             */
            struct tasklet_head {
                    struct tasklet_struct *head;
                    struct tasklet_struct **tail;
            };

            /*  --- per-cpu data ---  */

            /*  for TASKLET_SOFTIRQ  */
            static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);
            /*  for HI_SOFTIRQ       */
            static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);
            
          <linux/interrupt.h>
            /*  tasklet_init - initializes a tasklet_struct.
             *  @t:            pointer to the tasklet_struct structure.
             *  @func:         the function this tasklet_struct carrys.
             *  @data:         the data to @func.
             */
            extern void tasklet_init(struct tasklet_struct *t, void (*func)(unsigned long), unsigned long data);

            /*  tasklet_disable_nosync - disable a tasklet without synchronize.
             *  @t:                      the tasklet_struct object.
             *  #  this function atomic increase the data member tasklet_struct.count.
             *     for smp,the function smp_mb_after_atomic_inc() will be called to synchronize to another cpu.
             */
            static inline void tasklet_disable_nosync(struct tasklet_struct *t);

            /*  tasklet_disable - synchrounous version.
             *  @t:               the tasklet_struct object.
             *  #  this function just call to tasklet_disable_nosync() and then call to tasklet_unlock_wait(@t)
             *     to wait the tasklet gets end if it is running on some cpu.
             */
            static inline void tasklet_disable(struct tasklet_struct *t);

            /*  tasklet_enable - enable a disabled tasklet.
             *  @t:              the tasklet_struct object which had been disabled.
             *  #  this function atomic decrease the data member tasklet_struct.count .
             */
            static inline void tasklet_enable(struct tasklet_struct *t);

            /*  tasklet_schedule - schedule a tasklet with type of TASKLET_SOFTIRQ.
             *  @t:                the tasklet it should be scheduled.
             *  #  this function call to the __tasklet_schedule()<kernel/softirq.c>,
             *     before the calling,the @t->state will be setted to TASKLET_STATE_SCHED.
             */
            static inline void tasklet_schedule(struct tasklet_struct *t);

            /*  tasklet_hi_schedule - schedule a tasklet with type of HI_SOFTIRQ.
             *  @t:                   the tasklet it should be scheduled.
             *  #  this function call to the __tasklet_hi_schedule()<kernel/softirq.c>,
             *     before the calling,the @t->state will be setted to TASKLET_STATE_SCHED.
             */
            static inline void tasklet_hi_schedule(struct tasklet_struct *t);

            !  if @t->state == TASKLET_STATE_SCHED
               then
                 tasklet_schedule() and tasklet_hi_schedule() return without something have been done.
               else
                 call to the internal functions defined in <kernel/softirq.c>

            __tasklet_schedule() and __tasklet_hi_schedule() :
              these two functions almost do the same works:
                1>  store local irq flags.
                2>  @t->next = NULL.
                3>  push @t to the tasklet_head link-list.
                4>  call to raise_softirq_irqoff().
                5>  restore local irq flags.

              the difference:
                __tasklet_schedule() push @t to the tail of local @tasklet_vec link-list.
                __tasklet_schedule() call raise_softirq_irqoff() with TASKLET_SOFTIRQ.
                __tasklet_hi_schedule() push @t to the tail of local @tasklet_hi_vec link-list.
                __tasklet_hi_schedule() call raise_softirq_irqoff() with HI_SOFTIRQ.

          <kernel/softirq.c>
            /*  tasklet_action - action function for TASKLET_SOFTIRQ type.
             *                   it is installed through open_softirq() by softirq_init().
             *  @a:              the softirq action structure.
             */
            static void tasklet_action(struct softirq_action *a);

            /*  tasklet_hi_action - action function for HI_SOFTIRQ type.
             *                      it is installed through open_softirq() by softirq_init().
             *  @a:                 the softirq action structure.
             */
            static void tasklet_hi_action(struct softirq_action *a);

            the works these two functions do:
              1>  disable local irq and retrieve head of tasklet list.
              2>  empty the tasklet list.
              3>  enable local irq and enter a while-loop.
              4>  traverse the list to deal with each tasklet if it had not been disabled,
                  each tasklet only be activated once if it is not reactives itself.
                  if @t has been running on another cpu or it has been disabled,then it will
                  be push back to the tail of the tasklet list,raise_softirq_irqoff() will be
                  called before next loop.
              5>  traversed all tasklets in the list,function returns.
                  the new tasklet list will be deferred to the next time(it maybe contains
                  the running or disabled tasklets).

            !  while processing the tasklet,action function have to lock it(SMP) through tasklet_trylock() and
               unlock it after processed through tasklet_unlock().
               the two functions set tasklet_struct.state = TASKLET_STATE_RUN or unset it.
               /*  TASKLET_STATE_SCHED must be set before action by other kernel control path,
                *  otherwise,the bug occurred  */

          !  unless the tasklet function reactivates itself,every tasklet activation triggers at most one execution
             of the tasklet function.

        Work Queues :
          introduced in Linux 2.6 and replace a similar construct called "task queue" used in Linux 2.4.
          they allow kernel functions to be activated and later executed by special kernel threads called
          worker threads.

          the kernel functions in work threads are running in process context,it is not interrupt context.
          neither deferrable functions nor functions in a work queue can access to the User Mode address space
          of a process.

          <linux/workqueue.h>
            /*  work_func_t - a type definition for the worker function.  */
            typedef void (*work_func_t)(struct work_struct *work);

            /*  work_struct - it represents a work item.
             *  @data:        data to the @func.
             *  @entry:       works list.
             *  @func:        work function.
             */
            struct work_struct {
                    atomic_t data;
                    struct list_head entry;
                    work_func_t func;
            #ifdef CONFIG_LOCKDEP
                    struct lockdep_map lockdep_map;
            #endif
            };

            /* INIT_WORK - initialize @_work and set @_work->func to @_func */
            #define INIT_WORK(_work, _func)                   \
                    do {                                      \
                            __INIT_WORK((_work), (_func), 0); \
                    } while (0)

            /*  delayed_work - represents a delayed work.
             *  @work:         the work.
             *  @timer:        timer.
             */
            struct delayed_work {
                    struct work_struct work;
                    struct timer_list timer;
            };

          <kernel/workqueue.c>
            <linux/wait.h> typedef struct __wait_queue_head wait_queue_head_t;

            /*  cpu_workqueue_struct - the per-CPU workqueue(if single thread, always use the first
             *                         possible cpu).
             *  @lock:                 spin lock.
             *  @worklist:             works list,@worklist.next-->@work_struct.entry .
             *  @more_work:            wait queue where the worker thread waiting for more work to be done sleeps.
             *  @current_work:         the work currently pending on the cpu.
             *  @wq:                   the workqueue.
             *  @thread:               the associated worker thread.
             */
            struct cpu_workqueue_struct {
                    spinlock_t lock;
                    struct list_head worklist;
                    wait_queue_head_t more_work;
                    struct work_struct *current_work;
                    struct workqueue_struct *wq;
                    struct task_struct *thread;
            } ____cacheline_aligned;

            /*  workqueue_struct - the externally visible workqueue abstraction is an array of
             *                     per-CPU workqueues.
             *  @cpu_wq:           associated per-CPU workqueue struct.
             *  @list:             workqueue list.
             *  @name:             workqueue name.
             *  @singlethread:     if singlethread.
             *  @freezeable:       freeze threads during suspend.
             *  @rt:               run times count.
             */
            struct workqueue_struct {
                    struct cpu_workqueue_struct *cpu_wq;
                    struct list_head list;
                    const char *name;
                    int singlethread;
                    int freezeable;
                    int rt;
            #ifdef CONFIG_LOCKDEP
                    struct lockdep_map lockdep_map;
            #endif
            };

          Detail :
            every work is represented by a work_struct structure.
            queue a work is queue it into a cpu_workqueue_struct.
            workqueue_struct is a externally visiable abstraction of cpu_workqueue_struct.
            cpu_workqueue_struct structures is created by linux kernel automatically.
            __create_workqueue_key() is used to creates several cpu_workqueue_struct structures,
            and each cpu_workqueue_struct structure is associated to a same workqueue_struct
            structure(new created by the function).then per-CPU worker thread is created and started.
            each new created @wq is added into a static link-list @workqueues(defined in <kernel/workqueue.c>).
            the interfaces operate workqueue needs a workqueue_struct object as its parameter,it
            specify which workqueue to operating.
            @workqueue_struct.list is a nested link-list structure each elements on it is a workqueue_struct
            which associated to a cpu_workqueue_struct,generally,one workqueue_struct to one cpu_workqueue_struct.

          <linux/workqueue.h> <kernel/workqueue.c>
            /*  __create_workqueue_key - create a workqueue,and several cpu_workqueue_struct structures also
             *                           will be created and associated to the new created workqueue.
             *  @name:                   workqueue name.
             *  @singlethread:           if singlethread?
             *  @freezeable:             if freezeable?
             *  @rt:                     rt.
             *  @key:                    lock_class_key pointer.
             *  @lock_name:              lock_name.
             *  return - NULL or the new created workqueue_struct pointer.
             *  #  singlethread == F
             *       creates several worker threads.
             */
            extern struct workqueue_struct *
            __create_workqueue_key(const char *name, int singlethread, int freezeable,
                                   int rt, struct lock_class_key *key, const char *lock_name);

            #define __create_workqueue(name, singlethread, freezeable, rt)  \
                    __create_workqueue_key((name), (singlethread), (freezeable), (rt), NULL, NULL)
            #define create_workqueue(name)  __create_workqueue((name), 0, 0, 0);

            /*  destroy_workqueue - destroy a workqueue.
             *  @wq:                the workqueue_struct to be destroyed.
             */
            extern void destroy_workqueue(struct workqueue_struct *wq);

            /*  queue_work - queue a work.
             *  @wq:         the workqueue to queue.
             *  @work:       the work.
             *  return - queued,1;work already on the queue or failed,0.
             */
            extern int queue_work(struct workqueue_struct *wq, struct work_struct *work);

            /*  queue_delayed_work - queue a delayed work.
             *  @wq:                 the workqueue to queue.
             *  @work:               the work.
             *  @delay:              delay.(expires = timer.jiffies + @delay)
             *  return - queued,1;work already on the queue or failed,0.
             */
            extern int queue_delayed_work(struct workqueue_struct *wq, struct work_struct *work,
                                          unsigned long delay);

            /*  cancel_delayed_work - <linux/workqueue.h> static function cancel a delayed work.
             *  @work:                the delayed work.
             *  return - @ret >= 0.
            static inline int cancel_delayed_work(struct delayed_work *work);

            /*  flush_workqueue - block current process until all pending works had been done,
             *                    do not care the new incoming works.
             *  @wq:              the workqueue.(this workqueue associated to several cpu_workqueue_struct)
             */
            extern void flush_workqueue(struct workqueue_struct *wq);

          the predefined work queue :
            <kernel/workqueue.c>
              /*  keventd_wq - the kernel thread "events"'s workqueue.
               *               kernel offers the predefined workqueue to deal with general
               *               works,prevent creating a whole set of worker threads in order
               *               to run a function.
               */
              static struct workqueue_struct *keventd_wq __read_mostly;

              /*  schedule_work - queue a work up to keventd_wq.
               *  @work:          the work.
               *  return - queued,1;work already on the queue or failed,0.
               */
              extern int schedule_work(struct work_struct *work);

              /*  schedule_delayed_work - queue a delayed work up to keventd_wq.
               *  @work:                  the delaying work.
               *  @delay:                 delay,expires = @timer.jiffies + delay.
               *  return - queued,1;work alread on the queue or failed,0.
               */
              extern int schedule_delayed_work(struct delayed_work *work, unsigned long delay);

              /*  schedule_delayed_work_on - similar to schedule_delayed_work(),but it is possible
               *                             to assign cpu-id to specify which cpu to run this work
               *                             via @cpu.
               */
              extern int schedule_delayed_work_on(int cpu, struct delayed_work* work, unsigned long delay);

              /*  flush_scheduled_work - similar to flush_workqueue(),but the target to flush is keventd_wq.  */
              extern void flush_scheduled_work(void);

            !  functions executed in the predefined work queue should not block for a long time,because the execution
               of the pending functions in the work queue list is serialized on each CPU,a long delay negatively
               affects the other users of the predefined work queue.

            !  [kblockd] kernel thread deal with the works on the @kblockd_workqueue work queue,
               the works from block device layer.

    Returning from interrupts and Exceptions :
      resume execution of some program,several issues must be considered beforing doing it :
        >  Number of kernel control paths being concurrently executed
             if these just one,the CPU must switch back to User Mode
        >  Pending process switch requests
             if there is any request,the kernel must perform process scheduling,otherwise,control
             is returned to the current process(the interrupted process)
        >  Pending signals
             if a signal is sent to the current process,it must be handled
        >  Single-step mode
             if a debugger is tracing the execution of the current process,single-step mode must be
             restored before switching back to User Mode
        >  Virtual-8086 mode
             if the CPU is in virtual-8086 mode,the current process is executing a legacy Real Mode program,
             thus it must be handled in a special way

      flags used to keep track of pending switch requests,of pending signals,and of single step execution :
        thread_info.flags >
          TIF_SYSCALL_TRACE         syscalls are being traced
          TIF_NOTIFY_RESUME         not used in the 80x86 platform
          TIF_SIGPENDING            the process has pending signals
          TIF_NEED_RESCHED          scheduling must be performed
          TIF_SINGLESTEP            restore single step execution on return to User Mode
          TIF_IRET                  force return from system call via "iret" rather than "sysexit"
          TIF_SYSCALL_AUDIT         system calls are being audited
          TIF_POLLING_NRFLAG        the idle process is polling the TIF_NEED_RESCHED flag
          TIF_MEMDIE                the process is being destroyed to reclaim memory

        /*  these flags are defined in <arch/x86/include/asm/thread_info.h>  */

      ret_from_intr and ret_from_exception :
        they are two entry points of a piece of kernel code.
        the general flow >
          ret_from_intr:
                  if <nested kernel control paths?>
                          if <virtual 8086 mode?>
                                  goto resume_userspace
                          goto resume_kernel  /*  eflags.VM = 0 and CPL < 3(USER_RPL)  */
                  goto resume_userspace
          resume_kernel:
                  cli
                  if <kernel preemption enabled?>
                          goto need_resched
                  goto restore_all
          need_resched:
                  if <need reschedule?>
                          if <resuming a kernel control path with IF = 0?>
                                  goto restore_all
                          preempt_schedule_irq()
                          goto need_resched
          restore_all:
                  restore hardware context
                  /* finally,enter "irq_return" */
          resume_userspace:
                  cli
                  if <is there some works have to be done(rescheduling,signals,single step)?>
                          goto work_pending
                  goto  restore_all
          work_pending:
                  if <need to reschedule?>
                          goto work_resched
                  goto work_notifysig
          work_resched:
                  schedule()
                  goto resume_userspace
          work_notifysig:
                  if <virtual 8086 mode?>
                          save_v86_state()
                          do_notify_resume()
                          goto restore_all
                  do_notify_resume()
                  goto restore_all
          ret_from_exception:
                  cli
                  goto ret_from_intr
 
          /*  some function invoking might not returns,so "goto" means that function will switch
           *  to the next kernel control path.
           */
          /*  difference between ret_from_exception and ret_from intr when kernel preemption compilation
           *  option was selected :
           *    local interrupts(IF) are immediately disabled when returning from exceptions.
           */          
 
        !!  the assembly code of ret_from_intr and ret_from_exception is defined in <arch/x86/kernel/entry_32.S>
 
      resuming a kernel control path :
        resume_kernel:  /*  preemption support  */
          >  disables local interrupt
          >  if current->thread_info.preempt_count == 0
                     jmp need_resched
             else
                     jmp restore_all
          /*  preempt_count == 0 means preemption enabled  */

      checking for kernel preemption :
        need_resched:  /*  when the piece of code is executed,none of the unfinished kernel control path is an
                        *  interrupt handler,otherwise,the preempt_count field would be greater than zero.
                        */
          >  retrieve thread_info.flags
          >  if TIF_NEED_RESCHED != 1
                     jmp restore_all
             elif eflags.IF != 1
                     jmp restore_all
          >  call to preempt_schedule_irq
             /*  this function sets the PREEMPT_ACTIVE flag in the preempt_count field,enables the local interrupts,
              *  invokes schedule() to select another process to run.
              *  when the former process will resume,preempt_schedule_irq() clears the PREEMPT_ACTIVE flag,and disables
              *  local interrupts.
              *  preempt_schedule_irq() is defined in <kernel/sched.c>
              */
          >  jmp need_resched  /*  if the TIF_NEED_RESCHED flag of the current process is set,preempt_schedule_irq()
                                *  will be called again,thus,schedule() also be called again.
                                */
      
      resuming a User Mode program :
        resume_userspace:
          >  disbale local interrupts
          >  ends irq tracing
          >  retrieve thread_info.flags
          >  if TIF_WORK_MASK & thread_info.flags
                     jmp work_pending  /*  there has some works to be done before int/exception return  */
             else
                     jmp restore_all

        /*  work mask : TIF_SYSCALL_TRACE TIF_SYSCALL_AUDIT TIF_SINGLESTEP  */

      checking for rescheduling :
        work_pending:
          >  if thread_info.flags.TIF_NEED_RESCHED
                     jmp work_notifysig

        work_resched:
          >  call schedule
          >  disable local interrupts
          >  ends irq tracing
          >  retrieve thread_info.flags
          >  if thread_info.flags & TIF_WORK_MASK
                     if thread_info.flags & TIF_NEED_RESCHED
                             jmp work_resched
             jmp restore_all

      handling pending signals,virtual-8086 mode,and single stepping :
        work_notifysig:
          >  if eflags.VM == 1
                     jmp work_notify_x86
          >  call do_notify_resume
          >  jmp resume_userspace_sig

        work_notify_x86:
          >  save ti_flags /*  thread_info.flags  */
          >  call save_v86_state
          >  restore ti_flags
          >  call do_notify_resume
          >  jmp resume_userspace_sig

        /*  do_notify_resume() take care of pending signals and single stepping.  */
        /*  #ifdef CONFIG_VM86
         *  #define resume_userspace_sig  check_userspace
         *  #else
         *  #define resume_userspace_sig  resume_userspace
         */

      !  current->thread_info is stored in the process's kernel stack.


/* END OF CHAPTER 4 */


Chapter 5 : Kernel Synchronization
    How the Kernel Services Requests :
      two types of requests >
        1>  requests from processes
        2>  requests from devices
    
      the policy adopted by the kernel is the following >
        1>  if a dev-req comes while the kernel is idle,the kernel starts servicing.
        2>  if a dev-req comes while the kernel is servicing a process,the kernel
            stops servicing the process and start servicing the device.
        3>  if a dev-req comes while the kernel is servicing another device,the kernel
            stops servicing the first device and starts servicing the second one.
            when it finish servicing the new device,it resumes servicing the former one. 
        4>  one of the devices may induce the kernel to leave the process being currently
            services.
            after servicing the last request of the devices,the kernel may decide to drop
            temporarily the previous process and to pick up a new one.
            
        1--3>  Nested Execution of Exception and Interrupt Handlers
        4>     Kernel Preemption

      !  if CPU is not executing the code in the Kernel Mode,consider it is idle,or consider
         the kernel is idle and ready to service devices.
      !  dev-req correspond to interrupts(timer,keyboard,...)
         proc-req correspond to system calls or exception raised by User Mode processes

    Kernel Preemption : (Kernel Option : CONFIG_PREEMPT)
      Process Switch >
        >  a planned process switch :
             both in preemptive and nonpreemptive kernels,a process running in Kernel Mode can
             voluntarily relinquish the CPU(i.e. sleeping...)
           forced process switch :
             synchronous events cause a process switching,for instance,an interrupt handler that
             awakes a higher priority process.
             (react to such case,preemptive kernel is differs from nonpreemptive kernel)
  
        >  all process switches are performed by the switch_to macro.
           in both preemptive and nonpreemptive kernels,a process switch occurs when a process has
           finished some thread of kernel activity and the scheduler is invoked.
           however,in nonpreemptive kernels,the current process cannot be replaced unless it is about
           to switch to User Mode.
      
      !  the main characteristic of a preemptive kernel is that a process running in Kernel Mode can
         be replaced by another process while in the middle of a kernel function.

      !  nonpreemptive kernel forbids "forced process switch".
         /*  "forced process switch" also will happens when the process's time quantum expires.  */

      !  reason for making a kernel preemptive :
           reduce the "dispatch latency" of the User Mode processes.
           "dispatch latency" :
             the delay between the time processes become runnable and the time they actually begin running.
      
      kernel preemption is disabled when thread_info.preempt_count > 0,the following cases that preempt_count
      is greater than zero :
        1>  the kernel is executing an interrupt service routine
            /* hardirq counter of preempt_count > 0 */
        2>  the deferrable functions are disabled(always true when the kernel is executing a softirq or tasklet)
            /* softirq counter of preempt_count > 0 */
        3>  the kernel preemption has been explicitly disabled by setting the preemption counter to a positive 
            value
            /* preempt_count == 1 OR preempt_count[PREEMPT_ACTIVE] is set - preempt is progressing */

        !  thus,the kernel can be preempted only when it is executing an exception handler(in particular a system call)
           and the kernel preemption has not been explicitly disabled.
           /*  eflags.IF must be set up,otherwise,kernel preemption is not performed.  */

      macros defined in <linux/preempt.h> to accessing and manipulating preempt_count :
        <linux/preempt.h>
          /*  preempt_count - returns the value of preempt_count.  */
          #define preempt_count()  (current_thread_info()->preempt_count)

          /*  preempt_disable - disable kernel preemption.  */
          #define preempt_disable() \
                  do { \
                          inc_preempt_count(); \
                          barrier();           \
                  } while (0)

          /*  preempt_enable - enable kernel preeption.
           *                   if thread_info.flags & TIF_NEED_RESCHED is T,
           *                   call to preempt_schedule().
           */
          #define preempt_enable() \
                  do { \
                          preempt_enable_no_resched(); \  /*  *_no_resched means no scheduler calling.  */
                          barrier();                   \
                          preempt_check_resched();     \
                  } while (0)

          /*  preempt_schedule() is declared in <linux/preempt.h> and defined in <kernel/sched.c>
           *    if preempt_count > 0 OR irq disabled
           *            do nothing,and return
           *    else
           *            do-while loop until need_resched() returns zero
           *                    set PREEMPT_ACTIVE
           *                    call schedule()
           *                    unset PREEMPT_ACTIVE
           *                    barrier(),wait synchronization
           */

      macros defined in <linux/smp.h> associated to preempt_count :
        <linux/smp.h>
          /*  get_cpu - get cpu id with kernel preemption disabled.  */
          #define get_cpu()  ({ preempt_disable(); smp_processor_id(); })

          /*  put_cpu - put cpu,that is enable kernel preemption.  */
          #define put_cpu()  preempt_enable()

        /*  Linux 2.6,no macro "put_cpu_no_resched()" was defined.  */

      !!  kernel preemption may happen either when a kernel control path is terminated,or when an exception
          handler reenables kernel preemption by means of preempt_enable().
          kernel preemption may happens when deferrable functions are enabled(preempt_count.softirq_count == 0).
      !!

    When Synchronization Is Necessary :
      A critical region is a section of code that must be completely executed by the kernel control path that
      enters it before another kernel control path can enter it.
      interleaving kernel control path,the critical regions may occur in exception handlers,or interrupt handlers,
      or deferrable functions,kernel threads.
      once a critical region has been identified,it must be suitably protected to ensure that any time at most
      one kernel control path is inside that region.
      /*  a single CPU system,a critical region the kernel threads will enter it,just disable interrupt as well;
       *  a single CPU system,a critical region the system call service routines will enter it,just disable
       *  kernel preemption as well.
       *  THINGS IS MORE COMPLICATED ON SMP!(interrupt handler,exception handler,softirq...)
       */

    When Synchronization Is Not Necessary :
      1>  Interrupt handlers and tasklets need not be coded as reentrant functions.
          /*  THAT IS,after CPU received a IRQ,the correspond IRQ line will be disabled by driver until
           *  the interrupt handler finishes execution;
           *  and the same type tasklets cannot be executed by two or more CPUs simultaneously.
           */
      2>  Per-CPU variable accessed by softirqs and tasklets only do not require synchronization.
          /*  Different CPU has different Per-CPU variable.
           *  Softirq can run concurrently on several CPUs,so each Per-CPU variable is not same.
           *  tasklets have same type can not run on two or more CPUs at same time;if there are
           *  two tasklets with different types,they can run on two CPUs at same time,but the
           *  Per-CPU variable is not same.
           *  If Per-CPU variable is accessed by kernel threads(preemptible),then it must be
           *  protected by some synchronizing-mechanisms. 
           */

      3>  A data structure accessed by only one kind of tasklet does not require synchronization.
          /*  same type tasklets can not run simultaneously on two or more CPUs.  */

    !  the best synchronization technique consists in designing the kernel so as to avoid the need for
       synchronization in the first place.

    Synchronization Primitives :
      Linux kernel synchronization techniques :
        1>  Pre-CPU variables (all CPUs)
            duplicate a data structure among the CPUs

        2>  Atomic operation (all CPUs)
            atomic read-modify-write instruction to a counter
            
        3>  Memory barrier (local CPU or all CPUs)
            avoid instruction reordering

        4>  Spin lock (all CPUs)
            lock with busy wait

        5>  Semaphore (all CPUs)
            lock with blocking wait(sleep)

        6>  Seqlocks (all CPUs)
            lock based on an access counter

        7>  Local interrupt disabling (local CPU)
            forbid interrupt handling on a single CPU

        8>  Local softirq disabling (local CPU)
            forbid deferrable function handling on a single CPU

        9>  Read-Copy-Update(RCU) (all CPUs)
            lock-free access to shared data structures through pointers

      Per-CPU Variables :
        the simplest and most efficient synchronization technique.
        basically,a per-CPU variable is an array of data structures,one element
        per each CPU in the system.(__per_cpu_offset[NR_CPUS] used to save the
        offset values for per-CPU variable,each offset value corresponding to
        a CPU in the system.)

        a CPU only be allowed to operates its own element,there is no race condition.
        but per-CPU variables can be used only in particular cases--basically,when
        it makes sense to logically split the data across the CPUs of the system.

        !  per-CPU variable aligned in main memory so that each data structures falls
           on a different line of the hardware cache,concurrent accesses to the per-CPU
           array do not result in cache line snooping and invalidation.

        !  per-CPU variable provides protection against concurrent accesses from several
           CPUs,but can not protects accesses from asynchronous functions,in theses cases,
           additional synchronization technique is required.

        !  per-CPU variable must be accessed while kernel preemption is disabled.

        per-CPU variables operating primitives :
          <linux/percpu-defs.h>

            /*  __PCPU_ATTRS - set up percpu attributes for current cpu.
             *  #  This macro would extends to assembly language.
             */
            #define __PCPU_ATTRS(sec)  \
                    __percpu_attributes__((section(PER_CPU_BASE_SECTION sec)))  \
                    PER_CPU_ATTRIBUTES
  
            /*  section for the DECLARE and for the DEFINE must match  */
  
            #if !defined(ARCH_NEED_WEAK_PER_CPU) && !defined(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)

            /*  normal per-CPU definitions  */
            /*  DECLARE_PER_CPU_SECTION - declare a section for percpu data.
             *                            the section had been defined on other place.
             */
            #define DECLARE_PER_CPU_SECTION(type, name, sec)  \
                    extern __PCPU_ATTRS(sec) __typeof__(type) name
  
            /*  DEFINE_PER_CPU_SECTION - define a section for percpu data.  */
            #define DEFINE_PER_CPU_SECTION(type, name, sec)  \
                    __PCPU_ATTRS(sec) PER_CPU_DEF_ATTRIBUTES \
                    __typeof__(type) name
                    /*  assembly data object in the section ".data" or ".data.percpu",
                     *  type is __typeof__(type),
                     *  symbol is @name.
                     *  ".data" and ".data.percpu" means switch to the corresponding section.
                     */
  
            /*  DECLARE_PER_CPU - declare a per-CPU variable @name with type is @type.  */
            #define DECLARE_PER_CPU(type, name)  \
                    DECLARE_PER_CPU_SECTION(type, name, "")
  
            /*  DEFINE_PER_CPU - define a per-CPU variable @name with type is @type.  */
            #define DEFINE_PER_CPU(type, name)   \
                    DEFINE_PER_CPU_SECTION(type, name, "")
  
            #endif

          <asm-generic/percpu.h>
            #ifdef CONFIG_SMP
            #define PER_CPU_BASE_SECTION  ".data.percpu"

            /*  per_cpu - get the per-CPU variable for local CPU.
             *            SHIFT_PERCPU_PTR() will vertify @var at first,then
             *            call to RELOC_HIDE(&var, __per_cpu_offset[cpu]),
             *            which returns "&var + @offset",the address is the
             *            per-CPU variable instance for the certain cpu @cpu.
             *            finally,derefer to the address for get the per-CPU variable.
             */
            #define per_cpu(var, cpu)  \
              (*SHIFT_PERCPU_PTR(&(var), per_cpu_offset(cpu)))

            /*  __get_cpu_var - select the per-CPU variable @var in local CPU.  */
            #define __get_cpu_var(var)  \
              (*SHIFT_PERCPU_PTR(&(var), my_cpu_offset))

            #else
            #define PER_CPU_BASE_SECTION  ".data"

            /*  NONSMP versions  */
            #define per_cpu(var, cpu)  (*((void)(cpu), &(var)))
            #define __get_cpu_var(var)  (var)

            #endif

          <linux/percpu.h>
            /*  get_cpu_var - get @var from current CPU with preemption disabling.  */
            #define get_cpu_var(var)  (*({  \
                    preempt_disable();       \
                    &__get_cpu_var(var); }))

            /*  put_cpu_var - put @var to current CPU and enable preemption.
             *  #  It is not really write-back value to @var in current CPU,just
             *     enable preemption.
             *     write a per-CPU variable :
             *       assume there is a per-CPU variable is named "pCPUv" which is type of int,
             *       in local CPU.
             *       per_cpu(pCPUv, my_cpu_offset) = 16;  //  okay,macro extending => *Pointer = 16
             */
            #define put_cpu_var(var)  do {  \
                    (void)&(var);           \
                    preempt_enable();       \
                    } while(0)

            /*  get_cpu() and put_cpu() respectively disable preemption and enable preemption.  */

            extern void __percpu *__alloc_percpu(size_t size, size_t align);
            /*  alloc_percpu - alloc a per-CPU variable.
             *  @type:         the type of the per-CPU variable.
             *  return:        pointer points to the per-CPU variable.
             */
            #define alloc_percpu(type)  \
                    (typeof(type) __percpu *)__alloc_percpu(sizeof(type), __alignof__(type)))

            /*  free_percpu - destroy a per-CPU variable pointer got previously from alloc_percpu().
             *  @__pdata:     the pointer to be destroyed.
             */
            extern void free_percpu(void __percpu *__pdata);

            #ifdef CONFIG_SMP

            /*  per_cpu_ptr - return a pointer is typeof(ptr),the address is equal to 
             *                A := ptr + __per_cpu_offset[cpu].
             *                __per_cpu_offset is an array of unsigned long and size is
             *                equal to "sizeof(unsigned long) * NR_CPUS".
             *  !!  per_cpu_offset() is the offset that has to be added to a percpu variable
             *      to get to the instance for a certain processor.
             *  !!  __per_cpu_offset[NR_CPUS] is defined in <arch/x86/kernel/setup_percpu.c>.
             */
            #define per_cpu_ptr(ptr, cpu)  SHIFT_PERCPU_PTR((ptr), per_cpu_offset((cpu)))

            #else

            #define per_cpu_ptr(ptr, cpu)  ({ (void)(cpu); (ptr); })

            #endif

      Atomic Operations : 
        race condition in "read-modify-write" :
          p1 read 0x7fffffff32 in 4 bytes -> p1 modify the value readed from the memory unit -> write back
          result to 0x7fffffff32 in 4 bytes.
          p2 do the some things.
          !  But p1 and p2 process interleaving.
             p1 read -> p2 read -> p2 modify -> p1 modify -> p1 write back -> p2 write back
             now incorrect value is in 0x7fffffff32.

        memory arbiter : a hardware circuit that serializes accesses to the RAM chips.

        atomic operation : such "read-modify-write" operations can not be interrupted by other kernel control path.

        80x86 atomic operation mechanisms :
          1>  assembly language instructions that make zero or one aligned memory access are atomic.
              /*  an unaligned memory access is not atomic.  */
          2>  read-modify-write assembly language instructions(such as inc or dec),
              no other processor is granted to takes the memory bus if there is processor has been readed before write.
              memory bus stealing never happens in a uniprocessor system.
          3>  assembly language instructions whose opcode is prefixed by a rep byte(0xf2,0xf3,which forces the control
              unit to repeat the same instruction several times) are not atomic.
              the control unit checks for pending interrupts before executing a new iteration.
              /*  register ecx as the counter  */

        atomic_t type :
          linux kernel provides a special atomic_t type(an atomically accessible counter) and some functions and macros
          that act on atomic_t variables and are implemented as single,atomic assembly language instructions.
          On multiprocessor systems,each such instruction is prefixed by a "lock" byte.
          /*  macro LOCK_PREFIX is defined in <arch/x86/include/asm/alternative.h> used to replace the LOCK and LOCK
           *  LOCK_PREFIX macro used everywhere in the source code tree.
           *  #  LOCK instruction prefix is introduced in x86 architecture.
           */

          /*  POSIX provides sig_atomic_t type,operations on it is guaranteed no interuppts in middle.
           *  introduced in header <signal.h>
           *  normally,operation on int type and pointer type all is atomic.
           */

          atomic operations :
            <asm-generic/atomic.h>  /*  architecture independ  */
            /*  <arch/x86/include/asm/atomic.h>  architecture depend  */

              atomic_read(v)                ->       return *v
              atomic_set(v, i)              ->       *v = @i
              atomic_add(i, v)              ->       *v += @i
              atomic_sub(i, v)              ->       *v -= @i
              atomic_sub_and_test(i, v)     ->       return (*v -= @i) ? 0 : 1
              atomic_inc(v)                 ->       ++*v
              atomic_dec(v)                 ->       --*v
              atomic_dec_and_test(v)        ->       return (--*v) ? 0 : 1
              atomic_inc_and_test(v)        ->       return (++*v) ? 0 : 1
              atomic_add_negative(i, v)     ->       return ((*v += @i) < 0) ? 1 : 0
              atomic_inc_return(v)          ->       return ++*v
              atomic_dec_return(v)          ->       return --*v
              atomic_add_return(i, v)       ->       return *v += @i
              atomic_sub_return(i, v)       ->       return *v -= @i

              atomic_clear_mask(mask, addr) ->       clear all bits of *addr specified by @mask
              atomic_set_mask(mask, addr)   ->       set all bits of *addr specified by @mask

            <arch/x86/include/asm/bitops.h>
              test_bit(nr, addr)            ->       return the value of the @nr-th bit of *addr
                                                     result : @nr-th bit of *@addr != 0
              set_bit(nr, addr)             ->       set the @nr_th bit of *addr
              clear_bit(nr, addr)           ->       clear the @nr_th bit of *addr
              change_bit(nr, addr)          ->       invert the @nr_th bit of *addr
              test_and_set_bit(nr, addr)    ->       set the @nr_th bit of *addr and return its old value
                                                     the result is
                                                       @old & @mask != 0
                                                       |      |
                                                       |      +--> mask that the bit at @nr is 1,others are 0
                                                       +--> old value of *@addr 
              test_and_clear_bit(nr, addr)  ->       clear the @nr_th bit of *addr and return its old value
              test_and_change_bit(nr, addr) ->       invert the @nr_th bit of *addr and return its old value

      Optimization and Memory Barriers :
        optimizing compiler may reorder assembly language instructions,and modern CPU may process
        instructions parallely(depend on CPU architecture) thus memory access order may be changed,
        these must be avoided when dealing with synchronization.

        optimization barrier primitive ensures that the assembly language instructions corresponding
        to C statements placed before the primitive are not mixed by the compiler with the instructions
        corresponding to the C statements after the primitive.

        Linux barrier() macro :
          <linux/compiler-gcc.h>
            /*  keyword 'memory' forces the compiler to assume that all memory locations in RAM have been
             *  changed by the assembly language instruction.
             *  thus,the compiler cannot optimize the code by using the values of memory locations stored
             *  in CPU registers before the "__asm__" instruction.
             *  Note : optimization barrier do not forbid CPU mix the assembly language instructions.
             *         this is only take affect to assembly language source code.
             *         this is memory barrier.
             */
            #define barrier()  __asm__ __volatile__("": : :"memory")

          e.g.
            int x = 3;
            int y = x * 2;
            int z = 0;

            z = z + x * 2 + y;
            fprintf(stdout, "%d", z);
            /*  the instructions before barrier() will be finished before the statement "z = 6;"
             *  start.
             */

            barrier();

            z = 6;
            /*  suppose,int is not atomic,and compiler place "z = 6;" front to "z = z + x * 2 + y;",
             *  that is 
             *    z = 6;
             *    fprintf(stdout, "%d", z);  /*  print 6,but we expect 12  */
             *    z = z + x * 2 + y;
             */
    
        80x86,the assembly language instructions are said to be "serizalizting" :
          1>  all instructions that operate on I/O ports.
          2>  all instructions prefixed by the "lock" byte.
          3>  all instructions that write into control registers,system registers,or
              debug registers.(cli, sti, eflags.IF)
          4>  the 'lfence', 'sfence', 'mfence' assembly language instructions.
                   RMB       WMB       WRMB
          5>  a few special assembly language instructions;among them,the 'iret' instruction
              that terminates an interrupt or exception handler.

        Linux memory barrier macros :
          <arch/x86/include/asm/system.h>  /*  architecture depends  */
          /*  <asm-generic/system.h>           architecture independs  */

          /*  the following are X86_64 macros,X86_32 macros also are defined in the same file.
           *  e.g.
           *    #define mb()  alternative("lock; addl $0,0(%%esp)", "mfence", X86_FEATURE_XMM2)
           */

          /*  mb - memory barrier for MP and UP.  */
          #define mb()      asm volatile("mfence":::"memory")
        
          /*  rmb - read memory barrier for MP and UP.  */
          #define rmb()     asm volatile("lfence":::"memory")

          /*  wmb - write memory barrier for MP and UP.
           *  # some intel microprocessors never reorder write memory access,so this macro may
           *    instead of "barrier()",but it is keep compiler do not reorder instructions.
           */
          #define wmb()     asm volatile("sfence":::"memory")

          /*  smp_mb - CONFIG_SMP,otherwise "barrier()".  */
          #define smp_mb()  mb()

          /*  smp_rmb - CONFIG_X86_PPRO_FENCE,otherwise "barrier()".  */
          #define smp_rmb() rmb()

          /*  smp_wmb - CONFIG_X86_OOSTORE,otherwise "barrier()".  */
          #define smp_wmb() wmb()

          /*  the macros prefixed "smp_" only take affect on multiprocessor system.
           *  no such prefix are take affect on multiprocessor system and uniprocessor system.
           */

      Spin Lock :
        spin locks are a special kind of lock designed to work in a multiprocessor environment.
        if a spin lock had been held by another kernel control path,then current kernel control path
        who is waiting for the lock will repeatedly executing a tight instruction loop,until the lock
        is released.

        !  the instruction loop of spin locks represents a "busy wait".the waiting kernel control path
           keeps running on the CPU,even if it has nothing to do besides waste time.
           some works resource in kernel space may be locked a fraction of a millisecond only,spin lock
           would be far more time-conmusing to release the CPU and reacquire it later.

        !  kernel preemption is disabled in every critical region protected by spin locks.
           kernel preemption is still enabled in "busy wait" phase.

        <linux/spinlock_types.h>
          /*  raw_spinlock_t - raw spinlock type.
           *  @raw_lock:       it is the architecture depends spinlock structure,
           *                   which is defined in <arch/x86/include/asm/spinlock_types.h>,
           *                   typedef struct arch_spinlock arch_spinlock_t
           *                   it has member @slock is type of unsigned int .
           *  @break_lock:     flag signaling that a process is busy waiting for the lock.
           */
          typedef struct raw_spinlock {
                  arch_spinlock_t raw_lock;
          #ifdef CONFIG_GENERIC_LOCKBREAK
                  unsigned int break_lock;
          #endif
                  ...
          } raw_spinlock_t;

          typedef struct spinlock {
                  union {
                          struct raw_spinlock rlock;
                  };
          } spinlock_t;

          !!  UP and SMP have the different __ARCH_SPIN_LOCK_UNLOCKED macro definitions.
              UP :
                macro __ARCH_SPIN_LOCK_UNLOCKED is defined in <linux/spinlock_types_up.h>
                extend to { 1 } or { }  /*  CONFIG_DEBUG_SPINLOCK or !CONFIG_DEBUG_SPINLOCK  */
                on uniprocessor environment,spin lock is useless,because everytime only one
                kernel control path is executing.
                thus,acquire spin lock just decrease @slock in 1 and increase @slock in 1 to
                release spin lock.

              SMP :
                macro __ARCH_SPIN_LOCK_UNLOCKED is defined in <arch/x86/include/asm/spinlock_types.h>
                extend to { 0 }
                on Symmetrical Multi-Processor environment,spin lock is based ticket concept.a FIFO
                is constructed to queue the CPUs acquire the spin lock.@slock is encoded to represents
                the FIFO.

        <linux/spinlock.h>
          /*  spin_lock_init - initializer.
           *  @_lock:          a pointer to spinlock_t.
           */
          #define spin_lock_init(_lock)     \
          do {                              \
                  spinlock_check(_lock);    \
                  raw_spin_lock_init(&(_lock)->rlock);  \
          } while (0)

          /*  spin_lock - lock a spinlock.
           *  @lock:      a pointer point to spinlock_t object.
           */
          static inline void spin_lock(spinlock_t *lock);

          /*  spin_unlock - unlock.  */
          static inline void spin_unlock(spinlock_t *lock);

          /*  spin_unlock_wait - wait until spinlock gets unlocked.
           *                     it just query whether the spinlock
           *                     gets unlocked.
           *  #  arch_spin_unlock_wait() will be called,
           *     which enter a null cycle until the spinlock is released.
           */
          static inline void spin_unlock_wait(spinlock_t *lock);

          /*  spin_is_locked - it is locked?  */
          static inline void spin_is_locked(spinlock_t *lock);

          /*  spin_trylock - try to get spinlock.
           *                 return 1 -> succeed to get lock
           *                 return 0 -> failed to get lock               
           */
          static inline void spin_trylock(spinlock_t *lock);

          /*  all the functions above call to the corresponding raw_spin_*() functions,and them
           *  will call to _raw_spin_*() APIs,these APIs are declared in <linux/spinlock_api_smp.h>,
           *  they call to the primary functions with prefix "__raw_spin_".
           *  if CONFIG_INLINE_SPINLOCK was defined,then the primary functions are defined in the same
           *  header of _raw_spin_*(),otherwise,use the function definition in <kernel/spinlock.c> to
           *  instead.
           *  macron BUILD_LOCK_OPS(op, locktype) was defined in <kernel/spinlock.c>,this macro generate
           *  lockfunc such "__raw_##op##_lock(locktype##_t *lock)".
           */

        /*  very interesting things i had found out :
         *    the architecture specified functions in <arch/x86/include/asm/spinlock.h>
         *
         *      void __ticket_spin_lock(arch_spinlock_t *lock)
         *        asm volatile (
         *                LOCK_PREFIX "xaddw %w0, %1\n\t"  
         *                ...
         *                "cmpb %h0, %b0\n\t"
         *                "je 2f\n\t"  #  return
         *                ...
         *        );
         *        #  %0 -> @inc(0x0100) variable =Q (eax | ebx | ecx | edx)
         *           %h0 -> high 8-bits of the register  (i.e. ah)
         *           %b0 -> below b-bits of the register (i.e. al)
         *           %1 -> memory address of lock->slock
         *        #  xadd {  "w" a word
         *                   exchange %0 and %1
         *                   add %0 to %1,save result in %1
         *           }
         *        #  cmpb {
         *                   %b0 - %h
         *                   if result = 0 => ZF = 1
         *                   else => ZF = 0
         *           }
         *        #  default value of @lock->slock is zero,@lock->slock += 0x0100 to acquire lock
         *           high 8-bits equal to low-8-bits means no one is holding the lock
         *
         *      int __ticket_spin_trylock(arch_spinlock_t *lock)
         *        asm volatile (
         *                "movzwl %2, %0\n\t"  #  %2 -> &lock->slock, %0 -> tmp variable in eax
         *                                     #  zero-extend to eax,contents in ax is copied from
         *                                     #  @lock->slock
         *                "cmpb %h0, %b0\n\t"  #  same as above
         *                ...
         *        );
         *    !!  lock primitive waits until spin lock is able to be locked.
         *        trylock primitive does not wait.if it is can not get lock,return 0,otherwise,1.
         *
         *    these functions are implemented via inline assembly.
         *    ticket concept is introduced,that is :
         *      ticket is a queue has two parts,one is head,one is tail
         *      the @slock follows FIFO
         *      when need to acquire the lock,just atomically noting the tail and increase it by one
         *      then,wait until head becomes the initial value of the tail
         *
         *    so,acquire lock is not decrease @slock by one,it is increase @slock by 0x0100(256).
         *    unlock stay increase @slock by one.
         *
         *    Ticket Queue  <- @slock
         *    [high-32bits][high-16bits][high-8bits][low-8bits]  <= unsigned int
         *                               ah          al
         *                               TAIL        HEAD
         *      increment := 0x0100 -> 256     <- x86
         *      increment := 0x00000100 -> 256 <- x86-64
         *      ah in default : 00000000
         *      al in default : 00000000
         *  
         *      x86 =>
         *        kcpA try acquire lock
         *          ah = al => no one is holding the lock now
         *          ax += 0x0100
         *            =>  ah : 00000001
         *                al : 00000000
         *                ax : 0000000100000000
         *         
         *        kcpB try acquire lock
         *          ah != al => someone is holding the lock now
         *          set @condition_indicator := ax
         *            =>  ah : 00000001
         *                al : 00000000
         *          ax += 0x0100  -> queue itself to the tail of FIFO
         *            =>  ah : 00000010
         *                al : 00000000
         *          kcpB spin,enter a cycle
         *            movb @slock->al, @condition_indicator->al
         *            check if @condition_indicator->ah = @condition_indicator->al
         *       
         *        kcpA release lock
         *          @slock++
         *            =>  ah : 00000010
         *                al : 00000001
         *                ax : 0000001000000001
         *       
         *        kcpB finds out lock had been released
         *          movb @slock->al, @condition_indicator->al
         *            =>  ah : 00000001
         *            =>  al : 00000001
         *          lock is available
         *          @slock : 00000010 00000001
         *          return
         *
         *    even so,spin lock is stay allows one kernel control path to holding it at the same time.
         */

        The spin_lock macro with kernel preemption :
          when try to get a spinlock,the caller must call to spin_lock() function.
          which call to raw_spin_lock(),next the _raw_spin_lock() is called,finally,__raw_spin_lock() would
          be called.

          /*  kernel control path is not allowed to be preempted when it holds a spinlock.  */

          the following actions that __raw_spin_lock() takes :
            1>  enter a infinite for-cycle.
            2>  disable kernel preemption.
            3>  call to do_raw_spin_trylock(),thus,the architecture depended function arch_spin_lock()
                would be called,which is defined in <arch/x86/include/asm/spinlock.h> as a inline function.
            4>  if succeed to get the spinlock,break the cycle and return,otherwise,enable kernel preemption
                and get to the next.
            5>  set @lock->break_lock := 1,enter a nested cycle.
            6>  while !raw_spin_can_lock AND (lock)->break_lock
                        arch_spin_relax(&lock->raw_lock)
                /*  raw_spin_can_lock() checks whether the lock is able to be holden.
                 *  lock->break_lock = 1 means a kernel control path is waiting on the lock,but kernel
                 *  is allowed to be preempted.
                 *  arch_spin_relax() is equal to cpu_relax().
                 */
                
                if the lock can be holded,then break while-cycle and try to get the lock again.(jump to 2>)

        The spin_lock macro without kernel preemption(uniprocessor) :
          if no kernel preemption is configured,and no spinlock debug is configured too,
          then,the raw_spinlock_t is a NULL structure.
          for uniprocessor system,macro _raw_spin_lock() is macro __LOCK(),which just a compiler attribute
          "__context__(expression, in_context, out_context)".

          the __LOCK() just disable kernel preemption as well,it extended to :
            <linux/spinlock_api_up.h>
              /*  __LOCK - disable kernel preemption,check if lock is equal to 1(compiler checking).
               *  #  if kernel is not configured with preemption,then preempt_disable() do nothing,
               *     otherwise,the preempt count would increase and barrier() would be called.
               *  #  __acquire(x) is defined in <linux/compiler.h>,which is "__conext__(x,1)",
               *     conversely,__release(x) is defined in the same file,which is "__context__(x,-1)".
               */
              #define __LOCK(lock)  \
                do { preempt_disable(); __acquire(lock); (void)(lock); } while (0)

            /*  no CONFIG_PREEMPT on smp,it is same to CONFIG_PRREMPT configured on smp,but
             *  preempt_disable() and preempt_enable() do nothing as well.
             *  !  UP SPINLOCK DEFINITION IS DIFF TO SMP SPINLOCK DEFINITION.
             */

        The spin_unlock macro :
          it add 1 to spinlock_t object for release it(0->locked,1->unlocked),and then enable kernel preempt.
          /*  "lock" byte is not used,because write-only access always atomically executed by the current
           *  80x86 microprocessors.
           */

      Read/Write Spin Locks :
        rw spin lock,it is a lock allow more than one readers hold it for reading at same time;only one writer
        is allowed to holds it for writing at same time.

        <arch/x86/include/asm/rwlock.h>
          /*  RW_LOCK_BIAS - indicates the initializing state.no reader and no writer.  */
          #define RW_LOCK_BIAS        0x01000000

        <linux/rwlock_types.h>
          /*  rwlock_t - linux rw spin lock type.
           *  @raw_lock: architecture depends rw spin lock type.it is defined in
           *             <arch/x86/include/asm/spinlock_types.h>,
           *             it has a member @lock is type of unsigned int.
           *  @break_lock: whether there is a kernel control path waiting for the rw spin lock currently.
           *  #  @lock is encoded to includes two pieces of information :
           *       1>  bits [0, 23] -> the number of kernel control paths currently reading.
           *       2>  bit 24 -> unlock flag.1 -> unlocked, 0 -> locked.
           */
          typedef struct {
                  arch_rwlock_t raw_lock;
          #ifdef CONFIG_GENERIC_LOCKBREAK
                  unsigned int break_lock;
          #endif
                  ...
          } rwlock_t;

          /*  rwlock_init - initialize a rwlock_t object *@lock.
           *  @lock:        pointer points to an object is type of rw_lock_t.
           *  #  this version is no CONFIG_DEBUG_SPINLOCK defined.
           *  #  raw_lock.lock := RW_LOCK_BIAS
           *     break_lock := 0
           */
          #define rwlock_init(lock)        \
                  do { *(lock) = __RW_LOCK_UNLOCKED(lock); } while (0)

        Getting and releasing a lock for reading :
          <linux/rwlock.h>
            /*  read_lock - lock a rw spin lock for reading.
             *  @lock:      pointer points to an object is type of rwlock_t.
             */
            #define read_lock(lock)        _raw_read_lock(lock)

            /*  read_unlock - unlock a locked rwlock is for reading.
             *  @lock:        pointer points to an object is type  of rwlock_t which had been
             *                locked for reading.
             *  #  call list:
             *       _raw_read_unlock() -> __raw_read_unlock() {
             *                                     rwlock_release();
             *                                     do_raw_read_unlock();
             *                                     preempt_enable();
             *                             }
             */
            #define read_unlock(lock)      _raw_read_unlock(lock)

            /*  do_raw_read_trylock - try to get rwlock for reading.
             *  @rwlock:              pointer points to an object is type of rwlock_t.
             */
            #define do_raw_read_trylock(rwlock)        arch_read_trylock(&(rwlock)->raw_lock)

          <linux/rwlock_api_smp.h>
            /*  _raw_read_lock - internal routine for get rwlock to reading.
             *  @lock:           rwlock_t pointer,can not be NULL.
             *  #  this functions is defined in <kernel/spinlock.c>,if no
             *     CONFIG_INLINE_READ_LOCK defined.in the case,this function
             *     calls to __raw_read_lock(),which is build by macro BUILD_LOCK_OPS().
             *     thus,__raw_read_lock() is similar to __raw_spin_lock(),
             *     do_raw_read_trylock() would be called for get the rwlock.
             */
            void __lockfunc _raw_read_lock(rwlock_t *lock)        __acquires(lock);

          <arch/x86/include/asm/spinlock.h>
            /*  arch_read_trylock - architecture depended trylock for rwlock to reading.
             *  @lock:              pointer points to an object is type of arch_rwlock_t.
             *  return:             1 -> got lock,0 -> failed.
             */
            static inline int arch_read_tyrlock(arch_rwlock_t *lock);

            /*  this function used atomic operations on @lock :
             *    1>  convert @lock to atomatic_t *,because the structure just one member arch_rwlock_t.lock,
             *        so there is no padding in the structure,and the member is placed in the head of structure.
             *    2>  call to atomic_dec_return(@lock),it does --*v .
             *    3>  if ret value @v >= 0,then it is succeed to hold the lock for reading.
             *        the value maybe 0x01000000 - 1 = 0x00ffffff,or 0x00ffffff - 1 ...
             *        !  if a negative value is returend(0x00000000 - 1),that means there is a writer holding
             *           the rwlock,then call to atomic_inc() to restore the value of @lock.
             *    4>  if no writing is holding this lock,return 1,otherwise,increase @v and then return 0.
             *    !!  EVEN arch_rwlock_t.lock IS TYPE OF unsigned int,BUT atomatic_t IS TYPE OF int,THUS NO
             *        INCORRECT RESULT WILL APPEAR.
             */

          !!  it is also forbid holding rwlock to sleep,thus kernel preempt will be disabled after got the lock.
              rwlock locking action is busy waiting,too.
          !!  uniprocessor version of _raw_read_lock() is defined in <linux/spinlock_api_up.h>,it is a macro would
              be insteaded by "__LOCK(lock)".
          !!  Linux-2.6.34.1,BUILD_LOCK_OPS() macro builds preemption-friendly version for spin lock functions.
              thus,even kernel preemption is not selected,__raw_read_lock() stay defined by this macro.(no
              CONFIG_INLINE_READ_LOCK)
              "understanding the linux kernel" indicates if no kernel preemption is configured,read_lock() macro
              would be expanded to some assembly language code.
              /*  ENTRY(__read_lock_failed) stay existed in <arch/x86/lib/rwlock_64.S>,and arch_read_lock() will
               *  jump to the entry if failed to got rwlock for reading.actually,arch_read_lock() is composed in
               *  some assembly language code,not in C language code.
               */

        Getting and releasing a lock for writing :
          it is similar to getting and releaing a lock for reading.
          <linux/rwlock.h>
            /*  write_lock - get rwlock for writing.
             *  @lock:       pointer points to an object is type of rwlock_t.
             *  #  call list:
             *       _raw_write_lock() -> __raw_write_lock() -> do_raw_write_trylock() -> arch_write_trylock()
             */
            #define write_lock(lock)        _raw_write_lock(lock)

            /*  write_unlock - unlock a locked rwlock is for writing.
             *  @lock:         pointer points to an object is type of rwlock_t which had been locked for
             *                 writing.
             *  #  call list:
             *       _raw_write_unlock() -> __raw_write_unlock() {
             *                                      rwlock_release();
             *                                      do_raw_write_unlock();
             *                                      preempt_enable();
             *                              }
            #define write_unlock(lock)      _raw_write_unlock(lock)

          <arch/x86/include/asm/spinlock.h>
            /*  arch_write_trylock - architecture depended trylock for rwlock to writing.
             *  @lock:               pointer points an object is type of arch_rwlock_t.
             *  return:              1 -> succeed,0 -> failed.
             *  #  this function is similar to arch_read_trylock(),but it call to
             *     atomic_sub_and_test(RW_LOCK_BIAS, @lock) and atomic_add(RW_LOCK_BIAS, @lock) to
             *     instead atomic_dec_return(),atomic_add(),respectively.
             *     if @lock = RW_LOCK_BIAS(0x01000000)
             *       atomic_sub_and_test() return 1  /*  (*v -= @i) ? 0 : 1  */
             *       that means no kernel control path is holding this rwlock now
             *     else
             *       ret value = 0
             *       then,atomic_add() restore @lock
             *       /*  x := @lock - RW_LOCK_BIAS
             *        *  @lock := x + RW_LOCK_BIAS
             *        */
             */
            static inline int arch_write_trylock(arch_rwlock_t *lock);

      Seqlocks :
        sequence lock is similar to rw spin lock,but writer has higher priority than readers.
        of course,only one writer is activing at the same time,and reader may have to reads the 
        share data structure several times for get the valid copy.

        <linux/seqlock.h>
          /*  seqlock_t - a structure type represent seqlock.
           *  @sequence:  sequence value,only writer is allowed to change it.
           *              even number -> no writer is activing
           *              odd number  -> a writer is activing
           *  @lock:      spin lock as the basic lock for seqlock.
           */
          typedef struct {
                  unsigned sequence;
                  spinlock_t lock;
          } seqlock_t;

          /*  SEQLOCK_UNLOCKED - initialize value for the seqlock.
           *                     it equals to {0, __SPIN_LOCK_UNLOCKED(old_style_seqlock_init)}.
           */
          #define SEQLOCK_UNLOCKED        __SEQLOCK_UNLOCKED(old_style_seqlock_init)

          /*  seqlock_init - macro for initialize a seqlock @x.
           *                 the state of @x after this macro completed is same as
           *                 *@x = SEQLOCK_UNLOCKED
           *  @x:            pointer of seqlock.
           */
          #define seqlock_init(x)

          /*  write_seqlock - lock function for writer on seqlock.
           *  @sl:            pointer of seqlock.
           *  #  this function increase @sl->sequence and lock @sl->lock .
           */
          static inline void write_seqlock(seqlock_t *sl);

          /*  write_sequnlock - unlock function for writer on seqlock.
           *  @sl:              pointer of seqlock.
           *  #  this function increase @sl->sequence and unlock @sl->lock .
           */
          static inline void write_sequnlock(seqlock_t *sl);

          /*  because the basic lock is spin lock,thus when writer lock a seqlock,the kernel
           *  preempt is automatically disabled,and automatically enabled when the writer
           *  unlock the seqlock.
           *  stack order : lock spin lock -> ++sequence (lock)
           *                ++sequence     -> unlock spin lock (unlock)
           */

          /*  read_seqbegin - seqlock routine for reader begin to access the share data structure
           *                  which is protected by a seqlock.
           *  @sl:            pointer of seqlock.
           *  return:         @sl->sequence and it is a even number.
           *  #  this function should be called only once when reader enter a critical region
           *     to access the shared data.
           */
          static __always_inline unsigned read_seqbegin(const seqlock_t *sl);

          /*  read_seqretry - seqlock routine for reader to check if @sl->sequence == @start.
           *  @sl:            pointer of seqlock.
           *  @start:         a value which should get from read_seqbegin().
           *  return:         @sl->sequence != @start
           *  #  this function is used check whether a writer is writing the shared data.
           *     if @sl->sequence == @start(from read_seqbegin(),a even number),that means no writer
           *     is activing.
           *     else,a writer is activing and the copy readed previously is not completed,reader
           *     should read it again.
           */
          static __always_inline int read_seqretry(const seqlock_t *sl, unsigned start);

          e.g.
            =>  seqlock_t *seqlock
            unsigned int ret = 0;
            do {
                    ret = read_seqbegin(seqlock);  /*  ret must be a even number,otherwise,
                                                    *  read_seqbegin() will waiting until the writer
                                                    *  leaved.
                                                    */
                    /*  critical region,read the copy of shared data  */
                    ...
            } while (read_seqretry(seqlock, ret));
                    /*  if @ret != @seqlock->sequence
                     *          a writer changed the shared data
                     *          OR
                     *          a writer is activing
                     *  each case,reader have to read shared data again.
                     */

        seqcount_t :
          sequence mechanism for the code has its own mutex to protects the critical region.

          <linux/seqlock.h>
            /*  seqcount_t - version of sequence only.  */
            typedef struct {
                    unsigned int sequence;
            } seqcount_t;

          operations :
            read_seqcount_begin()
            read_seqcount_retry()
            write_seqcount_begin()
            write_seqcount_end()

            /*  obviously,suffix for write functions is "begin" or "end",because no lock in seqcount_t.  */

        rules for seqlock usage :
          1>  the structure does not include pointers that are modified by the writers and
              dereferenced by the readers.
          2>  the code in the critical regions of the readers does not have side effects.
              (otherwise,multiple reads would have different effects from a single read)
          3>  the critical regions of the readers should be short and writers should seldom acquire the seqlock.
              (read_seqbegin() is busy wait)

      Read-Copy Update (RCU) :
        RCU is lock free,allows many readers and many writers to proceed concurrently.
        it is designed to protect data structures that are mostly accessed for reading by several CPUs.

        the key for RCU :
          1>  only data structures that are dynamically allocated and referenced by means of pointers
              can be protected by RCU.
          2>  no kernel control path can sleep inside a critical region protected by RCU!!

        <linux/rcupdate.h>
          /*  rcu_head - rcu structure,usually embedded inside the data structure to be freed.
           *  @next:     next update requests in a list.
           *  @func:     actual update function to call after the grace period.
           */
          struct rcu_head {
                  struct rcu_head *next;
                  void (*func)(struct rcu_head *head);
          };

          /*  rcu_read_lock - mark the beginning of an RCU read-side critical section.  */
          static inline void rcu_read_lock(void);

          /*  rcu_read_unlock - mark the end of an RCU read-side critical section.  */
          static inline void rcu_read_unlock(void);

          !!  RCU mechanism do very little for read-side to prevent race conditions,write-side work a bit more.
          !!  no rcu_write_lock() is there,because there is no way for writers to lock out RCU readers,
              but writers must coordinate with each other.(use spinlock or other protection mechanism)
              RCU does not care how the writers keep out of each others' way.

          /*  call_rcu - queue an RCU callback for invocation after a grace period.
           *  @head:     structure to be used for queueing the RCU updates.
           *  @func:     actual update function to be invoked after the grace period.
           *  #  callback maybe deferred until all read-side critical sections exited,
           *     and read-side critical section is allowed be nested.
           */
          extern void call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *head));

        <kernel/rcutree_plugin.h>
          /*  synchronize_rcu - wait until a grace period has elapsed.
           *  #  function return maybe deferred until all read-side critical sections exited.
           *  #  EXPORT_SYMBOL_GPL(synchronize_rcu)
           */
          void synchronize_rcu(void);

        RCU "quiescent state" in Linux Kernel :
          classic RCU read-side critical sections are confined to kernel code and are not permitted
          to block.this means that any time a given CPU is seen either blocking,in the idle loop,or
          exiting the kernel,we know that all RCU read-side critical sections that were previously
          running on that CPU must have completed.
          such states are called "quiescent states",and after each CPU has passed through at least
          one quiescent state,the RCU grace period ends.

        RCU mechanism initialization in Linux Kernel :
          Linux kernel supports two kinds of RCU,one is for SMP environment,another is for uniprocessor
          environment - "bloatwatch edition".the headers are <linux/rcupdate.h> and <linux/rcutiny.h>,
          respectively.
          kernel initializes RCU mechanism during system booting through function rcu_init().

          <linux/rcupdate.h>
          /**
           * rcu_init - initializing function for initializes RCU mechanism for mutual exclusion
           */
          extern void rcu_init(void);
          /* void __init rcu_init(void); - <kernel/rcutree.c> */

          !!  RCU nodes are combined together through robin-tree.

          what rcu_init() to does :
            1>  announce rcu bootup.
            2>  initialize rcu flavor - @rcu_sched_state with @rcu_sched_data AND @rcu_bh_state with @rcu_bh_data.
            3>  call to static function __rcu_init_preempt() defined in <kernel/rcutree_plugin.h> to initialize
                rcu flavor - @rcu_preempt_state with @rcu_preempt_data.
                /**
                 * assign leaf node pointers into each CPU's rcu_data structure.
                 */
            4>  register RCU_SOFTIRQ deferrable function "rcu_process_callbacks()".
            5>  register cpu notifier @rcu_cpu_notify.
            6>  for each online CPUs call once rcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)@cpu).
                this will make CPUs to initialize per-CPU variables for sched state,bh state,and preempt state.

          <kernel/rcutree.c>
            /**
             * rcu_process_callbacks - the softirq RCU_SOFTIRQ handler
             */
            static void rcu_process_callbacks(struct softirq_action *unused);
            
            this function checks the quiescent state for @rcu_sched_state and @rcu_bh_state,if any callback is
            ready,then invoke it. (__rcu_process_callbacks())
            then deal with preemptable RCU @rcu_preempt_state.
            finally,check if any future RCU-related work will need to be done by current CPU.(through function
            rcu_needs_cpu_flush(),which call to rcu_needs_quick_check() to do primary checking.if there
            are some,then have to deal with them.even if none need be done immediately,returning 1 if so,but
            its return value is unused.)
                
        RCU principle :
          reader :
            read RCU data,it maybe old data or new data.

          writer :
            read RCU data -> make a copy -> modify copy -> write back.
            /*  writer back:
             *    use the new copy's address to instead the old address of the data structure
             *    it protected by RCU.(modify a pointer,operation on pointer is atomic)
             */

          usually,rcu_head is embedded in a data structure which is protected by RCU mechanism.
          each reader or writer sees either the old copy or the new one,no corruption in the
          data structure may occur.

          the old copy of the data structure cannot be freed right away when the writer updates
          the pointer.the old copy can be freed only afer all (potential) readers on the CPUs
          have executed the rcu_read_unlock() macro(exit read-side critical section).
          the kernel requires every potential reader to execute that macro before :
            1>  the CPU performs a process switch
            2>  the CPU starts executing in User Mode
            3>  the CPU executes the idle loop
            /*  in each of these cases,we say that the CPU has gone through a quiescent state.  */

          call_rcu() stores the @func in @head->func,and inserts @head in a per-CPU list of callbacks.
          when all CPUs have gone through a quiescent state,a local tasklet--whose descriptor is stored
          in the @rcu_tasklet per-CPU variable--executes all callbacks in the list.
          /*  kernel checks whether the local CPU has gone through a quiescent state,periodically.  */

        e.g.
          struct INTStruct {
                  int x;
                  struct rcu_head rcu;
          };

          INTStruct *pints = NULL;

          init:
            pints = kmalloc(sizeof(struct INTStruct, GFP_KERNEL));
            pints->x = 0;
            pints->rcu.next = NULL;
            pints->rcu.func = NULL;

          reader:
            rcu_read_lock();    /*  begin of RCU read-side section  */
            ...
            rcu_read_unlock();  /*  end of RCU read-side section  */

          writer:
            void pints_update_func(struct rcu_head *head)
            {
                    struct INTStruct *tmp = pints;

                    /*  if more than one writer will modify @pints concurrently,
                     *  there should have a synchronous mechanism to protect the
                     *  shared data structure. i.e. linux spin lock
                     */

                    pints = container_of(head, struct INTStruct, rcu);
                    kfree(tmp);  /*  free the old data structure  */
            }

            struct INTStruct *copy = kmalloc(sizeof(struct INTStruct), GFP_KERNEL);
            if (IS_ERR(copy))
              panic("RCU make copy was failed.");

            /*  make a copy  */
            rcu_read_lock();

            copy->x = pints->x;
            copy->rcu = pints->rcu;

            rcu_read_unlock();

            copy->x += 8;  /*  modify copy  */

            call_rcu(&copy->rcu, pints_update_func);  /*  queue an RCU callback  */

      Semaphores :
        Linux offers two kinds of semaphores :
          1>  Kernel semaphore,used by kernel control paths.
          2>  SystemV semaphore,used by User Mode processes.
          /*  3>  POSIX semaphore,used by User Mode processes.  */

        kernel semaphore is similar to spin lock,kernel semaphore does not allow a kernel control path
        to proceed unless the lock is open.
        kernel semaphores can be acquired only by functions that are allowed to sleep,that is kernel 
        control path would be blocked to wait for busy resource until it is released.

        <linux/semaphore.h>
          /*  semaphore - a structure represents the kernel semaphore.
           *  @lock:      kernel spin lock,concurrently writing protection for kernel semaphore.
           *              when sempahore APIs accessing to this structure,@lock will be locked and
           *              local interrupt request would be disabled until unlocked.
           *  @count:     @count > 0 => protected resource is free
           *              @count = 0 => semaphore busy,that is the resource is acquired by other.
           *                            "but no process is waiting for it"-(understanding linux kernel)
           *              "@count < 0 => resource is unavailable,and at least one process is waiting
           *                            for the resource"-(understanding linux kernel)
           *              #  but the type of @count is unsigned int,thus no negative value is possible
           *                 to exist(Linux 2.6.34.1)
           *              the initialized @count indicates how many processes are able to use the
           *              resource at the same time.
           *              because it is not recommend to access kernel semaphore members directly,
           *              thus,the initialized value of @count have to be determined when designing.
           *              #  creator is able to modify @count after initialized that is do not care
           *                 the suggestion or call up() once or more than once(@wait_list must be NULL).
           *                 IT IS NOT RECOMMEND,IT IS WORST,BECAUSE KERNEL DOES NOT PROVIDE SUCH
           *                 APIs TO CHANGE THE MAXIMUM NUMBER OF PROCESSES THAT ARE ALLOWED TO
           *                 ACCESS THE PROTECTED RESOURCE AT THE SAME TIME!!SUCH THING IS DETERMINED
           *                 AT DESIGNING.
           *  @wait_list: the sleeping processes on the kernel semaphore.
           *              if this list is NULL,that means no task is waiting for the semaphore.
           *  #  Must prevent access the member of this structure directly.
           *  #  if initialize a kernel semaphore with value 1,it will works as if a mutex.
           */
          struct semaphore {
                  spinlock_t lock;
                  unsigned int count;
                  struct list_head wait_list;
          };
            
          /*  DECLARE_MUTEX - declare and initialize a semaphore @name with value 1,it will works as if a
           *                  mutex.(@name.@count := 1)
           */
          #define DECLARE_MUTEX(name)        struct semaphore name = __SEMAPHORE_INITIALIZER(name, 1)

          /*  sema_init - initialize a kernel semaphore.
           *  @sem:       kernel semaphore pointer.
           *  @val:       the value.(@sem->@count := @val)
           */
          static inline void sema_init (struct semaphore *sem, int val);

          /*  init_MUTEX - initialize a mutex.  */
          #define init_MUTEX(sem)        sema_init(sem, 1)

          /*  init_MUTEX_LOCKED - initialize a mutex and lock it,at this time,
           *                      @sem->lock is unlocked,just @sem->count == 0(busy).
           */
          #define init_MUTEX_LOCKED(sem) sema_init(sem, 0)

        Getting and releasing semaphores :
          <linux/semaphore.h>
            /*  down - acquires the semaphore pointed by @sem.
             *  @sem:  kernel semaphore pointer.
             *  #  this function locks @sem->lock before checks @sem->count.
             *     if @sem->count > 0
             *       --(@sem->count)
             *     else
             *       call __down(@sem)
             *  #  if resource is unavailable,this function will put the task to sleep until
             *     the semaphore is released.
             *     this is,if semaphore is available,down() returns immediately with the side-effect
             *     "--(@sem->count)";otherwise,task is suspended until semaphore is available,in this
             *     case,"--(@sem->count)" would not occurs.of course,if a waiter is waiting for the
             *     semaphore,a up() calling would not process ++(@sem->count).
             *     another word,one leave and one enter,it is useless to restore value of semaphore.
             *  #  this function is deprecated.
             */
            extern void down(struct semaphore *sem);

            /*  down_trylock - try to acquire kernel semaphore without waiting.
             *  @sem:          kernel semaphore pointer.
             *  rerurn:        succeed to acquire -> 0
             *                 failed to acquire  -> 1
             */
            extern int __must_check down_trylock(struct semaphore *sem);

            /*  down_interruptible - acquire kernel semaphore,but set @current->state to
             *                       TASK_INTERRUPTIBLE.
             *  @sem:                kernel semaphore pointer.
             *  return:              0 OR -EINTR
             */
            extern int __must_check down_interruptible(struct semaphore *sem);

            /*  down_killable - acquire kernel semaphore,but set @current->state to
             *                  TASK_KILLABLE.
             *  @sem:           kernel semaphore pointer.
             *  return:         0 OR -EINR(interrupted by a fatal signal)
             */
            extern int __must_check down_killable(struct semaphore *sem);

            /*  down_timeout - acquire kernel semaphore,but have a timer to indicates
             *                 the maximum time to waiting for.
             *                 @current->state will be seted with TASK_UNINTERRUPTIBLE.
             *  @sem:          kernel semaphore pointer.
             *  @jiffies:      system timer jiffy.
             *                 schedule_timeout(@jiffies) would be called in assist routine.
             *  return:        0 OR -ETIME
             */
            extern int __must_check down_timeout(struct semaphore *sem, long jiffies);

            /*  up - releases the semaphore pointed by @sem.
             *  @sem:  kernel semaphore pointer.
             *  #  this function locks @sem->lock before checks @sem->count.
             *     if NULL(@sem->wait_list)
             *       ++(@sem->count)
             *     else
             *       call __up(@sem)
             *       in this case,the value of @sem->count will not increase,wake up the first 
             *       waiter in the waiting list to instead.
             */
            extern void up(struct semaphore *sem);

          <kernel/semaphore.c>
            /*  semaphore_waiter - structure represents a waiter on a specified kernel semaphore.
             *  @list:             linux list,every semaphore_waiter will be linked to a list
             *                     which is specified by @semaphore.list .
             *  @task:             struct task_struct *,the waiting process.
             *  @up:               indicate that whether a process which holding the protected resource
             *                     did up() calling.(that means someone released the kernel semaphore)
             */
            struct semaphore_waiter {
                    struct list_head list;
                    struct task_struct *task;
                    int up;
            };

            /*  __down_common - generic __down function.
             *  @sem:           the kernel semaphore.
             *  @state:         the task state expected to be seted when scheduling.
             *  @timeout:       timeout in jiffies.
             *  return:         0 OR -EINTR OR -ETIME
             *  #  other __down*() call to this function with different task state.
             *  #  this function create a semaphore_waiter{list, @current, 0},
             *     then list_add_tail @waiter into @sem->wait_list via @waiter->list.
             *     enter infinite for-cycle {
             *             if task is interrupted by signal,return -EINTR
             *             if expired,return -ETIME
             *             set task state with @state
             *             unlock @sem->lock
             *             call to schedule_timeout()
             *             lock @sem->lock
             *             if @waiter-> up == 1, return 0
             *     }
             *     !! before return,waiter would be removed from @sem->wait_list.
             */
            static inline int __sched __down_common(struct semaphore *sem, long state, long timeout);

            /*  __up - up function for the case one or more process is waiting for semaphore.
             *  @sem:  kernel semaphore pointer.
             *  #  this function retrieve the first waiter in @sem->wait_list,
             *     then set @waiter->up = 1,next wake up the task(@waiter->task).
             */
            static noinline void __sched __up(struct seamphore *sem);

          e.g.
            global namespace:
              DECLARE_MUTEX(ksem);

            kcp1:
              repeat:
                      if (!down_interruptible(&ksem)) {
                              ...
                              up(&ksem);
                      }
                      else
                              goto repeat;
              ...

            kcp2:
              if (!down_trylock(&ksem)) {
                      ...
                      up(&ksem);
              }
              else
                      printk(KERN_INFO "trylock kernel semaphore failed.");
              ...

          e.g.
            global namespace:
              struct semaphore ksem;
              sema_init(&ksem, 3);  /*  maximum processes number  */

            kpc1:  acquire ksem  /*  ksem.count -= 1 == 2  */
            kpc2:  acquire ksem  /*  ksem.count -= 1 == 1  */
            kpc3:  acquire ksem  /*  ksem.count -= 1 == 0  */
            kpc4:  acquire ksem => ksem.count <= 0 => __down(&ksem);

            ...

            kpc2:  release ksem => wake up kpc4  /*  ksem.count == 0  */
            kpc4:  waken up  /*  ksem.count == 0  */

            ...

            kpc1:  release ksem => no waiters => ++ksem.count  /*  ksem.count == 1  */
            kpc3:  release ksem => no waiters => ++ksem.count  /*  ksem.count == 2  */
            kpc4:  release ksem => no waiters => ++ksem.count  /*  ksem.count == 3  */

        !!  because semaphore is protected by kernel spin lock,thus kernel preemption is disabled
            when operating semaphore object(locked),kernel preemption is enabled after smeaphore
            is unlocked(operating finished).

        !!  kernel preemption is allowed when a kernel control path holding a semaphore.

      RW Semaphores :
        rw kernel semaphore is similar to kernel semaphore,but the operations are split to two kinds
        are read and write.

        <linux/rwsem-spinlock.h>
          /*  rw_semaphore - represent a kernel rw semaphore.
           *  @activity:     indicator.(default initialized to 0)
           *                 =  0 -> there are no active readers or writers
           *                 >  0 -> the number of active readers
           *                 = -1 -> one active writer
           *  @wait_lock:    rw_semaphore protector.
           *  @wait_list:    list of waiters.
           *  #  generic implement,the architecture-dependent implement is included in header
           *     <arch/x86/include/asm/rwsem.h> .
           */
          struct rw_semaphore {
                  __s32 activity;
                  spinlock_t wait_lock;
                  struct list_head wait_list;
          };

          /*  DECLARE_RWSEM - declare and initialize a local rw_semaphore object named with @name.  */
          #define DECLARE_RWSEM(name)  struct rw_semaphore name = __RWSEM_INITIALIZER(name)

          /*  init_rwsem - initialize a declared rw semaphore.
           *  @sem:        pointer of rw_semaphore.
           *  #  this function does same as DECLARE_RWSEM() macro.
           */
          #define init_rwsem(sem)

        rw_semaphore only allows a kernel control path holding a write lock at same time,but allows
        more than one kernel control paths to holding read lock.
        the default state of rw_semaphore is 0,means no readers and writers.
        if implementation is select to architecture-dependent(no CONFIG_RWSEM_GENERIC_SPINLOCK selected),
        then some macro definition would be used to represents the state what a rw_semaphore it is.
        for example :
          <arch/x86/include/asm/rwsem.h>
            RWSEM_ACTIVE_MASK  0xffffffffL(x86_64)  (0x0000ffffL x86)
            RWSEM_UNLOCKED_VALUE  0x00000000L
            RWSEM_ACTIVE_BIAS  0x00000001L
            RWSEM_WAITING_BIAS  (-RWSEM_ACTIVE_MASK - 1)
            RWSEM_ACTIVE_READ_BIAS  RWSEM_ACTIVE_BIAS
            RWSEM_ACTIVE_WRITE_BIAS  (RWSEM_WAITING_BIAS + RWSEM_ACTIVE_BIAS)

          /*  kernel rw semaphore is implemented as an library feature,thus the source code files
           *  under "top-level/lib"(i.e. rwsem.c),the artechiture-dependent source code files under 
           *  "arch/x86/lib"(i.e. rwsem_64.S).
           */

        it follow the strict FIFO rule,everytime wake-up always wake the first waiter up in the
        waiting list.
        up action is split two kinds that are up-read and up-write.
        after up-write,if the first waiter is waiting for read,then all read-waiter up to next
        write-waiter would be woken up,otherwise,the first waiter is waiting for write,then only
        it would be woken up in the waiting list.

        Operations :
          <linux/rwsem.h>
            /*  down_*() functions will put task to sleep,after the blocked task had woken up,
             *  the lock it acquired must be succeed,because,the blocking tasks enter
             *  TASK_UNINTERRUPTIBLE state,thus no signal is able to interrupt them.
             */
            extern void down_write(struct rw_semaphore *sem);
            extern void down_read(struct rw_semaphore *sem);

            extern void up_write(struct rw_semaphore *sem);
            extern void up_read(struct rw_semaphore *sem);

            /*  *trylock() functions try to acquire lock without blocking.
             *  return:  1 -> successful
             *           0 -> contention
             */
            extern int down_write_trylock(struct rw_semaphore *sem);
            extern int down_read_trylock(struct rw_semaphore *sem);

            /*  downgrade_write - downgrade a read semaphore which have holden by the task
             *                    to a write semaphore.if there is another reader is activing,
             *                    then block task until the last reader leaved.
             *  @sem:             rw_semaphore pointer.
             */
            extern void downgrade_write(struct rw_semaphore *sem);

        !!  rwsems is disallow to recurse(lock a rwsem multiple times),but it is allow that
            multiple locks of the same lock class might be taken,if the order of the locks 
            is always the same(prevent dead-lock).
            this feature is require CONFIG_DEBUG_LOCK_ALLOC,lockdep would be used for
            implement it,the associated APIs are suffix to *_nest .

      Completion :
        "understanding linux kernel" described that kernel synchronize mechanism "Completion"
        is imported for reslove kernel semaphore concurrently operating,but in Linux 2.6.34.1,
        the same kernel semaphore is disallow to be concurrently operated,kernel preemption and
        local interrupt request all are disabled when hold the spin lock embedded in the
        structure.thus the down() and up() running on different processes on different CPU would
        not bring dangling pointer problem.
        /*  process1-CPU0 destoryed kernel semaphore when process2-CPU1 is calling up() function.  */

        kernel Completion mechanism is based on wait queue and introduced for task wait for a
        condition is completed.
        only one task would be woken up after a condition is completed.in other words,only one
        kernel control path is allowed to enter critical region.

        !!  for wait queue see "How Processes Are Organized" .

        <linux/complete.h>
          /*  completion - structure represents a condition indicator.
           *  @done:       if the condition is completed.
           *               1 -> it is completed
           *               0 -> it is not completed
           *  @wait:       the tasks waiting for the condition.
           *  #  wait_queue_head_t contains a kernel spin lock and a kernel list as its own member.
           *     the items linked under wait_queue_head_t.task_list are wait instance each is 
           *     represents by structure wait_queue_t.
           */
          struct completion {
                  unsigned int done;
                  wait_queue_head_t wait;
          };

          /*  DECLARE_COMPLETION - declare and initialize a completion object.
           *                       initialized with {0, __WAIT_QUEUE_HEAD_INITIALIZER(@work.wait)} .
           */
          #define DECLARE_COMPLETION(work)  struct completion work = COMPLITION_INITIALIZER(work);

          /*  DECLARE_COMPLETION_ONSTACK - declare and initialize a completion object on kernel stack.  */
          #define DECLARE_COMPLETION_ONSTACK(work)  \
                  struct completion work = COMPLETION_INITIALIZER_ONSTACK(work)

          /*  init_completion - initialize a completion.
           *  @x:               pointer of the declared completion.
           *  #  does same thing as DECLARE_COMPLETION().
           */
          staic inline void init_completion(struct completion *x);

          /*  INIT_COMPLETION - reinitialize a completion.it is important after complete_all() is used.
           *  @x:               completion object,not a pointer.
           *  #  this macro does not modify wait list,just reset condition as well.
           */
          #define INIT_COMPLETION(x)  ((x).done = 0)

          /*  complete - signals a single thread waiting on this completion.
           *  @x:        completion pointer.
           *  #  this function hold spin lock with local interrupt request disabled,
           *     then does @x->done++,next call to __wake_up_common() with exclusive == 1.
           *     exclusive-wakeup : wake up one exclusive task and all non-exclusive tasks.
           */
          extern void complete(struct completion *x);

          /*  complete_all - signals all threads waiting on this completion.
           *  @x:            completion pointer.
           *  #  this function is similar to completion(),but it call to __wake_up_common() with
           *     exclusive == 0.
           *     non-exclusive-wakeup : wake everting up. 
           */
          extern void complete_all(struct completion *);

          /*  !!
           *  generally,the wait APIs add all non-exclusive threads to the head of wait queue,
           *  and add all exclusive threads to the tail of wait queue.
           *  but the wait queue of Completion mechanism only exclusive threads are queued.
           *  thus,do not mix Completion with wait queue APIs,just use Completion APIs only as well.
           *  !!
           */

          /*  wait_for_completion - let @current wait on @x and set task state to
           *                        TASK_UNINTERRUPTIBLE.
           *  @x:                   completion pointer.
           *  #  after queued completion waiting,@current will be put to sleep,another
           *     task would be selected by scheduler.
           */
          extern void wait_for_completion(struct completion *x);

          /*  TASK_INTERRUPTIBLE
           *  return:  1 -> condition completed
           *           -ERESTARTSYS -> interrupted by signal
           */
          extern int wait_for_completion_interruptible(struct completion *x);

          /*  TASK_KILLABLE
           *  return:  1 -> condition completed
           *           -ERESTARTSYS -> interrupted by signal
           */
          extern int wait_for_completion_killable(struct completion *x);

          /*  TASK_UNINTERRUPTIBLE
           *  @timeout:  time in jiffies.
           *  return:  1 -> condition completed
           *           0 -> timer expired
           */
          extern unsigned long wait_for_completion_timeout(struct completion *x,
                                                           unsigned long timeout);

          /*  TASK_INTERRUPTIBLE
           *  @timeout:  time in jiffies.
           *  return:  1 -> condition completed
           *           0 -> timer expired
           *           -ERESTARTSYS -> interrupted by signal
           */
          extern unsigned long wait_for_completion_interruptible_timeout(struct completion *x,
                                                                         unsigned long timeout);

          /*  try_wait_for_completion - try wait a condition completed without blocking.
           *  @x:                       completion pointer.
           *  return:                   0 -> decrement cannot be done without blocking
           *                            1 -> decrement succeed
           *  #  this function return 1,means the completion decrement succeed,thus the caller
           *     is able to use the condition right after control returned.
           *     because @x->done had been decreased,thus another task will see the condition
           *     is not completed(zero),they will wait for next completed.
           */
          extern bool try_wait_for_completion(struct completion *x);

          /*  completion_done - test to see if a completion has any waiters.
           *  @x:               completion pointer.
           *  return:           0 -> there are waiters
           *                    1 -> there are no waiters
           *  #  this function just checks if @x->done is equal to 0.
           *     if it is,that means a thread called to wait_for_completion*() did decrement to
           *     @x->done(become zero,it is in progress);
           *     else,@x->done is equal to 1,no any threads used the completed condition.
           */
          extern bool completion_done(struct completion *x);

      Local Interrupt Disabling :          
        interrupt disabling ensure that a sequence of kernel statements is treated as a critical section.
        the kernel control path entered the critical section,which will do not care about hardware event,
        thus providing an effective way to protect data structures that are also accessed by interrupt
        handlers.but this way in the case that interrupt handlers run on other CPUs(SMP) and access the
        protected data structures concurrently is useless,it is absolutely useful for uniprocessor system.

        for disable local interrupt,have to clear eflags.IF,the assembly instruction "cli" is take a charge
        to the feature.
        for enable local interrupt,have to set eflags.IF,the assembly instruction "sti" is take a charge to
        the feature.

        linux defined some macros or functions used to disable/enable local CPU interrupt :
          <linux/irqflags.h>
            /*  local_irq_disable - cli  */
            #define local_irq_disable()

            /*  local_irq_enable - sti  */
            #define local_irq_enable()

            /*  local_irq_save - save the older eflags state before disable local CPU interrupt
             *  @flags:          unsigned long type variable(not pointer) used to save the older
             *                   eflags state.
             */
            #define local_irq_save(flags)

            /*  local_irq_restore - restore eflags
             *  @flags:             the eflags state previously saved by local_irq_save().
             *  #  this function do not process "sti",it just call "popf" to restore eflags register.
             */
            #define local_irq_restore(flags)

            /*  irqs_disabled - return eflags.IF == 0  */
            #define irqs_disabled()

        !!  because interrupt is allowed to nest,eflags register may be changed by other interrupt handler,
            thus the kernel does not necessarily know what the IF flag was before the current control path
            executed.in the case,use local_irq_save() to save the older eflags state and restore it before
            exit critical region.

      Disabling and Enabling Deferrable Functions :
        deferrable functions do the work that is allowed to deferred to half-bottom of interrupt.
        the mechanism for half-bottom of interrupt contains :
          softirqs, tasklet, work_queue

        linux defined two macros used to disable or enable local softirqs :
          <linux/bottom_half.h>
            /*  local_bh_disable - increase preempt_count.softirq_counter  */
            extern void local_bh_disable(void);

            /*  local_bh_enable - decrease preempt_count.softirq_counter
             *  #  this function would checks
             *       if local_irq_pending() && !in_irq()
             *       true: call to do_softirq()
             *       false: decrease preempt_count.softirq_counter then call to preempt_check_resched()
             *     kernel preemption keep disabled until done with softirqs processing {
             *             preempt_count -= (255 = SOFTIRQ_OFFSET - 1)  bits[0, 7] => preempt count
             *             true-if
             *                     do_softirq()
             *             preempt_count -= 1    bits[8, 15] => softirq count
             *     }
             *     e.g.
             *       preempt_disable() => 000000001 => 1  /*/
             *       local_bh_disable() => 1 + 256 = 257 => 100000001
             *       preempt count = 257 - (SOFTIRQ_OFFSET - 1) = 255 => 2 => 000000010
             *       dec_preempt_count() => 2-- => 1 => 000000001  /*/
             */
            extern void local_bh_enable(void);

            /*  local_bh_enable_ip - enable local bh(bottom of interrupt),@ip is used to trace softirqs  */
            extern void local_bh_enable_ip(unsigned long ip);

        !!  tasklet is based softirq,thus local bottom disable/enable is take effect to tasklet,too.
        !!  preempt_disable() => increase preempt count; preempt_enable() => decrease preempt count
            preempt enabled if and only if preempt counter equals to zero,and preempt is able to be
            disabled more than once,each calling to preempt_disable() will increase preempt counter.

      Synchronizing Accesses to Kernel Data Structures :
        system performance may depends on which sychronization primitive is selected.
        there is a rule should to follow when select a synchronization primitive for protect
        kernel data structure: "always keep the concurrency level as high as possible in the system".

        the concurrency level in the system depends on two main factors:
          1>  the number of I/O devices that operate concurrently
          2>  the number of CPUs that do productive work

        !!  device IRQs is only can be disabled in a short periods of time.
        !!  CPUs in the system should avoid waste times on spin lock,that is do not select spin lock
            whenever possible.
            local IRQ disable is more efficiency than spin lock,it just clear eflags.IF .

        the examples that keep the concurrency level as high as possible :
          1>  a shared data is type of integer value it is accessed concurrently.
              declare it with atomic_t type,do not use spin lock or other different kernel synchronization
              primitives to protect it from concurrently access.

          2>  a shared linked list have to be updated.

              new->next = head->next;
              head->next = new;

              data corrupted to new->next when an interrupt handler modified head->next after the
              statement "new->next = head->next" finished.
                interrupt handler:
                  list_del(head->next);

                in this case,a kernel synchronization primitive is required:
                  unsigned long eflags;
                  local_irq_save(eflags);
                  new->next = head->next;
                  head->next = new;
                  local_irq_restore(eflags);

              data corrupted to these two statements there compiler optimizing or CPU out-of-order
              has taken a place:
                head->next = new => finished
                interrupt come
                interrupt handler access to head->next try to traverse list,but at this time,
                new->next == new
                /*  interrupt handler does not delete list item  */

                in this case,a memory write barrier is required:
                  new->next = head->next;
                  smp_wmb();
                  head->next = new;

      Choosing Among Spin Locks,Semaphores,and Interrupt Disabling :
        choosing the synchronization primitives depends on what kinds of kernel control paths access
        the data structure.

        kernel control path                   uniprocessor                      multiprocessor
        exceptions                            semaphore                         none
        interrupts                            local interrupt disabling         spin lock
        deferrable functions                  none                              none or spin lock
        exceptions + interrupts               local interrupt disabling         spin lock
        exceptions + deferrable functions     local softirq disabling           spin lock
        interrupts + deferrable functions     local interrupt disabling         spin lock
        exceptions + interrupts +             local interrupt disabling         spin lock
        deferrable functions

        /*  the primitives disable kernel preempt :
         *    spin lock
         *    local interrupt disable
         *    local bottom-half disable
         */

        protecting a data structure accessed by exceptions :
          the exception occurs in Kernel Mode only the "Page Fault".
          that is the subsequent "Page Fault" would be blocked until current is finished.
          the "Page Fault" handler(page_fault()) deal with it.

          a struct named mm_struct would be accessed by page_fault(),but it just accessed in read-only for
          scanning.  /*  page_fault() -> do_page_fault()  */

          /*  how synchronize to the shared data structures placed in the page for concurrently accessing
           *  is the work of kernel control paths,page_fault() does not care about,it does not access to
           *  such shared data structures,it just replaces pages.
           */

          the process in User Mode is able to call to system-call via "int" or "int3",and linux implement
          syscall in traps.(default the vector of system trap gate is 0x80,and handler is system_call())
          thus,the shared data structures accessed by syscall usually represents a resource that can be
          assigned to one or more processes.
          race condition occurs when more than one syscalls try to access the resource,such resource can be
          protected by semaphore primitive,because semaphore is allow processes to sleep until the resource
          is available.

          kernel preemption does not create problem when syscall owns a semaphore :
            process1 holds semaphore sem1 ->
            process2 preempt process1 ->
            process2 try to acquires sem1 ->
            sem1 is unavailable ->
            process2 entry sleep ->
            scheduler pick up next process ->
            ...
            scheduler pick up process1 to run ->
            process1 finished accessing to the resource ->
            sem1 is available now ->
            process2 is woken up

          !!  the kernel preemption must to be disabled in a syscall that is the syscall wants to
              access Per-CPU variable.

        protecting a data structure accessed by interrupts :

          data structure accessed by the top-half of interrupt:

            data structure only accessed by the interrupt handlers have the same type>
              do not need any synchronization,because such interrupt handlers are serialized.

            data structure accessed by interrupt handlers and each has the different type>
            /*  interrupt can be nested  */

              uniprocessor :
                enter any critical region in the interrupt handler must disable local irq at first.
                /*  semaphore brings blocking,but interrupt context forbid such action.
                 *  spin lock,interrupt handler holding a spin lock maybe interrupted by another
                 *  interrupt,and the interrupt handler request the same spin lock => dead-lock
                 */

              multiprocessor :
                1>  disable local irq before enter any critical region in the interrupt handler
                    /*  disable local irq won't interfere other CPU,the interrupt with the
                     *  same type may handled on other CPU
                     */
                2>  acquire a spin lock or a rw spin lock before access to the data structure
                    /*  acquire spin lock after local irq disabled,then no interrupt has highter
                     *  priority would interrupts current interrupt handler.
                     *  acquire spin lock disabling the concurrently access to the data structure
                     *  by interrupt handlers with the same type on other CPU.
                     *  SPIN LOCK MIGHT SPIN UNTIL UNLOCK,BUT TOP-HALF OF INTERRUPT MUST BE
                     *  SHORT AND FAST,THUS INTERRUPT HANDLER ACQUIRE SPIN LOCK WOULD NOT
                     *  FREEZE SYSTEM.
                     */

          linux macros coupled spin lock with local irq enabling/disabling :
            spin_lock_irq
            spin_unlock_irq
            spin_lock_bh
            spin_unlock_bh
            spin_lock_irqsave
            spin_unlock_irqrestore
            
            read_lock_irq                   read_seqbegin_irqsave
            read_unlock_irq                 read_seqretry_irqrestore
            read_lock_bh
            read_unlock_bh
            read_lock_irqsave
            read_unlock_irqrestore            
            
            write_lock_irq                  write_seqlock_irqsave
            write_unlock_irq                write_sequnlock_irqrestore
            write_lock_bh                   write_seqlock_irq
            write_unlock_bh                 write_sequnlock_irq
            write_lock_irqsave              write_seqlock_bh
            write_unlock_irqrestore         write_sequnlock_bh

        protecting a data structure accessed by deferrable functions : (bottom-half)
          !!  no race condition may exist in uniprocessor system in the case that data structure
              accessed only by deferrable functions.
              /*  deferrable function can not be interrupted by another,serialized.  */

          multiprocessor :
            softirq => softirqs has same type can run concurrently on several CPUs at a time
            tasklet => tasklets has same type can not run on more than one CPU at a time
                                                 
                                                 /*  protection  */
            softirqs            require          spin lock
            one tasklet         require          none
            many tasklets       require          spin lock

            /*  many tasklets might access to the same data structure concurrently,
             *  thus a synchronizing mechanism is demand.
             */

          !!  work_queue is dealt with by scheduler,in process context,thus any type of
              synchronization is OK except local irq disable(it is useless).

        protecting a data structure accessed by exceptions and interrupts :
          interrupt handler can not be interrupted by exceptions.          
          (non-reentrant)

          uniprocessor :
            just disable local irq as well,prevent interrupt handler is interrupted by other
            interrupts with different type.
            if just one kind of interrupt handler access to the data structure,do not need
            to disable local irq.

          multiprocessor :
            disable local irq
              =>  prevent interrupt handler interrupted by other
            select an appropriate kernel synchronizing mechanism to protect the data structure
              =>  prevent concurrently access to the data structure by I/E handlers on other CPU

            /*  on multiprocessor system,semaphore is preferable to replace spin lock.
             *    interrupt handler can not sleep
             *    exception handler can not sleep
             *    syscall is allowed to sleep
             *    only "Page Fault" occurs in Kernel Mode(only one type)
             *  thus,interrupt handler can use down_trylock() to acquire semaphore,it works like
             *  a spin lock.
             */

        protecting a data structure accessed by exceptions and deferrable functions :
          softirq AND tasklet in interrupt context
          exception handler "Page Fault" in interrupt context
          syscall in process context
          work_queue in process context

          multiprocessor :
            exception handler         require     disable bottom-half(or disable local irq)
                                                  spin lock

            deferrable function       require     spin lock
            /*  no exception can be raised when a deferrable
             *  function is running.
             *  exception can not interrupt interrupt handler
             */

          of course,semaphore can used to replace spin lock,but the handler in interrupt context
          must use down_trylock() to instead down().

        protecting a data structure accessed by interrupts and deferrable functions :
          interrupt handler can interrupt deferable function,but deferrable function can not
          interrupt an interrupt handler.

          multiprocessor :
            deferrable function        require        disable local irq
                                                      spin lock

        protecting a data structure accessed by exceptions,interrupts and deferrable functions :
          exception can not be raised when interrupt handler or deferrable function is running.
          interrupt handler can interrupt exceptions and deferrable functions.

          multiprocessor :
            deferrable function        require        disable local irq
                                                      spin lock
                                                      /*  dont care about exception  */

            exception handler          require        disable local irq
                                                      spin lock or semaphore
                                                      /*  disable local irq influence to
                                                       *  interrupt and softirq
                                                       */

            interrupt handler          require        disable local irq
                                                      spin lock

          !!  essentailly,deferrable function activated right after when interrupt handlers finished,
              thus exception handler do not to explicitly disable bh.

      Examples of Race Condition Prevention :

        Reference Counters >
          a reference counter is just an atomic_t counter associated with a specific resource such
          inode,file,memory page,module,etc.

          a kernel control path starts use the resource,increase reference counter;
          a kernel control path finishes use the resource,decrease reference counter;
          when reference counter is equals to zero,it can be released if necessary.

        Big Kernel Lock >
          In Linux 2.6.34.1,BKL is used to protect old code(functions to VFS or to several filesystems).
          BKL as an optional feature configured by CONFIG_LOCK_KERNEL.

          the corresponding APIs is defined in <linux/smp_lock.h>,and implementions are included in 
          <top-level/lib/kernel_lock.c> .

          In Linux 2.6.11,BKL is based on a meaningful semaphore,but in the earlier 2.6,it based on a
          spin lock.
          !!  LINUX 2.6.34.1,BKL IS BASED ON A SPIN LOCK NAMED @kernel_flag WHICH IS DEFINED IN
              <lib/kernel_lock.c> IS TYPE OF raw_spinlock_t.

          <linux/smp_lock.h>
            /*  lock_kernel - acquire BKL,the kernel preemption would be disabled after
             *                got BKL.kernel preempt is enabled during spin.
             *
             *  #  this macro call to _lock_kernel() defined in <lib/kernel_lock.c> .
             *  #  _lock_kernel() would checks whether @current->lock_depth is equal to -1.
             *       if it is,then call to __lock_kernel() which spin on @kernel_flag for
             *       get the spin lock.
             *     increase @current->lock_depth before return to called.
             */
            #define lock_kernel()

            /*  unlock_kernel - release BKL and enable kernel preemption.
             * 
             *  #  this macro call to _unlock_kernel() defined in <lib/kernel_lock.c> .
             *  #  must call to _unlock_kernel() when @current->lock_depth is greater than or
             *     equal to zero,otherwise,it is a BUG.
             *     this function decrease @current->lock_depth at first,
             *     if the value becomes less than zero,
             *       then call to __unlock_kernel() to release BKL.
             *     else does nothing.
             */
            #define unlock_kernel()

            /*  release_kernel_lock - release BKL,this macro used by scheduler.
             *                        after released,enable kernel preemption without
             *                        reschedule.
             *
             *  #  if @tsk->lock_depth >= 0,then call to __release_kernel_lock() .
             *  #  __release_kernel_lock() is defined in <lib/kernel_lock.c> ,which
             *     does not decrease @current->lock_depth,only decrease preempt counter,
             *     if preempt counter is equal to zero,that means kernep preempt enabled.
             */
            #define release_kernel_lock(tsk)

            /*  reacquire_kernel_lock - reacquire the BKL.after got lock,disable 
             *                          kernel preempt.
             *  @tsk:                   task pointer used to check @lock_depth
             *  return:                 -EAGAIN -> need reschedule but failed to
             *                                     acquire lock
             *                          0 -> succeed to acquire lock,before return,
             *                               kernel preempt would be disabled.
             *  #  this function checks whether @tsk->lock_depth is greater than or
             *     equal to zero first.
             *     if it is,call to __reacquire_kernel_lock()
             *     else return 0.
             */
            static inline int reacquire_kernel_lock(struct task_struct *tsk);

            !!  if a process is holding BKL,and call to schedule() to relinquish CPU,
                BKL is automatically unlocked during process switch by schedule(),
                but BKL would be automatically reacquired by schedule() when the process
                is selected to run by scheduler.

          <lib/kernel_lock.c>
            /*  kernel_flag - the BKL.  */
            static __cacheline_aligned_in_smp DEFINE_RAW_SPINLOCK(kernel_flag);

          !!  task_struct.lock_depth is crucial for allowing interrupt handlers,exception
              handlers,and deferrable functions to take the BKL,without it,every asynchronous
              function that tries to get the BKL could generate a deadlock if the current
              process already owns the BKL.
              /*  these three kinds of kernel control paths are use the process kernel stack.
               *  @lock_depth is process exclusive.
               *  @lock_depth == -1 -> unlocked
               *  because @lock_depth,the same process issues two or more consecutive requests for
               *  BKL would not hang processor.
               */

          !!  In Linux 2.6.11,BKL is implemented via semaphore,semaphore is allows kernel preempt
              inside a critical region.
              thus,if a process is holding a semaphore inside a critical region,it maybe preempted
              by other.
              but in the case,semaphore should not be released to prevent the possible data corruption.
              preempt_schedule_irq() temporarily sets @lock_depth to -1,then schedule() does not
              release BKL,because @lock_depth < 0;preempt_schedule_irq() restores the @lock_depth
              before the preempted process is selected to run but have not been replaced.

        Memory Descriptor Read/Write Semaphore >
          @mm_struct defined in <linux/mm_types.h> has a member named @mmap_sem is type of 
          struct rw_semaphore,when Page Fault occurred,page_fault() get read-semaphore to scans
          the memory descriptors.
          @mmap_sem used to protects the mm_struct when it is shared by several lightweight processes.
          e.g.
            process A shared mm_struct @mm with process B

            process A call to do_mmap() to extends virtual memory.
            if no free memory is available,A would be suspended.
            for extending,write-semaphore have to be holden.

            process B is stay running,it is able to use the shard memory but can not read/write @mm.

        Slab Cache List Semaphore >
          struct kmem_list3 is defined in <mm/slab.c>,a member named @list_lock is type of spin lock
          used to protect concurrently access to this structure.
          struct kmem_cache is defined in <linux/slab_def.h>,it contains a member is named @nodelists
          is an array of struct kmem_list3.

          @kmem_list3 is the slab lists for all objects.when kernel control path wants to alloc a new
          node in a nodelist,it have to acquire the @list_lock for grants an exclusive right to access
          and modify the list.

        Inode Semaphore >
          struct indoes is defined in <linux/fs.h>,which contains some kernel primitives such spin lock,
          mutex,rw semaphore,etc.
          
          a member named @i_lock is type of spinlock_t used to protects the struct inode get accessing
          concurrently to several file-system drivers or syscalls.
          any process wants to modify inode have to acquire this lock at first.

        !!  whenever more than one locks is to request,the changes of dead-lock to occur would
            increase significantly.for reduce dead-lock probability,must acquire the locks in a
            predefined order.


/* END OF CHAPTER5 */
          

Chapter 6 : Timing Measurements
    Linux Kernel must performs two main kinds of timing measurement :
      1>  keeping the current time and data
      2>  maintaining timers(timers notify kernel,user program a certain interval of time has elapsed)

      timing measurements are performed by several hardware circuits based on fixed frequency
      oscillators and counters.

    Clock and Timer Circuits :
      clock circuits :
        keep track of the current time of day and to make precise time measurements.
        the timestamp can be re-synchronized by kernel.

      timer circuits :
        they are programmable circuits,they issue the timer interrupts at a fixed,predefined frequence,
        kernel is able to set the frequence.

      <===IBM-compatible===>


      Real Time Clock(RTC) : (Hardware,often integered into Motherboard of Computer)
       real time clock is independent of the CPU and all other chips.
       even the PC power off it stay tick,because it is energized by a small battery,the COMS RAM
       and RTC are integrated in a single chip.
       RTC issue periodic interrupts on IRQ8 at frequencies raning between 2Hz -- 8192Hz .
       it is programmable to make it works like an alarm clock(active IRQ8 when RTC reaches a specific value).

       Linux use RTC only to derive the time and date,it allows processes to program RTC via "/dev/rtc"
       device file.
       the kernel accesses the RTC through the 0x70 and 0x71 I/O ports.

     Time Stamp Counter(TSC) :
       80x86 microprocessor include a CLK inpt pin,CPU receive a clock signal through it from an external
       oscillator.
       Starting with the Pentium,80x86 microprocessor has a counter that is increased at each clock signal,
       the counter is accessible through the 64-bit Time Stamp Counter (TSC) register.

       /** 
        * the data of TSC is abled to be readed by assembly instruction "rdtsc".
        * everytime a clock signal raised on the CLK input pin,TSC automatically increase.
        */

       Linux can use TSC to get more accurate time measurements than Programmable Interval Timer.
       different PC might have the different clock signal frequency,thus the kernel does not specify
       a specialized clock signal frequency when comiling.

       Linux determine the clock signal frequency when initializing the system,to figure out the
       actual clock signal frequency of a CPU,the function calibrate_tsc() is used to counting the
       number of clock signals that occur in a time interval of approximately 5 milliseconds.

       /* x86_init_ops is defined in <arch/x86/include/asm/x86_init.h> */
       x86_init_ops is a structure which contains some primitives used when boot the system on 
       x86 platform.

       the structure x86_platform_ops contains a member named "calibrate_tsc" is type of 
       unsigned long function with none of parameters.
       @calibrate_tsc would be set to the architecture depending calibrate tsc function
       native_calibrate_tsc() .

       finally,the time constant is produced by properly setting up one of the channels of
       the Programmable Interval Timer.
       /** 
        * 1Hz means the thing appears once in a periodic
        * 100Hz means the thing appears 100 times in a periodic
        * so,we can define the N Hz is the number of clock signal is appear in a second
        * then we know after N times clock signal is past,a second is past
        */

       !! to avoid losing significant digits in the integer divisions,calibrate_tsc()
          returns the duration,in microseconds,of a clock tick multiplied by 2^32 .
          /*  clock tick,it is alike to that the clock hanged on the wall tick once
           *  to tell us a second is past.
           */

     Programmable Interval Timer(PIT) :
       PIT notify the kernel that a specified time interval has elapsed.it issues a special
       interrupt called "timer interrupt" to the kernel after a time interval has elapsed.
       after set up PIT,it continuously issues interrupt at a fixed frequency specified by kernel.

       /*  PIT is also used to drive an audio amplifier connected to the computer's
        *  internal speaker.
        *  IBM-compatiable PC cotains at least a PIT,it implemented by an 8254 CMOS chip using
        *  the 0x40--0x43 ports.
        */

       such hardware interrupt is issued on IRQ0 at a (roughly) 1000Hz(or 100Hz) frequency,
       the time interval approximately equal to 1 millisecond and called a tick.

       <kernel/time/ntp.c>
         /*  tick_nsec - the tick in nanoseconds.
          *  #  it approximately equal to 999.848 nanoseconds.
          *     clock signal frequency of about 1000.15Hz .
          *     this variable may automatically adjusted by kernel when it synchronizes to
          *     an external clock.
          */
         unsigned long tick_nsec;
        
       shorter ticks result in higher resolution timers,that will help to some featrues are
       requie more accurate timer to do something.(ie. I/O multiplexing or multimedia playback ...)
       but shorter ticks means CPU have to traps into Kernel Mode more frequently(timer interrupt).
       /*  the frequency of timer interrupts depends on the hardware architecture.  */

       some macro definitions used by kernel to yields time constant :
         CONFIG_HZ - yields the approximately number of timer interrupts per second.
                     this is defined in kernel config file,and Makefile use this macro 
                     to produces HZ in header <timeconst.h> .
                     default value on x86 platform is 1000

         CLOCK_TICK_RATE - it is the 8254 chip's internal oscillator frequency.
                           in header <arch/x86/include/asm/timex.h>,it is deined with value
                           PIT_TICK_RATE,which is defined in <linux/timex.h> with value
                           1193182ul .

         LATCH - it is used by PIT,defined in <linux/jiffies.h> with value
                 (CLOCK_TICK_RATE + HZ/2) / HZ
    
       on x86 platform,the PIS is model i8253,and some associated macros and functions are defined
       in header <arch/x86/include/asm/i8253.h>,implementations in <arch/x86/kernel/i8253.c> :
         #define PIT_MODE 0x43
         #define PIT_CH0 0x40
         #define PIT_CH2 0x42

         /**
          * setup_pit_timer - register a clock event device for PIT,and set the clock event
          *                   as the global clock event.
          * 
          * #  a static object named @pit_ce is type of struct clock_event_device
          *    defined in <i8253.c>,the definition of struct clock_event_device in
          *    <linux/clockchips.h> .
          *    the static object @pit_ce's @set_mode member is assigned to init_pit_timer() which
          *    is also in <i8253.c> as a static function.
          *    the default mode of PIT device is CLOCK_ENV_FEAT_PERIODIC | CLOCK_ENV_FEAT_ONESHOT .
          *    so it supports two kinds of work modes.
          *    when booting system,the boot CPU have to initialize PIT through communicate to the
          *    device via I/O ports :
          *      case CLOCK_ENV_FEAT_PERIODIC:
          *              outb_pit(0x34, PIT_MODE);           =>  work on periodic mode.
          *              outb_pit(LATCH & 0xff, PIT_CH0);    =>  LSB of LATCH
          *              outb_pit(LATCH >> 8, PIT_CH0);      =>  MSB of LATCH
          *
          *      case CLOCK_ENV_FEAT_ONESHOT:
          *              outb_pit(0x38, PIT_MODE);           =>  work on oneshot mode.
          *
          *      0x30 means PIT UNUSED,in this case,frequency of internal oscillator is set to zero.
          *
          *    outb_pit -> outb_p -> outb
          *                          inline function constructed by BUILDIO macro.
          *    outb has same function as IO instruction "outb" that is write first operand
          *    to the second operand which should be I/O port.
          *    how many length of data can be written is depends on hardware,sometimes,delay to
          *    consecutive operations is required for device to read data from I/O port.
          */
         extern void setup_pit_timer(void);

     CPU Local Timer :
       local APIC provides another time-measuring device: the CPU local timer.

       CPU local timer is similar to PIT but has same difference :
         >  APIC's timer counter is 32-bits,PIT's timer counter is 16-bits,therefore,CPU local
            timer can programmed to issues timer interrupt at very lower frequency.
            (counter stores the number of ticks that must elapsed before issue a timer interrupt)

         >  CPU local timer issues timer interrupt only to its CPU,PIT raise global clock event.

         >  CPU local timer based on the bus clock signal,it can be programmed in such a way to
            decrease the timer counter every 1, 2, 4, 8, 16, 32, 64, 128 bus clock signals.
            PIT make use its own clock signals,can be programmed in a more flexible way.

     High Precision Event Timer(HPET) :
       timer chip developed jointly by Intenl and Microsoft.

       HPET provides a number of hardware timers that can be exploited by the kernel.
       basically,it includes up to eight 32-bit or 64-bit independent counters.
       each counter is driven by its own clock signal,and the frequency at least is 10Hz.
       any counter associated with at most 32 timers and the timer is composed by a
       "comparator" and a "match register".

       comparator used to checks whether the value of counter is matched in the match register,
       if it is,then raise a hardware interrupt.
       /**
        * some timers support to generate a periodic interrupt.
        * for program them,have to write instructions into the I/O address where mapped to 
        * their register,such mapping is constructed during bootstraping phase by BIOS.
        * HPET allows kernel process I/O operation on counter and match register,and set the
        * oneshot mode or periodic mode(work mode depends on timer).
        */

                +-------+
                |counter|  <== 32bit or 64bit
                +-------+
                  | | |
                  | | +------->  timer1 { comparator match-register }
                  | +--------->  timer2
                  +----------->  timer3
                  ...            ...

     ACPI Power Management Timer(ACPI PMT) :
       ACPI PMT is another clock device included in almost all ACPI-based motherboards,and its
       clock signal frequency is fixed at roughly 3.58Hz.
       it actually a simple counter increased at each clock tick,kernel can read its value through
       an I/O port whose address mapping is established by BIOS.

       ACPI PMT is preferable to the TSC if the OS or the BIOS may dynamically lower the frequency
       or voltage of the CPU to save power.
       /**
        * CPU enter the power save mode,TSC would automatically changes its frequency,
        * ACPI PMT would not.
        * however,a HPET is present,it should always be preferred to the other circuits because
        * of its richer architecture.
        */

    The Linux Timekeeping Architecture :
      several time-related activitys that kernel must carry on >
        <  updates the time elapsed since system startup
        <  updates the time and date
        <  time slots checking for the processes on CPUs
        <  updates resource usage statistics
        <  checks whether the interval of time associated with each software timer has eplased

      linux timekeeping architecture is the set of time-related kernel data structures and some functions.
      it depends on the availability of the TSC,of the ACPI PMT,and of the HPET.
      kernel uses two basic timekeeping functions :
        one to keep the current time up-to-date
        another to count the number of nanoseconds that have eplased within the current second
      /**
       * 80x86 based multiprocessor machine have a different timekeeping architecture to uniprocessor
       *   uniprocessor : all time-keeping activities are triggered by interrupts raised by the
       *                  global timer(PIT or HPET)
       *   multiprocessor : all general activities are triggered by the interrupts raised by the global
       *                    timer,while CPU-specific activities are triggered by the interrupts raised
       *                    by the local APIC timer
       *                    general activity - such software timer
       *                    CPU-specific - monitoring the execution time of the currently running process
       *   #  the early SMP system based on 80486 Intel processor did not have local APICs.
       *      on some SMP motherboards,the local timer interrupts are not usable.
       */

      Data Structures of the Timekeeping Architecture :
        the timer object :
          "understanding the linux kernel" has introduced a structure is named timer_opts,which represents
          a timer object.
          but such structure had been removed in patch 
            "Time: i386 Conversion - part 4: Remove Old timer_opts Code"
          at Fri,17 Mar 2006 .
          SO THE Linux 2.6.34.1 HAS NOT SUCH STRUCTURE NOW!

        timer_list :
          Linux 2.6.34.1 has a structure is named @timer_list which is inclued in <linux/timer.h> .

          <linux/timer.h>
            /**
             * timer_list - list is constructed by timers,and each entry represents a timer
             * @entry:      embedded list_head object
             * @expires:    expire jiffies(setting jiffies + timeout)
             * @function:   what to do after expired
             * @data:       argument for @function(deviceID or other meaningful data)
             * @base:       time vector,indicates which vector this timer based
             * @start_site: which instruction started this timer
             * @start_comm: command name
             * @start_pid:  pid of the process started this timer
             */
            struct timer_list {
                    struct list_head entry;
                    unsigned long expires;
                    void (*function)(unsigned long);
                    unsigned long data;
                    struct tvec_base *base;

            #ifdef CONFIG_TIMER_STATS
                    void *start_site;
                    char start_comm[16];
                    int start_pid;
            #endif
                    ...
            };

            /** 
             * structure tvec_base is defined in <kernel/timer.c>.
             * different timer belongs to different time vector.
             * it has a spin lock member used to prevent race condition on it.
             */

          timer_list is one of components for high-resolution timers.
          high-resolution timers is newer timer mechanism(timer wheel has been abandoned).
          timer_list as the component brings the two new benefits :
            >  time ordered enqueuing into a rb-tree
            >  independent of ticks (the processing is based on nanoseconds)

          hrtimer base data structure - ktime_t
            every time value,absolute or relative,is in a special nanoseconds-resolution type
            ktime_t.
            
            <linux/ktime.h>
              union ktime {
                      s64 tv64;
              #if BITS_PER_LONG != 64 && !defined(CONFIG_KTIME_SCALAR)
                      struct {
              # ifdef __BIG_ENDIAN
                              s32 sec, nsec;
              # else 
                              s32 nsec, sec;
              # endif } tv;
              #endif
              };

              typedef union ktime ktime_t;

              #define KTIME_MAX ((s64)~((u64)1 << 63))

              #if (BITS_PER_LONG == 64)
              # define KTIME_SEC_MAX (KTIME_MAX / NSEC_PER_SEC)
              #else
              # define KTIME_SEC_MAX LONG_MAX
              #endif

        jiffies :
          there are two jiffies existed,one is @jiffies_64 is defined in <kernel/timer.c>,
          another is @jiffies which is defined in the linker script.

          <linux/jiffies.h>
            #define __jiffy_data __attritbue__((section(".data")))

            extern u64 __jiffy_data jiffies_64;
            extern unsigned long volatile __jiffy_data jiffies;

            /**
             * initial value for jiffies,the counter will overflow five minutes after the
             * system boot.
             */
            #define INITIAL_JIFFIES ((unsigned long)(unsigned int) (-300 * HZ))

          jiffies is a counter that stores the number of elapsed ticks since the system was started.
          it is increased by one when a timer interrupt occurs,that is,on every tick.

          jiffies would wraparound after data type is overflowed.
          linux handles cleanly the overflow of jiffies through some macros :
            time_after, time_after_eq, time_before, time_before_eq
            /* these macro yield the correct value even if a wraparound occurred. */

          in some cases,the kernel needs the real number of system ticks eplased since the system boot,
          regardless of the overflows of jiffies,so @jiffies_64 saves the ticks eplased,and linker
          equated @jiffies to the LSB 32bits of @jiffies_64.
          /*
           * value of @jiffies_64 is not atomic on 32-bit architecture,
           * therefore,read @jiffies_64 must through function get_jiffies_64().
           * the function acquires @xtime_lock before reading,any writing(updating) to
           * @jiffies_64 must wait until the reading is accomplished.
           * any updating to @jiffies_64 will synchronize to @jiffies(LSB 32bits of @jiffies_64).
           */

        xtime :
          xtime is a variable used to save the current time and date,it is type of struct timespec.
          
          <linux/time.h>
            struct timespec {
                    /* __kernel_time_t is a type definition of "long" */
                    __kernel_time_t tv_sec;  /* seconds */
                    long tv_nsec;            /* nanoseconds */
            };

          @xtime.tv_sec - number of seconds that have eplased since midnight of January 1,1970(UTC)
          @xtime.tv_nsec - number of nanoseconds that have eplased within the last second

          <kernel/time/timekeeping.c>
            /* xtime_lock - @xtime sequence lock */
            __cacheline_aligned_in_smp DEFINE_SEQLOCK(xtime_lock);

            /* xtime - current time and date */
            struct timespec xtime __attribute__ ((aligned (16)));

            /*
             * the function get_jiffies_64() above acquires @xtime_lock reading lock before read from
             * @jiffies_64.
             */

          @xtime variable is usually updated once in a tick,the user programs get the current time and
          date through it,and kernel use it to updating inode timestamps,etc.

          !!  @xtime_lock IS USED TO PROTECT SEVERAL CRITICAL REGIONS OF THE TIMEKEEPING ARCHITECTURE.

      Timekeeping Architecture in Uniprocessor Systems :
        on uniprocessor platform,all time-related activity are triggered by the PIT interrupt on IRQ0.
        some of these activities are executed right after the interrupt raised,the remaining activites
        are carried on by deferrable functions.

        initialization phase :
          during kernel initialization,the function time_init() is used to initializes system timekeeping
          architecture.

          <arch/x86/kernel/time.c>
            /**
             * time_init - initializes TSC and delay the periodic timer init to late x86_late_time_init()
             *  
             * #  @late_time_init is a pointer to a function which receives nothing and returns nothing,
             *    if it is not NULL,the function start_kernel() will call to it.
             */
            void __init time_init(void);

            /*
             * x86_late_time_init - x86 platform timekeeping architecture initializer
             *
             * #  this function calls to x86_init.time.timer_init() to sets up timer,
             *    then calls to tsc_init() to sets up TSC.
             *    generally,the @timer_init of @x86_init points to hpet_time_init().
             */
            static __init void x86_late_time_init(void);

            /**
             * hpet_time_init - set up system timer
             * 
             * # if hpet_enable() returned false,use PIT as a substitution,else,use the HPET.
             *   set up default timer interrupt via setup_default_timer_irq(),it associates
             *   IRQ0 with @irq0 is type of struct irqaction.
             */
            void __init hpet_time_init(void);

            /**
             * irq0 - the interrupt action of interrupts raised on IRQ0
             * @handler: time_interrupt() as the interrupt handler
             * @flags:   disables IRQ0 during interrupt handler is processing |
             *           no irq balancing |
             *           interrupt is used for polling |
             *           timer interrupt
             * @name:    "timer"
             */
            static struct irqaction irq0 = {    
                    .handler = timer_interrupt,
                    .flags = IRQF_DISABLED | IRQF_NOBALANCING | IRQF_IRQPOLL | IRQF_TIMER,
                    .name = "timer
            };

            /**
             * hpet_enable() is defined in <arch/x86/kernel/hpet.c>.
             * if hpet is capable,@boot_hpet_disable must be false,and the @hpet_address is the
             * the memory address mapped to hpet I/O ports.
             * if hpet is enabled,@hpet_legacy_int_enabled must be 1,its value is setup by
             * hpet_enable_legacy_init(),this function is called by hpet driver used to init HPET.
             */

          @wall_to_monotonic is type of struct timespec,it is defined in <kernel/time/timekeeping.c>.
          this variable is what kernel need to add to @xtime to get to monotonic time,or @xtime
          corrected for sub jiffie times.
          its @tv_sec maybe negative,but @tv_nsec always be positive.

          !!  <kernel/time/clocksource.c> defined @curr_clocksource,it is type of struct clocksource,
              and function clocksource_select() sets up it to the best clocksource available in the
              system.
              generally,the variable @timekeeper is type of struct timekeeper and used to represents the
              timekeeper is activied in the system,a member named @clock is point to @curr_clocksource.
              /* static function change_clocksource() used to modify @clock member. */

          @xtime is initialized by function timekeeping_init() which is defined in <kernel/time/timekeeping.c>,
          start_kernel() calls to timekeeping_init() to initializes the clocksource and common timekeeping
          values.
            /* read the persistent clock to get @now,it is specified during system booting. */
            @xtime.tv_sec = now.tv_sec;
            @xtime.tv_nsec = now.tv_nsec;
            ...
            if @boot.tv_sec == 0 && @boot.tv_nsec == 0
            then
                    @boot.tv_sec = @xtime.tv_sec
                    @boot.tv_nsec = @xtime.tv_nsec
            /* initializes @wall_to_monotonic with the boot time */
            set_normalized_timespec(&wall_to_motonotic, -@boot.tv_sec, -@boot.tv_nsec);

            /**
             * macro definition NSEC_PER_SEC = 1000000000L in <linux/time.h>.
             *
             * @nsec = -@boot.tv_nsec, @sec = -@boot.tv_sec
             * loop until @nsec < NSEC_PER_SEC
             *         @nsec -= NSEC_PER_SEC
             *         ++@sec
             * loop until @nsec > 0
             *         @nsec += NSEC_PER_SEC
             *         --@sec
             * finally,setup @wall_to_monotonic
             * 
             * the first loop will be skipped because @nsec < 0,the second loop ensure
             * @wall_to_monotonic.tv_nsec is positive and @wall_to_monotonic.tv_sec is
             * negative.
             * so,use @xtime + @wall_to_monotonic is able to get boot time.
             * but @wall_to_monotonic is no longer the boot time,for get boot time,have to
             * use getboottime(),the real boot time is get through @wall_to_monotonic and
             * @total_sleep_time.
             */
      
        timer_interrupt :
          timer_interrupt() is the ISR for timer interrupt.
          
          <arch/x86/kernel/time.c>
            /**
             * timer_interrupt - common timer interrupt handler
             * @irq:             IRQ line number,it usually is IRQ0
             * @dev_id:          device exclusive data
             * return:           IRQ_HANDLED
             */
            static irqreturn_t timer_interrupt(int irq, void *dev_id);

          timer_interrupt just a common timer interrupt ISR,what it will does is depends on what
          the timer device is selected by kernel.
          generally it does the following steps :
            1>  increase irq_cpustat_t.__nmi_count to update the NMI interrupt counter.
            2>  if @timer_ack is TRUE
                then
                        acquire spin lock @i8259A_lock
                        "outb" 0x0c on PIC_MASTER_OCW3 - it is PIC_MASTER_ISR <- PIC_MASTER_CMD(0x20)
                        "inb" on PIC_MASTER_POLL - it also is PIC_MASTER_ISR
                        release spin lock
                        /* 
                         * these two assembly instructions acknowledge to PIC we have handled the interrupt
                         * AEOI will end it automatically.
                         * 
                         * this IF is optimized out for !IO_APIC AND x86_64
                         * AEOI means "Automatic End Of Interrupt",it is work mode of intel 8259A PIC
                         */
            3>  call to @global_clock_event->event_handler
                if no HPET chip had probed,it generally is PIC
            4>  if MCA_bus is TRUE
                then
                        /* acknowledge irq0 by setting bit 7 in port 0x61 */
                        outb_p(inb_p(0x61) | 0x80, 0x61)
                /* MCA - Microchannel Architecture Bus */
            5>  return

          !!  @event_handler is setup by framework,the struct clock_event_device is described in
              <linux/clockchips.h>.
         
          
          !!  if a timer device is selected by kernel used for periodic tick,then,@event_handler
              of the clock_event_device belongs to such device will be assigned to @tick_handle_periodic.

              function tick_setup_device() is defined in <kernel/time/tick-common.c>,it changes the clock
              event device of a tick device to a newer clock event device.
                if the tick device is used for periodic tick,then call to tick_setup_periodic(),this function
                sets the @event_handler to function @tick_periodic_handler if no broadcast is enabled,
                otherwise,set to @tick_periodic_handler_broadcast.

                if the tick device is used for oneshot tick,then call to tick_setup_oneshot(),this function 
                sets the @event_handler of the newer clock event device to the event handler of current tick
                device,and changes the work mode of the clock devent device to CLOCK_EVT_MODE_ONESHOT.

             !!  IF THE TICK DEVICE HAVE NONE OF CLOCK EVENT DEVICE NOW,THEN CAN NOT SET IT AS AN ONESHOT
                 TICK DEVICE,THUS ITS WORK MODE WILL AUTOMATICALLY SET TO TICKDEV_MODE_PERIODIC.

             !!  CONFIG_NO_HZ => dynamic clock (nohz)
                                 allow to stop periodic clock when system is in idle state

          <kernel/time/tick-common.c>
            /**
             * tick_handle_periodic - the common periodic tick handler
             * @dev:                  the underlying clock event device
             *
             * #  the main works are accomplished by tick_periodic().
             *    it checks whether @tick_do_timer_cpu is equals to current cpu at first
             *      TRUE
             *        acquires @xtime_lock
             *        updates @tick_next_period to keep tracing to next periodict event
             *        call to do_timer(1)
             *        release @xtime_lock
             *    call to update_process_times() to updates CFQ time slots of processes
             *    call to profile_tick(CPU_PROFILING)
             *    return
             */
            void tick_handle_periodic(struct clock_event_device *dev);

          <kernel/timer.c>
            /**
             * do_timer - updates timekeeping architecture associated data
             * @ticks:    tick delta
             * 
             * #  this function add @ticks to @jiffies_64
             *    call to update_wall_time() in order to updates wall time
             *    call to calc_global_load() for re-calculating system global loading
             *    if a time slot of a process had exhausted,scheduler have to pick next
             *    process for running
             */
            void do_timer(unsigned long ticks);
            
      Timekeeping Architecture in Multiprocessor systems :
        multiprocessor system can rely on two different sources of timer interrupts :
          1>  PIT or High Precision Event Timer(hrtimer)
              /* PIT or HPET used for the global timer event */
          2>  CPU local timer
              /* CPU local timer interrupt signals timekeeping activities related to the local CPU. */

        initialization phase :
          linux uses interrupt vector 239(0xef) as the local APIC timer interrupt.

          <arch/x86/kernel/irqinit.c>
            /**
             * smp_intr_init - initializes CPU-to-CPU interrupt gates
             * 
             * #  if no CONFIG_SMP AND no CONFIG_X86_64 OR CONFIG_X86_64_APIC defined
             *    this function does nothing.
             *    otherwise,setup IPI.
             */
            static void __init smp_intr_init(void);

            /**
             * apic_intr_init - initializes APIC related interrupt gates
             *
             * #  if no CONFIG_X86_64 OR CONFIG_X86_64_APIC defined
             *    kernel will not creates some APIC related interrupt gates.
             *    kernel will not creates local CPU APIC timer interrupt gate,
             *    otherwise,create local APIC timer interrupt gate on vector 0xef(LOCAL_TIMER_VECTOR),
             *    the handler is apic_timer_interrupt().
             */
            static void __init apic_intr_init(void);

          apic_timer_interrupt() is a low-level interrupt handler,after acknowledged interrupt,it calls to
          local_apic_timer_interrupt() to deal with the local CPU APIC timer interrupt.
          /**
           * symbol "apic_timer_interrupt" is re-defined to "smp_apic_timer_interrupt" in <arch/x86/kernel/entry_64.S>.
           * and function smp_apic_timer_interrup() is defined in <arch/x86/kernel/apic/apic.c>.
           * local_apic_timer_interrupt() is defined in the same header,which retrieve current CPU-id from SMP
           * environment,next retrieve the per-CPU variable @lapic_events is type of struct clock_event_device,
           * if @event_handler is NULL,then call to lapic_timer_setup() to sets the clock event device work on
           * CLOCK_EVT_MODE_SHUTDOWN,that is disable it;
           * otherwise,call to the @event_handler set by framework.(NMI counter increase)
           */

          function "static void __init calibrate_APIC_clock(void)" is used to calibrate the local APIC timer's
          frequency of booting CPU during a tick(1ms),the exact value is used to program the local APICs.
          /**
           * calibrate_APIC_clock() is called by "void __init setup_boot_APIC_clock(void)",it as an operation
           * inclued by x86_init_ops as the member @setup_percpu_clockev.
           * global variable @x86_cpuinit is type of struct x86_cpuinit_ops,member @setup_percpu_clockev is set
           * to "void __cpuinit setup_secondary_APIC_clock(void)",this function will call to
           * "static void __cpuinit setup_APIC_timer(void)".
           * function setup_APIC_timer() copies the calibrated value from boot CPU and use it to program its
           * local APIC timer.
           */

          !!  setup_boot_APIC_clock() is called by APIC_init_uniprocessor(),which is called by smp_sanity_check().
              start_kernel -> rest_init -> kernel_init -> smp_prepare_cpus -> smp_sanity_check
              -> APIC_init_uniprocessor -> setup_boot_APIC_clock

          !!  @x86_cpuinit is figure in __cpuinitdata,this macro expands to "__section(.cpuinit.data)".
          
          !!  under SMP environment,each cpu has a per-CPU variable named @tick_cpu_device is type of 
              struct tick_device.
              /* uniprocessor environment,the CPU stay has such variable,but only one is existed in the system. */
              in function setup_tick_device(),if the clock event device of tick device is unassigned,
              then function checks whether @tick_do_timer_cpu is equals to TICK_DO_TIMER_BOOT(value is -2)
              /* no CPU took the do_timer update,assign it to current cpu */
                TRUE
                        sets @tick_do_timer_cpu to current cpu  /* the CPU becomes do timer cpu */
                        sets @tick_next_periodic = ktime_get()  /* set next periodic */
                        sets @tick_period = ktime_set(0, NSEC_PER_SEC / HZ) /* set timer tick frequency */
              
              IN FUNCTION tick_periodic(),IF CURRENT CPU IS NOT THE @tick_do_timer_cpu,THEN ONLY UPDATES THE PROCESSES'
              TIMES ON CURRENT CPU AND PROFILE TICK.
              THE CPU IS @tick_do_timer_cpu,THEN CALL TO do_timer(1) TO UPDATES "Timekeeping Architecture" RELATED
              DATA.

              as described above,SMP environment,it is still only one CPU deal with do_timer update,and finally,
              does the same thing as what uniprocessor does.

      Updating the Time and Date :
        function do_timer() updates @jiffies_64,call to function update_wall_time() to updates wall time.
    
        <kernel/time/timekeeping.c>
          /**
           * update_wall_time - uses the current clocksource to increment the wall time
           * 
           * #  function is called from the timer interrupt,and @xtime_lock is required.
           */
          void update_wall_time(void);

          the things update_wall_time() does :
            1>  get current clocksource of @timekeeper.
                /**
                 * struct clocksource is defined in <linux/clocksource.h>,it is hardware
                 * abstraction for a free running counter,provides mostly state-free accessors
                 * to the underlying hardware.
                 */

            2>  calculate offset,it is number of clock cycles in one NTP interval.
                /**
                 * clock cycle - time is not a consecutive line,it is separated short lines 
                 *               linked together,each of them is a clock cycle,means a time
                 *               periodic.
                 *               generally,tick represents such time periodic.
                 *               HZ specified how many tick in a second,in other word,how long
                 *               the separated line is in one second.
                 */

            3>  get shift,it is the shift value of current clocksource.
                /**
                 * for convert clock cycles to nanoseconds,have to calculate
                 *   (clock cycles / FREQUENCY) * NSEC_PER_SEC => nanoseconds
                 *
                 * division is deprecated in kernel,thus for remove division,must specify
                 * multiple and shift to convert the formula above to
                 *   clock cycles * multiple >> shift
                 *
                 * clocksource.mult => multiple AND clocksource.shift => shift
                 */

            4>  use offset to updates shift.
                /**
                 * this is implemented via a while-cycle,and inside the cycle,
                 * function logarithmic_accumulate(offset, shift) is called to
                 * update @xtime.tv_sec,new offset is got from result of the function.
                 */
                then call to timekeeping_adjust(offset) to adjust NTP error.

            5>  store full nanoseconds in @xtime.tv_nsec.
                @xtime.tv_nsec = @timekeeper.xtime_nsec >> @timekeeper.shift + 1
                /**
                 * timekeeper.xtime_nsec stores the clock shifted nano seconds remainder not
                 * stored in @xtime.tv_nsec.
                 */

            6>  use @xtime to update @timekeeper.xtime_nsec and @timekeeper.ntp_error

            7>  call to update_xtime_cache() with the converted nanoseconds.
                /** 
                 * @xtime_cache is cache for @xtime,@xtime.tv_nsec stored the full nanoseconds.
                 *   @xtime_cache = @xtime + converted nanoseconds
                 *
                 * at step 5,@xtime stored the remainded nanoseconds from most recently timer
                 * interrupt.
                 * the converted nanoseconds is calculated by offset -- clock cycle value at
                 * this time(the timer interrupt).
                 */

            8>  call to update_vsyscall(&xtime, timekeeper.clock, timekeeper.mult) to the data
                readonly from vsyscalls,such data is written by timer interrupt or systc.

      Updating Local CPU Statistics :
        updating process times :
          tick_periodic() call to update_process_times(user_mode(get_irq_regs())) to updating
          Local CPU Statistics about to processes running on current CPU.
          
          <kernel/timer.c>
            /**
             * update_process_times - charge one tick to the current process
             * @user_tick:            1 => the user time tick
             *                        0 => the system time tick
             */
            void update_process_times(int user_tick);

            the things that update_process_times() to does :
              1>  fetch @current process on current CPU.

              2>  call to account_process_tick(@current, @user_tick).
                  the function is defined in <kernel/sched.c>,
                  if @user_tick == 1
                          => account_user_time()
                  else if @current != this_rq()->idle || irq_count() != HARDIRQ_OFFSET
                          => account_system_time()
                  else
                          => account_idle_time()

                  account_user_time() is also defined in <kernel/sched.c>,which updates current
                  process's associated times(@utime, @utimescaled, group user time).
                    utime += @cputime               /* via cputime_add() */
                    utimescaled += @cputime_scaled  /* utime - time spent in User Mode */
                    /* cputime is @cpu_one_jiffy,cputime_scaled is cputime_to_scaled(@cpu_one_jiffy) */

                  next add user time to cpu_usage_stat,it converts cputime_t @cputime to 64-bit
                  then checks whether TASK_NICE(@current) > 0,if it is,update cpu_usage_stat.nice;
                  otherwise,update cpu_usage_stat.user.
                  finally call to account_update_integrals(@current) to account for user time used,this
                  function will updates the fields associated to mm member of the task.

                  account_system_time() is also defined in <kernel/sched.c>,which account system cpu time
                  to a process.it is similar to account_system_time(),but it might updates @irq,@softirq,or
                  @system in cpu_usage_stat.
                  in the case that @current->flags & PF_VCPU && irq_count() - @hardirq_offset == 0
                  then just account guest cpu time to a process and return. /* HARDIRQ_OFFSET */
                  /* account_system_time() call to account_update_integrals(),too. */

                  account_idle_time() might updates @iowait or @idle in cpu_usage_stat depends on 
                  this_rq()->nr_iowait > 0.

              3>  call to run_local_timers() to starts hrtimers,then raise a softirq is type of 
                  TIME_SOFTIRQ,before returning,call to softlockup_tick() to checks whether the
                  watchdog thread has hung or not.
                  /* run_local_timers() is called from per-CPU time interrupt of local-CPU on SMP */

              4>  call to rcu_check_callbacks() to checks whether this CPU is in a non-context-switch
                  quiescent state.
                  if no rcu pending on this CPU,do nothing.
                  if @user_tick == 1 OR (current CPU is idle AND rcu active AND not in softirq  AND
                  hardirq_count() <= (1 << HARDIRQ_SHIFT))
                  then note a sched quiescent state and a bottom-half quiescent state.
                  if not in softirq,then note a bottom-half quiescent state only.

                  before return,checks a quiescent state from the current CPU through call to
                  rcu_preempt_check_callbacks(),if @current->rcu_read_lock_nesting == 0,then disable
                  flag RCU_READ_UNLOCK_NEED_QS in @current->rcu_read_unlock_sepcial,next note a preempt
                  quiescent state;otherwise,just enable RCU_READ_UNLOCK_NEED_QS as well.
                  finally,raise a softirq is type of RCU_SOFTIRQ and returns.
                  /* rcu_process_callbacks() handler will deal with this softirq. */
                  /* callbacks of RCU are queued by call_rcu() */

              5>  checks if there are some messages have to be logged in kernel message loop-queue and 
                  print them.(via printk_tick() to checks and prints)

              6>  checks if there are some performance events pending and deal with them.

              7>  call to scheduler_tick() to does scheduling,this will cause run-queue clock
                  updating,cpu-load updating.

                             (for real-time process,task_tick_fair() for non-real-time process)
                  it call to task_tick_rt() updates current task's runtime statistics in the run-queue
                  of this CPU.
                  reduce one to current task's real-time time_slice,if it is greater than 0,then returns;
                  otherwise,set it equals to DEF_TIMESLICE,next checks the @run_list of a real-time 
                  scheduling class which the curr is associated to whether contains another processes,
                  if it is,then requeue current task to the end,and setup its task need resched flag.
                  /**
                   * time_slice is not equal to 0 means the time slot of current is not exhausted.
                   */

                  it call to perf_event_task_tick(rq->curr) to adjust performance event related robin
                  tree.

                  /* on SMP,loading rebalance will be triggered before return. */

              8>  call to run_posix_cpu_timers(@current),this function is a piece of POSIX.1 standard,
                  which checks whether there are some posix timer on current CPU need to be fired.

      Profiling the Kernel Code :
        Linux kernel includes a minimalist code profiler called "readprofile" used to discover where
        the kernel spends its time in Kernel Mode.

        Linux 2.6.34.1 supports to four kinds of profiler :
          <linux/profile.h>
            #define CPU_PROFILING   1
            #define SCHED_PROFILING 2
            #define SLEEP_PROFILING 3
            #define KVM_PROFILING   4

            /* these functions are implemented in <kernel/profile.c> */

            /**
             * profile_setup - setup function for kernel profiler
             * @str:           kernel parameter,it must be form "<type>,<prof_shift>"
             *                 supported forms -
             *                   "schedule,<prof_shift>"
             *                   "sleep,<prof_shift>"
             *                   "kvm,<prof_shift>"
             *                   "<prof_shift>"  =>  cpu profiling
             *                   <prof_shift> is a parameter denotes the size of the code fragments
             *                   to be profiled,the size equals to 2^<prof_shift>.
             *
             * #  this function will sets @prof_on to the macro definitions above to indicates
             *    kernel profiling is on.
             *    the kernel parameter must be the forms :
             *      profile=<type>,<prof_shift> OR profile=<prof_shift>
             */
            int profile_setup(char *str);

            /**
             * profile_init - initialize kernel profiler
             * return:        0 => succeed | profiler off
             *                -ENOMEM => failed
             *
             * #  this function calculates size of the code to profiling,setup corresponding data objects.
             */
            int profile_init(void);

            /**
             * profile_tick - do profiling on a tick
             * @type:         what type of profiler to activate
             *
             * #  if @type == CPU_PROFILING AND @timer_hook != NULL
             *      then call to @timer_hook() to process profiling
             *    else if "in Kernel Mode" AND @prof_cpu_mask != NULL AND "current cpu in prof cpu mask"
             *      then call to @profile_hit(type, (void *)profile_pc(regs)) to process profiling
             */
            void profile_tick(int type);

            !!  tick_periodic() call to profile_tick() with CPU_PROFILING.
                @timer_hook is installed via register_timer_hook() and uninstalled
                via unregister_timer_hook(),it is defined in <kernel/profile.c> is type of 
                  int (*timer_hook)(struct pt_regs *)

      Checking the Watchdogs :
        NMI - nonmaskable interrupt

        Watchdog system :
          it is useful to detect kernel bugs that cause a system freeze.

        Linux kernel supports two kinds of watchdog :
          touch watchdog every tick came
          touch watchdog every NMI occurred

        hard lockup :
          Linux kernel initializes NMI in trap gate,the handler is assembly entry "nmi" defined in
          <arch/x86/kernel/entry_64.S>,it call to do_nmi() to deal with this exception.
          function setup_apic_nmi_watchdog() is defined in <arch/x86/kernel/apic/nmi.c> used to
          setup NMI watchdog during system booting.
          the watchdog is Local APIC version deal with "hard lockup",it checks the value of @irq0_irqs.
          if "this" timer interrupt handler accomplished,@irq0_irqs must increased 1.
          if lapic watchdog detected current value as same as the older,that means the last
          timer interrupt handler did not execute completely,it turn out CPU has some problem.
          /**
           * !!  interrupt can interrupts exception.
           *     when CPU processing a timer interrupt,the local IRQ is disabled until timer interrupt
           *     exited.
           * For activate it,must boot kernel with parameter 'nmi_watchdog' .
           * local APIC is able to product NMI exception,such NMI is regonized by CPU only.
           */

        soft lockup :
          function update_process_times() call to softlockup_tick() to touch watchdog is tick-version.
          it just deal with "soft lockup".in this case,watchdog as a kernel thread named "khungtaskd",
          the "main" function of this thread is watchdog() which defined in <kernel/hung_task.c>.
          the primary works function watchdog() to does are :
            1>  enter a infinite loop.
            2>  enter another while-loop,hang itself until timeout.
                /* task state set to TASK_INTERRUPTIBLE */
            3>  call to check_hung_uninterruptible_tasks() with a timeout value.
                check_hung_uninterruptible_tasks() scan all threads with the specified timeout value,
                if there is a thread the time it hung is greater than @timeout,then report this problem.
                prints CPU registers,kernel stack,etc.
            4>  goto 2> .

     Software Timers and Delay Functions :
       a timer is a software facility that allows functions to be invoked at some future moment,after a given
       time interval has eplased;a timeout denotes a moment at which the time interval associated with a timer
       has eplased.

       Linux introduced two types of timers :
         dynamic timer - used by the Kernel
         interval timer - used by the User Mode processes

       Linux kernel cannot ensure that timer functions will start right at their expiration times,it can only
       ensure that they are executed either at the proper time or after with a delay of up to a few hundreds of
       milliseconds(none real-time strictly)
       /* reason is the checking timer functions usually dealt with by deferrable functions */

       Dynamic Timers :
         dynamically created and destroyed,no limit to the number of currently active timers.

         Data structures for dynamic timers :         
           dynamic timers are linked by structure "struct timer_list".
                                        (introduced in Data Structures of Timekeeping Architecture)

           <kernel/timer.c>
             #define TVN_BITS (CONFIG_BASE_SMALL ? 4 : 6)
             #define TVR_BITS (CONFIG_BASE_SMALL ? 6 : 8)
             #define TVN_SIZE (1 << TVN_BITS)
             #define TVR_SIZE (1 << TVR_BITS)
             #define TVN_MASK (TVN_SIZE - 1)
             #define TVR_MASK (TVR_SIZE - 1)

             /* tvec - structure represents timer vector node */
             struct tvec {
                     struct list_head vec[TVN_SIZE];
             };

             /* tvec_root - structure represents timer vector root */
             struct tvec_root {
                     struct list_head vec[TVR_SIZE];
             };

             /**
              * structures @tvec and @tvec_root used to partitions the expiration values into
              * blocks of ticks,that allows dynamic timers to percolate efficiently from lists
              * with larger expiration values to lists with smaller ones.
              */
              

             /**
              * tvec_base - structure collected all data needed by dynamic timer API
              * @lock:      race condition protector
              * @running_timer: the dynamic timer is handled now by local CPU
              * @timer_jiffies: the earliest expiration time of the dynamic timers yet
              *                 to be checked :
              *                   @it == @jiffies => no backlog of deferrable functions has
              *                                      accumulated
              *                   @it < @jiffies  => lists of dynamic timers that refer to
              *                                      previous ticks must be dealt with
              *                 @it is increased once when run_timer_softirq() handles dynamic timer
              * @next_timer:    next expires time
              * @tv1:           all dynamic timers
              * @tv2:           dynamic timers in next 2^14 - 1 ticks
              * @tv3:           dynamic timers in next 2^20 - 1 ticks
              * @tv4:           dynamic timers in next 2^26 - 1 ticks
              * @tv5:           dynamic timers do not belong to @tv2 -- @tv4
              */
             struct tvec_base {
                     spinlock_t lock;
                     struct timer_list *running_timer;
                     unsigned long timer_jiffies;
                     unsigned long next_timer;
                     
                     struct tvec_root tv1;
                     struct tvec tv2;
                     struct tvec tv3;
                     struct tvec tv4;
                     struct tvec tv5;
             } ____cacheline_aligned;

             !!  each CPU in system has a per-CPU variable @tvec_bases it is a pointer points to
                 an object is type of struct tvec_base.

                 except boot CPU,all another CPUs allocate an object is type of struct tvec_base
                 then use the address setup its per-CPU variable @tvec_bases;
                 /**
                  * allocating and setup is handled by function "static int init_timer_cpu(int cpu);"
                  * defined in <kernel/timer.c>.
                  * in that function,@timer_jiffies and @next_timer are assigned with the value
                  * of @jiffies.(of course,these list_head arraies are initialized,too)
                  */
                 boot CPU's @tvec_bases use a static object is named boot_tvec_bases defined in
                 <kernel/timer.c>.

         a dynamic timer can be created through three methods :
           1>  static global variable in the code
               /* use macro DEFINE_TIMER(_name, _function, _expires, _data) <linux/timer.h> */
           2>  local variable inside a function,in this case,the object is stored in Kernel Mode stack
           3>  dynamically allocated descriptor

         <linux/timer.h>
           /**
            * init_timer - initialize a dynamic timer
            * @timer:      pointer points to an object is type of struct timer_list
            */
           #define init_timer(timer)

           !!  if want a timer is deferrable,must initialize it through init_timer_deferrable().
               in this case,timer_set_deferrable() would open the flag TBASE_DEFERRABLE_FLAG of
               its base.
               <kernel/timer.c> #define TBASE_DEFERRABLE_FLAG (0x1)
               #  all tvec_base are 2 byte aligned,the lower bit of base in timer_list is guaranteed
                  to be zero.
                  (unsigned long)base | TBASE_DEFERRABLE_FLAG => deferrable timer (flag opened)


           /* TIMER_NOT_PINNED - timer is not pinned can be rebalanced to another CPU on SMP */
           #define TIMER_NOT_PINNED 0
           /* TIMER_PINNED - timer is pnned can not be relanced to another CPU on SMP */
           #define TIMER_PINNED     1

           /**
            * add_timer_on - add a timer on a particular cpu
            * @timer:        the timer to add
            * @cpu:          cpu ID to place timer
            *
            * #  this function is implemented in <kernel/timer.c>.
            *    it fetch per-CPU variable @tvec_bases is type of struct tvec_base *.
            *    setup start info(address of instruction,process command, pid).
            *    setup base for @new_timer,and if necessary,updates @tvec_bases->next_timer.
            *                                               ( it := @new_timer->expires)
            *    queue this @new_timer to the tail of the @base,which list to place is depend on
            *    @new_timer->expires - @base->timer_jiffies,use the result to determin time vector.
            *    wake up idle CPUs to make them check condition to determine need to reevaluate 
            *    the timer wheel when nohz is active.
            */
           extern void add_timer_on(struct timer_list *timer, int cpu);

           /* add_timer - no CPU specified version,the dynamic timer may assigned to another CPU */
           extern void add_timer(struct timer_list *timer);

           /**
            * del_timer - deactive a timer
            * @timer:     the timer to deactive
            * return:     0 => delete an inactive timer
            *             1 => delete an active timer
            *
            * #  clean the timer info through timer_stats_timer_clear_start_info().
            *    if @timer is pending,then detach it,and updates @next_timer of its base
            *    if necessary.
            *    return.
            */
           extern int del_timer(struct timer_list *timer);
           /**
            * del_timer_sync() is synchronized version to wait until @timer->@function accomplished.
            * it fetch and lock @timer's base(spin lock).
            * if @timer is running then unlock @base and returns,because it is running on another CPU,
            * there is no way to stop function it running.
            * otherwise,does the same things as what del_timer() does.
            * return: -1 => timer running 
            *          0 => timer is not pending and deactived 
            *          1 => timer is pending and deactived
            */

           /**
            * mod_timer - modify a timer with a new expiration in TIMER_NOT_PINNED state
            * @timer:     the timer to modify
            * @expires:   new expiration
            * return:     1 => succeed
            *             0 => fault
            *
            * #  this function call to an internal function __mod_timer() to process primary works.
            *                                               <kernel/timer.c>
            *    fetch and lock the @timer's base,if this timer is pending,then detach it.
            *    update its @base if necessary.
            *    fetch per-CPU variable @tvec_bases,if @base != @new_base,then reset base for @timer
            *    to @new_base.
            *    update expiration of @timer,and update its base's new_timer member if necessary.
            *    call to internal_add_timer() to queue @timer.
            *
            * !! vector is determined by "expires - timer_jiffies",thus,modification maybe associate
            *    @timer with a new vector.
            */
           extern int mod_timer(struct timer_list *timer, unsigned long expires);

           /* mod_timer_pending - modify a pending timer's timeout */
           extern int mod_timer_pending(struct timer_list *timer, unsigned long expires);

           /* mod_timer_pinned - modify a timer with a new expires in TIMER_PINNED state */
           extern int mod_timer_pinned(struct timer_list *timer, unsigned long expires);

           /* timer_pending - if @timer->entry.next != NULL => pending OTHERWISE not pending */
           static inline int timer_pending(const struct timer_list *timer);

           e.g.
             int kthread_example(void *data)
             {
                     struct timer_list *example_timer = kmalloc(sizeof(struct timer_list), GFP_KERNEL);
                     if (IS_ERR(example_timer)) {
                             printk(KERN_DEBUG "Failed to allocate memory for @example_timer.");
                             return -ENOMEM;
                     }

                     init_timer(example_timer);

                     /**
                      * before timer added to timer list of a CPU,it is safely to assign values for members
                      * of it.
                      * but in the case timer is added and activated,modify jiffies can only through mod_timer().
                      * @function and another properties can not be modified furthuer.
                      */
                     /* if @function is NULL,it must be a bug */
                     example_timer->function = example_timer_fn;
                     example_timer->expires = jiffies + (unsigned long)16;

                     add_timer_on(example_timer, smp_processor_id());
                     ...
                     mod_timer(example_timer, jiffies + (unsigned long)32);
                     ...
                     del_timer(example_timer);
                     ...
                     kfree(example_timer);
                     return 0;
             }

         run_timer_softirq :
           TIMER_SOFTIRQ handler is run_timer_softirq().
           function init_timers() call to open_softirq() to setup this function as a handler for softirq TIMER_SOFTIRQ,
           it also setup per-CPU variable @tvec_bases and CPU timer notifier,such notifier is used for support to
           CPU hotplug(migrate timers on the CPU ready to offline).

           <kernel/timer.c>
             /**
              * run_timer_softirq - handler for softirq TIMER_SOFTIRQ
              * @h:                 softirq action acciated to this handler
              */
             static void run_timer_softirq(struct softirq_action *h);

             !!  run_timer_softirq() call to hrtimer_run_pending() to expire hrtimers,it is called from
                 timer softirq every jiffy.
                 function hrtimer_run_pending() checks whether high-resolve timer is active
                   activated => return
                   not activated => check if high-resolution timer mode is enabled AND nohz mode inactive
                                      TRUE  => checkout to HRT
                                      FALSE => stay work on nohz mode
                 /* clocksource switch happens in the timer interrupt with @xtime_lock is held. */
                 /**
                  * tick for hrtimer works on one-shot mode,the corresponding timer interrupt handler
                  * is hrtimer_handler() which is defined in <kernel/hrtimer.c>
                  */
                 AS DESCRIBED ABOVE,WE HAD KNOWN THAT HRT USE THE SAME TYPE OF SOFTIRQ AS NOHZ TIMER.

             /**
              * __run_timers - internal routine for run_timer_softirq()
              * @base:         the per-CPU variable @tvec_bases fetched
              *                by run_timer_softirq()
              */
             static inline void __run_timers(struct tvec_base *base);

             __run_timers() does :
               1>  acquire lock to local @base
               2>  enter a while-loop,iterating until @timer_jiffies before @jiffies
                                                 older                                   newer
                                                 [ - - - - @timer_jiffies - - - @jiffies ]
                                                 [ - - - (@timer_jiffies @jiffies) - - - ] coincided
                                                 [ - - - - @jiffies - - - @timer_jiffies ]
                                                 after                                   before
               3>  cascade timers -- place the dynamic timers they had not expired to new vectors in
                   the @base(adjust tv2 -- tv5)
               4>  increase @timer_jiffies once
               5>  for each expired dynamic timer,detach it from @base,and set @running_timer points to
                   it to indicate that "this" timer is dealt with,now.
               6>  call to "this" timer's @function with its @data.
               7>  goto 2>

               /* detach_timer() is defined in <kernel/timer.c>,which dequeue a timer from timer_list. */

         An Application of Dynamic Timers - the nanosleep() System Call :
           system call "nanosleep" is defined in <kernel/hrtimer.c> in system call definition method 2.

           <kernel/hrtimer.c>
             SYSCALL_DEFINE2(nanosleep, struct timespec __user *, rqtp, struct timespec *, rmtp);
             =>
               <linux/syscalls.h>
                 SYSCALL_DEFINE2(name, ...) = SYSCALL_DEFINEx(2, _##name, __VA_ARGS__)
                 =>  finally :
                   asmlinkage long sys_nanosleep(struct timespec __user *rqtp, struct timespec __user *rmtp);

             /**
              * sys_nanosleep - system call nanosleep
              * @rqtp:          pointer points to an object is type of struct timespec as the requested time
              *                 interval
              * @rmtp:          remained time interval to save when this syscall is interrupted
              * return:         0 => time interval elapsed completely
              *                 -EFAULT => error,copy to(/from) user failed
              *                 -EINVAL => error,invalid timespec
              *                 -ERESTARTNOHAND => error,restart syscall with no handler
              *                 -ERESTART_RESTARTBLOCK => error,interrupted by signal and restarted
              *                                           in this case,@rmtp saves the remained time interval
              *                                           if syscall restart have forbidden,just saves the
              *                                           remained time interval only,do not to restart syscall
              */

           the syscall sys_nanosleep() creates hrtimer on the stack of the process in Kernel Mode,before 
           function hrtimer_nanosleep() exit,such hrtimer would be destroyed.
                    (sys_nanosleep -> hrtimer_nanosleep)

           the primary sleeping routine is handled by function do_nanosleep().

           /**
            * do_nanosleep - main routine of hrtimer_nanosleep()
            * @t:            hrtimer sleeper created by hrtimer_nanosleep()
            * @mode:         work mode of hrtimer
            * return:        T => time interval elasped completely
            *                F => interrupted
            */
           static int __sched do_nanosleep(struct hrtimer_sleeper *t, enum hrtimer_mode mode);

           what do_nanosleep does :
             1>  set @t->function to hrtimer_wakeup(),
                 set @t->task to @current
             2>  enter do-while loop until hrtimer completed(@t->task == NULL) or interrupted by an signal,
             3>  set @current to TASK_INTERRUPTIBLE
             4>  start timer expiring through hrtimer_start_expires(&@t->timer, @mode)
             5>  if timer inactive,set sleeper's @task to NULL
                 this means,the hrtimer had timeout
             6>  if @t.task != NULL,call to schedule() relinquish CPU
             7>  cancel hrtimer and change @mode to HRTIMER_MODE_ABS
             8>  goto 3>
             9>  exit do-while loop,set @current to TASK_RUNNING
             10> return @t->task == NULL

           !!  IF @current IS RE-PICKED AS THE NEXT TASK TO RUN,AND THE SPECIFIED TIME INTERVAL HAVE NOT
               EPLASED,THEN THE do-while LOOP WILL RESTART A HRTIMER AGAIN AND RELINQUISH CPU.
               @expires := @request + @jiffies

       Delay Functions :
         the minimum wait time for dynamic timer is 1-milliseconds,it is useless when kernel need a timer
         to wait a time interval less than 1-milliseconds.(1-microseconds / 1-nanoseconds)
         such case appears often in device driver.

         <arch/x86/include/asm/delay.h>
           /**
            * udelay - delay function used to wait a time interval in microseconds
            * @n:      how many microseconds to wait for
            */
           #define udelay(n)  (__builtin_constant_p(n) ? \
                               ((n) > 20000 ? __bad_udelay() : __const_udelay((n) * 0x10c7ul)) \
                               __udelay(n))

           !!  __udelay => __const_udelay(n * 0x000010c7)

           /**
            * ndelay - delay function used to wait a time interval in nanoseconds
            * @n:      how many nanoseconds to wait for
            */
           #define ndelay(n)  (__builtin_constant_p(n) ? \
                               ((n) > 20000 ? __bad_ndelay() : __const_udelay((n) * 0x5ul)) \
                               : __ndelay(n))

           !!  __ndelay => __const_udelay(n * 0x00005)

           /**
            * __const_udelay - main delay function
            * @xloops:          how many loops to iterates
            */
           extern void __const_udelay(unsigned long xloops);

           /**
            * use_tsc_delay - switch the delay clock source to TSC
            */
           void use_tsc_delay(void);

           !!  kernel must take a way to convert microseconds and nanoseconds to loops,and have to
               calibrate how many loops in a tick.
               the convertion is :
                 32-bit mull
                 =>  eax : @xloops *= 4
                     edx : cpu_data(raw_smp_processor_id()).loops_per_jiffy * (HZ / 4)

                 @loops_per_jiffy is the loops in a tick which is calibrated during system booting through
                 calibrate_delay() defined in <init/delay.c>.
                 the cpu_data() fetch an object is type of struct cpuinfo_x86 is introduced in
                 <arch/x86/include/asm/processor.h>.

               for boot CPU,just set @loops_per_jiffy to its clock frequency as well,but for the rest CPUs,
               must call to calibrate_delay() to determine the value of its @loops_per_jiffy.
               the @loops_per_jiffy can be set through kernel parameter "lpj=<v>" .

           !!  <arch/x86/lib/delay.c>
                 /**
                  * delay_fn - the real delay function setup during system booting
                  * @arg1:     loops to delay
                  */
                 static void (*delay_fn)(unsigned long) = delay_loop;

                 delay_loop() is defined in the same source code file as default delay function.
                 static function delay_tsc() is the delay function based TSC,and use_tsc_delay()
                 switch @delay_fn to delay_tsc().

           !!  HPET or TSC as the delay clock source,one loop corresponds to one CPU cycle -- 
               time interval between two consecutive CPU clock signals.
               PIT  or unspecified,one loop corresponds to the time duration of a single iteration
               of a tight instruction loop.

     System Calls Related to Timing Measurements :
       some system call primitives related to timing measurement >

         SUS :
           time
           gettimeofday
           adjtimex
           /* settimeofday REQUEST _BSD_SOURCE */
           setitimer
           alarm           

         POSIX.1 :
           clock_gettime
           clock_settime
           clock_getres
           timer_create
           timer_gettime
           timer_settime
           timer_getoverrun
           timer_delete
           clock_nanosleep

       !!  Linux System Call routines always return long int.
       SUS System Call :
         <kernel/time.c>
           /**
            * sys_time - system call time to retrieve time in seconds since the Epoch
            * @tloc:     pointer points to an object is type of time_t,
            *            if @tloc is not NULL,then write the time into
            *            *@tloc
            * return:    time in seconds since UTC 1970-01-01 00:00
            *            -EFAULT => failed to write time into @tloc
            */
           SYSCALL_DEFINE1(time, time_t __user *, tloc);

           !!  the system call sys_time() is able to be implemented at user-level using
               sys_gettimeofday(),but this interface is remained for compatible.

           /**
            * sys_gettimeofday - system call gettimeofday to retrieve time of day
            * @tv:               pointer points to an object is type of struct timeval,
            *                    the object is used to save time info
            * @tz:               pointer points to an object is type of struct timezone,
            *                    if @tz is not NULL,then write @sys_tz(system timezone) to @tz
            * return:            0 => succeed
            *                    -EFAULT => tv == NULL OR (tv != NULL AND write time info to @tv failed)
            *                               tz != NULL AND write @sys_tz to @tz failed
            *
            * #  this function call to do_gettimeofday() to fetch current system time,function 
            *    timekeeping_get_ns() would be called by it,and @xtime_lock will be acquired during
            *    read current time from the clock source of @timekeeper.
            */                         
           SYSCALL_DEFINE2(gettimeofday, struct timeval __user *, tv, struct timezone __user *, tz);

           !!  under linux,the second field of struct timezone @tz_dsttime is never been used.

           /**
            * sys_adjtimex - system call adjtimex used to tune kernel clock
            * @txc_p:        pointer points to an object is type of struct timex,
            *                it saves the new parameters used by kernel to tunes clock
            * return:        -EFAULT => failed to copy current values of kernel clock to @txc_p
            *                TIME_OK => succeed
            *                TIME_ERROR => failed to adjust kernel clock
            *                ...
            *                TIME_* => represent clock synchronization status,introduced in <linux/timex.h>
            *
            * 
            * #  this function use @txc_p to tunes kernel clock,if succeed,the paramter
            *    @txc_p save the copy of the current values of kernel clock.
            *    @xtime_lock is write-lock during tuning kernel clock.
            *    real-time-clock (RTC) integered in motherboard also be modified.
            */
           SYSCALL_DEFINE1(adjtimex, struct timex __user *, txc_p);

           !!  structure timex is defined in <linux/timex.h> used to discipline kernel clock oscillator.
           !!  clock synchronization status :
                 TIME_OK     0           =>  synchronized,no leap second
                 TIME_INS    1           =>  insert leap second
                 TIME_DEL    2           =>  delete leap second
                 TIME_OOP    3           =>  leap second in progress
                 TIME_WAIT   4           =>  leap second has occurred
                 TIME_ERROR  5           =>  not synchronized
                 TIME_BAD    TIME_ERROR  =>  bw compat

         <kernel/itimer.c>
           /**
            * sys_setitimer - system call allow User Mode processes to set special timers
            *                 - interval timers(no relating to PIT)
            * @which:         which policy to apply -
            *                   ITIMER_REAL    => actual elapsed time,SIGALRM
            *                   ITIMER_VIRTUAL => time spent by the process in User Mode,SIGVTALRM
            *                   ITIMER_PROF    => time spent by the process both in User Mode and in
            *                                     Kernel Mode,SIGPROF
            * @value:         itimer parameter to set
            * @ovalue:        the older itimer parameter to save
            * return:         0 => succeed
            *                 -EINVAL => invalid itimer parameter
            *                 -EFAULT => failed when copy from user or copy to user
            */
           SYSCALL_DEFINE3(setitimer, int, which, struct itimerval __user *, value,
                           struct itimerval __user *, ovalue);

           !!  structure itimerval is defined in <linux/time.h>
                 struct itimerval {
                         struct timeval it_interval;
                         struct timeval it_value;
                 };

                 @it_interval means the time interval must to be waitted before send an signal
                 @it_value means the times for signals have to be sent

                 /**
                  * everytime after an signal had sent,@it_value been decreased,stop when it becomes to zero.
                  * everytime an signal was sent and @it_value is not equal to zero,reset itimer to
                  * @it_interval for next signal is going to be raised.
                  */

           !!  structure task_struct has member named @signal is type of a pointer for structure signal_struct.
               members {
                 struct hrtimer real_timer            /* high-resolution timer */
                 struct pid *leader_pid               /* process group id */
                 ktime_t signal_struct.it_real_incr   /* time interval in ktime_t,conversion occurred */
               } reserved for ITIMER_REAL for @current task.

           !!  ITIMER_VIRTUAL and ITIMER_PROF are migrated to cpu itimer now,not as the process itimer.
               for these two policies,cpu itimer is set to CPUCLOCK_VIRT and CPUCLOCK_PROF,respectively.
               function set_cpu_itimer() is defined in <kernel/itimer.c>,it operates an array contains two
               elements included in signal_struct has name @it is type of struct cpu_itimer.
               parameter @clockid of set_cpu_itimer() has argument CPUCLOCK_VIRT or CPUCLOCK_PROF.
           
           /**
            * sys_alarm - system call alarm,send called SIGALRM after an especified time interval in seconds 
            *             had been elapsed.
            * @seconds:   time interval in seconds.
            * return:     0 => timer is not activated,that is,time interval elapsed and signal raised
            *             > 0 => remaining time in seconds of a pending timer
            *
            * #  this function call to alarm_setitimer() to set an itimer(ITIMER_REAL) at oneshot mode
            *    with parameter @seconds.
            */
           SYSCALL_DEFINE1(alarm, unsigned int, seconds);

       !!  POSIX.1b API returns 0 means succeed,otherwise,returns error code.
       POSIX.1 System Call : 
         <linux/time.h>
           /* system clocks for POSIX.1b interval timers */
           #define CLOCK_REALTIME            0          /* resolution = 999848 */
                   /**
                    * real-time clock of the system.
                    * essentially,the value of @xtime.
                    */
           #define CLOCK_MONOTONIC           1          /* resolution = 999848 */
                   /**
                    * real-time clock of the system purged of every time warp due to the
                    * synchronization with an external time source.
                    * essentially,it represented by the sum of @xtime and @wall_to_monotonic.
                    */
           #define CLOCK_PROCESS_CPUTIME_ID  2
           #define CLOCK_THREAD_CPUTIME_ID   3
           #define CLOCK_MONOTONIC_RAW       4
           #define CLOCK_REALTIME_COARSE     5
           #define CLOCK_MONOTONIC_COARSE    6

         <linux/posix-timers.h>
           struct k_itimer;        /* POSIX.1b interval timer */

           /* struct k_clock - POSIX timer operations */
           struct k_clock {
                   int res;  /* in nanoseconds */
                   int (*clock_getres)(const clockid_t which_clock, struct timespec *tp);
                   int (*clock_set)(const clockid_t which_clock, struct timespec *tp);
                   int (*clock_get)(const clockid_t which_clock, struct timespec *tp);
                   int (*timer_create)(struct k_itimer *timer);
                   int (*nsleep)(const clockid_t which_clock, int flags, struct timerspec *,
                                  struct timespec __user *);
                   int (*nsleep_restart)(struct restart_block *restart_block);
                   int (*timer_set)(struct k_itimer *timr, int flags,
                                     struct itimerspec *new_setting,
                                     struct itimerspec *old_setting);
                   int (*timer_del)(struct k_itimer *timr);
           #define TIMER_RETRY 1
                   void (*timer_get)(struct k_itimer *timr,
                                     struct itimerspec *cur_setting);
           };

         <kernel/posix-timers.c>
           /* posix_clocks - table for POSIX clocks,MAX_CLOCKS = 16 defined in <linux/time.h> */
           static struct k_clock posix_clocks[MAX_CLOCKS];

           /**
            * CLOCK_DISPATCH - generic caller routine used to dispatch clock from posix_clocks table
            *                  and does @call on it
            * @clock:          which clock
            * @call:           which routine to call
            * @arglist:        arguments enclosed by parentheses
            *
            * #  @clock < 0,negative index was selected in the table,thus call to posix cpu clock routine
            * #  @call == NULL,no such routine is existed,thus call to common routine
            */
           #define CLOCK_DISPATCH(clock, call, arglist)  \
                   ((clock) < 0 ? posix_cpu_##call arglist :  \
                           (posix_clocks[clock].call != NULL  \
                            ? (*posix_clocks[clock].call) arglist : common_##call arglist))

           /**
            * sys_clock_gettime - POSIX.1b API,get time in seconds and in nanoseconds from a clock
            * @which_clock:       indicates which clock to get the time,it is type of 
            *                       clockid_t => __kernel_clockid_t => int
            * @tp:                pointer points to an object is type of struct timespec,it saves
            *                     the time retrieve from @posix_clocks[@which_clock]
            * return:             0 => get time succeed
            *                     -EINVAL => invalid clock id
            *                     -EFAULT => copy to user failed
            */
           SYSCALL_DEFINE2(clock_gettime, const clockid_t, which_clock, struct timespec __user *, tp);

           /* sys_clock_settime - POSIX.1b API,set time in seconds and in nanoseconds for a clock */
           SYSCALL_DEFINE2(clock_settime, const clockid_t, which_clock, const struct timespec __user *, tp);

           /* sys_clock_getres - POSIX.1b API,get @res(timer resolution) from a clock */
           SYSCALL_DEFINE2(clock_getres, const clockid_t, which_clock, struct timespec __user *, tp);

           !!  typedef int __kernel_timer_t;
               typedef __kernel_timer_t timer_t;

           /* sys_timer_create - POSIX.1b API,create and start an interval timer */
           SYSCALL_DEFINE3(timer_create, const clockid_t, which_clock, struct sigevent __user *,
                           timer_event_spec, timer_t __user *, create_timer_id);

           !!  structure sigevent is defined in <asm-generic/siginfo.h>
                 #define SIGEV_SIGNAL     0        /* notify via signal */
                 #define SIGEV_NONE       1        /* other notification: meaningless */
                 #define SIGEV_THREAD     2        /* deliver via thread creation */
                 #define SIGEV_THREAD_ID  4        /* deliver to thread */


                 #ifndef __ARCH_SIGEV_PREAMBLE_SIZE
                 #define __ARCH_SIGEV_PREAMBLE_SIZE  (sizeof(int) * 2 + sizeof(sigval_t))
                 #endif

                 #define SIGEV_MAX_SIZE  64
                 #define SIGEV_PAD_SIZE  ((SIGEV_MAX_SIZE - __ARCH_SIGEV_PREAMBLE_SIZE)  \
                                          / sizeof(int))

                 /* structure for notification from asynchronous routines(aio_read, aio_write,etc.) */
                 typedef struct sigevent {
                         sigval_t sigev_value;
                         int sigev_signo;
                         int sigev_notify;
                         union {
                                 int _pad[SIGEV_PAD_SIZE];
                                 int _tid;
                                 
                                 struct {
                                         void (*_function)(sigval_t);
                                         void *_attribute;        /* really pthread_attr_t */
                                 } _sigev_thread;
                         };
                 } sigevent_t;

           /* sys_timer_gettime - POSIX.1b API,get the remaining time from an interval timer */
           SYSCALL_DEFINE2(timer_gettime, timer_t, timer_id, struct itimerspec __user *, setting);

           /* sys_timer_settime - POSIX.1b API,set an interval timer */
           SYSCALL_DEFINE4(timer_settime, timer_t, timer_id, int, flags, const struct itimerspec __user *,
                           new_setting, struct itimerspec __user *, old_setting);

           /* sys_timer_getoverrun - POSIX.1b API,get the number of overruns of an interval timer */
           SYSCALL_DEFINE1(timer_getoverrun, timer_t, timer_id);

           /* sys_timer_delete - POSIX.1b API,delete an interval timer */
           SYSCALL_DEFINE1(timer_delete, timer_t, timer_id);

           /**
            * sys_clock_nanosleep - POSIX.1b API,suspend process until an especial time interval(in nanoseconds)
            *                       elapsed
            * @which_clock:         clock id
            * @flags:               control flags 
            *                         0 => @rqtp is an interval related to the current value of @which_clock
            *                         TIMER_ABSTIME => @rqtp is a absolute time
            * @rqtp:                request
            * @rmtp:                remained time
            * return:               0 => time elapsed completely
            *                       -EINVAL => invalid time
            *                       -EFAULT => failed when copy to user / copy from user
            */
           SYSCALL_DEFINE4(clock_nanosleep, const clockid_t, which_clock, int, flags,
                           const struct timespec __user *, rqtp, struct timespec __user *, rmtp);


/* END OF CHAPTER6 */


Chapter 7 : 
    Scheduling Policy :
      the set of rules used to determine when and how to select a new process to run is called "Scheduling Policy".

      Linux scheduling is based on time sharing technique :
        several processes run in "time multiplexing",the CPU time is divided into slices,one for each runnable
        process.                                                               /* OR called quantum */
        !!  time sharing technique relies on timer interrupts and is thus transparent to processes.

      Linux scheduling is also based on ranking processes :
        process priority.
        each process is associated with a value that tells the scheduler how appropriate it is to let the
        process run on a CPU.
        !!  linux process priority is dynamic,the scheduler keep track of what processes are doing and adjusts
            their priorities periodically.

      Process classify :
        I/O-Bound
          make heavy use of I/O devices and spend much time waiting for I/O operations to complete
        CPU-Bound
          number-crunching applications that require a lot of CPU time

        alternative three for classifications >
          1>  interactive processes
                interact constantly with their users,and therefore spend a lot of time waiting for
                keypresses and mouse operations.
                the average delay must fall between 50 -- 150 milliseconds.
                /* shell, text editor, graphical application, etc. */

          2>  batch processes
                do not need user interaction,often run in the background.
                scheduler can decreases their priority as well.
                /* language compiler, database searching, scientific computation, etc. */

          3>  real-time processes
                very stringent scheduling requirements.
                never be blocked by lower-priority processes,should have a short guaranteed response
                time with a minimum variance.
                /* video / sound application, robot controller, program collects data from physical sensor, etc. */

          !!  batch process can be I/O-Bound and CPU-Bound.
              interactive process often be I/O-Bound.
              real-time program are explicitly recognized as such by the scheduling algorithm in Linux.

      !!  PROGRAMMER CAN MODIFY THE PROCESS PRIORITY THROUGH SOME SYSTEM-CALL API.
          SCHEDULER TENDS TO FAVOR THE PROCESS IS INTERACTIVE OVER BATCH.

      Process Preemption :
        when a process enter TASK_RUNNING state,kernel checks whether its priority is greater than the @current running
        process,if it is,then scheduler will be invoked,@current would be interrupted and another process is selected to
        run(usually,"this" process").
        when a process's time slice had exhausted(TID_NEED_RESCHED set),scheduler would be called after timer interrupt
        terminated,and @current running process would be preempted by anoter process selected by scheduler.

        !!  A process had preempted is not suspended,it stay in TASK_RUNNING state,but it is no longer uses the CPU.
            Since Linux 2.6,the Kernel is preemptive,that is,a process can be preempted even when it is executing the
            Kernel Code in Kernel Mode.
            /* interrupt can not be preempted but can be interrupted by another */

      How Long Must a Quantum Last ? :
        quantum duration is critical for system performance,it should be neither too long nor too short.

        suppose,quantum duration is 5s :
          batch process 1 run at first,and wait for N seconds to start next execution.
            N = 5s * total(processes - 1)
          and,maybe an interactive process preempt batch process 1.
        suppose,quantum duration is 5ms,and process switch require 5ms :
          process 1 run,and exhausted time slice,scheduler does process switch
            time consumed = 5ms + 5ms => 10ms
          CPU time duration = 1s
            1s = 1000ms
          times for process switch in 1s = 1000ms / 10ms / 2 = 50
          times for a process runs in 1s = 1000ms / 10ms / 2 = 50
          => 50% CPU time had consumed on process switch

        Linux quantum duration rule : choose a duration as long as possible,while keeping good system response time.
                                      /* compromise */

    The Scheduling Algorithm :
      Scheduling Algorithm for earlier verions of Linux :
        at every process switch,kernel scanned the list of runnable processes,computed their priorities,and selected the
        "best" process to run.  /* too costly */

      Scheduling Algorithm of Linux 2.6 :
        /* scale well with the number of runnable processes */
        select the process to run in constant time,independently of the number of runnable processes.
        /* scale well with the number of processors */
        each CPU has its own queue of the runnable processes.
        distinguishing interactive processes and batch processes.

        swapper process (PID 0) :
          executes only when the CPU cannot execute other processes.
          each CPU has its own swapper process.
          scheduler always can find a process to run.
    
        scheduling classes : /* every Linux process always scheduled according to a scheduling class */
          SCHED_FIFO 
            First-In First-Out real-time process
            when the scheduler assigns the CPU to the process,it leaves the process descriptor in its current position in
            the runqueue list.if no other high-priority process is runnable,the process continues to use the CPU as long as
            it wishes,even if other processes that have the same priority are runnable.

          SCHED_RR
            Round Robin real-time process
            when the scheduler assigns the CPU to the process,it puts the process descriptor at the end of the runqueue list.
            this ensure a fair assignment of CPU time to all SCHED_RR processes that have the same priority.

          SCHED_NORMAL
            Conventional time-shared process

          Scheduling of Conventional Processes :
            every conventional process has a static priority,kernel represents the static priority use the number ranging from
            100(highest-priority) -- 139(lowest-priority).
            !!  A NEW PROCESS ALWAYS INHERITS THE STATIC PRIORITY OF ITS PARENT.
                USER CAN CHANGE STATIC PRIORITY OF A PROCESS THROUGH nice() OR setpriority() SYSTEM CALL.BUT A USER CAN ONLY 
                CONTROL THE PRIORITY OF THE PROCESS WHICH IS BELONGS TO "this" USER,UNLESS "this" USER IS root.

            Base time quantum :
              /* FORMULA1 */
                                                    (140 - static priority) * 20 if static priority < 120
              base time quantum(in milliseconds) = 
                                                    (140 - static priority) * 5  if static priority >= 120

              base time quantum is assigned when a process has exhausted its previous time quantum.

              btq     =>  base time quantum
              sp      =>  static priority
              nv      =>  nice value
              idelta  =>  interactive delta
              stt     =>  sleep time threshold
                                            sp      nv      btq         idelta  stt
              highest static priority       100     -20     800ms       -3      299ms
              high static priority          110     -10     600ms       -1      499ms
              default static priority       120       0     100ms       +2      799ms
              low static priority           130     +10      50ms       +4      999ms
              lowest static priority        139     +19       5ms       +6      1199ms

            Dynamic priority and average sleep time :
              every conventional process also has a dynamic priority is a value ranging from 100(high) -- 139(low).
              this property is the number actually looked up by the scheduler when selecting the new process to run.

              empirical formula :
                dynamic priority = max(100, min(static priority - bonus + 5, 139))
                @bonus : [0, 10]
                         related to the average sleep time of the process

              average sleep time :
                average number of nanoseconds that the process spent while sleeping.
                sleeping in TASK_INTERRUPTIBLE state contributes to the average sleep time in a different
                way from sleeping in TASK_UNINTERRUPTIBLE state.
                the average sleep time is decreases while a process is running.

                ast => average sleep time
                b   => bonus
                g   => granularity

                /**
                 * the times that process is sleep greater,the average sleep time is lower.
                 *   total sleep time / times(process sleep) := average sleep time
                 */
                ast                         b           g
                    0 <= ast < 100ms        0           5120
                100ms <= ast < 200ms        1           2560
                200ms <= ast < 300ms        2           1280
                300ms <= ast < 400ms        3            640
                400ms <= ast < 500ms        4            320
                500ms <= ast < 600ms        5            160
                600ms <= ast < 700ms        6             80
                700ms <= ast < 800ms        7             40
                800ms <= ast < 900ms        8             20
                900ms <= ast < 1000ms       9             10
                   1s == ast                10            10

                average sleep time is also used by scheduler to determine whether the process is an
                interactive process or a batch process.
                a process is considered "interactive" if it satisfies 
                  dynamic priority <= 3 * static priority / 4 + 28
                  <=>
                  bonus - 5 >= static priority / 4 - 28
                               /* interactive delta */

            Active and expired processes :
              for prevent process starvation,when a process finishes its time quantum,it can be replaced
              by a lower priority process whose time quantum has not yet been exhausted.

              two disjoint sets of runnable processes :
                Active processes {
                        runnable processes have not yet exhausted their time quantum
                        allowed to run
                }                
            
                Expired processes {
                        runnable processes have exhausted their time quantum
                        forbidden to run until all active processes expire
                }

              an active batch process exhausted its quantum always becomes expired.
              an active interactive process exhausted its quantum usually remains active - scheduler refills
              its time quantum and leaves it in the set of active processes.
              /**
               * if waited_long_time(eldest_expired_process) || higher_priority(expired_process)
               * then
               *         quantum exhausted-interactive process => expired interactive process
               *         #  do not refill quantum
               */

          Scheduling of Real-Time Processes :
            every real-time process associated with a real-time priority,it is a value ranging from
            1(highest priority) -- 99(lowest priority).
            a real-time process inhibits the execution of every lower-priority process while it remains
            runnable.
            real-time processes are always considered "active".

            !!  USER CAN CHANGE REAL-TIME PRIORITY THROUGH SYSTEM CALL sched_setparam() AND sched_setscheduler().

            if same_priority(some_real_time_processes)
            then
                    scheduler : select first_runnable(PerCPU_Fetch(smp_cpu_id(), runqueue))

            conditions that real-time process to be replaced : /* one of the following */
              1>  real_time_priority(@current) < real_time_priority(rtp)
              2>  set_task_state(@current, TASK_INTERRUPTIBLE) || set_task_state(@current, TASK_UNINTERRUPTIBLE)
                  /* blocking */
              3>  task_state(@current) == TASK_STOPPED / TASK_TRACED / EXIT_ZOMBIE / EXIT_DEAD
              4>  relinquish_cpu(smp_cpu_id(), @current) through system call sched_yield()
              5>  scheduling_class(@current) == SCHED_RR && is_quantum_exhausted(@current)

            !!  SYSTEM CALLS nice() AND setpriority() ON A REAL-TIME PROCESS WITH SCHED_RR DO NOT CHANGE THE
                REAL-TIME PRIORITY BUT RATHER THE DURATION OF THE BASE TIME QUANTUM .
                THE BASE TIME QUANTUM OF A REAL-TIME PROCESS WITH SCHED_RR DO NOT DEPEND ON THE REAL-TIME
                PRIORITY,BUT RATHER ON THE STATIC PRIORITY OF IT,ACCORDING TO THE FORMULA1.

      Data Structures Used by the Scheduler :
        The runqueue Data Structure :
          each CPU on the system has a Per-CPU variable is named @runqueues,it is type of struct rq introduced 
          in <kernel/sched.c>.

          <kernel/sched.c>
            /**
             * struct rq - the structure used to represent runqueue
             * @lock:                  runqueue lock
             * @nr_running:            number of runnable processes in the runqueue lists
             * @cpu_load:              CPU load factors based on the average number of processes in the runqueue
             * @in_nohz_recently:      indicate whether timekeeping in nohz mode recently
             * @load:                  load weight
             * @nr_load_updates:       the times that @load updated
             * @nr_switches:           number of process switches performed by the CPU
             * @cfs:                   completely fair scheduling runqueue
             * @rt:                    real time runqueue
             * @leaf_cfs_rq_list:      leaf of robin tree of the cfs rq
             * @leaf_rt_rq_list:       leaf of robin tree of the rt rq
             * @nr_uninterruptible:    number of processes that in TASK_UNINTERRUPTIBLE state in the runqueue
             * @curr:                  current task
             * @idle:                  idle task - @swapper mentioned abovde
             * @next_balance:          the jiffies next balance to occurs
             * @prev_mm:               memory descriptor of previous task
             * @clock:                 sched clock,monotonic per cpu clock
             * @nr_iowait:             number of processes that were previously in the runqueue lists and are now
             *                         waiting for a disk I/O operation to complete
             * @rd:                    the root scheduling domain
             * @sd:                    the base scheduling domain of this CPU
             * @active_balance:        flag set if some process shall be migrated from this runqueue to another
             * @push_cpu:              not used
             * @cpu:                   the cpu this runqueue belongs to
             * @online:                is the @cpu online?
             * @avg_load_per_task:     average load for per task
             * @migration_thread:      the "migration" kernel thread
             * @migration_queue:       list of processes to be removed from the runqueue
             * @hrtick_timer:          high-resolution timer
             * @yld_count:             the times that sys_sched_yield() was called by the task in this runqueue
             * @sched_count:           the times that schedule() was called by the task in this runqueue
             * @sched_goidle:          the times that no task can be selected to run thus selected idle process to
             *                         execute
             * @bkl_count:             the times that big kernel locked
             */
            struct rq {
                    raw_spinlock_t lock;
                    unsigned long nr_running;
                    #define CPU_LOAD_IDX_MAX 5
                    unsigned long cpu_load[CPU_LOAD_IDX_MAX];

            #ifdef CONFIG_NO_HZ
                    unsigned char in_nohz_recently;
            #endif
                    
                    struct load_weight load;
                    unsigned long nr_load_updates;
                    u64 nr_switches;

                    struct cfs_rq cfs;
                    struct rt_rq rt;

            #ifdef CONFIG_FAIR_GROUP_SCHED
                    struct list_head leaf_cfs_rq_list;
            #endif
            #ifdef CONFIG_RT_GROUP_SCHED
                    struct list_head leaf_rt_rq_list;
            #endif

                    unsigned long nr_uninterruptible;

                    struct task_struct *curr, *idle;
                    unsigned long next_balance;
                    struct mm_struct *prev_mm;

                    u64 clock;

                    atomic_t nr_iowait;

            #ifdef CONFIG_SMP
                    struct root_domain *rd;
                    struct sched_domain *sd;
                    ...
                    int active_balance;
                    int push_cpu;
                    int cpu;
                    int online;
                  
                    unsigned long avg_load_per_task;

                    struct task_struct *migration_thread;
                    struct list_head migration_queue;
            #endif

                    ...
            
            #ifdef CONFIG_SCHED_HRTICK
                    ...

                    struct hrtimer hrtick_timer;
            #endif

                    ...

            #ifdef CONFIG_SCHEDSTATS
                    ...
                    unsigned int yld_count;

                    unsigned int sched_switch;
                    unsigned int sched_count;
                    unsigned int sched_goidle;
                    ...
                    unsigned int bkl_count;
            #endif
            };

          because each CPU has its own @runqueues,thus a task only existed just one @runqueues of a CPU,but
          when load balancing occurred,it maybe migrated to another @runqueues of different CPU in the system.

          !!  HAVE TO BE AWARE CLEARLY THAT struct rq IS ASSOCIATED TO THE CPU,NOT TO THE PROCESS.WHEN SELECT
              A CPU FOR PROCESS,THAT IS SELECT A RUNQUEUE OF THE CPU FOR PROCESS,THE PROCESS WILL RESIDE ON IT.
          !!  HAVE TO BE AWARE CLEARLY THE DIFFERENCE BETWEEN 
                @rq->curr(CURRENT RUNNING TASK OF RUNQUEUE)
                @current(THIS THREAD)
                @p(TASK)

        Process Descriptor :
          structure task_struct is defined in <linux/sched.h> used to represents a task.
          each task correspond to a process normally,but it a process contains several threads,every thread
          correspond to a task. (in other words,a task correspond to a thread)

          struct task_struct had been introduced in Chapter 3 Processes.

          the members of struc task_struct that related to Linux scheduler :
            __u32 thread_info->flags         =>  store the TIF_NEED_RESCHED flag,which is set if the scheduler 
                                                 must be invoked  
            __u32 thread_info->cpu           =>  logical number of the CPU owning the runqueue to which the
                                                 runnable process belongs
            volatile long state              =>  process state
            int prio                         =>  dynamic priority
                static_prio                  =>  static priority,converted from USER NICE VALUE
                normal_prio                  =>  normal priority (CFS)
            struct list_head rt.run_list     =>  real-time runnable list where @current is linked into
            unsigned int policy              =>  scheduling class (SCHED_NORMAL, SCHED_RR, SCHED_FIFO)
            cpumask_t cpus_allowed           =>  bit mask of the CPUs that can execute the process
            unsigned int rt.time_slice       =>  real-time ticks left in the time quantum of the process
            u64 se.slice_max                 =>  maximum time slice
            unsigned int rt_priority         =>  real-time priority

            /**
             * @rt is type of struct sched_rt_entity
             * @se is type of struct sched_entity,CFS use it to records process times
             */

          !!  LINUX 2.6.34.1 APPLY CFS ON THE PROCESS SCHEDULING.

          when static function copy_process() defined in <kernel/sched.c> is called for fork a child process,
          it call to dup_task_struct() to make a copy of the parent's task_struct for child process.
          so,@rt and @se will be copied as well,thus child has the same time informations as its parent's.
          !!BUT!!
            copy_process() call to sched_fork(@child) later to performs scheduling entity setup,almost
            all fields would be initialized to 0.of course,@se.slice_max is set to 0,and @rt unchanged.
            copy_process() just assign child to a CPU through sched_fork() but not start it due to child is
            in TASK_WAKING state,and sched_fork() let child inherits the @normal_prio from its parent as the
            @prio(dynamic priority).

        Sched Clock Data :
          sched clock for unstable cpu clocks.
          create a semi stable clock from a mixture of other events :
            TSC
            GTOD
            explicit idle event

          <kernel/sched_clock.c>
            /**
             * sched_clock_data - structure used to record clock data fro scheduler
             * @tick_raw:         raw tick,get from TSC
             * @tick_gtod:        GTOD,gettimeofday relating facility,as base and the unstable clock deltas
             * @clock:            clock value
             */
            struct sched_clock_data {
                    u64 tick_raw;
                    u64 tick_gtod;
                    u64 clock;
            };

        Sched Class :
          schedule class defined a set of primitives to describe how to deal with linux scheduling.
          <linux/sched.h>
            struct sched_class;

          member @next is a const pointer points to an object is also type of struct sched_class,thus all sched_class
          are linked.

          the primitives :
            <specifier>@<type>@<name> => data type describing
            <specifier>@<type>@r      => return type
            task: struct task_struct
            i:    int
            ui:   unsigned int
            cpumask*: struct cpumask *
        
            /**
             * "rq" related parameters and "p" related parameters are pass-by-pointer,
             *  and types can be deduced from their names.
             */

            enqueue_task            @rq @p i@wakeup
            dequeue_task            @rq @p i@sleep
            yield_task              @rq
            check_preempt_curr      @rq @p i@flags
            task@r pick_next_task   @rq
            put_prev_task           @rq @p

          #ifdef CONFIG_SMP
            i@r select_task_rq      @p i@sd_flag i@flags
            pre_schedule            @this_rq @task
            post_schedule           @this_rq
            task_waking             @this_rq @task
            task_woken              @this_rq @task
            set_cpus_allowed        @p const@cpumask*@newmask
            rq_online               @rq
            rq_offline              @rq
          #endif /*CONFIG_SMP */

            set_curr_task           @rq
            task_tick               @rq @p i@queued
            task_fork               @p
            switched_from           @this_rq @task
            switched_to             @this_rq @task i@running
            prio_changed            @this_rq @task i@oldprio i@running
            ui@r get_rr_interval    @rq @task
           
          #ifdef CONFIG_FAIR_GROUP_SCHED
            moved_group             @p i@on_rq
          #endif /* CONFIG_FAIR_GROUP_SCHED */  

          These primitives are setup by scheduler initializing routine.Because Linux 2.6.34.1 introuduced CFS -
          - Completely Fair Scheduler,thus the sched_class in kernel is set to cfs class in normally.

      Functions Used by the Scheduler :
        The scheduler_tick() function :
          <arch/x86/kernel/tsc.c>
            /**
             * native_sched_clock - architecture depended sched_clock(),used to returns current time in nanosec units
             * return:              time data readed from TSC and converted to nanoseconds
             */
            u64 native_sched_clock(void);

          #ifndef CONFIG_PARAVIRT
            unsigned long long
            sched_clock(void) __attribute__((alias("native_sched_clock")));
          #endif

          <kernel/sched_clock.c>
            sched_clock - generic scheduler clock
              => (jiffies - INITIAL_JIFFIES) * (NSEC_PER_SEC / HZ)
            /* architecture can overrides this */

          <linux/sched.h>
            /**
             * scheduler_tick - called from timer code to updates scheduling related data
             */
            extern void scheduler_tick(void);

            what scheduler_tick() does :
              1>  retrieve cpu id,runqueue of local cpu,the task current running of runqueue.
              2>  call to sched_clock_tick() to update scheduler clock.
                  /**
                   * sched_clock_tick() call to sched_clock_local(),it calculate scheduler
                   * clock from TSC, GTOD, Explicit Idle Event .
                   */
              3>  acquire runqueue lock.
              4>  update runqueue clock.
                  /**
                   * call to sched_clock_cpu().in normally,sched_clock_local() will be called,
                   * except current CPU is not a local CPU,then called to sched_clock_remote().
                   */
              5>  update cpu load info of the @runqueue.
                  /**
                   * @rq->cpu_load[].
                   * @rq->calc_load_update is treated as a timer to indicate that 
                   * @this_rq should go to account the number of active processes.
                   * (@calc_load_update after(expired) or equal to(timeout) @jiffies)
                   */
              6>  call to @curr->sched_class->task_tick(@rq, @curr, 0)
                  to update run-time statistics of the @current.
                  if @cfs_rq->nr_running > 1 || !sched_feat(WAKEUP_PREEMPT)
                  then
                          check_preempt_tick(@cfs_rq, @curr)
                          /**
                           * should @current be preempted?
                           * new task with higher priority => TRUE
                           * starving => TRUE
                           */
                  task_tick() of cfs will call entity_tick() on all cfs_rq related
                  to @current->se,that is all @se in the se.@group_node would be
                  updated. (each @se has a member named @cfs_rq is type of struct cfs_rq)
                  /**
                   * @se.sum_exec_time increase and @se.vruntime decrease
                   * @delta get from scheduler clock substract to @se.exec_start
                   * @se.exec_start updated to @clock
                   */
              7>  release runqueue lock.
              8>  call to perf_event_task_tick(@curr) sign a performance event associated
                  with task tick.
              #ifdef CONFIG_SMP
              9>  call to idle_cpu(cpu) to check whether local cpu is idle.
                  @rq->idle_at_tick = 1 => local cpu is idle
                                      0 => local cpu is busy
              10> trigger load balance on "this" runqueue with local cpu.
                  arise SCHED_SOFTIRQ if it is time to do periodic load balancing.
              #endif /* CONFIG_SMP */

            local cpu maybe idle even its runqueue is not empty(CPU hyper-threading technology)

            !!  update_process_times() call to scheduler_tick(),and task_tick_rt() is called
                if @current is a real-time process,otherwise,task_tick_fair() is called in normally.
                and what task_tick_rt() does has introduced in the description what update_process_times()
                does at early Chapter "Timing Measurements".

        The try_to_wake_up() function :
          awake a sleeping or stopped process by setting its state to TASK_RUNNING and inserting it into
          the runqueue of the local CPU.

          <kernel/sched.c>
            /**
             * try_to_wake_up - try to wake up threads are satisfied condition
             * @p:              task to wake up
             * @state:          process state mask
             * @wake_flags:     wake up flags,used to determine whether should do a
             *                  synchronous wakeup
             * return:          1 => woken up
             *                  0 => task already active
             */
            static int try_to_wake_up(struct task_struct *p, unsigned int state,
                                      int wake_flags);

            !!  @current is always on the run-queue unless the actual re-schedule is
                in progress,and in such case,just change process state to
                TASK_RUNNING as well.

            !!  TASK_WAKING state is set by scheduler used to make the process is waking
                here,this guarantees that nobody will actually run it,and a signal or
                other external event cannot wake it up and insert it on the runqueue either.

            what try_to_wake_up() does :
              1>  check scheduler feature whether SYNC_WAKEUPS is disabled,if it is,then
                  clear WF_SYNC in @wake_flags.
              2>  retrieve local cpu via get_cpu().
                  retrieve the runqueue which @p is residing,and lock this_rq,disable
                  interrupt.
                  update runqueue clock.
              3>  check whether @p->state & @state == 1 ?
                  if @p does not satisfy the condition,then do not wake up it.
              4>  check if @p is enqueued into a runqueue,if it is,then goto
                  "out_running"(process is already active,@se on a runqueue).
                  /**
                   * task enqueued into a runqueue,then @task->se.on_rq set to 1.
                   * @task->rq != NULL means this task is residing on @rq,but
                   * it is not guarantee to the @task had been enqueued into @rq.
                   */
              5>  save local CPU id,because on SMP,task maybe migrated to another CPU
                  by load balancing.
              #ifdef CONFIG_SMP
              6>  if task is already active,then goto "out_activate"
                  else
                          fix up the @nr_uninterruptible count of this @rq,
                          if @p is contributes to load,then decrease @rq->nr_uninterruptible
                          /* @p is in TASK_UNINTERRUPTIBLE and do not with flag PF_FREEZING */

                          set @p tp TASK_WAKING
                          call to @p->sched_class->task_waking() if it is not NULL
                          release @rq->lock but local interrupt stay disabled

                          get CPU id of @p through select_task_rq()
                          /**
                           * called with SD_BALANCE_WAKE,schedule balancing take
                           * place be there.
                           * which @rq to select follows these  rules :
                           *   > the CPU is idle - prefer to previously executing CPU
                           *     and to the local CPU.
                           *   > if select previously CPU cause lower workload,then
                           *     select the previous CPU.
                           *   > if the process has been executed recently,then select
                           *     the old CPU.
                           *   > if migrate the process to the local CPU will reduce the
                           *     unbalance between the CPUs,then select local CPU.
                           */
                                  if current CPU id is not equal to older,
                                  must task migrating occurred,set task's cpu to
                                  current CPU.

                          retrieve runqueue of this CPU and lock it,then update
                          its clock.
                          updates schedule statistic @rq->ttwu_count,and if it is
                          necessary,update @rq->ttwu_local @rq->ttwu_wake_remote,
                          too.
              #endif /* CONFIG_SMP */
            "out_activate":
              7>  increase schedule statistics @p->se.nr_wakeups.
                  if WF_SYNC then increase @p->se.nr_wakeups_sync.
                  if task migrating occurred then increase @p->se.nr_wakeups_migrate.
                  if no task migrating occurred then increase @p->se.nr_wakeups_local.
                  else,increase @p->se.nr_wakeups_remote.
              8>  activate task @p on @rq via call to function activate_task().
                  it enqueue @p into @rq and increase @nr_running of @rq.
              9>  check whether current in interrupt context.
                  if it is not,then update average of @current->se.avg_wakeup,
                  and set @se.last_wakeup to @se.sum_exec_runtime.
                  /**
                   * at there,no task_rq_unlock() was called,that is local interrupt
                   * stay disabled.
                   */
            "out_running": /* in this case,return 0 */
              10> check preempt of @p,if any process can preempt @p,then replace
                  it.
                  /**
                   * allow @rq->curr is preempted by @p require to close WAKE_SYNC bit 
                   * in @wake_flags the parameter of try_to_wake_up().
                   * preemption is not occurs immediately,it just mark @rq->curr is
                   * TIF_NEED_RESCHED.
                   * on SMP,function resched_task() will checks whether task is polling,
                   * (the target CPU is not actively polling the status of TIF_NEED_RESCHED
                   *  flag of process)
                   * if it is not,must send an IPI via smp_send_reschedule() to force
                   * rescheduling on the target CPU(@task's CPU).
                   */
              11> set @p to TASK_RUNNING.(@p is runnable,now)
              #ifdef CONFIG_SMP
              12> call to @p->sched_class->task_woken() if it is not NULL.
                  if @rq->idle_stamp != 0,then update @rq->avg_idle.
              #endif /* CONFIG_SMP */
              13> unlock @rq,enable interrupt then put cpu and return.

        The recalc_task_prio() function :
          !!  LINUX 2.6.34.1 NO SUCH FUNCTION.
              EACH struct rq HAS MEMBER NAMED "cfs_rq",THAT IS THE CFS
              SCHEDULING,IF THE SCHEDULER IS CFS POLICY,THEN EVERY
              RUNQUEUE HAS A CFS ENTITY IT ATTACH TO THIS RUNQUEUE.

          because cfs,the priority is related to the weight.to avoid the subversion of
          "niceness" due to uneven distribution of tasks with abnormal "nice" values
          across CPUS,the contribution that each task makes to its runqueue's load
          is weighted according to its scheduling class and "nice" value.
          /* for SCHED_NORMAL,it just a scaled version of the time slice allaction */

          nice levels are multiplicative with a gentle 10% change for every nice 
          level changed.i.e. a CPU bound task goes from nice 0 to nice 1,it will
          get an estimated 10% less CPU time than another task on this CPU.

          the "10% effect" is relative and cumulative :
            from any nice level >
              go up 1 level => -10% CPU usage
              go down 1 level => +10% CPU usage
              /**
               * @p1 -10% and @p +10%,then the relative distance between them is 
               * an estimated 25%.
               * to achieve this,use a multiplier of 1.25 .
               */
          
          <kernel/sched.c>
            static const int prio_to_weight[40] = {
                    88761, 71755, 56483, 46273, 36291, /* [-20, -16] */
                    28154, 23254, 18705, 14949, 11916, /* [-15, -11] */
                     9548,  7620,  6100,  4904,  3906, /* [-10, -6] */
                     3121,  2501,  1991,  1586,  1277, /* [-5, -1] */
                     1024,   820,   655,   526,   423, /* [0, 4] */
                      335,   272,   215,   172,   137, /* [5, 9] */
                      110,    87,    70,    56,    45, /* [10, 14] */
                       36,    29,    23,    18,    15, /* [15, 19] */
            };
          
          struct task_struct.@static_prio is converted from nice value.
          struct task_struct.@normal_prio is based on @static_prio.
          
          <linux/sched.h>
            #define MAX_USER_RT_PRIO 100
            #define MAX_RT_PRIO MAX_USER_RT_PRIO     /* 100 */

            #define MAX_PRIO (MAX_RT_PRIO + 40)      /* 140 */
            #define DEFAULT_PRIO (MAX_RT_PRIO + 20)  /* 120 */

          <kernel/sched.c>
            #define NICE_TO_PRIO(nice) (MAX_RT_PRIO + (nice) + 20)
            #define PRIO_TO_NICE(prio) ((prio) - MAX_RT_PRIO - 20)
            #define TASK_NICE(p) PRIO_TO_NICE(p->static_prio)

            #define USER_PRIO(p) ((p) - MAX_RT_PRIO)
            #define TASK_USER_PRIO(p) USER_PRIO((p)->static_prio)
            #define MAX_USER_PRIO (USER_PRIO(MAX_PRIO))

            /**
             * __normal_prio - return the priority based on static_prio
             * @p:             the task
             * return:         @p->static_prio
             */
            static inline int __normal_prio(struct task_struct *p);

            /**
             * normal_prio - calculate the expected normal priority
             * @p:           the task
             * return:       MAX_RT_PRIO - 1 - @p->rt_prioriy => @p is real-time
             *               __normal_prio(@p) => @p is not real-time
             */
            static inline int normal_prio(struct task_struct *p);

            /**
             * effective_prio - calculate the current priority
             * @p:              the task
             * return:          the current priority
             * # this function set @p->normal_prio to __normal_prio(@p),
             *   then check if the task has RT policy or boosted to RT
             *   priority,if it is not,return @normal_prio,otherwise,
             *   return @prio.in other words,that is keep the priority
             *   if task is RT or boosted to RT prioriry.
             */
            static int effective_prio(struct task_struct *p);

        The schedule() function :
          the linux scheduler entry point.it must to find out a process in rq and assign the CPU
          to it.it is invoked by several kernel routines directly or in a lzay way(deferred).

          <linux/sched.h>
            /* __sched - indicates the following text is reside on section ".sched.text" */
            #define __sched __attribute__((__section__(".sched.text")))

            /**
             * schedule - the linux scheduler entry point
             * # asmlinkage __sched schedule(void) { ... } EXPORT_SYMBOL(schedule);  
            asmlinkage void schedule(void);

          direct invocation :
            called from @current,because the resource it needs is not available now,the process
            would be blocked until the resource is available.
            the following steps @current have to do :
              1>  enqueue itself into an appropriate wait queue.
              2>  set process state to TASK_UNINTERRUPTIBLE or TASK_INTERRUPTIBLE.
              3>  call to schedule().
              4>  check whether resource is available now. /* selected by scheduler */
                  if it is not,goto 2>                     /* continues right after 3> */
                                                           /* scheduler set process to */
                                                           /* TASK_RUNNING */
              5>  dequeue itself from the wait queue.

          lazy invocation :
            invoked by set TIF_NEED_RESCHED flag of @current.
            a check on the value of this flag is always made before resuming the execution of
            a User Mode process,schedule() will definitely be invoked at some time in the
            near future.

            lazy invocation instances :
              >  scheduler_tick(),@current has used up its quantum of CPU.
              >  a process is woken up by try_to_wake_up(),and it has higher priority than
                 @current,then TIF_NEED_RESCHED of @rq->curr is set by resched_task().
              >  when a sched_setscheduler() system call is issued.

          what to do before context switch :
            1>  disable preemption.
                retrieve @rq of this CPU.
            2>  save @rq->curr in a local variable.
                release the locks had holden by @prev.
            3>  call to schedule_debug(@prev) to do time-debugging checks and statistics.
                if kernel control path is not atomic with preempt off currently AND
                   @prev is not in exit state
                   /* do_exit() call to schedule() atomically */
                then
                        scheduling bug occurred
                        call to __schedule_bug(@prev) to print scheduling info
                else
                        increase this_rq()->@sched_count
                        #ifdef CONFIG_SCHEDSTATS
                        AND @prev->lock_depth >= 0
                        then
                                increase this_rq()->@bkl_count
                                increaase @prev->sched_info.bkl_count

            4>  if scheduler feature HRTICK is enabled
                then
                        call to hrtick_clear(@rq) to cancel @rq->hrtick_timer,if it is
                        active
            5>  raw spin lock irq to @rq->lock,update @rq's clock,disable TID_NEED_RESCHED of 
                @prev
            6>  check if @prev is not in TASK_RUNNING and preemption is disabled,
                if it is,then check if @prev in signal pending state.
                if @prev is signal pending,then set @prev to TASK_RUNNING,
                otherwise,deactivate @prev.
                set unsigned long pointer @switch_count to @prev->nivcsw.
            7>  call to pre_schedule(@rq, @prev).
                it is a wrapper of function pre_schedule() of @prev's sched class.
            8>  if @rq no running process is exist,then call to idle_balance().
                idle_balance() is defined in <kernel/sched_fair.c>,which is used to
                pull tasks from other CPUs if the CPU(its parameter) is going to idle.
                /**
                 * pull through move_tasks(),the parameter @idle of it is set to
                 * CPU_NEWLY_IDLE.
                 * BUT idle_balance() IS A STATIC FUNCTION WITHOUT EXPORT SYMBOL
                 */
            9>  put @prev task.
                if @prev is TASK_RUNNING,then have to updates the time data related to scheduling
                of it.
                call to sched call put_prev_task() before return.
            10> pick next task through pick_next_task(@rq).
                it select the next task from robin-tree of @rq.
            11> if @prev != @next
                then
                        process sched info switch between @prev and @next
                        sign a performance event about to task sched out
                        increase @rq->nr_switches
                        set @rq->curr to @next,it as the current process of @rq
                        increase *@switch_count /* @prev->nivcsw */

            /**
             * above are the works did by schedule() before context_switch().
             * context_switch() finally would call to architecture depend function switch_to().
             * what the context_switch() does has described in Chapter 3 Processes :: Process Switch.
             */

          what to do after context switch :
            12> after context_switch(),update @cpu and @rq of @cpu.
            13> if @prev == @next,that means no other task in @rq should be selected to run.
                call to raw_spin_unlock_irq(@rq->lock) to release lock.
            14> call to post_schedule(@rq).
                call this function without @rq->lock hold and preempt disabled.
                it checks if @rq->post_schedule is TRUE?
                if it is
                then
                        acquire @rq->lock with interrupt disabled
                        if the function post_schedule() of sched class of @rq->curr is not NULL,
                        then call to it
                        release @rq->lock and enable interrupt
                        set @rq->post_schedule to 0
                /* post_schedule() is correspond to pre_schedule() */
            15> if reacquire_kernel_lock(@current) < 0 => if failed to reacquire kernel locks
                then
                        set @prev := @rq->curr
                        set @switch_count := &@prev->nivcsw
                        goto need_resched_nonpreemptible(3>)
            16> enable preempt without resched.
            17> if need_resched() is TRUE? => @current's thread flag
                => (struct thread_info *)(asm("esp") & ~(THREAD_SIZE - 1)) :: TIF_NEED_RESCHED is on
                   /* process's thread_info object is saved in kernel stack of the process */
                if it is
                then
                        goto need_resched(1>)

      Runqueue Balancing in Multiprocessor Systems :
        three types of multiprocessor machines :
          1>  classic multiprocessor architecture
                all CPUs share a common set of RAM chips.
          2>  hyper-threading
                invented by Intel.
                a hyper-threaded chip is a microprocessor that executes
                several threads of execution at once.
                includes several copies of internal registers,and quickly switches between them.
                deal with another thread when current thread is stalled for memory access.
                Linux considers a hyper-threaded physical CPU as several different logical CPUs.
          3>  NUMA
                CPUs and RAM chips are grouped in local "nodes"(usually,one CPU with several RAM chips).
                CPU access to the RAM chips inside the local node is little or no contention,and fast;
                CPU access to the RAM chips outside to the local node(remote node) is slower.
                /* Memory arbiter is the bottleneck for Classic Multiprocessor Architecture */

        For SMP,a CPU can only run the runnable process inside the runqueue of its owns,that is,a runnable
        process is always stored in exactly one runqueue - no runnable process ever appears in two or more
        runqueues(task bind to CPU,if it remains runnable).
        /* task bind to CPU may induce a severe performance penaly - CPU overloaded */

        runqueue balancing : if necessay,move the processes in a overloaded runqueue to another of other CPU.
                             load balancing algorithm should take into consideration the topology of the CPUs
                             in the system.
                             since Linux 2.6.7,algorithm based on the notion of "scheduling domains".
                             /**
                              * easily tune for all kinds of existing multiprocess architecture,include
                              * multi-core architecture.
                              */

        Scheduling Domains :
          essentially,a scheduling domain is a set of CPUs whose workloads should be kept balanced by the
          kernel.
          scheduling domains are hierarchically organized :
                  the top scheduling domain {
                          span all CPUs in the system

                          child scheduling domain {
                                  span CPU0, CPU1, CPU2

                                  child scheduling domain {
                                          span CPU0
                                  }

                                  child scheduling domain {
                                          span CPU1, CPU2
                                  }

                          }

                          child scheduling domain {
                                  span CPU3
                          }

                          child scheduling domain {
                                  span CPU4, CPU5, CPU6, CPU7

                                  ...
                          }
                          
                          ...
                  }
          !!  base domain => the scheduling domain at the bottom of a scheduling domain
                             it usually span one CPU
                             
          every scheduling domain is partitioned in one or more groups,each of which represents a subset
          of the CPUs of the scheduling domain.workload balancing is always done between groups of a 
          scheduling domain.
          /**
           *
           * @phys_domains @sched_group_phys     group1                group2
           * tsd : { CPU0, CPU1, CPU2, CPU3 } => csd1 : { CPU0, CPU 1} csd2 : { CPU2, CPU3 }
           * @p->CPU0
           * balance @p : move @p to CPU2
           * => if and only if group1.workload > group2.workload
           */          

          <linux/sched.h>
            /* scheduling domain flags */
            #ifdef CONFIG_SMP

            #define SD_LOAD_BALANCE         0X0001  /* do load balancing on this domain */
            #define SD_BALANCE_NEWIDLE      0x0002  /* balance when about to become idle */
            #define SD_BALANCE_EXEC         0x0004  /* balance on exec */
            #define SD_BALANCE_FORK         0x0008  /* balance on fork,clone */
            #define SD_BALANCE_WAKE         0x0010  /* balance on wakeup */
            #define SD_WAKE_AFFINE          0x0020  /* wake task to waking CPU */
            #define SD_PREFER_LOCAL         0x0040  /* prefer to keep tasks local to this domain */
            #define SD_SHARE_CPUPOWER       0x0080  /* domain members share CPU power */
            #define SD_POWERSAVINGS_BALANCE 0x0100  /* balance for power savings */
            #define SD_SHARE_PKG_RESOURCES  0x0200  /* domain members share cpu pkg resources */
            #define SD_SERIALIZE            0x0400  /* only a single load balancing instance */

            #endif

            /* scheduling domain levels */
            enum sched_domain_level {
                    SD_LV_NONE = 0,
                    SD_LV_SIBLING,
                    SD_LV_MC,
                    SD_LV_CPU,
                    SD_LV_NODE,
                    SD_LV_ALLNODES,
                    SD_LV_MAX
            };

            /**
             * sched_group - structure sched_group used to represent a group of
             *               sched_domain instances
             * @next:        next scheduling domain group
             * @cpu_power:   CPU power of this group,SCHED_LOAD_SCALE being max power
             *               for a single CPU
             * @cpumask:     the CPUs this group covers
             *               NOTE : this filed is variable length alike to
             *                      struct sched_domain.@span
             */
            struct sched_group {
                    struct sched_group *next;
                    unsigned int cpu_power;
                    unsigned long cpumask[0];
            };

            /**
             * sched_domain - structure sched_domain represent the scheduling domain used by Linux
             *                load balancing algorithm
             * @parent:       parent of this domain
             * @child:        child of this domain
             * @groups:       groups this domain belong to
             * @min_interval: minimum balance interval in ms
             * @max_interval: maximum balance interval in ms
             * @busy_factor:  less balancing by factor if busy
             * @imbalance_pct: no balance until over watermark
             * @cache_nice_tries: leave cache hot tasks for # tries
             * @busy_idx:     busy index
             * @idle_idx:     idle index
             * @newidle_idx:  new idle index
             * @wake_idx:     wake index
             * @forkexec_idx: fork execution iindex
             * @smt_gain:     SMT Hyper-Threading support - Simulate MultiThreading
             * @flags:        scheduling domain flag
             * @level:        scheduling domain level
             * @last_balance: jiffies for the last balance
             * @balance_interval: balance interval in ms
             * @nr_balance_failed: number of balance failed
             * @last_update:  jiffies for the last scheduling domain info updating
             * @span:         span of all CPUs in this domain
             *                NOTE : this filed is variable length.
             *                       allocated dynamically by attaching extra space to
             *                       the end of the structure,depending on how many CPUs
             *                       the kernel has booted up with.
             *                       # it is also be embedded into static data structures
             *                         at build time.
             */
            struct sched_domain {
                    /* these fields must be setup */
                    struct sched_domain *parent;
                    struct sched_domain *child;
                    struct sched_group *groups;
                    unsigned long min_interval;
                    unsigned long max_interval;
                    unsigned int busy_factor;
                    unsigned int imbalance_pct;
                    unsigned int cache_nice_tries;
                    unsigned int busy_idx;
                    unsigned int idle_idx;
                    unsigned int newidle_idx;
                    unsigned int wake_idx;
                    unsigned int forkexec_idx;
                    unsigned int smt_gain;
                    int flags;
                    enum sched_domain_level level;

                    /* runtime fields */
                    unsigned long last_balance;
                    unsigned int balance_interval;
                    unsigned int nr_balance_failed;

            #ifdef CONFIG_SCHEDSTATS
                    ...
            #endif
            #ifdef CONFIG_SCHED_DEBUG
                    ...
            #endif
                
                    unsigned long span[0];
            };

          <kernel/sched.h>
            /**
             * static_sched_group - structure static_sched_group used to
             *                      define per-CPU variable @sched_group_phys
             * @sg:                 the group @cpus belong to
             * @cpus:               cpu bitmap
             */
            struct static_sched_group {
                    struct sched_group sg;
                    DECLARE_BITMAP(cpus, CONFIG_NR_CPUS);
            };

            /**
             * static_sched_domain - structure static_sched_domain used to
             *                       define per-CPU variable @phys_domains
             * @sd:                  the scheduling domain @span belong to
             * @span:                cpu bitmap
             */
            struct static_sched_domain {
                    struct sched_domain sd;
                    DECLARE_BITMAP(span, CONFIG_NR_CPUS);
            }

            /* phys_domains - represent physical CPUs scheduling domain */
            static DEFINE_PER_CPU(struct static_sched_domain, phys_domains);

            /* sched_group_phys - represent the group of physical CPUs scheduling domain */
            static DEFINE_PER_CPU(struct static_sched_group, sched_group_phys);

          @phys_domains => the top scheduling domain  /* span all physical CPUs */
          @sched_group_phys => the group of the top scheduling domain

          The rebalance_tick() function :
            !!  LINUX 2.6.34.1 HAS NO SUCH FUNCTION WAS DEFINED.
                INSTEAD,WORKLOAD REBALANCING IS TRIGGERED IN FUNCTION scheduler_tick() IF
                NECESSARY WITH CONFIG_SMP DEFINED.
                FUNCTION trigger_load_balance(@rq, @cpu) IS USED TO TRIGGER REBALANCING.
                /* @cpu => smp_processor_id() @rq => cpu_rq(@cpu) */

            /* based on cfs */
            <kernel/sched_fair.c>
              /**
               * trigger_load_balance - trigger softirq SCHED_SOFTIRQ to mark load 
               *                        rebalance
               * @rq:                   this rq
               * @cpu:                  the cpu is processing scheduler_tick()
               *
               * # to check if this @cpu is need to be rebalanced.
               *   if @jiffies time_after_eq @rq->next_balance AND
               *      cpu_rq(@cpu)->sd != NULL
               *   then
               *           raise_softirq(SCHED_SOFTIRQ);
               * ! if defined CONFIG_NO_HZ,the checking will be more complicated.
               *   it is the place where to nominate a new idle load balancing owner,
               *   or decide to stop the periodic load balancing if the whole system
               *   is idle.
               */
              static inline void trigger_load_balance(struct rq *rq, int cpu);

              #ifdef CONFIG_NO_HZ
              /**
               * nohz - idle load balancing recorder when periodic ticking is stopped
               * @load_balancer:        load balancer for nohz mode
               * @cpu_mask:             cpumask for idle CPUs
               * @ilb_grp_nohz_mask:    mask of groups of scheduling domains the CPUs
               *                        inside them appropriate to be nominated as
               *                        @load_balancer
               */
              static struct {
                      atomic_t load_balancer;
                      cpumask_var_t cpu_mask;
                      cpumask_var_t ilb_grp_nohz_mask;
              } nohz ____cacheline_aligned = {
                      .load_balancer = ATOMIC_INIT(-1),
              };
              ...
              #endif

              what trigger_load_balance() to does with CONFIG_NO_HZ :
                1>  if @rq recently in nohz mode AND this cpu is not idle at this tick
                    then
                            set @rq->in_nohz_recently := 0
                            check if @nohz.load_balancer == cpu
                            /* this cpu is nominated to does load balancing for all CPUs */
                                    clear this cpu from @nohz.cpu_mask
                                    set @nohz.load_balancer := -1
                                    /* this cpu is no longer be the load balancer,busy now */
                            check if @nohz.load_balancer == -1
                                    find a new idle load balancing cpu
                                    if @ilb < @nr_cpu_ids /* unfound */
                                            call to resched_cpu(@ilb)
                                            /* set TIF_NEED_RESCHED for cpu_rq(@ilb)->curr */
                2>  if @rq is idle at this tick => this cpu is idle AND
                       @nohz.load_balancer == @cpu AND
                       cpumask_weight(@nohz.cpu_mask) == num_online_cpus()
                    then
                            resched_cpu(@cpu)
                            return
                            /**
                             * this cpu is idle and doing idle load balancing for all the
                             * cpus with ticks stopped.
                             */
                3>  if @rq is idle at this tick AND
                       @nohz.load_balancer != @cpu AND
                       cpumask_test_cpu(@cpu, @nohz.cpu_mask)
                    then
                            just return
                            /**
                             * this cpu is idle and the idle load balancing is done by someone,
                             * no need to raise the SCHED_SOFTIRQ.
                             */

          The run_rebalance_domains() function :
            function run_rebalance_domains() defined in <kernel/sched_fair.c> is the softirq handler
            for SCHED_SOFTIRQ which is registered by function sched_init() defined in <kernel/sched.c>.
            if necessay,trigger_load_balance() may raise such softirq from scheduler_tick().

            <kernel/sched_fair.c>
              /**
               * run_rebalance_domains - ISR for softirq SCHED_SOFTIRQ
               *                         it is used to deal with workload rebalancing
               * @h:                     softirq action
               */
              static void run_rebalance_domains(struct softirq_action *h);

              what run_rebalance_domains() does :
                1>  retrieve local CPU id.
                    use the CPU id to retrieve @runqueues.
                    declare a enum type cpu_idle_type variable named @idle,
                    if @this_rq->idle_at_tick => T
                    then @idle := CPU_IDLE       /* this cpu is idle */
                    else @idle := CPU_NOT_IDLE   /* this cpu is busy */
                                  /* the possible values for such enum type */

                2>  call to auxiliary routine rebalance_domains(@this_cpu, @idle).
                #ifdef CONFIG_NO_HZ
                3> if this cpu is the owner for idle load balancing,then do the
                   balancing on behalf of the other idle cpus whose ticks are stopped.
                   if @this_rq->idle_at_tick AND @nohz.load_balancer == this_cpu
                   then
                           for each cpu in the @nohz.cpu_mask
                                   check if balance cpu is this cpu
                                           T => continue to next cycle
                                   check if need_resched() is true,that is @rq->curr
                                   has flag TIF_NEED_RESCHED
                                           break
                                   call to rebalance_domains(the approriate cpu, CPU_IDLE)
                                   let the @cpu to does the scheduling domain rebalancing
                                   /**
                                    * let the @cpu does load balancing,because it is idle
                                    * now,so scheduler can migrate some tasks in the same
                                    * group of another CPU(runqueues) to this cpu.
                                    */

                                   update local variable @rq through cpu_rq(@balance_cpu)
                                   check if @this_rq->next_balance time after @rq->next_balance
                                   then
                                           update @this_rq->next_balance to @rq->next_balance
                #endif /* CONFIG_NO_HZ */

          The rebalance_domains() function :
            this function checks each scheduling domain to see if it is due to be balanced,
            and initiates a balancing operation if so.

            <kernel/sched_fair.c>
              /**
               * rebalance_domains - auxiliary routine of run_rebalance_domains(),
               *                     does the real rebalancing for scheduling domains
               * @cpu:               the cpu to does rebalancing
               * @idle:              cpu state => CPU_IDLE OR CPU_NOT_IDLE
               * # balancing paramters are set up in arch_init_sched_domains().
               */
              static void rebalance_domains(int cpu, enum cpu_idle_type idle);

              what rebalance_domains() does :
                1>  retrieve the runqueue associated with @cpu.
                    initializes local variable @next_balance to @jiffies + 60 * HZ
                    as the record.
                    initializes local variable @balance to 1.
                    initializes local variable @update_next_balance to 0.
                2>  traverse all domains through macro for_each_domain(the cpu, the scheduling domain).
                    this macro function is defined in <kernel/sched.c>,it traverse up to the top
                    scheduling domain.
                3>  in the cycle of for each domain : (the initial value of @sd is cpu_rq(@cpu)->sd)
                      if @sd->flags & SD_LOAD_BALANCE => F
                      then
                              continue to next cycle
                              /* this scheduling domain does not need load balancing */

                      set local variable @interval to @sd->balance_interval
                      check if @idle != CPU_IDLE is true
                      then
                              set @interval *= @sd->busy_factor
                              /* @cpu is not idle,then set @interval to an empirical value */
                      convert @interval(ms) to jiffies
                      check if @interval is zero,then set @it to 1
                      check if @interval > HZ * NR_CPUS / 10,if it is,set @interval to the result

                      set local variable @need_serialize to @sd->flags & SD_SERIALIZE
                      /* indicator used to tell whether need to lock the balancing spin lock */
                      try to lock spin lock @balancing if @need_serialize is TRUE,if failed to
                      lock,then goto out /* serializing required but failed to acquire lock */

                      check if @jiffies time after or equal to @sd->last_balance + @interval
                      => T    /* it is time to do rebalancing */
                              call to load_balance(@cpu, @rq, @sd, @idle, &@balance)
                              if the return value is TRUE,then update @idle CPU_NOT_IDLE
                              set @sd->last_balance to @jiffies
                              /**
                               * return value is TRUE,means the tasks had pulled over,
                               * so the @cpu is no longer idle,or one of SMT siblings is
                               * not idle.
                               */
                      if @need_serialize is TRUE,then unlock the spin lock
                  out:

                      check if @next_balance time after @sd->last_balance + @interval
                      => T
                              update @next_balance to @sd->last_balance + @interval
                              set @update_next_balance to 1

                      check if @balance is false
                      => T
                              break  /* stop the load balance at this level */
                              /**
                               * @balance is past by address to load_balance(),if @balance
                               * is false now,then there is another CPU in the sched group
                               * which is doing load balancing more actively.
                               */
                      next cycle :
                4>  check if @update_next_balance is T
                    => T
                            set @rq->next_balance to @next_balance
                            /* the next timepoint to does workload balancing for @rq */

          The load_balance() function :
            this function called by rebalance_domains() to check this_cpu to ensure it is 
            balanced within domain.if imbalance have detected,then attempt to move tasks.

            <kernel/sched_fair.c>
              /**
               * load_balance - check if this cpu is balanced,try to move tasks if imbalance
               *                detected
               * @this_cpu:     this cpu to check
               * @this_rq:      runqueue of this cpu
               * @sd:           the domain at current level
               * @idle:         idle state of this cpu
               * @balance:      indicator used to tell caller if there is anoter CPU active
               * return:        the number of tasks they have moved
               */
              static int load_balance(int this_cpu, struct rq *this_rq,
                                      struct sched_domain *sd, enum cpu_idle_type idle,
                                      int *balance);

              what load_balance() does :
                1>  initializes local variables :
                      @all_pinned := 0 => whether the tasks is pinned on a CPU
                      @active_balance := 0 => indicate balancing is active
                      @sd_idle := 0 => indicates the scheduling domain at this level is idle
                      @ld_moved := unspecified => tell how many tasks load_balance() moved
                2>  get per-CPU variable @load_balance_tmpmask is type of cpumask_var_t to
                    @cpus.
                    copy @cpus to @cpu_active_mask(activated cpus in this system).
                                  /* defined in <kernel/cpu.c> with EXPORT_SYMBOL */
                3>  check if this cpu is idle AND
                             @sd->flags set SD_SHARE_CPUPOWER AND
                             @sd->parent unset SD_POWERSAVINGS_BALANCE
                    then
                            @sd_idle := 1 => this scheduling idle is idle now
                                             even there maybe slibing is busy
                            /**
                             * parent sd enabled power savings policy.
                             * idle sibling can pick up load irrespective of busy siblings.
                             * idle sibling percolate up state as CPU_IDLE.
                             */
                4>  increase @sd->lb_count[@idle].
                                  /* load balance statistics */
           redo:
                5>  call to update_shares(@sd),update @sd->last_update and call to walk_tg_tree() with
                    @tg_nop(@down) @tg_shares_up(@up) @sd(@tree),if necessary.
                    /**
                     * walk_tg_tree() => iterate full tree
                     *                   call to @down when first entering a node
                     *                   call to @up when leave it for the final time
                     * tg_shares_up(task group, the data)
                     * =>  re-compute the task group their per cpu shares over the given domain
                     */
                6>  find the busiest group through find_busiest_group(),@balance will pass to it.
                    check if *@balance => false
                    then
                            goto out_balanced /* no busiest group */
                    check if @group => true
                    then
                            increase @sd->ld_nobusyg[@idle]
                            goto out_balanced
                7>  find busiest runqueue through find_busiest_queue().
                    check if @busiest => false
                    then
                            increase @sd->lb_nobusyq[@idle]
                            goto out_balanced
                8>  add @imbalance(set by find_busiest_queue()) to @sd->lb_imbalance[@idle]
                    set @ld_moved to zero ready for moving
                9>  check if @busiest->nr_running > 1
                    then
                            do tasks moving :  /* another task in @busiest */
                                    disable local interrupt
                                    lock @this_rq and @busiest
                                    call to move_tasks(@this_rq, @this_cpu, @busiest,
                                                       @imbalance, @sd, @idle, &@all_pinned)
                                    set @ld_moved to the return value
                                    release locks
                                    enable local interrupt
                            check if @ld_moved => true AND
                                     @this_cpu != smp_processor_id()
                            then
                                    set TIF_NEED_RESCHED for cpu_rq(@this_cpu)->curr
                                    /**
                                     * some other cpu did the load balance.
                                     * task moved,so it is possible to encounter the case
                                     * @this_cpu != smp_processor_id()
                                     * @this_cpu is selected to do load balance,
                                     * but another one have done it.
                                     */
                            check if @all_pinned => true
                            then
                                    /* all tasks on this runqueue were pinned by CPU affinity */
                                    call to cpumask_clear_cpu() to remove the cpu of
                                    @busiest from @cpus
                                    check if @cpus is NULL
                                    then
                                            goto redo
                                            /**
                                             * after removed the cpu of @busiest,we
                                             * encountered no cpu is working for
                                             * loadl balance.
                                             * we need re-do it.
                                             */
                                    else
                                            goto out_balanced
                10> check if @ld_moved => zero
                    then
                            update load balance statistics
                            /**
                             * failed to done load balance,no tasks moved even there
                             * is a busiest runqueue.
                             */
                            if need to active balance for @sd,then acquire @busiest->lock with
                            local interrupt disabled,and check if the @curr task on @busiest
                            cpu cannot be moved to @this_cpu,then release lock and enable
                            local interrupt,set @all_pinned to 1(can not be moved),finally
                            goto out_one_pinned.

                            if @busiest do not active balance,then set @busiest->active_balance
                            to 1(some task of @busiest need to be moved),and push @this_cpu to
                            @busiest->push_cpu,finally,set @active_balance to 1.

                            release @busiest->lock and enable local interrupt.
                            check if @active_balance => true
                            then
                                    wake up @busiest->migration_thread
                                    /**
                                     * the @migration_thread walks the chain of the scheduling domain,
                                     * from the base domain of the @busiest to the top domain,looking
                                     * for an idle CPU.if found,invoke move_tasks() to move one
                                     * process into the idle runqueue.
                                     */
                            set @sd->nr_balance_failed := @sd->cache_nice_tries + 1
                    else
                            set @sd->nr_balance_failed to 0
                            /* @ld_moved > 0 => task moved */
                11> check if @active_balance => false
                    then
                            set @sd->balance_interval to @sd->min_interval
                            /**
                             * time interval for @sd to balance
                             */
                    else
                            check if @sd->balance_interval < @sd->max_interval
                            then
                                    set @sd->balance_interval *= 2
                12> check if @ld_moved => zero AND
                             @sd_idle => zero AND
                             @sd->flags set @sd_SHARE_CPUPOWER AND
                             @sd'parent does not set SD_POWERSAVINGS_BALANCE
                    then
                             /**
                              * no task moved,and this sd is not idle
                              * domain members share power and parent does not
                              * enable power savings
                              */
                             set @ld_moved to -1
                13> goto out
    out_balanced:
                14> increase @sd->ld_balanced[@idle]
                    set @sd->nr_balance_failed to zero
                    /* load is balanced */
  out_one_pinned:
                15> tune up the balancing interval
                    check if @sd_idle => false AND
                             @sd->flags set SD_SHARE_CPUPOWER AND
                             @sd'parent does not set SD_POWERSAVINGS_BALANCE
                    then
                            set @ld_moved to -1   /* members share power */
                    else
                            set @ld_moved to zero /* move tasks failed */
             out:
                16> check if @ld_moved => true /* negative number also is T */
                    then
                            call to update_shares(@sd) to update @last_update,
                            if necessary
                    return @ld_moved
                           /**
                            * -1 => cpu power sharing
                            * 0  => failed to move tasks
                            * positive => moved tasks
                            */

          The move_tasks() function :
            the function move tasks from a busiest runqueue to another lower-load runqueue
            within scheduling domains(usually,the current load balancing CPU).

            <kernel/sched_fair.c>
              /**
               * move_tasks - migrate tasks
               * @this_rq:    this rq of this cpu
               * @this_cpu:   this cpu is the cpu nomimated to does load balancing
               * @busiest:    the busiest runqueue in the same scheduling domain
               * @max_load_move:  the maximum number of tasks allowed to be migrated
               * @sd:         the scheduling domain of current level
               * @idle:       state of this cpu
               * @all_pinned: indicator used to tell whether all tasks pinned by CPU
               *              affinity
               * return:      if total load moved is greater than zero
               * # this function call to the schedule class associated load balance
               *   routine,for cfs,it is load_balance_fair().
               */
              static int move_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,
                                    unsigned long max_load_move, struct sched_domain *sd,
                                    enum cpu_idle_type idle, int *all_pinned);

              what move_tasks() does :
                1>  get the best priority of this rq.
                    that is,the priority of @this_rq->curr.
                2>  enter do-while cycle,endup if @load_moved is false OR @max_load_move
                    is less than @total_load_moved.
                3>  in the do-while cycle :
                      call to
                        load_balance_fair(@this_rq, @this_cpu, @busiest,
                                          @max_load_move - @total_load_moved,
                                          @sd, @idle, @all_pinned, &@this_best_prio)
                      update @total_load_moved to @total_load_moved add @load_moved

                #ifdef CONFIG_PREEMPT
                      check if @idle is CPU_NEWLY_IDLE AND 
                               @this_rq->nr_running > 0
                      then
                              break
                              /**
                               * if @idle is equal to CPU_NEWLY_IDLE,it represents
                               * this function is called from idle_balance(),which is
                               * called by schedule().
                               * the CPU entering idle(rq->nr_running == 0),then pull
                               * tasks.rq->nr_running is zero before,and now,it at least
                               * one task is running.
                               */
                      check if @this_rq->lock or @busiest->lock is contended
                      then
                              /* contended => lock->break_lock <-> T */
                              break
                              /**
                               * another process is waiting for the task.
                               * we have finished once load_balance_fair(),thus
                               * can free the locks.
                               */
                #endif 
                4>  return @total_load_moved > 0

          The load_balance_fair() function :
            suppose CONFIG_FAIR_GROUP_SCHED defined.

            <kernel/sched_fair.c>
              /**
               * load_balance_fair - load balance for cfs
               * @this_rq:           the load balance runqueue
               * @this_cpu:          the load balance cpu
               * @busiest:           the busiest runqueue of this scheduling domain
               * @max_load_move:     the maximum number of tasks this function should move
               * @sd:                the current level scheduling domain
               * @idle:              cpu state
               * @all_pinned:        all pinned indicator
               * @this_best_prio:    the best priority of @this_rq
               * return:             number of moved tasks
               */
              static
              unsigned long load_balance_fair(struct rq *this_rq, int this_cpu, struct rq *busiest,
                                              unsigned long max_load_move, struct sched_domain *sd,
                                              enum cpu_idle_type idle, int *all_pinned,
                                              int *this_best_prio);

              what load_balance_fair() does :
                1>  retrieve the busiest CPU,acquire RCU read lock,update h load for this
                    busiest CPU.
                2>  traverse all entry in the @task_groups,each is a task group.
                      retrieve this busiest CPU's cfs_rq
                      update h load and load weight for this busiest cfs_rq
                      
                      check if the @task_weight of the busiest cfs_rq is zero
                      T =>
                              continue to next entry

                      updae remain load
                      /**
                       * @rem_load = @rem_load_move * @busiest_weight
                       *             # initialized to @max_load_move before enter cycle
                       * @rem_load = div_u64(@rem_load, @busiest_h_load + 1)
                       *                                # prevent zero
                       */

                      call to balance_tasks() to pull tasks
                      /**
                       * balance_tasks() defined in the same file.
                       * it traverse all tasks in the busiest cfs_rq.
                       *
                       * function can_migrate_task() is used to check if the tasks
                       * in the @busiest runqueue are able to be migrated to another.
                       * # do not migrate :
                       *     1>  task is running
                       *     2>  disallowed CPU @this_cpu
                       *     3>  hardware cache hot on remote CPU of the task
                       * # aggressive to migrate :
                       *     1>  cold cache
                       *     2>  too many balance attempt have failed
                       *
                       * function pull_task() is used to set the @cpu of a task,which
                       * is defined in the same file.
                       * # of course,the runqueue of the task it resides to be changed,too.
                       *   and if necessary,set TIF_NEED_RESCHED for the task.
                       *
                       * in the cycle,it may modifies @this_best_prio if there is a new
                       * process pulled from busiest rq to this rq which has higher prio
                       * than @this_rq->curr.
                       *
                       * #ifdef CONFIG_PREEMPT
                       * it will checks @idle if equal to CPU_NEWLY_IDLE,if it is,then
                       * break cycle and return.(for schedule(),just leave current CPU
                       * is not idle as well,dont care about other sibling domains)
                       * #endif 
                       *
                       * it will set @allpinned to 1,if the tasks pinned on the target
                       * CPU which we wish to pull tasks from it.
                       */
                      check if @moved_load is zero
                      T =>
                              continue to next entry

                      convert @moved_load to the number of tasks pulled
                      @rem_load_move -= @moved_load
                      break cycle if @rem_load_move less than zero
                3>  release RCU read lock
                4>  return @max_load_move - @rem_load_move

    CFS Completely Fair Scheduler :
      The new "desktop" process scheduler implemented by Ingo Molnar and merged in Linux 2.6.23.
      CFS basically models an "ideal, precise, multi-tasking CPU" on real hardware.

      ideal - precise - multi-tasking CPU :
        suppose a CPU has 100% power,and each task running on it at precise equal speed,in parallel,
        if there is nr_running tasks are present,then each at the 1/nr_running speed;if there just
        2 tasks running,then each at 50% physical CPU power.
        but there just one task can running at the same in the actual hardware.thus,introduced a
        new property "virtual runtime"(schedule entity @vruntime).
        
      vruntime : virtual runtime
        specifies when the task's timeslice would start execution on the ipmtCPU.
        in practice,it is the task's actual runtime normalized to the total number of
        running tasks.
        /**
         * "when" is not "how long the time it would running on the CPU".
         */

        on ipmtCPU,all tasks have the same @vruntime,ipmtCPU is able to runs all tasks at the same
        time with equal speed,whatever how many the tasks present.

      pick up policy :
        always pick up the task has smallest @vruntime as the next runnable task.CFS always split up
        CPU times as close to ipmtCPU times as possible.
        CFS uses an rbtree to saves all tasks in this runqueue,thus,the left-most leaf contains the
        task has smallest @vruntime,the total number of running tasks in the runqueue is accounted
        through the @rq->cfs.load value(sum of weights of the tasks in this runqueue).
        /**
         * therefore,we can have known that CFS use the system load to does task scheduling,guarantees
         * system load of the task's as equal to each other as possible.
         */

        CFS picked up taskA to run ->
        ... scheduling occurred(schedule tick) or task scheduled CPU usage ->
        taskA's vruntime += the time just spent using the physical CPU
                            /* max(the smallest CFS timeslice, actually spent) */
        ->
        vruntime of taskA get higher,then it is no longer the left-most leaf,if necessary.
                                                /* taskA preempted by another */

      the minimum CFS timeslice granularity :
        it is 1ms => 10 * 10^5 ns
        if this time is too long,then brings lower interactivity;if it is too short,then a lot of
        CPU times are spent on task switch.

      CFS scheduling policies :
        SCHED_NORMAL    -  regular tasks
        SCHED_BATCH     -  batch jobs(can run longer and make better use of caches at cost of interactivity)
        SCHED_IDLE      -  tasks even weaker than +19,but not a true idle timer scheduler

      CFS scheduling class :
        CFS scheduling class is implemented in <kernel/sched_fair.c>,the APIs in such file are included
        by a static sched_class object is named @fair_sched_class defined in the same file.

        all APIs manipulate the CFS structure is type of "struct cfs_rq" which is defined in
        <kernel/sched.c>,and the structure "struct rq" contains a member named @cfs.

      The structure cfs_rq :
        <kernel/sched.c>
          /**
           * cfs_rq - structure used to represent CFS scheduling
           * @load:              load weight,contains two member @weight @inv_weight
           *                     both are unsigned long
           * @nr_running:        number of processes in running
           * @exec_clock:        scheduler clock
           * @min_vruntime:      minmum value of virtual runtime
           * @tasks_timeline:    rbtree of tasks
           * @rb_leftmost:       left-most leaf for O(1) access
           * @tasks:             tasks list(linked task_struct)
           * @balance_iterator:  balance iterator,used to traverses scheduler queue
           *                     !! UNUSED,AND REMOVE IN RECENT KERNEL
           * @curr:              currently running entity on this cfs_rq
           * @next:              next entity on this cfs_rq
           * @last:              the last running entity on this cfs_rq
           * @nr_spread_over:    scheduling statistic
           * @rq:                the runqueue associated to this cfs_rq
           * @leaf_cfs_rq_list:  ties together list of leaf cfs_rq's in a cpu
           * @tg:                group that owns this runqueue
           * @task_weight        the part of @load.weight contributed by tasks
           * @h_load:            h_load = weight * f(tg)
           *                     f(tg) is the recursive weight fraction assigned
           *                     to this group
           * @shares:            this cpu's part of tg->shares
           * @rq_weight:         @load.weight at the time set shares                    
           */
          struct cfs_rq {
                  struct load_weight load;
                  unsigned long nr_running;
                  u64 exec_clock;
                  u64 min_vruntime;
                  struct rb_root tasks_timeline;
                  struct rb_node *rb_leftmost;
                  struct list_head tasks;
                  struct list_head *balance_iterator;
                  struct sched_entity *curr, *next, *last;
                  unsigned int nr_spread_over;
          #ifdef CONFIG_FAIR_GROUP_SCHED
                  struct rq *rq;
                  struct list_head leaf_cfs_rq_list;
                  struct task_group *tg;
          #ifdef CONFIG_SMP
                  unsigned long task_weight;
                  unsigned long h_load;
                  unsigned long shares;
                  unsigned long rq_weight;
          #endif
          #endif
          };

      About group scheduling :
        CFS operates on individual tasks and strives to provide fair CPU tiem to each task.
        sometimes,it may be desirable to group tasks and provide fair CPU time to each such
        task group.
        /* CONFIG_FAIR_GROUP_SCHED */        

        two mutually exclusive mechanisms to group tasks for CPU bandwidth control
        proposes:
          1>  based on user id(CONFIG_USER_SCHED)
          2>  based on "cgroup" pseudo filesystem(CONFIG_CGROUP_SCHED)
          
    System Calls Related to Scheduling :
      Non root user can only lower the priorities of processes it owns to.

      The nice() System Call :
        <kernel/sched.c>
          /**
           * sys_nice - system call nice() used to lower priority of a process of a non-root
           *            user
           *            only root user can set new nice value for all processes in the system
           * @increment: increment value
           *             used to determine what nice it will be
           * return:     0 => succced
           *             -EPERM => permisson deny OR capability unsatisfied
           * # @increment := -40 if @increment < -40
           *   @increment := +40 if @increment > +40
           */
          SYSCALL_DEFINE1(nice, int, increment);

        !!  function set_user_nice() used to set @current's nice,it convert nice to process
            priority(and CFS load weight.
            if @current is enqueued into a runqueue,then have to check process preemption,
            set TIF_NEED_RESCHED if necessary.
        !!  system call nice() only used for backward compatibility,it has been replaced by the
            setpriority() system call.

      The getpriority() and setpriority() System Calls :
        <kernel/sys.c>
          /**
           * getpriority - system call getpriority() used to retrieve the lowest nice(high-prio)
           *               field value among all processes in a given group
           * @which:       which class?
           *                 PRIO_PROCESS - select the processes according to their process ID
           *                 PRIO_PGRP    - select the processes according to their group ID
           *                 PRIO_USER    - select the processes according to their user ID
           * @who:         value of pid OR pgrp OR uid
           * return:       nice value of the target
           *               -EINVAL => invalid @which
           *               -ESRCH  => no such process existed
           */
          SYSCALL_DEFINE2(getpriority, int, which, int, who);

          /**
           * setpriority - system call setpriority() used to setup the nice value of process or
           *               processes
           * @which:       which class?(same as getpriority())
           * @who:         pid OR pgrp OR uid
           * @niceval:     new nice value,and would be converted to priority or load weight
           * return:       0 => succeed to set priority
           *               -EINVAL   =>  invalid @which
           *               -EPERM    =>  permisson deny
           *               -EACCESS  =>  exceed system resource limition OR capability unsatisfied
           * # of course,if necessary,set TIF_NEED_RESCHED.
           */
          SYSCALL_DEFINE3(setpriority, int, which, int, who, int, niceval);

      The sched_getaffinity() and sched_setaffinity() System Calls :
        <kernel/sched.c>
          /**
           * sched_getaffinity - get the cpu affinity of a process
           * @pid:               process ID
           * @len:               length in bytes of the buffer @user_mask_ptr
           * @user_mask_ptr:     the buffer will saves the cpumask about the process
           *                     @pid
           * return:             min(@len, cpumaks_size()) => succed to get affinity
           *                     -EINVAL => @len is invalid,it must be too small
           *                     -ENOMEM => failed to allocate memory for cpumask_var_t object
           *                     -EFAULT => failed to copy result to user space buffer
           *                     -ESRCH  => no such process
           */
          SYSCALL_DEFINE3(sched_getaffinity, pid_t, pid, unsigned int, len,
                          unsigned long __user *, user_mask_ptr);

          /**
           * sched_setaffinity - set the cpu affinity for a process
           * @pid:               process ID
           * @len:               length in bytes of the buffer @user_mask_ptr
           * @user_mask_ptr:     the buffer saves the given cpumask from user space
           * return:             0 => succeed to set cpu affinity for @pid
           *                     -ENOMEM => failed to allocate memory for cpumask_var_t object
           *                     -EFAULT => failed to copy cpumask from user space
           *                     -ESRCH  => no such process
           *                     -EPERM  => permission deny
           *                     -EINVAL => cpumask disabled all CPUs OR
           *                                (PF_THREAD_BOUND AND @pid != @current AND
           *                                 @pid is not allowed on the given CPUs)
           */
          SYSCALL_DEFINE3(sched_setaffinity, pid_t, pid, unsigned int, len,
                          unsigned long __user *, user_mask_ptr);

        !!  routines find_process_by_pid() and find_process_by_vpid() is used to search the
            target.
            /* @pid = 0 means @current process */ 
        !!  CPU switch(if necessary) dealt with by @migration_thread of the runqueue @pid is
            residing now.

    System Calls Related to Real-Time Processes :
      If a process want to becomes a real-time process,it must have the CAP_SYS_NICE capability to modifies
      the values of the task_struct.@rt_priority and task_struct.@police inside its task descriptor.

      The sched_getscheduler() and sched_setscheduler() system calls :
        <kernel/sched.c>
          /**
           * sys_sched_getscheduler - get thread's scheduling policy
           * @pid:                    the process
           * return:                  the policy value
           *                          -EINVAL => @pid < 0
           *                          -ESRCH  => no such process
           *                          -EPERM  => permission deny # security checking (security_task_getscheduler())
           */
          SYSCALL_DEFINE1(sched_getscheduler, pid_t, pid);

          /**
           * sys_sched_setscheduler - set/change the scheduler policy and RT priority
           * @pid:                    process ID
           * @policy:                 scheduling policy
           *                            SCHED_FIFO
           *                            SCHED_RR
           *                            SCHED_NORMAL
           *                            SCHED_BATCH
           *                            SCHED_IDLE
           * @param:                  scheduler parameter
           *                          <linux/sched.h>
           *                            struct sched_param {
           *                                    int sched_priority;
           *                            };
           * return:                  0 => succeed to set/change scheduler
           *                          -EINVAL => invalid @policy OR invalid @sched_priority
           *                          -EFAULT => copy from user failed
           *                          -ESRCH  => no such process
           *                          -EPERM  => permission deny(maybe resource limition over)
           * # if @p->sched_reset_on_fork is set,@policy will disable SCHED_RESET_ON_FORK.
           *   this function can not be called to in interrupt context.
           *   for safely change the policy of a task,the runqueue it is residing must be locked up.
           *   # dealt with by __sched_setscheduler().
           *   # before entery __sched_setscheduler(),rcu_read_lock() is called to indicates a RCU read-side
           *     critical section is started.
           * # after priority and scheduler settings are accomplished,check the runqueue to determine whether
           *   allows preemption.
           * # the fourth parameter of __sched_setscheduler() is named @user with boolean type,if the caller
           *   is a real user in the system,then it is true;otherwise,false(i.e. shutdown, etc.),in
           *   this case,should call this function from sched_setscheduler_nocheck().
           *   # the "user" is a temporary created high priority thread worker.
           */
          SYSCALL_DEFINE2(sched_setscheduler, pid_t, pid, int, policy, struct sched_param __user *, param);

          !!  the valid @sched_priority for SCHED_FIFO and SCHED_RR is between 1..MAX_USER_RT_PRIO - 1(99).
              the valid @sched_priority for SCHED_NORMAL, SCHED_BATCH and SCHED_IDLE is 0.
          !!  the user must have CAP_SYS_NICE capability to modify scheduler.
              /* -EPERM */
          !!  DISALLOWED TASKS MOVE OUT SCHED_IDLE,IF THE TASK IS SCHED_IDLE NOW,THEN CAN NOT MODIFY IT.
              /* -EPERM */
          !!  DO NOT ALLOW REALTIME TASKS INTO GROUPS THAT HAVE NO RUNTIME ASSIGNED.
              /* -EPERM, if defined CONFIG_RT_GROUP_SCHED */ 
          !!  allow unprivileged RT tasks to decrease priority,but disallowed increase priority.
                    /* non root user */
          !!  for modify a task's priority,have to dequeue it from the runqueue,and enqueue it again later.
          !!  @p->pi_lock would be locked with irqsave during adjusting runqueue.

      The sched_getparam() and sched_setparam() system calls :
        <kernel/sched.c>
          /**
           * sys_sched_getparam - get the RT priority of a thread
           * @pid:                process ID
           * @param:              scheduler parameter bufffer
           * return:              0 => succeed to get parameter
           *                      -EINVAL => @pid < 0 OR @param == NULL
           *                      -ESRCH  => no such process
           *                      -EPERM  => permission deny
           *                      -EFAULT => copy to user failed
           * # RCU read-side critical section starts before find_procees_by_pid().
           */
          SYSCALL_DEFINE2(sched_getparam, pid_t, pid, struct sched_param __user *, param);

          /**
           * sys_sched_setparam - set the RT priority of a thread
           * @pid:                process ID
           * @param:              new parameter
           * return:              0 => succeed to set parameter
           *                      error values are same as do_sched_setscheduler(),it was called
           *                      by sys_sched_setscheduler()
           * # because this function only modifies the real-time priority for @pid,thus,the argument
           *   of @policy paramter of __sched_setscheduler() is -1,in this case,keep @p->policy
           *   unchanged.
           */
          SYSCALL_DEFINE2(sched_setparam, pid_t, pid, struct sched_param __user *, param);

      The sched_yield() system calls:
        <kernel/sched.c>
          /**
           * sys_sched_yield - yield processor time
           * # this function updates runqueue statistics of the rq which @current is residing now,
           *   call to scheduling class API yield_task(this_rq()) to does some scheduler depended works.
           *   finally,it enable preemption without rescheduling,then call to schedule().
           */
          SYSCALL_DEFINE0(sched_yield);

          !!  even @current has yielded processor time,it stay in TASK_RUNNING state.
              scheduling class API yield_task() may let @it go to the end of this runqueue.
              /* this system call mainly used by real-time process with SCHED_FIFO */

      The sched_get_priority_min() and sched_get_priority_max() system calls :
        <kernel/sched.c>
          /**
           * sys_sched_get_priority_min - get minimum priority of the corresponding policy
           * @policy:                     the policy to query
           * return:                      minimum priority of @policy
           *                              -EINVAL => invalid @policy
           */
          SYSCALL_DEFINE1(sched_get_priority_min, int, policy);

          /**
           * sys_sched_get_priority_max - get maximum priority of the corresponding policy
           * @policy:                     the policy to query
           * return:                      maximum priority of @policy
           *                              -EINVAL => invalid @policy
           */
          SYSCALL_DEFINE1(sched_get_priority_max, int, policy);

      The sched_rr_get_interval() system call :
        <kernel/sched.c>
          /**
           * sys_sched_rr_get_interval - return the default timeslice of a process
           * @pid:                       process ID
           * @interval:                  user space buffer
           * return:                     0 => succeed to query default timeslice
           *                             -EINVAL => @pid < 0
           *                             -EPERM  => permission deny
           *                             -EFAULT => copy to user failed
           */
          SYSCALL_DEFINE2(sched_rr_get_interval, pid_t, pid, struct timespec __user *, interval);

          !!  @time_slice is get from scheduling class API get_rr_interval()/* for real-time process */
              in jiffies,function jiffies_to_timespec() convert jiffies to timespec.
              before process finding,have to sign a RCU read-side critical section,and,have to acquire
              runqueue lock before call to scheduling class API.
          !!  timespec value "0" means the timeslice is infinity(real time process only,SCHED_FIFO).


/* END OF CHAPTER7 */


Chapter 8 : Memory Management
    Page Frame Management :
      Linux adopt 4kB page frame size as the standard memory allocation unit.
      The reasons :
        1>  Page Fault exceptions issued by the paging circuitry are easily interpreted.
            (page existed but disallow process accesses to,or page is not exist)

        2>  Although both 4kB and 4MB are multiples of all disk block sizes,trasfers of data
            between main memory and disks are in most cases more efficient when the smaller size
            is used.

      Kernel Memory Layout :
        0  0x1000                                 _end
        +-------------------------------------------------------+- - - - -+------------+
        |  |                |        |            |             |         |            |
        |  |  Dynamic       |        |            |  Dynamic    |         | Unmapped   |
        |  |  Memory        |        |            |  Memory     |         |   High     |
        |  |                |        |            |             |         |  Memory    |
        |  |                |        |            |             |         |            |
        +-------------------------------------------------------+- - - - -+------------+
         |                      |          |
         |                      |          |
         |                      |          |
         +----------+-----------+          |
                    |                      V
                    |                  Reversed Kernel
                    V
             Reserved Hardware

      Page Descriptors :
        Kernel must keep track of the status of each page frame.
        The pages inside a page frame are belong to a process,or contain Kernel Code,or contain Kernel Data.
        The pages inside a page frame are dynamic memory or free.

        !  A page frame in dynamic memory is free,if it does not contain any useful data.
           A page frame in dynamic memory is not free,if the page fram contains data of a User Mode process,
           data of software cache,dynamically allocated kernel data structures,buffered data of a device
           driver,code of a kernel modul,etc.

        struct page :
          State information of a page frame is kept in a page descriptor of type @page.
          <linux/mm_types.h>
            /**
             * page - structure page represent a physical page
             * @flags:        atomic flags,32 flags describe the status of the page frame
             * @_count:       usage count
             *                # this field would decrease whenever a process releases this page frame
             *                  or COW is executed on it
             *                # !! THE BOOK HAS INTRODUCED WHEN @_count IS SET TO -1 MEANS NO ONE
             *                     IS USING IT.BUT IN KERNEL VERSION 2.6.34.1,NOWHERE HAS SET THIS
             *                     FIELD TO -1
             *                     FUNCTION page_count() JUST atomic_read() THIS FIELD,BUT THE BOOK
             *                     SAID THIS ROUTINE RETURN @_count + 1 AS THE RESULT,IT IS NOT
             *                     CORRECT IN KERNEL 2.6.34.1
             *                     IN HEADER <linux/mm.h>,IT SAID page_count() == 0 MEANS THE PAGE
             *                     IS "FREE",@page->lru IS THEN USED FOR FREELIST MANAGEMENT IN THE
             *                     BUDDY ALLOCATOR;page_count() > 0 MEANS THE PAGE HAS BEEN ALLOCATED
             *                     
             *                     EVEN IN BOOTING PHASE,FUNCTION init_page_count() SET THIS FIELD
             *                     TO 1,THIS FUNCTION IS CALLED IN memmap_init_zone(),WHICH IS
             *                     DEFINED IN <mm/page_alloc.c> AS A "__meminit" FUNCTION
             *
             *                # when got a new page,function prep_new_page() would be called on the
             *                  kernel control path.in which,function set_page_refcounted() will be
             *                  invoked,and this routine set page's @_count field to 1(of course,
             *                  have to done some BUG checking before set @_count.this routine
             *                  is defined in <mm/internal.h>)
             * @_mapcount:    count of ptes mapped in mms to show when page is mapped
             *                AND limit reverse map searches
             *                # Number of Page Table entries that refer to the page frame
             *                  (-1 if none)
             * @inuse:
             * @objects:      
             *                SLUB data
             * @private:      mapping-private opaque data,usually used for buffer_heads if
             *                PagePrivate set;used for swp_entry_t if PageSwapCache;
             *                indicates order in the buddy system if PG_buddy is set
             * @mapping:      if low bit clear,points to inode address_space,or NULL
             *                if page mapped as anonymous memory,low bit is set,and
             *                it points to anon_vma object
             * @ptl:          split pt lock
             * @slab:         SLUB: pointer to slab
             * @first_page:   compound tail pages
             * @index:        offset within mapping
             * @freelist:     SLUB: freelist req. slab lock
             * @lru:          pageout list
             * @virtual:      kernel virtual address,NULL if not kmapped
             * @debug_flags:  atomic bitops,page debug flags
             * @shadow:       pointer to a status block,NULL if not tracked
             */
            struct page {
                    unsigned long flags;
                    atomic_t _count;
                    union {
                            atomic_t _mapcount;
                            struct {
                                    u16 inuse;
                                    u16 objects;
                            };
                    };
                    union {
                            struct {
                                    unsigned long private;
                                    struct address_space *mapping;
                            };
            #if USE_SPLIT_PTLOCKS
                            spinlock_t ptl;
            #endif
                            struct kmem_cache *slab;
                            struct page *first_page;
                    };
                    union {
                            pgoff_t index;
                            void *freelist;
                    };
                    struct list_head lru;
            #if defined(WANT_PAGE_VIRTUAL)
                    void *virtual;
            #endif
            #ifdef CONFIG_WANT_PAGE_DEBUG_FLAGS
                    unsigned long debug_flags;
            #endif
            #ifdef CONFIG_KMEMCHECK
                    void *shadow;
            #endif
            };

          All page descriptors are stored in the @mem_map array :
            <mm/memory.c>
            #ifndef CONFIG_NEED_MULTIPLE_NODES
              /* max_mapnr - the maximum number of page mapping */
              unsigned long max_mapnr;
              /* mem_map - array used to saves all mapped pages(memory) */
              struct page *mem_map;

              EXPORT_SYMBOL(max_mapnr);
              EXPORT_SYMBOL(mem_map);
            #endif
            /**
             * these definitions also are existed in <mm/nommu.c> for the
             * case that no Memory Management Unit is available.
             */ 

          Macro virt_to_page() is defined in <arch/x86/include/asm/page.h> used to
          get the linear address associated with the page descriptor :
            #define virt_to_page(kaddr)    pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)

          Macro pfn_to_page() is defined in <asm-generic/memory_model.h> used to
          get the page descriptor associated with the page frame number @pfn :
            #define __pfn_to_page(pfn)    (mem_map + ((pfn) - ARCH_PFN_OFFSET))
            #define pfn_to_page    __pfn_to_page

          Page Flags for struct page.flags :
            <linux/page-flags.h>
              enum pageflags {
                      PG_locked,                /* page is locked,do not touch */
                      PG_error,
                      PG_referenced,
                      PG_uptodate,
                      PG_dirty,
                      PG_lru,
                      PG_active,
                      PG_slab,
                      PG_owner_priv_1,          /* owner use.if pagecache,fs may use */
                      PG_arch_1,
                      PG_reserved,
                      PG_private,               /* if pagecache,has fs-private data */
                      PG_private_2,             /* if pagecache,has fs aux data */
                      PG_writeback,             /* page is under writeback */
              #ifdef CONFIG_PAGEFLAGS_EXTENDED
                      PG_head,                  /* a head page */
                      PG_tail,                  /* a tail page */
              #else
                      PG_compound,              /* a compound page */
              #endif
                      PG_swapcache,             /* swap page: swp_entry_t in private */
                      PG_mappedtodisk,          /* has blocks allocated on-disk */
                      PG_reclaim,               /* to be reclaimed asap */
                      PG_buddy,                 /* page is free,on buddy lists */
                      PG_swapbacked,            /* page is backed by RAM/swap */
                      PG_unevicatable,          /* page is "unevictable" */
              #ifdef CONFIG_MMU
                      PG_mlocked,               /* page is vma mlocked */
              #endif
              #ifdef CONFIG_ARCH_USES_PG_UNCACHED
                      PG_uncached,              /* page has been mapped as uncached */
              #endif              
              #ifdef CONFIG_MEMORY_FAILURE
                      PG_hwpoison,              /* hardware poisoned page,do not touch */
              #endif
                      __NR_PAGEFLAGS

                      /* Filesystems */
                      PG_checked = PG_owner_priv_1,
                      PG_fscache = PG_private_2,        /* page backed by cache */
                      /**
                       * these two page bits are conscripted by FS-Cache to maintain local
                       * caching state,they are set on pages belonging to the netfs's inodes
                       * when those inodes are being locally cached.
                       */

                      /* XEN */
                      PG_pinned = PG_owner_priv_1,
                      PG_savepinned = PG_dirty,

                      /* SLOB */
                      PG_slob_free = PG_private,

                      /* SLUB */
                      PG_slub_frozen = PG_active,
                      PG_slub_debug = PG_error,
              };

              !!  page flag operations are defined in same file with these format :
                    /* Name first character is uppercase */
                    Page<Name without "PG_" prefix>(page descriptor pointer)
                    SetPage<Name without "PG_" prefix>(page descriptor pointer)
                    ClearPage<Name without "PG_" prefix>(page descriptor pointer)

                    Page  => return the value of corresponding bit
                    Set   => enable corresponding bit
                    Clear => disable corresponding bit

                    e.g.
                            PageError(pdp);
                            SetPageError(pdp);
                            ClearPageError(pdp);

                  each macro is correspond to a static inline function in <linux/page-flags.h>.

      Non-Uniform Memory Access(NUMA) :
        NUMA,the access times for different memory locations from a given CPU may vary,the physical
        memory of the system is partitioned in several nodes.

        The time for CPUs to access the pages in a single node is same,but in the case several nodes
        are existed,it is different.
        For minimize the number of accesses to costly nodes,Kernel carefully selecting where the Kernel
        data structures that are most often referenced by the CPU are stored.
        /**
         * the Kernel make use of NUMA even for some peculiar uniprocessor systems that have huge "holes"
         * in the physical address space.
         */

        ! physical memory inside each node can be split into several zones.

        Each node has a descriptor of type "pg_data_t" :
          <linux/mmzone.h>
            /**
             * pglist_data(pg_data_t) - structure used to represent a memory node
             * @node_zones:             array of zone descriptors of the node
             * @node_zonelists:         array of zonelist data structures used by the page allocator
             *                          each zonelist includes all memory zones in current node
             *                          with the same zone type
             * @nr_zones:               number of zones in the node
             * @node_mem_map:           array of page descriptors of the node
             * @node_page_cgroup:       cgroup data used for memory resource control
             * @bdata:                  used in the kernel initialization phase
             * @node_size_lock:         memory chip hotplug support
             * @node_start_pfn:         page frame number of the start for this node
             * @node_present_pages:     total number of physical pages
             * @node_spanned_pages:     total size of physical page range,including holes
             * @node_id:                node id
             * @kswapd_wait:            kernel thread kswapd wait queue
             * @kswapd:                 task structure for kswapd
             * @kswapd_max_order:       logarithmic size of free blocks to be created by kswapd
             */
            typedef struct pglist_data {
                    struct zone node_zones[MAX_NR_ZONES];
                    struct zonelist node_zonelists[MAX_ZONELISTS];
                    int nr_zones;
            #ifdef CONFIG_FLAT_NODE_MEM_MAP
                    struct page *node_mem_map;
            #ifdef CONFIG_CGROUP_MEM_RES_CTLR
                    struct page_cgroup *node_page_cgroup;
            #endif
            #endif
            #ifndef CONFIG_NO_BOOTMEM
                    struct bootmem_data *bdata;
            #endif
            #ifdef CONFIG_MEMORY_HOTPLUG
                    spinlock_t node_size_lock;
            #endif
                    unsigned long node_start_pfn;
                    unsigned long node_present_pages;
                    unsigned long node_spanned_pages;
                    int node_id;
                    wait_queue_head_t kswapd_wait;
                    struct task_struct *kswapd;
                    int kswapd_max_order;
            } pg_data_t;
            /* on NUMA,each node has a pg_data_t structure to describe its memory layout. */

            #ifndef CONFIG_NEED_MULTIPLE_NODES

            /* contig_page_data - node descriptor for node 0,used for single node */
            extern struct pglist_data contig_page_data;
            #define NODE_DATA(nid)    (&contig_page_data)
            #define NODE_MEM_MAP(nid)    mem_map

            #else

            #include <asm/mmzone.h>
            /**
             * CONFIG_NEED_MULTIPLE_NODES - NUMA
             *         in this case,an array @node_data is defined in <arch/x86/include/asm/mm_zone64.h>,
             *         macro NODE_DATA(nid) <-> @node_data[@nid]
             */

            #endif

          ! even NUMA is not configured for Kernel,it stay use a single pg_data_t object to includes all
            system physical memory.
          ! pg_data_t.@node_zones is initialized at boot time with the address of all zone descriptors
            designate to this node.

      Memory Zones :
        Real computer architectures have hardware constraints that may limit the way page frames can be used.
          80x86 :
            1> DMA(Direct Memory Access) processor for old ISA buses have a strong limition :
                 they are able to address only the first 16MB of RAM
            2> In modern 32-bit computers with lots of RAM,the CPU cannot directly access all physical memory
               because the linear address space is too small

          Linux Kernel have to deal with these two kind of problems.
          Linux partitions the physical memory of  every memory node into three zones.
            80x86 UMA :
              ZONE_DMA => contains page frames of memory below 16MB
              /* ISA compatible */
              ZONE_NORMAL => contains page frames of memory at and above 16MB and below 896MB
              /* Access through linear address */
              ZONE_HIGH => contains page frames of memory at and above 896MB
              /* Cannot access through linear address > 4GB */
              /* 64-bit ZONE_HIGH is different to 32-bit */

            /* for use of ZONE_HIGH,Kernel have to makeup temporary memory mapping or permanent memory mapping */

        struct zone :
          <linux/mmzone.h>
            /**
             * zone - structure zone represent memory zone inside a memory node
             * @watermark:        zone watermarks,access with *_wmark_page(zone) macros
             * @lowmem_reserve:   low memory reserve used to avoid totally wasting several GB of ram
             *                    when going to allocate will be freeable or/and it will be released
             *                    eventually
             *                    this array is recalculated at runtime if the sysctl_lowmem_reserve_ratio
             *                    sysctl changes
             *                    (low-on-memory critical situations)
             * @node:             NUMA node id
             * @min_unmapped_pages:        minimum number of pages unmapped
             * @min_slab_pages:            minimum number of pages for slab allocator
             * @pageset:                   per-CPU pageset
             *                             used to implement special caches of single page frames
             * @lock:                      concurrent protection
             * @all_unreclaimable:         all pages pinned
             * @span_seqlock:              memory hotplug support
             * @free_area:                 blocks of free page frames in this zone,
             *                             used by buddy system
             * @lru_lock:                  LRU mechanism lock
             * @lru:                       LRU zones to represent inactive pages and active pages
             *                             for memory cache
             * @pages_scanned:             since last reclaim,how many pages scanned
             * @flags:                     zone flags
             * @vm_stat:                   zone statistics
             * @prev_priority:             holds the scanning priority for this zone([12, 0])
             * @wait_table:                holding the hash table of wait queues of processes waiting
             *                             for one of the pages of the zone
             * @wait_table_hash_nr_entries:        size of the hash table
             * @wait_table_bits:                   power-of-2 order of the size of the wait queue
             *                                     hash table
             *                                     @wait_table_size == (1 << wait_table_bits)
             * @zone_pgdat:                which node this zone belongs to
             * @zone_start_pfn:            page frame number of the start of this zone
             * @spanned_pages:             total size of zone in pages,including holes
             * @present_pages:             amount of memory(excluding holes)
             * @name:                      rarely used,the conventional name are "DMA",
             *                             "Nonrmal",or "HighMem"
             */
            struct zone {
                    unsigned long watermark[NR_WMARK];
                    unsigned long lowmem_reserve[MAX_NR_ZONES];
            #ifdef CONFIG_NUMA
                    int node;
                    unsigned long min_unmapped_pages;
                    unsigned long min_slab_pages;
            #endif
                    struct per_cpu_pageset __percpu *pageset;
                    spinloc_t lock;
                    int all_unreclaimable;
            #ifdef CONFIG_MEMORY_HOTPLUG
                    seqlock_t span_seqlock;
            #endif
                    struct free_area free_area[MAX_ORDER];

                    ...

                    spinlock_t lru_lock;
                    struct zone_lru {
                            struct list_head list;
                    } lru[NR_LRU_LISTS];

                    ...

                    unsigned long pages_scanned;
                    unsigned long flags;

                    atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS];

                    int prev_priority;

                    ...

                    wait_queue_head_t *wait_table;
                    unsigned long wait_table_hash_nr_entries;
                    unsigned long wait_table_bits;

                    struct pglist_data *zone_pgdat;
                    unsigned long zone_start_pfn;

                    unsigned long spanned_pages;
                    unsigned long present_pages;

                    const char *name;

            } ____cacheline_internodealigned_in_smp;

            each page descriptor contains a link to the memory node and a link to the zone inside memory
            node that including this page.the links are not saved as pointers as usually,they are
            encoded as indices stored in the high bits of the page.@flags field.
            /**
             * the number of @flags that characterize a page frame is limited,thus it is always possible
             * to reserve the most significant bits of the @flags field to encode the proper memory node 
             * and zone number.
             */

           <linux/mm.h>
           static inline struct zone *page_zone(struct page *page)
           {
                   return &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum(page)];
                          /**
                           * page_to_nid() retrieve @nid from @page->flags
                           * NODE_DATA() retrieve node structure from @nid
                           * page_zonenum() retrieve @zonenum from @page->flags
                           */
           }

           ! memory allocation function must determines which memory zone should be selected by
             zone class indicator ZONE_DMA / ZONE_NORMAL / ZONE_HIGH .
             select DMA if only if ZONE_NORMAL have no free space to available.
             pg_data_t.@zonelists linked zone descriptors,one allocation request operates on a 
             zonelist,the first one is the 'goal' of the allocation,the other zones are fallback
             zones,in decreasing priority.
             /**
              * Page Allocator selected ZONE1,and found out the page frames presented can not
              * satisfied the requestion,then go to search the fallback zones via @zonelists.
              * # Number of free page frames currently is less than required.
              */

           /**
            * structure zonelist and structure zoneref are defined in <linux/mmzone.h>.
            * struct zoneref has two members :
            *         struct zone *zone /* actual zone */
            *         int zone_idx      /* idx in zonelist (/
            * struct zonelist has several members :
            *         struct zonelist_cache *zlcache_ptr /* zonelist traversing speedup cache */
            *         struct zoneref _zonerefs[MAX_ZONES_PER_ZONELIST + 1]
            *                        /* zone reference objects in this zonelist */
            *         struct zonelist_cache zlcache /* #ifdefined CONFIG_NUMA */
            */

      The Pool of Reserved Page Frames :
        memory allocation requests can be satisfied in two different ways :
          1> required memory is available now,return immediately
          2> required memory is not available now,block wait for memory reclaiming

        inside blocking-forbidden critical region or interrupt context,the kernel control path
        should issues "atomic memory allocation" requests(GFP_ATOMIC flag of memory allocator).
        "atomic memory allocation" never blocks,if there is not enough memory available,just
        fails.
        /**
         * for minimize the likelihood of GFP_ATOMIC failed event,kernel reserves a pool
         * of page frames for atomic memory allocation requests,to be used only on
         * "low-on-memory" conditions.
         */

        the amount of the reserved memory(in kilobytes) :
          <mm/page_alloc.c>
            int min_free_kbytes = 1024;
          /* it can be modified during runtime via sysctl */
          /**
           * Its value depends on the number of page frames inclued in the ZONE_DMA and ZONE_NORMAL
           * memory zones :
           *         reserved pool size = floor(sqrt(16 * directly mapped memory)) (kilobytes)
           * ! can not lower than 128 or greater than 65536
           */
          ZONE_DMA and ZONE_NORMAL memory zones contribute to the reserved memory with a number of
          page frames proportional to their relative sizes.(X : Y)
          /**
           * struct zone.@lowmem_reserve[MAX_NR_ZONES] indicates how many page frames of the memory zone
           * contributes to reserved memory.
           */

      The Zoned Page Frame Allocator :
        zoned page frame allocator : (kernel subsystem)
          handles the memory allocation requests for groups of contiguous page frames.
          /* requests for allocation and deallocation of dynamic memory */

        ! it search a memory zone that includes a group of contiguous page frames that can
          satisfy the request.

        buddy system :
          handles page frames allocation inside a specified memory zone.

        performance optimization :
          cache a small number of page frames for quickly satisfy the allocation request of
          single page frames.

        Zoned Page Frame Allocator :
          { Zone Allocator }
          |   |   |
          |   |   | ZONE_DMA Memory Zone {
          |   |   +-------->per-CPU page frame cache
          |   |   |             |
          |   |   |             V
          |   |   +-------->buddy system
          |   |     }
          |   |
          |   |     ZONE_NORMAL Memory Zone {
          |   +----------->per-CPU page frame cache
          |   |                |
          |   |                V  
          |   +----------->buddy system
          |         }
          |
          |         ZONE_HIGHMEM Memory Zone {
          +--------------->per-CPU page frame cache
          |                    |
          |                    V
          +--------------->buddy system
                    }

        Requesting and Releasing Page Frames :
          /**
           * The implementations for memory layout are under "mm" directory.
           * Architecture depended implementations are under <arch/x86/...>.
           */

          <linux/gfp.h>
            /**
             * alloc_pages - allocate contiguous page frames
             * @gfp_mask:    GFP mask
             * @order:       number of contiguous page frames = 2^@order
             * return:       succeed => page descriptor pointer of the first page frame
             *               failed => NULL
             */
            static inline struct page *
            alloc_pages(gfp_t gfp_mask, unsigned int order);

            /**
             * alloc_page - just allocate one page frame
             */
            #define alloc_page(gfp_mask)  alloc_pages(gfp_mask, 0)

            /**
             * __get_free_pages - get free contiguous page frames
             * @gfp_mask:         GFP mask
             * @order:            the number of page frames = 2^@order
             * return:            succeed => linear address of the first page frame
             *                    failed  => 0
             */
            extern unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order);

            /**
             * __get_free_page - just get a free page frame
             */
            #define __get_free_page(gfp_mask)  __get_free_pages((gfp_mask), 0)

            /**
             * get_zeroed_page - get a page frame filled with zero
             * @gfp_mask:        GFP mask
             * return:           succeed => linear address of the page frame
             *                   failed => 0
             */
            extern unsigned long get_zeroed_page(gfp_t gfp_mask);

            /**
             * __get_dma_pages - get pages from ZONE_DMA
             */
            #define __get_dma_pages(gfp_mask, order)  __get_free_pages((gfp_mask) | GFP_DMA, (order))

            GFP bitmasks :
              each flag has a corresponding binary bit in @gfp_t.
              VM => Virtual Memory Area

              /* zone modifiers */
              __GFP_DMA                ZONE_DMA
              __GFP_HIGHMEM            ZONE_HIGH
              __GFP_DMA32              ZONE_DMA32
              __GFP_MOVABLE            Page is movable(page migration mechanism or reclaimed)
              GFP_ZONEMASK (__GFP_DMA | __GFP_HIGHMDEM | __GFP_DMA32 | __GFP_MOVABLE)

              /**
               * __GFP_DMA  = 1 => memory allocation search from ZONE_DMA
               * __GFP_HIGH = 0 => memory allocation search from ZONE_DMA and ZONE_NORMAL
               * __GFP_HIGH = 1 => memory allocation search from ZONE_DMA,ZONE_NORMAL and ZONE_HIGH
               *                   |
               *                   +-> # in order of prefence
               */

              /* action modifiers */
              __GFP_WAIT               Can wait and reschedule(suspend caller,if necessary)
              __GFP_HIGH               Should access emergency pools
              __GFP_IO                 Can start physical IO
              __GFP_FS                 Can call down to low-level FS
              __GFP_COLD               Cache-cold page required
              __GFP_NOWARN             Suppress page allocation failure warning
              __GFP_REPEAT             Try hard to allocate the memory(attempt _might_ fail)
              __GFP_NOFAIL             The VM implementation _must_ retry infinitely
                                       (deprecated)
              __GFP_NORETRY            The VM implementation _must_ not retry indefinitely
              __GFP_COMP               Add compound page metadata
              __GFP_ZERO               Return zeroed page on success
              __GFP_NOMEMALLOC         Do not use emergency reserves
              __GFP_HARDWALL           Enforce hardwall cpuset memory allocs
              __GFP_THISNODE           No fallback,no policies
              __GFP_RECLAIMABLE        Page is reclaimable

              __GFP_NOTRACK            Do not track with kmemcheck
                                       #ifdef CONFIG_KMEMCHECK => 0x200000u
                                       #else => 0
                                       #endif

            Linux Predefined GFP bitmasks combinations :
              __GFP_BITS_SHITF          22
              __GFP_BITS_MASK           1 << __GFP_BITS_SHIFT - 1

              GFP_NOWAIT                GFP_ATOMIC & ~__GFP_HIGH
              GFP_ATOMIC                __GFP_HIGH
              GFP_NOIO                  __GFP_WAIT
              GFP_NOFS                  __GFP_WAIT | __GFP_IO
              GFP_KERNEL                __GFP_WAIT | __GFP_IO | __GFP_FS
              GFP_TEMPORARY             __GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_RECLAIMABLE
              GFP_USER                  __GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL
              GFP_HIGHUSER              __GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL | \
                                        __GFP_HIGHMEM
              GFP_HIGHUSER_MOVABLE      __GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL | \
                                        __GFP_HIGHMEM | __GFP_MOVABLE
              GFP_IOFS                  __GFP_IO | __GFP_FS
              GFP_THISNODE              __GFP_THISNODE | __GFP_NOWARN | __GFP_NORETRY
                                        #ifndef CONFIG_NUMA => 0
              GFP_DMA                   __GFP_DMA
              GFP_DMA32                 __GFP_DMA32E

              /* makeup all the page movable related flags */
              GFP_MOVABLE_MASK          __GFP_RECLAIMABLE | __GFP_MOVABLE
              /* control page allocator reclaim behavior */
              GFP_RECLAIM_MASK          __GFP_WAIT | __GFP_HIGH | __GFP_IO | __GFP_FS | \
                                        __GFP_NOWARN | __GFP_REPEAT | __GFP_NOFAIL | \
                                        __GFP_NORETRY | __GFP_NOMEMALLOC
              /* control slab gfp mask during early boot */
              GFP_BOOT_MASK             __GFP_BITS_MASK & ~(__GFP_WAIT | __GFP_IO | __GFP_FS)
              /* control allocation constraints */
              GFP_CONSTRAINT_MASK       __GFP_HARDWALL | __GFP_THISNODE
              /* do not use these(dma32, highmem, ~bits_mask) with a slab allocator */
              GFP_SLAB_BUG_MASK         __GFP_DMA32 | __GFP_HIGHMEM | ~__GFP_BITS_MASK

            /**
             * __free_pages - release allocated pages
             * @page:         the first page's page descriptor of pages to be released
             * @order:        number of free pages = 2^@order
             */
            extern void __free_pages(struct page *page, unsigned int order);
            
            /**
             * free_pages - release allocated pages
             * @addr:       linear address of the first page of pages to be released
             * @order:      number of free pages = 2^@order
             */
            extern void free_pages(unsigned long addr, unsigned int order);

            /* these just free a page frame */
            #define __free_page(page)  __free_pages((page), 0)
            #define free_page(addr)  free_pages((addr), 0)

            !!  free pages routines finally call to __free_page_ok(),which is static defined in
                <mm/page_alloc.c>,if @order is zero,they would call to free_hot_cold_page().
                before free pages,local interrupt would be disabled until free accomplished.
                if the argument of the pages to be released are not satisfied condition checking,
                then just return to caller as well.

      Kernel Mappings of High-Memory Page Frames :
        linear address corresponding to mapped physical address,that are ZONE_DMA and ZONE_NORMAL.
        the beginning of high memory is stored in the @high_memory variable,which is defined in
        <linux/mm.h> a void pointer.
        for 32-bit system,this variable is set to 896MB,so page frames above 896MB boundary are
        not generally mapped in the fourth gigabyte of the kernel linear address spaces,and kernel
        can not access them directly.

        !!  __get_free_pages(GFP_HIGHMEM, 0) returns 0,and kernel will lost track to the pages.

        Resolutions for this problem :
          1> use alloc_pages() or alloc_page() to request high-memory page frames.
             return linear address of page descriptor instead to return linear address 
             of the page frames.
             /* page descriptor allocated in low memory and forever during the kernel initialization */
          2> kernel reversed the last 128MB linear address space used to mapping high-memory page frames.
             this is temporary mapping,by recycling linear addresses the whole high memory can be
             accessed at different times.

        !! on the system with PAE support have up to 64GB of RAM will be less than 128MB linear address
           space left for kernel mapping.

        The ways Linux Kernel selected :
          permanent kernel mapping >
            establish permanent kernel mapping may block current process if no free page table entry
            is available.                      /* thus can not be invoked in interrupt context */

            kernel mapping use a dedicated Page Table in the master kernel page tables,variable
              @pkmap_page_table is type of "pte_t *" which is defined in <mm/highmem.c>

            macro LAST_PKMAP <arch/x86/include/asm/pgtable_32_types.h> 
              number of entries in the page table,has value 512(PAE) OR 1024(non-PAE).
                                                           /* 2MB once OR 4MB once */
                                                           /* sizeof(Page Frame) = 4kB */

            macro PKMAP_BASE <arch/x86/include/asm/pgtable_32_types.h>
              ((FIXADDR_BOOT_START - PAGE_SIZE * (LAST_PKMAP + 1)) & PMD_MASK)
              /* the page table maps the linear addresses starting from @PKMAP_BASE */

            static page kernel mapping counters array <mm/highmem.c>
              static int pkmap_count[LAST_PKMAP];
              /* each element in the array corresponding to a entry in @pkmap_page_table */

            page kernel mapping counter :
              counter == 0
                the corresponding Page Table entry does not map any high-memory page frame
                and is usable
              counter == 1
                the corresponding Page Table entry does not map any high-memory page frame,
                but it cannot be used because the corresponding TLB entry has not been
                flushed since its last usage
              counter > 1
                the corresponding Page Table entry maps a high-memory page frame,which is
                used by exactly @counter - 1 kernel components

            page kernel mapping track :
              <mm/highmem.c>
                /**
                 * page_address_slot - hash table bucket slot for page kernel mapping
                 * @lh:                list of @page_address_maps
                 * @lock:              bucket protector
                 *
                 * page_address_htable - page mapping hash table
                 */
                static struct page_address_slot {
                        struct list_head lh;
                        spinlock_t lock;
                } ____cacheline_aligned_in_smp page_address_htable[1 << PA_HASH_ORDER];

                /**
                 * page_address_map - page->virtual association
                 * @page:             page frame
                 * @virtual:          virtual linear address mapping
                 * @list:             list linker
                 */
                struct page_address_map {
                        struct page *page;
                        void *virtual;
                        struct list_head list;
                };

                /* page_address_maps - buffer for all page address mapping */
                static struct page_address_map page_address_maps[LAST_PKMAP];

            permanent page kernel mapping primitives :
              macro function PageHighMem(__p) is defined in <linux/page-flags.h>,which
              would be expanded to is_highmem(page_zone(__p)) to determine whether
              @__p is a page frame in high-memory.

              <linux/mmzone.h>
                /**
                 * is_highmem - check whether a given zone descriptor is high-memory zone
                 * @zone:       zone descriptor
                 * return:      1 => it is
                 *              0 => it is not
                 */
                static inline int is_highmem(struct zone *zone)
                {
                        int zone_off = (char *)zone - (char *)zone->zone_pgdat->node_zones;
                        return zone_off == ZONE_HIGHMEM * sizeof(*zone) ||
                               (zone_off == ZONE_MOVABLE * sizeof(*zone) &&
                                zone_movable_is_highmem());
                }

              !! FOR SOME PERMANENT PAGE KERNEL MAPPING PRIMITIVES,IF PAGE FRAME IS NOT
                 IN HIGH-MEMORY,THEN THE ROUTINES FOR HIGH-MEMORY WOULD NOT BE INVOKED.
                 e.g.  /* PageHighMem(@page) => FALSE */
                   kmap -> page_address
                   page_address -> lowmem_page_address
                   kunmap -> return

              <mm/highmem.c>
                /**
                 * page_address - get the mapped virtual address of a page
                 * @page:         the page
                 * return:        page->virtual
                 *                NULL => incorrect page descriptor
                 * # this function traverse page_address_slot to find the
                 *   page_address_map object which represent this @page.
                 */
                void *page_address(struct page *page);

              <arch/x86/mm/highmem_32.c>
                /**
                 * kmap - map a page to virtual address
                 * @page: the page frame to map
                 * return:  mapped linera address
                 *          NULL => failed to map this @page
                 * # this function call to kmap_high(),which is defined in
                 *   <mm/highmem.c>.
                 * # if @page is not in high-memory,just invoke page_address()
                 *   to get the linear address as well
                 */
                void *kmap(struct page *page);

              <mm/highmem.c>
                /**
                 * kmap_high - map a high memory page frame to virtual address
                 * @page:      the page frame in high-memory to be mapped
                 * return:     mapped linear address
                 *             NULL => failed to map this @page
                 * # this will locks @kmap_lock up before call to page_address(),
                 *   if @page had been mapped,then return value of page_address()
                 *   is the virtual address;otherwise,return NULL,in this case,
                 *   kmap_high() call to map_new_virtual() try to map @page.
                 *   before unlock @kmap_lock,it have to updates counter in 
                 *   @pkmap_count[PKMAP_NR(@vaddr)](increase 1).
                 *   finally,return @vaddr.
                 */
                void *kmap_high(struct page *page);

                /** 
                 * last_pkmap_nr - the number in range [0, LAST_PKMAP] used by the most recently
                 *                 page kernel mapping
                 * # if this number is zero,that means all zero pkmaps in TLB cache must be
                 *   flushed at first.
                 */
                static int last_pkmap_nr;
                
                /**
                 * map_new_virtual - map a new virtual address for a mapped page frame
                 * @page:            the page
                 * return:           new virtual address of this page frame
                 *                   0 => failed to map a new virtual address for @page
                 * # for map a new virtual address,it have to find a usable page table entry.
                 *   it start a infinity for-cycle :
                 *     the first step is to traverse all page table entry try to find a usable
                 *     entry.(from element at @last_pkmap_nr + 1 to the last element in the array pkmap_count[])
                 *     if @last_pkmap_nr is zero,flush TLB caches for all zero pkmaps,then set
                 *     @last_pkmap_nr to LAST_PKMAP(use the last page table entry).
                 *     if pkmap_count[@last_pkmap_nr] is zero,an entry(@last_pkmap_nr) is available.
                 *     if there is no such entry available,then ready to suspend caller.
                 *       set state of @current to TASK_UNINTERRUPTIBLE
                 *       enqueue this kernel control path to wait queue
                 *       unlock @kmap_lock
                 *       call to schedule()
                 *     after process switch(completion)
                 *       lock @kmap_lock
                 *       check if there is somebody have mapped @page
                 *       => T
                 *               just return the mapped virtual address
                 *       => F
                 *               goto step1 try to find a usable page table entry again
                 *   outside to for-cycle,map @page by itself(a free page table entry is available)
                 *     convert @last_pkmap_nr to virtual address through PKMAP_ADDR(),the @vaddr
                 *     call to set_pte_at() to makeup association between @page with @vaddr
                 *     # passed arguments
                 *     #   &@init_mm, @vaddr
                 *         &@pkmap_page_table[@last_pkmap_nr], mk_pte(@page, @kmap_prot)
                 *     update counter to 1 => @pkmap_count[@last_pkmap_nr] := 1
                 *     set @virtual of @page_address_map for this @page frame to @vaddr through set_page_address()
                 *     return @vaddr
                 */
                static inline unsigned long map_new_virtual(struct page *page);

              <arch/x86/mm/highmem_32.c>
                /**
                 * kunmap - unmap a page frame
                 * @page:   the page frame to unmap
                 * # this function will call to kunmap_high() which is defined in
                 *   <mm/highmem.c>.
                 * ! can not invoke this function in interrupt context.
                 */
                void kunmap(struct page *page);

              <mm/highmem.c>
                /**
                 * kunmap_high - unmap a high-memory page frame
                 * @page:        the page frame in high-memory to be unmapped
                 * # what this function does :
                 *     1> lock @kmap_lock
                 *        get virtual address of @page,if NULL is got,that is a BUG
                 *     2> use the virtual address to get PKMAP number via PKMAP_NR(@vaddr)
                 *     3> decrease @pkmap_count[@nr]
                 *        check the value of @pkmap_count[@nr]
                 *        == 0
                 *                BUG
                 *                ! A counter must never go down to zero without a TLB flush
                 *        == 1
                 *                activate wait queue @pkmap_map_wait through waitqueue_activate()
                 *     4> unlock @kmap_lock
                 *        check if return value of waitqueue_activate() is TRUE
                 *        ==> T
                 *                call to wake_up(@pkmap_map_wait) to tell others page table entry
                 *                is available
                 */
                void kunmap_high(struct page *page);
            

          temporary kernel mapping >
            establish temporary kernel mapping do not block current process but limit to the number
            of temporary mappings existed at same time.
            and a kernel control path use a temporary kernel mapping must ensure that no other
            kernel control path is using the same mapping.
            (@it can never block,otherwise the same page table "window" maybe used by others)

            /* Page Table entry */
            "window" is reserved fro temporary kernel mapping is quite small,each CPU has its own
            set of 13 windows,represented by the "enum km_type" data structure.
            /* it seems like Linux 2.6.34.1 introduced 18 kinds of enum km_type values */
            
            <asm-generic/kmap_types.h>

              #ifdef __WITH_KM_FENCE
              #define KMAP_D(n)  __KM_FENCE_##n ,
              #else
              #define KMAP_D(n)
              #endif

              /**
               * km_type - kernel mapping type enumerating
               * # each enumerating value is an index of a fix-mapped linear address.
               *   (except KM_TYPE_NR)
               *   because kernel must avoid two kernel control paths using a
               *   window at same time,each enumerate value in @km_type is dedicated
               *   to one kernel component and is named after the component.
               */
              enum km_type {
                      KMAP_D(0)        KM_BOUNCE_READ,
                      KMAP_D(1)        KM_SKB_SUNRPC_DATA,
                      KMAP_D(2)        KM_SKB_DATA_SOFTIRQ,
                      KMAP_D(3)        KM_USER0,
                      KMAP_D(4)        KM_USER1,
                      KMAP_D(5)        KM_BIO_SRC_IRQ,
                      KMAP_D(6)        KM_BIO_DST_IRQ,
                      KMAP_D(7)        KM_PTE0,
                      KMAP_D(8)        KM_PTE1,
                      KMAP_D(9)        KM_IRQ0,
                      KMAP_D(10)       KM_IRQ1,
                      KMAP_D(11)       KM_SOFTIRQ0,
                      KMAP_D(12)       KM_SOFTIRQ1,
                      KMAP_D(13)       KM_SYNC_ICACHE,
                      KMAP_D(14)       KM_SYNC_DCACHE,
              /* UML specific,for copy_*_user - used in do_op_one_page */
                      KMAP_D(15)       KM_UML_USERCOPY,
                      KMAP_D(16)       KM_IRQ_PTE,
                      KMAP_D(17)       KM_NMI,
                      KMAP_D(18)       KM_NMI_PTE,
                      KMAP_D(19)       KM_TYPE_NR
              };

            ! "enum fixed_addresses" is defined in <arch/x86/include/asm/fixmap.h>.
              it defined all "compile-time" special addresses,they has a constant
              address during "compile-time" but set the physical addresses in the
              boot process.
              /**
               * "compile-time allocated" memory buffers are fixed-size 4kB pages,
               * they start from the end of virtual address and backwards.
               *                 (x86_32 0xfffff000)
               * routine set_fixmap(idx, phys) associate physical memory with fixmap
               * indices.
               * ! TLB cache for such buffers would not be flushed across task switches.
               */
            ! @fixed_addresses - FIX_KMAP_BEGIN : begin of kmap
                                 FIX_KMAP_END := FIX_KMAP_BEGIN + (KM_TYPE_NR * NR_CPUS) - 1
                                                : end of kmap
              so the kmap memory block size is "4kB * (FIX_KMAP_END - FIX_KMAP_BEGIN)"

            <arch/x86/mm/init_32.c>
              /**
               * kmap_pte - page table entry type data object for fixmap
               * # function kmap_init() in the same file initializes @it to
               *   kmap_get_fixmap_pte(kmap_vstart),and @kmap_vstart is get from
               *   __fix_to_virt(FIX_KMAP_BEGIN).
               *   and variable @kmap_prot is set to PAGE_KERNEL.
               *                (an object is type of page table entry protection flags)
               */
              pte_t *kmap_pte;

            <arch/x86/mm/highmem_32.c>
              /**
               * kmap_atomic - establish a kernel fix mapping
               * @page:        page frame to map
               * @type:        what kind of kernel fix mapping
               * return:       NULL => failed to map
               *               > 0  => linear address for this kmap
               * # this function call to kmap_atomic_prot() with @kmap_prot to finish
               *   primary works.
               */
              void *kmap_atomic(struct page *page, enum km_type type);

              /**
               * kmap_atomic_prot - establish a kernel fix mapping with specified
               *                    page protection flags for single entry
               * @page:             page frame to map
               * @type:             what kind of kernel fix mapping
               * @prot:             entry protection flags
               * return:            NULL => failed to map
               *                    > 0  => linear address for this kmap
               * # if @page is not in high-memory,then just call to page_address() as well.
               */
              void *kmap_atomic_prot(struct page *page, enum km_type type, pgprot_t prot);

              what kmap_atomic_prot() does :
                1> disable page fault trap
                2> calculate @idx from "@type + KM_TYPE_NR * smp_processor_id()",
                   calculate @vaddr from __fix_to_virt(FIX_KMAP_BEGIN + @idx).

                3> !! this is a BUG if page table entry for @kmap_pte - @idx is not existed.

                4> call to set_pte() fill a page table entry "@kmap_pte - @idx" with value
                   returned by mk_pte(page, prot).           (backwards)
                                                             /**
                                                              * fill page table entry,no matter
                                                              * to how linear address grows
                                                              */
                5> return @vaddr.

              !! FORBID SLEEP WHEN HOLDING AN ATOMIC KERNEL MAPPING.
              
              /**
               * kunmap_atomic - unmap an atomic kernel mapping
               * @kvaddr:        kernel mapping virtual address
               * @type:          what kind of kernel fix mapping
               * # this function get @vaddr through "@kvaddr & PAGE_MASK",
               *   use @type to calculate @idx.
               *   if @vaddr != __fix_to_virt(FIX_KMAP_BEGIN + @idx)
               *   => T
               *           BUG_ON(@vaddr < PAGE_OFFSET)
               *           BUG_ON(@vaddr >= (unsigned long)high_memory
               *   else
               *     call to kpte_clear_flush(@kmap_pte - @idx, @vaddr),
               *     clear and flush a page table entry which has used for
               *     atomic kernel mapping.
               *   enable page fault trap.
               */
              void kunmap_atomic(void *kvaddr, enum km_type type);

          noncontiguous memory allocations

      The Buddy System Algorithm :
        a well-known memory management problem -> "external fragmentation"
          frequent requests and releases of groups of contiguous page frames of different
          size may lead to a situation in which several small blocks of free page frames
          are "scattered" inside blocks of allocated page frames.
          as a result,it may become impossible to allocate a large block of contiguous
          page frames even if there are enough free pages to satisfy the request.

        the ways to avoid "external fragmentation" :
          1> use the paging circuitry to map groups of noncontiguous free page frames into
             intervals of contiguous linear addresses.
          2> develop a suitable technique to keep track of the existing blocks of free
             contiguous page frames,avoiding as much as possible the need to split up
             a large free block to satisfy a request for a smaller one.

          /**
           * Linux Kernel prefer to 2>
           * reasons :
           *   1> in some cases,contiguous page frames are really necessay,because contiguous
           *      linear addresses are not sufficient to satisfy the request.(i.e. DMA)
           *                                                                 # it dont care paging
           *   2> even if contiguous page frame allocation is not strictly necessary,it offers the
           *      big advantage of leaving the kernel paging tables unchanged.(TLB cache)
           *   3> large chunks of contiguous physical memory can be accessed by the kernel through
           *      4MB pages.(PAE support)
           *      # reduces TLB misses
           */
        
        Buddy System algorithm :
          all free page frames are grouped into 11 lists of blocks that contain groups of
            1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024
          contiguous page frames,respectively.
          the physical address of the first page frame of a block is multiple of the group size.
          /* e.g. list-16-block : 16 * 2^12(4096,regular page size) */
          /* 1024 => 4MB chunk of contiguous RAM */
          /**
           * contiguous RAM request :
           *   require a group of X(above) contiguous page frames
           *
           * buddy system :
           *   try to find a free block in list-X
           *   if found out
           *           split this block if necessay
           *           allocate the free block
           *           split and insert remaining blocks(if existed) to corresponding list
           *   else
           *           goto next level and retry(R := X, X *= 2)
           *
           *   if all retry have failed 
           *           failed to allocate page frames         
           */
         
          releasing :
            kernel attempt to merge pairs of free buddy blocks of size B together into a single
            block of size 2B.
            two blocks are considered buddies if :
              > both blocks have the same size B
              > they are located in contiguous physical addresses
              > the physical address of the first page frame of the first block is a multiple
                2 * B * 2^12
                /* e.g. 16 * 2^12 <-> 2 * 8 * 2^12 */

              /* this algorithm is iterative : B => 2B => 4B => 8B => ... */

        Buddy System data structures :
          Linux 2.6 use a different buddy system for each zone.
          so,in 80x86 architecture,there are three kinds of buddy systems :
            > one for ISA DMA
            > one for NORMAL
            > one for HIGH

          each buddy system relies on the following main data structures :
            @mem_map array has introduced previously,each @zone is concerned with
            a subset of @mem_map elements
            /* represented by @zone.zone_start_pfn AND @zone.spanned_pages */

            member @free_area is an array of structure @free_area
            <linux/mmzone.h>
              #ifndef CONFIG_FORCE_MAX_ZONEORDER
              #define MAX_ORDER  11
              #else
              #define MAX_ORDER  CONFIG_FORCE_MAX_ZONEORDER
              #endif
              
              #define MIGRATE_UNMOVABLE  0
              #define MIGRATE_RECLAIMABLE  1
              #define MIGRATE_MOVABLE  2
              #define MIGRATE_PCPTYPES  3  /* the number of types on pcp(per-CPU pages) lists */
              #define MIGRATE_RESERVE  3
              #define MIGRATE_ISOLATE  4  /* can not allocate from here */
              #define MIGRATE_TYPES  5

              /* suppose "k" is the idx of @free_area */
              /**
               * free_area - structure free_area represent a free area,and the
               *             free area is composed by 5 kinds of groups of
               *             page frames each group in different status
               * @free_list: linker array
               *             each element represents a linker which collects the
               *             page descriptors each has the same migrate type and
               *             associated with the free blocks of 2^k pages
               *             # the pointers to the adjacent elements in the list
               *               are stored in the @lru field of page descriptor
               *               if the page is not free,then @lru represent other
               *               meaning
               * @nr_free:   number free blocks of size 2^k pages
               */
              struct free_area {
                      struct list_head free_list[MIGRATE_TYPES];
                      unsigned long nr_free;
              };

              struct zone {
                      ...
                      struct free_area free_area[MAX_ORDER];  /* 11 lists of blocks */
                      /* contiguous page frames 2^0 --> 2^10 */
                      ...
              };

        Allocating a block :
          static routine __rmqueue() is used to remove an element from the buddy allocator,
          must call to this routine within struct zone.lock acquired.

          <mm/page_alloc.c>
            /**
             * __rmqueue - remove an element from the buddy allocator and then return it
             * @zone:      the zone to find a page
             * @order:     order for @free_area
             * @migratetype:  migrate type for the page to find
             * return:     NULL => failed to find such page
             *             pointer to the page descriptor which is the first page frame
             */
            static struct page *__rmqueue(struct zone *zone, unsigned int order,
                                          int migratetype);

            what __rmqueue() does :
              1> use __rmqueue_smallest() to find an available smallest free page
                 from the freelists.
                 /* __rmqueue_smallest() might fallback list-X to list-2X list-4X ... */
              2> if @page unfound AND @migratetype is not equal to MIGRATE_RESERVE
                 then
                         call to __rmqueue_fallback() try to find appropriate page frames
                         /**
                          * migrate type fall back and search again.
                          */
    
                         if unfound
                         then
                                 set @migratetype to MIGRATE_RESERVE
                                 /**
                                  * @migratetype := MIGRATE_RESERVE,it means that
                                  * only once fallback is be wanted.
                                  */
                                 goto 1>
              3> deal with buddy system tracing.
              4> return @page.

            what __rmqueue_smallest() does :
              /* it is a static inline function and defined in the same file */

              1> start a for-cycle ready to traverse the rest of freelists in this
                 @zone from @order.

                 for (@current_order = order; @current_order < MAX_ORDER; ++@current_order) {
                         ...
                 }
              2> for-cycle :
                 in cycle,all NULL free area will be skipped.
                 @page is get through list_entry() with parameters
                   @free_list[@migratetype].next
                   struct page
                   lru
                 => struct page { lru := list->next }
              3> for-cycle :
                 delete @page from @page->lru
                 clear page buddy for this @page
                 set @page->private to 0
                 decrease @nr_free of the @free_area
              4> for-cycle :
                 expand @page {
                        low is @order
                        high is @current_order
                        area is the free area where @page frame get from
                        /**
                         * notice,the @area past to expand() is the address
                         * of the @free_area[idx page frame come from],thus
                         * @area points to the element of @free_area[] is the
                         * last @free_area object between [0, this area].
                         *                                # list-0 list-X
                         */
                        @size initialized to 1 << high,it means the sizeof
                        an element in list-X contains @size page frames(contiguous)
                        
                        until high <= low
                          area--  /* not it points to previous element */
                          high--
                          @size >>= 1

                          add @page[@size].lru to @area.free_list[@migratetype]
                          @area.nr_free++
                          set page order for @page[size] to high

                        this procedure subdivide the page frames start at
                        @page to two parts,each size is half of current order,
                        and add the second part to the free area at order is
                        "current order - 1",set @private member of the first
                        page frame of second part to the order.

                        co->current order
                        @page0 @page1 @page2 ... @pageN (all 2^co page frames)
                        
                        each has 2^(co - 1) page frames
                        part1 : @page0 @page1 ... @page(2^(co - 1) - 1)
                        part2 : @page(2^(co - 1)) @page(2^(co - 1) + 1) ... @pageN

                        ca->current area
                                          /* the previous free area */
                        insert part2 into (--ca)->free_list[@migratetype]
                        ++ca->nr_free  /* a new element has inserted */
                        
                        set first_of(part2)->@private := co
                 }

              !! if for-cycle finished but no appropriate page frame is found out,
                 function return NULL.

            what __rmqueue_fallback() does :
              /**
               * if __rmqueue_fallback() was called by __rmqueue(),it must be
               * __rmqueue_smallest() was failed,it turns out free area between
               * @order to MAX_ORDER,no appropriate page frames.
               */

              /**
               * fallbacks - two dimensional array used to represents fallback lists for
               *             different migrate type
               *             function __rmqueue_fallback() use this array try to find out
               *             appropriate page frames
               */
              static int fallbacks[MIGRATE_TYPES][MIGRATE_TYPES - 1] = {
                      [MIGRATE_UNMOVABLE] = { MIGRATE_RECLAIMABLE, MIGRATE_MOVABLE, MIGRATE_RESERVE },
                      [MIGRATE_RECLAIMABLE] = { MIGRATE_UNMOVABLE, MIGRATE_MOVABLE, MIGRATE_RESERVE },
                      [MIGRATE_MOVABLE] = { MIGRATE_RECLAIMABLE, MIGRATE_UNMOVABLE, MIGRATE_RESERVE },
                      [MIGRATE_RESERVE] = { MIGRATE_RESERVE, MIGRATE_RESERVE, MIGRATE_RESERVE },
              };

              @fallbacks is used when desired block of page frames with @migratetype is not existed,
              in this case,try to traverse fallback list to find out such block.request for @migratetype
              can not be satisfied,then we can try to use a substituted migrate type to instead this
              @migratetype.

              what __rmqueue_fallback() does is use another migrate type from @fallbacks as substitution
              to traverse free areas from MAX_ORDER - 1 to @order try to find out required page frames.

              if breaking a large block of pages,move all free pages to the preferred allocation list.
              if falling back for a reclaimable kernel allocation,be more aggressive about taking
              ownership of free pages.(or @page_group_by_mobility_disabled is TRUE)
              /**
               * move freepages block,the freepages will be unlinked from its original LRU and 
               * insert to @free_list[@migratetype] again.(current migrate type fall backed)
               * move_freepages_block() also deal with page alignment.
               * if necessary,modify migrate type of these pages to @migratetype.
               */

              /* of course,expand() is invoked as same as __rmqueue_smallest() called */

        Freeing a block :
          !! LINUX 2.6.34.1 HAVE NOT FUNCTION __free_pages_bulk().
             FUNCTION WITH SUFFIX "_bulk" FOR FREE PAGES IS THE ONLY ONE __free_pcppages_bulk().
             THAT IS,FREE PAGES HAVE TO BE HANDLED BY __free_pages().

          As described above,struct page.@flags contains the information about what memory zone
          this page frame is belong to.

          <mm/page_alloc.c>
            /**
             * free_hot_cold_page - free a hot page or a cold page
             * @page:               page frame descriptor
             * @cold:               indicator for whether @page is cold or hot
             * # this function is invoked by __free_pages() when @order is given 0,
             *   that means only one page frame have to free.
             */
            void free_hot_cold_page(struct page *page, int cold);
          
            what free_hot_cold_page() does :
              1> get memory zone descriptor @zone from @page.

                 call to kmemcheck_free_shadow() to handle kernel memory check,
                 if @page->shadow is not NULL,that means this page is tracked,
                 in this case,@shadow points a status block and status for each
                 bytes in this page frame,and kmemcheck_free_shadow() will free the
                 page frames used for kernel memory check.(it get page descriptor
                 from @page->shadow)

                 if @page is mapped in an anonymous way,then set @page->mapping to
                 NULL.(check via PageAnon().if page is PAGE_MAPPING_ANON,then @mapping
                 points to its anon_vma,not a address_space)

                 if @page failed on free_pages_check(),then return.
                 /**
                  * TRUE :
                  *   page_mapcount(page) OR
                  *   page->mapping != NULL OR
                  *   page->count != 0 OR
                  *   page->flags & PAGE_FLAGS_CHECK_AT_FREE
                  *   {
                  *           bad_page page
                  *           return 1
                  *   }
                  * Otherwise FLASE(and disable PAGE_FLAGS_CHECK_AT_PREP)
                  */

                 if @page is not high-memory,then does debug checking.

              2> call to architecture free-page to does some especial works.
                 call to kernel_map_pages() to does page-alloc debug.

                 set @page->private to @migratetype
                 disable local interrupt
                 count virtual memory event with type PGFREE

              3> deal with unmovable,reclaimable,movable on pcp lists only.
                 free ISOLATE pages back to the allocator because they are 
                 being offlined but treat RESERVE as movable pages,get them
                 back if necessary.

                 check if @migratetype == MIGRATE_ISOLATE
                 T =>
                         free one page with arguments (@zone, @page, 0, @migratetype)
                         enable local interrupt
                         return to caller

              4> if @migratetype is not MIGRATE_ISOLATE
                 then
                         get @pcp lists of this CPU

                 /* Memory Page Frame LRU cold/hot cache */
                 check if @could is TRUE
                 T =>
                         add @page->lru to the tail of @pcp->lists[@migratetype]
                 F =>
                         add @page->lru to the head of @pcp->lists[@migratetype]

                 increase counter of @pcp
                 
                 check if @pcp->count is greater than or equal to @pcp->high
                 /**
                  * per-CPU page cache watermark.
                  */
                 T =>
                         free pcppages bulk (@zone, @pcp->batch, @pcp)
                         /**
                          * free_pcppages_bulk() finally call to __free_one_page() to
                          * release page frames to per-CPU page frame cache.
                          */
                         @pcp->count -= @pcp->batch
              5> enable local interrupt
                 return to caller

            /**
             * __free_pages_ok - free page frames 
             * @page:            page frame
             * @order:           order for the block of @page frames
             * # this function is invoked by __free_pages() when @order
             *   is greater than 0
             */
            static void __free_pages_ok(struct page *page, unsigned int order);

            what __free_pages_ok() does :
              1> similart to __free_hot_cold_page(),it starts free event tracing,
                 and does kernel memory checking.

              2> for each page frame in the block,call free_pages_check() on them,
                 if any bad page frame was detected
                 then
                         return to caller
                         /* bad page frame detected,we can not free it to buddy system */

              3> if @page is not in high-memory,then does debug checking.
              4> call to architectur depened free-page primitive to does some
                 especial works.
              5> call to kernel_map_pages() to does page-alloc debug.
              6> disable local interrupt
                 count virtual memory events with type PGFREE,amount of page frames is
                 "1 << @order"
              7> call to free_one_page() to release the block of page frames
              8> enable local interrupt
                 returb to caller

            !! both free_hot_cold_page() and __free_pages_ok() are called to
                       __TestClearPageMlocked(@page);
               but i did not find out such macro is defined in kernel,and i suppose
               that macro is added into kernel only USE_SPLIT_PTLOCKS defined.
               and if @page is Mlocked most recently,then have to free page mlock
               before free page.

            /**
             * __free_one_page - release allocated page frames and do block merging
             * @page:            the page frame
             * @zone:            which memory zone
             * @order:           what is the order of free area of @zone
             * @migratetype:     migrate type
             * # this function is the main page frame releasing routine,it have to
             *   take charge of buddy system to merge blocks if necessary.
             * # free one page,but is able to free more than one page frames. :)
             */
            static inline void __free_one_page(struct page *page, struct zone *zone,
                                               unsigned int order, int migratetype);

            what __free_one_page() does :
              1> check if @page is compound page
                 T =>
                         check if any page frame is bad in this compound page
                         T =>
                                 /**
                                  * number of page frames in this compound page 
                                  *         1 << @order
                                  *         if compound order of @page is different to @order,OR
                                  *         @page->flags & PG_head => TRUE
                                  *         then,this @page is a bad page.
                                  * check all page frames in this compound page,if any page with
                                  * PageTail() => FALSE OR @first_page != @page
                                  * then it also a bad page.
                                  */
                                 just return to caller
              2> get @page_idx through "pfn of this @page & ((1 << MAX_ORDER) - 1)"
                                                              # 2047 => 01111111111
                                       /* pfn - page frame number */
              3> start a while-cycle until @order >= (MAX_ORDER - 1 := 10)
                 /**
                  * dont handle @order-10,because list-10 is the last list-X
                  * dont handle order < @order,because merge smallers to a large
                  */
                 in the while-cycle,it have to find the buddy for this @page,it the buddy is
                 existed,then merge them,iterative to fallback lists.
              4> while-cycle {
                        call to __page_find_buddy() with @page, @page_idx, @order.
                        the page descriptor of its buddy is calculated from the expression
                                @page + ((@page_idx ^ (1 << order)) - @page_idx)
                                /**
                                 * memory mapping for virtual address is linear,all page frame
                                 * descriptors are saved in @mem_map.
                                 */
                        check if @buddy is @page's buddy through page_is_buddy() with
                        @page, @buddy, @order
                               @buddy is the real buddy if only if PG_buddy flag is set in
                               @buddy->flags AND page order of @buddy is equal to @order
                        if no buddy is found,then break cycle,because there is no more merging
                        have to be done.

                        update merging informations :
                          list_del @buddy from its LRU(@it inserted by expand() formerly)
                          decrease @nr_free of free area has @order
                          remove @buddy's order
                          calculate combined index through __find_combined_index(@page_idx, @order)
                                  @page_idx & ~(1 << @order)
                          /* combined_idx - index of page frame after it merged to its buddy */
                          update @page to @page + (@combined_idx - @page_idx)
                          /* page frame address for merged page frames(@page and its buddy) */

                          update @page_idx to @combined_idx
                          @order++
                 }
              5> call to set_page_order() to set @page's order to @order,value of @order is the
                 last while-cycle where no buddy of @page is found.
                 /* the order is stored in @private member of the page descriptor */
                 list add @page->lru to @zone->free_area[@order].free_list[@migratetype]
                 update @nr_free

        What is Buddy :

          list-X
                    (same size)
            page's buddy        page
            page                page'buddy
          +--------------------------------+
          |              |                 |
          |              |                 |
          |              |                 |
          +--------------------------------+
          <----------- 2^X ---------------->
          <- 2^(X - 1) ->   <- 2^(X - 1) ->

          allocate sizeof(@page) from list-x
          but the size of block in list-X is sizeof(@page) * 2
          thus,split it
          now,there are two parts,one is @page,another is its buddy
          before allocating,@page and its buddy should be placed together in list-X
          but memory request have came and list-(X-1) did not satisfy the request
          so @page have to get apart with its buddy :(
          finally,@page is allocated to a kernel control path,and its buddy been
          expanded to list-(X-1),the @buddy waiting for its buddy come back
          ... /* more memory request similar to this */

          free @page,release it now
          because,order of @page is log_2(sizeof(@page)),it has the same size with
          its buddy,so it will be placed inside list-(X-1)
          and @page's buddy is expanded to list-(X-1),too

          @page try to find out its buddy in list-(X-1)

          if its buddy is existed(have not been required by other kernel control path)
          then @page and its buddy can be merged,they can be there together again
          but list-(X-1) is too small for them,thus they must move to list-X
          if there also a buddy in list-X,can do the same

          if its buddy is not existed,that means no more buddy have to meet,no more
          page frame merging have to be done
          /**
           * this just for current page frames,because of their buddy is not existed 
           * is maybe @buddy required by other kernel control path.
           * after @buddy come back,it will must be go to meet these page frames as
           * soon as possible. :)
           */

      The Per-CPU Page Frame Cache :
        the kernel often requests and releases single page frames,to boost system performance,
        each memory zone has a per-CPU page frame cache.
        the cache includes some pre-allocated page frames to be used for single memory requests
        issued by the local CPU.
        there are two caches for each memory zone and for each CPU :
          hot cache
            stores page frames whose contents are likely to be included in the CPU's hard cache
          cold cache
            stores page frames whose contents are not accessed in a long time

        in face,every access to a memory cell of the page frame will result in a line of the
        hardware cache being "stolen" from another page frame,unless the hardware cache already
        includes a line that maps the cell of the "hot" page frame just accessed.

        taking a page frame from the "cold" cache is convenient if the page frame is going to
        be filled with a DMA operation. /* the CPU is not involved and no line will be modified */

        structure per_cpu_pageset :
          <linux/mmzone.h>
            /**
             * per_cpu_pages - per-CPU pre-allocated pages
             * @count:         number of pages in the list
             * @high:          high watermark,emptying needed
             *                 cache depletion watermark
             * @batch:         chunk size for buddy add/remove
             *                 number of page frames
             * @lists:         lists of pages,one per migrate type stored on the pcp-lists
             *                 "cold" page frame from @list->prev,and "hot" page frame from
             *                 @list->next
             */
            struct per_cpu_pages {
                    int count;
                    int high;
                    int batch;
                    struct list_head lists[MIGRATE_PCPTYPES];
            };

            /**
             * per_cpu_pageset - structure per_cpu_pageset represents the set of per-CPU
             *                   page frame caches
             * @pcp:             pages pre-allocated for this CPU
             * @expire:          NUMA expire
             * @stat_threshold:  threshold defines how large a memory zone is
             * @vm_stat_diff:    zone statistics
             */
            struct per_cpu_pageset {
                    struct per_cpu_pages pcp;
            #ifdef NUMA
                    s8 expire;
            #endif
            #ifdef CONFIG_SMP
                    s8 stat_threshold;
                    s8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];
            #endif        
            };

        !! MACRO "__percpu" IS DEFINED IN <linux/compiler.h>,MACRO EXPANDED TO
             "__attribute__((noderef, address_space(3)))"
           "noderef" AND "address_space" ARE NOT COMPILER ATTRIBUTES,THEY ARE "Sparse"
           ATTRIBUTES.
           AND "Sparse" IS USED TO CHECK LINUX KERNEL CODE ERROR.

        !! in <mm/page_alloc.c>,each CPU has a per-CPU variable is type of struct per_cpu_pageset
           with name @boot_pageset.
           before "void __init setup_per_cpu_pageset(void)" is called,only @boot_pageset is available,
           this function call to alloc_percpu() to allocate cpu cache for @zone->pageset to each
           populated zone.
           /* __init specifier,thus each CPU on this system will processes this function */
           static function setup_pageset() is called by __build_all_zonelists(),the function initializes
           @boot_pageset to zero,and setup its fields to default values.
           /**
            * function setup_pageset() also will be called by setup_per_cpu_pageset(),setup_per_cpu_pageset()
            * allocate cpu cache for @zone->pageset,and setup_pageset() initializes them.
            * values of @batch and @high essentially depend on the number of page frames included in
            * the memory zone.
            * function setup_pageset() set @high to 6 * batch,and set @batch to max(1UL, 1 * batch)
            */

        it is interpreted to an array and each element corresponding to a CPU in the system.
        actually,an element consists of two per_cpu_pageset object,one for hot cache,another for
        cold cache.

        /* LINUX 2.6.34.1 ONLY ONE WATERMARK */
        kernel monitors the watermark @high.
        if number of page frames in the cache is arrived "high",then kernel release
        @batch page frames to buddy system.


        Allocating page frames through the per-CPU page frame caches :
          function buffered_rmqueue() allocates page frames in a given memory zone.it makes use of
          the per-CPU page frame caches to handle single page frame requests.

          <mm/page_alloc.c>
            /**
             * buffered_rmqueue - handle page frames request from pre-allocated cache
             * @preferred_zone:   used for NUMA,used as an argument of zone_statistics()
             *                    if @preferred_zone->zone_pgdat == @zone->zone_pgdat => NUMA_HIT
             *                    ...
             * @zone:             the zone used for this allocation
             * @order:            how many page frames to be allocated at this allocation
             * @gfp_flags:        allocator flags
             * @migratetype:      page frame migrate type
             * return:            page descriptor pointer
             *                    NULL => have fault
             */     
            static inline
            struct page *buffered_rmqueue(struct zone *preferred_zone, struct zone *zone, int order,
                                          gfp_t gfp_flags, int migratetype);

            what buffered_rmqueue() does :
              again:
              1> if @order is zero,then only one page frame is required.
              2> disable local interrupt.
                 get @pcp from per-CPU pcp cache of this @zone.(this_cpu_ptr(zone->pageset)->pcp)
                 get page frame list with migrate type @migratetype.
              3> if page frame list is empty,that means no page frames is cached with @migratetype.
                 thus,call to rmqueue_bulk() to allocate page frames as cache,and update @pcp->count.
                                                         /* @batch is the size for this allocation */
                 check page frame list again,if it stay empty,then we failed to allocating,goto failed.
              4> if page frame list is not empty,then check if __GFP_COLD is enabled in @gfp_flags.
                 if "cold" is specified,that means allocation must operate on "cold" cache,
                 so get page frames from the tail of page frame list;otherwise,get page frames from
                 the head of page frame list.(list->next)
              5> due to some page frames is allocated for the request,delete it from its LRU list.
                 decrease @pcp->count.

              !! BECAUSE LINUX 2.6.34.1 NO FIELD "low" IN PER_CPU_PAGES,SO IF LIST IS EMPTY,THEN HAVE
                 TO ALLOCATE PAGE FRAMES AS PER-CPU CACHE.

              6> if @order is not zero,that is more than one page frames are required.
                 /**
                  * in this case,per-CPU cache can not be used for this allocation.
                  */
              7> if __GFP_NOFAIL is enabled,then print warning once if @order > 1.
                                                                       /* may be failed */
                 /**
                  * __GFP_NOFAIL is not to be used in new code!
                  */
              8> disable local interrupt.
                 get page frames through __rmqueue().
                 enable local interrupt.
              9> if no page frames is acquired,then goto failed.
                 otherwise,modify zone page state(@vm_stat_diff[NR_FREE_PAGES] + -(2^@order)).

              public path :
                count zone virtual memory PGALLOC events,number of events is 2^@order.
                call to zone_statistics() with @preferred_zone and @zone,which update NUMA statistics.
                enable local interrupt.
                
                check VM BUG,if @page from @zone in a bad range,then this is a BUG.

                call to prep_new_page() to set some field of the @page(first page frame) and
                process kernel memory checking.
                AND
                if __GFP_ZERO is enabled,have to initializes all byte in the pages to zero.
                if @order >= 1 AND __GFP_COMP is enabled,then have to prepare compound page.
                this function have to checks all page frames from this allocation,if any checking for
                a page frame is failed,function returns 1;otherwise return 0.
                
                if @prep_new_page() returned 1,jump to label "again" to process allocation again.

              failed:
                enable local interrupt
                return NULL

        Releasing page frames to the per-CPU page frame caches :
          function free_hot_cold_page() is used to free per-CPU page frame.
          this function will call to free_pcppages_bulk() to process the main
          works.

          <mm/page_alloc.c>
            /**
             * free_pcppages_bulk - free a bulk of per-CPU page frames
             * @zone:               memory zone of the cached page frame
             * @count:              how many page to free,in normally,it is @batch
             * @pcp:                the per_cpu_pageset
             * # this function will processes three of cycles,one for @count
             *   page frames,one for find out appropriate @pcp->lists,the last one for
             *   free 3one page frame
             */
            static void free_pcppages_bulk(struct zone *zone, int count,
                                           struct per_cpu_pages *pcp);

            what free_pcppages_bulk() does :
              /**
               * this function only be used when @pcp->count >= @pcp->high,that maximum
               * watermark is arrived at,so have to free the exceeded page frames to
               * buddy system.
               * if it is not arrived at watermark,then just release one page frame
               * and add it to the @pcp->lists[X] then increase @pcp->count as well.
               */
              1> acquire @zone->lock.
                 set @zone->all_unreclaimable and @zone->pages_scanned to zero.
                 modify zone statistics NR_FREE_PAGES to "origin_value += @batch".
              2> while-cycle until @coun == 0
              3> in while-cycle :
                   do-while cycle : until @list is not empty,that is find out a
                   list of page frames with appropriate @migratetype.
                   in each cycle stage,@batch_free will increase 1(initialized to zero)
                   whenever an empty list is encountered.
                   if @migratetype == MIGRATE_PCPTYPES,have to reset it to zero and
                   traverse @pcp->lists again.
              4> in while-cyle :
                   from step 3>,we have find out a list of page frames is not empty,now
                   have to handle page frames free on it.

                   do-while cycle : until @count == 0 OR @batch_free == 0 OR @list is empty
                   get page descriptor from the tail of LRU.
                   unlink this @page from LRU.
                   /* MIGRATE_MOVABLE list may include MIGRATE_RESERVEs */
                   call to __free_one_page(@page, @zone, 0, page_private(@page)) to free
                   this page frame.
                   trace memory page event for per-CPU drain.
              5> outside to while-cycle
                 release @zone->lock
                 return

      The Zone Allocator :
        the zone allocator is the frontend of the kernel page frame allocator.
        the goals that zone allocator must satisfy :
          > it should protect the pool of reserved page frames.
          > it should trigger the page frame reclaiming algorithm when memory is scarce and
             blocking the current process is allowed;once some page frames have been freed,the
             zone allocator will retry the allocation.
          > it should preserve the small,precious ZONE_DMA memory zone,if possible.

        the __alloc_pages() function :
          macro alloc_pages() ends up invoking alloc_pages_node() and which ends up invoking
          __alloc_pages().
          __alloc_pages() is a zone allocator,which is defined in <linux/gfp.h> as a static
          inline function.

          <linux/gfp.h>
            /**
             * __alloc_pages - zone allocator
             * @gfp_mask:      gfp flags
             * @order:         will allocate 2^@order page frames
             * @zonelist:      memory zone list used to find oud appropriate page frames
             * return:         page descriptor pointer of the first page
             *                 NULL => failed on allocation
             * # this function only call to __alloc_pages_nodemask() with NULL node mask
             */
            static inline
            struct page *__alloc_pages(gfp_t gfp_mask, unsigned int order,
                                       struct zonelist *zonelist);

          <mm/page_alloc.c>
            /**
             * __alloc_pages_nodemask - allocate page frames from the zones in a specified
             *                          NUMA node
             * @gfp_mask:               gfp flags
             * @order:                  2^@order pages to allocate
             * @zonelist:               memory zone list for page frames searching
             * @nodemask:               mask of NUMA nodes
             * return:                  page descriptor of the first page frame
             *                          NULL => failed on allocation
             * # this function is the "heart" of the zoned buddy allocator
             */
            struct page *__alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,
                                                struct zonelist *zonelist, nodemask_t *nodemask);

            what __alloc_pages_nodemask() does :
              1> get zone_type from @gfp_mask.
                 get migrate type from @gfp_mask through allocflags_to_migrate().
                 set @gfp_mask to the gfp allowed mask.
                 start kernel control path tracing for memory allocation,call to might_sleep_if()
                 if __GFP_WAIT is enabled.
              2> call to should_fail_alloc_page(),this will check @gfp_mask and @order whether
                 satisfied kernel resource requirement.
                 if the function returned TRUE,then __alloc_pages_nodemask() should returns
                 NULL immediately.
              3> check the zones suitable for the @gfp_mask contain at least one valid zone.
                 it is possible to have an empty zonelist as a result of GFP_THISNODE and a
                 memoryless node.
                 /**
                  * this will check if @zonelist->zonerefs->zone is FALSE,if it is,then return
                  * NULL.
                  */
              4> get first zones from zonelist and save the address to @preferred_zone(local var).
              5> if @preferred_zone is NULL,then return NULL.
              6> @preferred_zone is not NULL,then get page from free list.
                 this is done by function get_page_from_freelist(),call it with arguments

                   @gfp_mask | __GFP_HARDWALL, @nodemask, @order, @zonelist, @high_zoneidx(zone type),
                   ALLOC_WMARK_LOW | ALLOC_CPUSET, @preferred_zone, @migratetype
                        
                 /**
                  * get_page_from_freelist() traverse all zones in @zonelist try to allocate page frame.
                  */

              7> if @page is NULL,that means failed on the first allocation,then retry on slow path.
                 this is done by function __alloc_pages_slowpath() with arguments
                   @gfp_mask, @order, @zonelist, @high_zoneidx, @nodemask, @preferred_zone, @migratetype
              8> shutdown tracing
                 return @page

            /**
             * get_page_from_freelist - traverse memory zones try to find out required page frames
             * @gfp_mask:               GFP flags
             * @nodemask:               NUMA node mask
             * @order:                  exponent
             * @zonelist:               memory zone list
             * @high_zoneidx:           the zone type specified in GFP flags
             * @alloc_flags:            allocation flags
             * @preferred_zone:         preferred memory zone,generally it is the memory zone of local NUMA node
             * @migratetype:            page frame migrate type
             * return:                  page descriptor pointer
             *                          NULL => faild on allocation
             */
            static struct page *
            get_page_from_freelist(gfp_t gfp_mask, nodemask_t *nodemask, unsigned int order,
                                   struct zonelist *zonelist, int high_zoneidx, int alloc_flags,
                                   struct zone *preferred_zone, int migratetype);

            what get_page_from_freelist() does :
              1> set local variable @classzone_idx to @preferred_zone's idx
              zonelist_scan:
              2> ready to scan all memory zones in @zonelist.
                 for_each_zone_zonelist_nodemask : @zone @zoneref @zonelist @high_zoneidx @nodemask
                   /**
                    * allocation can not span several memory zones.
                    */
              3> check if NUMA_BUILD is TRUE AND
                          /* zonelist cache */
                          @zlc_active is TRUE AND
                                                                     /* local-var zonelist_cache
                                                                      * approximation */
                          zlc_zone_worth_trying(@zonelist, @zoneref, @allowednodes) is FALSE
                 then
                         /**
                          * zlc_zone_worth_trying() does two quickly checkings :
                          *   1> if the zone is not thought to be full
                          *   2> if the zone nodes is allowed
                          */
                         continue to next cycle
              4> check if @alloc_flags & ALLOC_CPUSET is TRUE AND
                          cpuset_zone_allowed_softwall(@zone, @gfp_mask) is FALSE
                 then
                          skip current zone and goto try_next_zone
              5> call to BUILD_BUG_ON(ALLOC_NO_WATERMARKS < NR_WMARK),it is a building-time macro
                 used for debug.
              6> check if @alloc_flags & ALLOC_NO_WATERMARKS is TRUE
                 then
                         get watermark for current @zone use the idx @alloc_flags & ALLOC_WMARK_MASK
                         check if this watermark is OK through function zone_watermark_ok()
                         T =>
                                 goto try_this_zone
                         check if @zone_reclaim_mode is zero
                         then
                                 goto this_zone_full,this @zone is full and can not reclaim page frames
                 call to zone_reclaim(@zone, @gfp_mask, @order) try to reclaim page frames.
                 if this function returned ZONE_RECLAIM_NOSCAN,then try next zone(goto try_next_zone),
                 because we try to reclaim page frames inside this @zone but did not scan them,thus do not
                 know how many page frames are available.
                 if this function returned ZONE_RECLAIM_FULL,then goto this_zone_full.even we want to reclaim
                 page frames,but there is nothing can be done.
                 other returned values,check if zone watermark is OK,it is not,then goto this_zone_full.
                 /**  
                  * zone_watermark_ok(@zone, @order, @mark, @classzone_idx, @alloc_flags)
                  *
                  * free pages is calculated via expression "zone_page_state(@zone, NR_FREE_PAGES) - 2^@order + 1"
                  * if free pages in this @zone is less than or equal to minimum watermark + low-memory reserve,
                  * then return 0,that is after allocated 2^@order page frames,the free pages would not satisfy
                  * memory zone minimum requirement.
                  * if more free pages are available,then scan free areas from order 0 to order @order - 1
                  * {
                  *         !! besides the page frames to be allocated,there are at least min / 2^k free page
                  *            frames in blocks of order at least k,for each k between 1 and the order of the
                  *            allocation.
                  *         free pages -= Nth free area's nr_free << N
                  *         minimum watermark /= 2 /* require fewer higher order pages to be free */
                  *         if free pages <= minimum watermark,then return 0
                  * }
                  *
                  * !! @mark as an argument passed to zone_watermark_ok(),and local variable @min in
                  *    zone_watermark_ok() is initialized to @mark.
                  *    if ALLOC_HIGH is enabled,then @min -= @min / 2       => __GFP_HIGHMEM
                  *    if ALLOC_HARDER is enabled,then @min -= @min / 4     => __GFP_WAIT OR (@current is RT)
                  */
              try_this_zone:
              7> call to buffered_rmqueue(@preferred_zone, @zone, @order, @gfp_mask, @migratetype).
                 if we succeed to acquire page frames,then stop traversing and return these page frames.
              this_zone_full:
              8> check if NUMA_BUILD is TRUE
                 then
                         /* mark zonelist cache for @zonelist that the zone has reference @zoneref is full */
                         call to zlc_mark_zone_full(@zonelist, @zoneref)
              try_next_zone:
              9> check if NUMA_BUILD is TRUE AND
                          @did_zlc_setup is FALSE AND
                          /* more than one NUMA nodes are online */
                          @nr_online_nodes > 1
                 then
                         set @allowednodes to the result of zlc_setup(@zonelist, @alloc_flags),to setup zonelist cache
                         set @zlc_active = 1 to indicate that zonelist cache is activated
                         set @did_zlc_setup = 1 to indicate that zonelist cache had been setup
              10> outside to for_each_zone_zonelist_nodemask
                  check if NUMA_BUILD is TRUE AND
                           NULL page descriptor is acquired AND
                           zonelist cache is activated
                  then
                          reset @zlc_active to zero and goto zonelist_scan to try again
              11> return acquired page descriptor

            /**
             * __alloc_pages_slowpath - allocate page frames from slowpath
             *                          the meaning of slowpath as its name,we maybe need to
             *                          waiting for memory reclaim or dirty page writeback .etc
             *                          until some page frames are available
             * @gfp_mask:               GFP flags
             * @order:                  exponent
             * @zonelist:               memory zones to scan
             * @high_zoneidx:           zone type specified by GFP flags
             * @nodemask:               NUMA node mask
             * @preferred_zone:         preferred memory zone
             * @migratetype:            migration type of page frames
             * return:                  page descriptor pointer
             *                          NULL => failed to allocation on slowpath
             * # __alloc_pages() call to this function when get_pages_from_freelist() was failed
             */
            static inline struct pages *
            __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order, struct zonelist *zonelist,
                                   enum zone_type high_zoneidx, nodemask_t *nodemask,
                                   struct zone *preferred_zone, int migratetype);

            what __alloc_pages_slowpath() does :
              1> check if @order is greater than or equal to MAX_ORDER
                 then
                         print operation warning if __GFP_NOWARN is not enabled
                         return NULL
              2> check if NUMA_BUILD is TRUE AND
                                              /* __GFP_THISNODE | __GFP_NORETRY | __GFP_NOWARN */
                          @gfp_mask & GFP_THISNODE == GFP_THISNODE
                 then
                         /**
                          * GFP_THISNODE should not cause reclaim since the subsystem(f.e. slab)
                          * using GFP_THISNODE may choose to trigger reclaim using a large set
                          * of nodes after it has established that the allowed per node queues
                          * are empty and that nodes are over allocated.
                          */
                         goto nopage
              restart:
              3> wake up all kswapd to writeback dirty page frames for recycle memory pages
              4> get @alloc_flags from @gfp_mask through gfp_to_alloc_flags.
              5> call to get_page_from_freelist() with arguments
                   @gfp_mask, @nodemask, @order, @zonelist, @high_zoneidx,
                   @alloc_flags disabled ALLOC_NO_WATERMARKS, @preferred_zone, @migratetype
                 because kswapds in this system had been woken up,if we are luck maybe there
                 are some free page frames are available now
              6> if page descriptor is not NULL,then goto got_pg.
              rebalance:
              /**
               * allocate without watermarks if the context allows.
               * GFP_ATOMIC
               */
              7> check if @alloc_flags enabled ALLOC_NO_WATERMARKS
                 then
                         try to get page frames through __alloc_pages_high_priority()
                         if page descriptor is not NULL,then goto got_pg
              8> if @gfp_mask did not specify __GFP_WAIT,then goto nopage,because we are no times
                 to waste for waiting page frames available.
              9> if @current enabled PF_MEMALLOC,then goto nopage,this is for avoid recursion of direct
                 reclaim.
                 if current task is doing memory allocating,then should not do it again.
              10> check if thread flag TIF_MEMDIE is TRUE AND
                           @gfp_mask disabled __GFP_NOFAIL
                  then
                          /**
                           * TIF_MEMDIE,this thread is recycling memory now,but memory allocation is
                           * allowed to be fail,then we just report no page frame is available as well
                           */
                          goto nopage
              11> get page frames from __alloc_pages_direct_reclaim(),this function try reclaim memory
                  pages directly and then try to allocate page frames.
                  /**
                   * address of local variable @did_some_progress is passed to this function.
                   *
                   * __alloc_pages_direct_reclaim() might cause this kernel control path to sleep.
                   * it call to cond_resched() to check whether some other process needs the CPU.
                   *            # cond_resched() not really sched this task,it will check if should resched
                   *              at first.
                   *              TIF_NEED_RESCHED is enabled and preempt_counter() & PREEMPT_ACTIVE is F
                   *                                              it represents a preemption counter
                   *
                   * enabled PF_MEMALLOC in process flags,set @current->reclaim_state->reclaimed_slab = 0.
                   * call to try_to_free_pages(),and use its return value to set *@did_some_progress.
                   * after this process is woken up,it clear process @reclaim_state,and disable PF_MEMALLOC,
                   * call to cond_resched() again.
                   * check if @order is not equal to zero
                   * T =>
                   *         call to drain_all_pages()
                   * check if @did_some_progess is TRUE
                   * T =>
                   *         call to get_page_from_freelist() try allocate memory again
                   * finally,return a page descriptor pointer.if it got some pages,then the pointer points to
                   * the first page frame of block of page frames,otherwise,it equal to NULL.
                   */

                  if have got page frames,then goto got_pg
                  /**
                   * if some page frames are reclaimed,then @did_some_progress will records the number
                   * of reclaimed page frames by __alloc_pages_direct_reclaim().
                   */
              12> we failed to get page frames in 11> through __alloc_pages_direct_reclaim(),
                  that means we are running out of options and have to consider going OOM.
                  check if @did_some_progress is zero
                  then
                          check if @gfp_mask enabled __GFP_FS AND
                                   @gfp_mask disabled __GFP_NORETRY
                          then
                                  check if @oom_killer_disabled
                                  then
                                          /**
                                           * if out-of-memory killer is disabled,then we can not
                                           * free memory by the way kill some "best" processes.
                                           */
                                          goto nopage
                                                         /* allocate pages may out-of-memory */
                          try to get page frames through __alloc_pages_may_oom()
                                                         /**
                                                          * this function may will call to out_of_memory()
                                                          * try to kill some processes for prevent
                                                          * the situation that no memory is available.
                                                          */
                          /**
                           * before call to out_of_memory(),__alloc_pages_may_oom() will try scannin
                           * freelist again with very high watermark.
                           * this is only to catch a parallel oom killing,we must fail if we are 
                           * still under heavy pressure.
                           */

                          if have got page frames,then goto got_pg.
                          check if @order > PAGE_ALLOC_COSTLY_ORDER ADN
                                   @gfp_mask disabled __GFP_NOFAIL
                          then
                                  goto nopage,OOM,just report no page frames are available as well

                          goto restart,because @order is satisied requirement and __GFP_NOFAIL is
                          specified,thus we have to try again.

              13> update @pages_reclaimed += @did_some_progress for check if we should retry the allocation.
                  check if should_alloc_retry(@gfp_mask, @order, @pages_reclaimed) is TRUE
                  then
                          /* wait for some write requests to complete then retry */
                          /* these write requests ma submitted by kswapd process */
                          call to congestion_wait(BLK_RW_ASYNC, HZ / 50) to wait a moment
                          goto rebalance
              nopage:
              14> no page frames are available.
                  check if @gfp_mask disabled __GFP_NOWARN AND
                           printk_ratelimit() is TRUE
                  then
                          print kernel message KERN_WARNING about memory allocation failed
                          dump kernel calling stack
                          print memory figure
                  return page descriptor,in this case,it must be NULL
              got_pg:
              15> check if kernel memory checking is enabled
                  then
                          do kernel memory checking for page allocation through 
                          kmemcheck_pagealloc_alloc(@page, @order, @gfp_mask)
                  return page descriptor,we finally got some page frames.

        releasing a group of page frames :
          all kernel macros and functions that release page frames rely on the __free_pages() function,
          and which function will call to free_hot_cold_page() if logarithmic size of page frames is zero,
          call to __free_pages_ok() if logarithmic size of page frames is greater than zero.

      Memory Area Management :
        memory area,that is sequence of memory cells having contiguous physical addresses and an arbitrary length.

        the Buddy System adopts the page frame as the basic memory area,it works fine if the memory request always
        be N page frames.but if the memory request require N memory cells(each cell 8bits) and N is not a multiple
        of page size,then this will let Buddy System try to allocate 0.x page frame or x.y page frames.of course,
        Buddy System can not do that.(quite wasteful)

        the solution is introduce a new data structure that describe how small memory areas are allocated within
        the same page frame.
        a new problem will be introduced - internal fragmentation :
          it is caused by a mismatch between the size of the memory request and the size of the memory area allocated
          to satisfy the request.
        a classical solution(earyly Linux versions) :
          memory areas whose sizes are geometrically distributed,that is the size depends on a power of 2 rather
          than on the size of data to be stored.
          /**
           * in this way,the internal fragmentation is always smaller than 50%.
           * kernel creates 13 geometrically distributed lists of free memory areas whose sizes range from
           *         [32, 131072] (bytes)
           * Buddy System is invoked for obtain additional page frames needed to store new memory areas,and
           * for to release page frames that no longer contain memory areas.
           * # a dynamic list is used to keep track of the free memory areas contained in each page frame.
           */

      The Slab Allocator :
        running a memory area allocation algorithm on top of the buddy system is not particularly efficient,
        a bettery algorithm is derived from the "slab allocator" schema that was adopted for the first time
        in the Sun Microsystems Solaris 2.4.
        "slab allocator" based on the following premises :
          > the type of data to be stored may affect how memory areas are allocated.
            /**
             * slab allocator views memory areas as objects consisting of both a set of data structures and
             * a couple of functions or methods called the 'constructor' and 'destructor'.
             * to avoid initializing objects repeatedly,the slab allocator does not discard the objects that
             * have been allocated and then released,but instead saves them in memory.
             * the later request for such object can be taken from memory without having to be reinitialized.
             */
          > the kernel functions tend to request memory areas of the same type repeatedly.
            /* slab allocator allows the data object to be saved in a cache and reused quickly. */
          > requests for memory areas can be classified according to their frequency.
            /**
             * requests of a particular size that are expected to occur frequently can be handled most efficiently
             * by creating a set of special-purpose objects that have the right size,thus avoiding internal fragmentation.
             */
          > there is another subtle bonus in introduing objects whose sizes are not geometrically distributed :
              the initial addresses of the data structures are less prone to be concentrated on physical
              addresses whose values are a power of 2
          > hardware cache performance creates an additional reason for limiting calls to the buddy system allocator
            as much as possible.
            /**
             * every call to a buddy system function "dirties" the hardware cache,thus increasing the average
             * memory access time.
             */

        slab allocator groups objects into caches,each cache is a "store" of objects of the same type.
        the area of main memory contains a cache is divided into slabs,and each slab consists of one or more
        contiguous page frames that contain both allocated and free objects.

          [MAIN MEMORY AREA] => {
                  /* struct kmem_cache */
                  [CACHE] => {
                          /* struct kmem_list3 */
                          [KMEM LIST3] => {
                                  [SLABS_PARTIAL] => {
                                          /* struct slab */
                                          [SLAB] => { PG1 PG2 ... PGn }
                                                      |
                                                      +--> { allocated objects | free objects }
                                                             /*         SAME TYPE          */
                                          [SLAB] => { PG }
                                                      ...
                                          ...
                                  }
                                  [SLABS_FULL] => {
                                          [SLAB]
                                          [SLAB]
                                          ...
                                  }
                                  [SLABS_FREE] => {
                                          [SLAB]
                                          [SLAB]
                                          ..
                                  }
                          }
                  }
          }

          !! the basic storage cell is memory unit.and an area of main memory is grouped of some memory cells.
             if the main memory area has a slab cache,then it must saved some objects of the same type T.
             and the cache is divided to several slab nodes,every slab node stored some objects have type
             T.

          /* kernel scan to caches periodically and release the page frames corresponding to empty slabs */

        Slab Flags :
          <linux/slab.h>
            /* flags to pass to kmem_cache_create() */
                                                        /* CONFIG_SLAB_DEBUG */
            #define SLAB_DEBUG_FREE  0x00000100UL       /* perform (expensive) checks on free */
            #define SLAB_RED_ZONE    0x00000400UL       /* red zone objs in a cache */
            #define SLAB_POISON      0x00000800UL       /* poison objects */
            #define SLAB_STORE_USER     0x00010000UL    /* store the last owner for bug hunting */
                                                        /* END OF CONFIG_SLAB_DEBUG */

            #define SLAB_HWCACHE_ALIGN  0x00002000UL    /* align objs on cache lines */
            #define SLAB_CACHE_DMA      0x00004000UL    /* use GFP_DMA memory */
            #define SLAB_PANIC          0x00040000UL    /* panic if kmem_cache_create() fails */

            #define SLAB_DESTROY_BY_RCU  0x00080000UL   /* defer freeing slabs to RCU */
            #define SLAB_MEM_SPREAD      0x00100000UL   /* spread some memory over cpuset */
            #define SLAB_TRACE           0x00200000UL   /* trace allocations and frees */

            #ifdef CONFIG_DEBUG_OBJECTS
            #define SLAB_DEBUG_OBJECTS   0x00400000UL   /* prevent checks on free */
            #else
            #define SLAB_DEBUG_OBJECTS   0x00000000UL
            #endif

            #define SLAB_NOLEAKTRACE     0x00800000UL   /* avoid kmemleak tracing */

            #ifdef CONFIG_KMEMCHECK
            #define SLAB_NOTRACK         0x01000000UL   /* do not track use of uninitialized memory */
            #else
            #define SLAB_NOTRACK         0x00000000UL
            #endif
            
            #ifdef CONFIG_FAILSLAB
            #define SLAB_FAILSLAB        0x02000000UL   /* fault injection mark */
            #else
            #define SLAB_FAILSLAB        0x00000000UL
            #endif

            /* flags effect the page allocator grouping pages by mobility */
            #define SLAB_RECLAIM_ACCOUNT 0x00020000UL   /* objects are reclaimable */
            #define SLAB_TEMPORARY       SLAB_RECLAIM_ACCOUNT  /* objects are short-lived */

          about SLAB RCU :
            SLAB_DESTROY_BY_RCU delays freeing the SLAB page by a grace period,it does not delay
            object freeing.this is means that if we do kmem_cache_free() that memory location is free
            to be reused at any time.thus it may be possible to see another object there in the same
            RCU grace period.
            this feature only ensures the memory location backing the object stays valid,the trick to
            using this is relying on an independent object validation pass.

        Cache Descriptor :
          each cache is described by the kernel data structure "kmem_cache" which is defined in <linux/slab_def.h>.
          
          <linux/slab_def.h>
            /**
             * kmem_cache - structure represents a kernel memory cache for slab allocator
             * @array:      per-CPU array of pointers to local caches of free objects
             * @batchcount: number of objects to be transferred in bulk to or from the local caches
             * @limit:      maximum number of free objects in the local caches,tunable
             * @shared:     cache sharing
             * @buffer_size: cache buffer size,it is the size of object in this slab cache
             * @@reciprocal_buffer_size: reciprocal buffer size used to calculate index of an object
             *                           in a slab of this slab cache
             * @flags:      slab flags
             * @num:        objects per slab
             * @gfporder:   order of page frames per slab
             * @gfpflags:   force GFP flags
             * @colour:     number of colors for the slabs
             * @colour_off: basic alignment offset in the slabs,
             * @slabp_cache: pointer to the general slab cache containing the slab descriptors
             *               NULL => internal slab descriptors are used
             * @slab_size:  the size of a single slab,it is equal to
             *                      sizeof(struct slab) + @num * sizeof(kmem_bufctl_t)
             * @dflags:     set of flags that describe dynamic properties of the cache
             * @ctor:       pointer to constructor method associated with the cache
             * @name:       name of the cache
             * @next:       next cache descriptor
             * @nodelists:  array of pointers are type of "struct kmem_list3",each element
             *              is a slab list object for all objects,and each slab list as
             *              a node of the slab cache
             *              # "struct kmem_list3" is defined in <mm/slab.c>
             * # omitted statistics fields and debug fields
             */
            struct kmem_cache {
            /* per-CPU data, touched during every alloc/free */
                    struct array_cache *array[NR_CPUS];

            /* Cache tunables.Protected by @cache_chain_mutex */
                    unsigned int batchcount;
                    unsigned int limit;
                    unsigned int shared;

                    unsigned int buffer_size;
                    u32 reciprocal_buffer_size;
            /* touched by every alloc & free from the backend */
                    unsigned int flags;
                    unsigned int num;
            /* cache_grou/shrink */
                    unsigned int gfporder;
                    gfp_t gfpflags;
                    size_t colour;
                    unsigned int colour_off;
                    struct kmem_cache *slabp_cache;
                    unsigned int slab_size;
                    unsigned int dflags;
                    void (*ctor)(void *obj);
            /* cache creation/removal */
                    const char *name;
                    struct list_head next;
            /* statistics */
            #ifdef CONFIG_DEBUG_SLAB
                    ...
            #endif
                    struct kmem_list3 *nodelists[MAX_NUMNODES];
            };
            
            !! CAN NOT PLACE ANY FIELDS AFTER @nodelists[],BECAUSE IT IS EXTENDED
               DYNAMICALLY ACCORDING TO @nr_node_ids SLOTS INSTEAD OF 'MAX_NUMNODES'.

            !! we have to think of where the slab descriptor to be stored.
               there are two positions used to store slab descriptor :
                 1> a slab cache used to store slab descriptors
                 2> store slab descriptor inside to the page frame allocated
                    for objects
               if off-slab is disabled,then slab allocator will stores slab descriptor
               in the page frames that allocated for the objects managed by this slab;
               otherwise,it stores the slab descriptor in a general cache with the name
               "size-<memory area size>" in @malloc_sizes,and such memory areas are 
               initialized by kmem_cache_init().
               # "memory area size" must greater than or equal to sizeof(struct slab).

          <mm/slab.c>
            /**
             * kmem_list3 - structure represents a slab list,and each slab list
             *              is a node of a slab cache
             * @slabs_partial: list for both free and non-free objects
             * @slabs_full: list for non-free objects
             * @slabs_free: list for free objects
             * @free_objects: number of current free objects
             * @free_limit: maximum number of free objects in this node
             * @colour_next: per-node cache coloring
             * @list_lock:  node concurrent protector
             * @shared:     shared per node 
             *              pointer to a local cache shared by all CPUs
             *              # this field makes the task of migrating free
             *                objects from a slab local cache to another
             *                easier
             * @alien:      slab local caches on other nodes
             * @next_reap:  used by slab allocator's page reclaiming algorithm
             * @free_touched: used by slab allocator's page reclaiming algorithm
             * # @next_reap and @free_touched can be updated withou locking
             */
            struct kmem_list3 {
                    struct list_head slabs_partial;
                    struct list_head slabs_full;
                    struct list_head slabs_free;
                    unsigned long free_objects;
                    unsigned int free_limit;
                    unsigned int colour_next;
                    spinlock_t list_lock;
                    struct array_cache *shared;
                    struct array_cache **alien;
                    unsigned long next_reap;
                    int free_touched;
            };

            /**
             * slab - structure represents a slab object,this structure manages the objs
             *        in a slab,placed either at the begining of mem allocated for a slab,
             *        or allocated from an general cache
             * @list: pointer for one of the three slab list
             *        # container_of(&slab->list, struct kmem_list3, slabs_free)
             * @colouroff: offset of the first object in the slab
             * @s_mem: address of first object(allocated OR free) in the slab
             * @inuse: number of objects in the slab that are currently used
             * @free: index of next free object in the slab,or BUFCTL_END if there are no
             *        free objects left
             * @nodeid: nodeid in slab cache
             * # @slab is an abstracted type used to manages all possible data type in kernel
             *   if we have a "flip" cache,then all objs in this cache must of type "flip",
             *   and @slab is used to manages them
             * # slabs are chained into three lists
             */
            struct slab {
                    struct list_head list;
                    unsigned long colouroff;
                    void *s_mem;
                    unsigned int inuse;
                    kmem_bufctl_t free;
                    unsigned short nodeid;
            };

            # suppose @inuse = 16 => idx [0, 15]
              suppose start = @s_mem
              &[0] = start
              &[1] = start + 1 * sizeof(data type)
              ...
              &[15] = start + 15 * sizeof(data type)

              if all objects in this slab is allocated,then this slab must be chained into
              @slabs_full of struct kmem_list3.

              "slab" do not care about the actual data type,it just control how memory in
              a page frame to be allocated.
              so,kernel code have to use memory cache corresponding to the actual data type.
                /* this will create a new slab cache if there is no such data type is cached before */
                struct kmem_cache *pointer = kmem_cache_create("data", sizeof(@data), 0, SLABFLAGS, NULL);

                /* e.g. */
                /* pre-allocated slab cache for "struct fs_struct" @fs_cachep is defined in <kernel/fork.c> */

          slab descriptors can be stored in two possible places :
            1> external slab descriptor
               stored outside the slab,in one of the general caches not suitable for ISA DMA
               pointed to by @cache_sizes
            2> internal slab descriptor
               stored inside the slab,at the beginning of the first page frames assigned to the slab
            
            ! slab allocator selects 2> when the size of the objects is smaller than 512MB or when
              internal fragmentation leaves enough space for the slab descriptor and the object
              descriptors inside the slab.
              struct kmem_cache.@flags & CFLGS_OFF_SLAB => F
                slab descriptor is stored inside the slab
              struct kmem_cache.@flags & CFLGS_OFF_SLAB => T
                slab descriptor is stored outside the slab

          relationship between cache and slab descriptors :
            
            +----------------------------------------------------------------------------------+
            |                                                                                  |
            |    Cache Descriptor <-@next-> Cache Descriptor <-@next-> Cache Descriptor ...    |
            |     ^      ^     ^             ^            ^                    ^     ^         |
            |     |      |     |             |            |                    |     |         |
            |     |      v     |             |            |                    v     |         |
            |     |     FSD    |             |     PSD <--+                   FSD    |         |
            |     |            |             |                                 ^     |         |
            |     |     PSD <--+             +---> ESD                         |     |         |
            |     |                                                            v     |         |
            |     +---> ESD                                                   FSD    |         |
            |                                                                        |         |
            |                                                                 PSD <--+         |
            |                                                                                  |
            |    @next: struct kmem_cache.next is type of struct list_head                     |
            |    FSD : Full Slab Descriptor             (chained by @slabs_full)               |
            |          FSD1 <-@slabs_full-> FSD2 <-@slabs_full-> ... <-@slabs_full-> FSDn      |
            |    PSD : Partically Full Slab Descriptor  (chained by @slabs_partial)            |
            |    ESD : Empty Slab Descriptor            (chained by @slabs_free)               |
            |          PSDs and ESDs are similar to FSD                                        |
            +----------------------------------------------------------------------------------+

        General and Specific Caches :
          there are two types of caches :
            > general caches
              used only by the slab allocator for its own purpose
            > specific caches
              used by the remaining parts of the kernel

          general caches :

            struct cache_sizes :
              <linux/slab_def.h>
                /**
                 * cache_sizes - size description struct for general caches
                 * @cs_size:     size of memory area
                 * @cs_cachep:   slab cache descriptor associated with this cache size
                 * @cs_dmacachep: DMA slab cache descriptor associated with this cache size
                 */
                struct cache_sizes {
                        size_t cs_size;
                        struct kmem_cache *cs_cachep;
                #ifdef CONFIG_ZONE_DMA
                        struct kmem_cache *cs_dmacachep;
                #endif
                };

            "kmem_cache" => a first cache whose objects are the cache descriptors of the remaining
                            caches used by the kernel.
                            /* name is "kmem_cache" */
            @cache_cache => contains the descriptor of "kmem_cache".

            the range of the memory area sizes typically includes 13 geometrically distributed sizes.
            @malloc_sizes => table of type "cache_sizes",this table includes 26 cache descriptors 
                             associated with memory area of size(in bytes)
                               32, 64, 128, 256, 512, 1024, 2048,
                               4096, 8192, 16384, 32768, 65536, 131072
                             each size is associated with two caches :
                               one for ISA DMA allocations
                               another one for normal allocations
                             /* @malloc_sizes is an array of struct cache_sizes */

            ! @cache_cache and @malloc_sizes are defined in <mm/slab.c>.
            ! struct cache_sizes is defined in <linux/slab_def.h>
              header <linux/kmalloc_sizes.h> defined the memory area sizes

            <linux/slab.h>
              /**
               * kmem_cache_init - setup the general caches during system booting
               * # this function is implemented in <mm/slab.c>,it is called after
               *   the page allocator have been initialised and before smp_init()
               */
              void __init kmem_cache_init(void);

          specific caches :
            specific caches used by the remaining parts of the kernel code.
            kernel subsystem is able to creates specific caches for its special purpose.
            for create a specific cache,must through the kernel API kmem_cache_create().

            <linux/slab.h>
              /**
               * kmem_cache_create - create a specific slab cache
               * @name:              name of the slab cache
               * @size:              size of objects to be created in this cache
               * @align:             required alignment for the objects
               * @flags:             SLAB flags
               *                       SLAB_POISON
               *                         poison the slab with a known test pattern (a5a5a5a5)
               *                         to catch references to uninitialised memory
               *                       SLAB_RED_ZONE
               *                         insert 'Red' zones around the allocated memory to check
               *                         for buffer overruns
               *                       SLAB_HWCACHE_ALIGN
               *                         align the objects in this cache to a hardware cacheline
               * @ctor:              constructor for the objects
               * return:             pointer to slab cache descriptor
               *                     NULL => failure
               * # this function can not be called from interrupt,but it can be interrupted
               *   @ctor is run when new pages are allocated by the cache
               *   @name must be valid until the cache is destroyed,and kmem_cache_name() is
               *   not guaranteed to return the same pointer
               */
              struct kmem_cache *kmem_cache_create(const char *name, size_t size, 
                                                   size_t align, unsigned long flags,
                                                   void (*ctor)(void *));

              brief description for kmem_cache_create() :
                global variable @cache_chain is type of struct list_head,mutex @cache_chain_mutex is
                used to protect @cache_chain from concurrent accessing.
                @cache_chain is initialized by kmem_cache_init(),which place @cache_cache.next to
                @cache_chain,of course the kmalloc caches also is created by this function(@malloc_sizes).
                kmem_cache_create() traverse all slab caches on @cache_chain to see
                  if there is a slab cache lost its name,then print KERN_ERR message and continue
                  if @name is duplicated,then cancel to slab cache creating
                kmem_cache_create() next try to allocate a memory area from @cache_cache through API
                kmem_cache_zalloc(). /* allocate memory area and fill it with zero */
                we have described @cache_cache is a slab cache descriptor for slab cache "kmem_cache",
                thus the object allocated by kmem_cache_zalloc() is type of struct kmem_cache.
                if succeed to allocate such an object,then go to initialize it and insert it into @cache_chain;
                otherwise,we failed on create kmem_cache,so goto label "oops",in this case,NULL would be returned.

              /**
               * kmem_cache_destroy - destroy a slab cache which is allocated via kmem_cache_create() previously
               * @cachep:             pointer to the slab cache
               * # must ensure that before kmem_cache_destroy() is called,pointer for @name field of the slab cache
               *   is valid
               */
              void kmem_cache_destroy(struct kmem_cache *cachep);

              brief description for kmem_cache_destroy() :
                it call to get_onlin_cpus() at first,this will "lock up" all online CPUs,so the another kernel control
                paths try to process CPU operation after online CPUs were got lock would failed.
                next,lock @cache_chain_mutex,and unlink @cachep from @cache_chain.(list delete @cachep->next)
                check if __cache_shrink(cachep) is TRUE,then raise slab-error that "Can't free all objects",in this case
                chain @cachep again and put online CPUs,we failed to destroy this slab cache.
                if @cachep is not shrink cache,then call to __kmem_cache_destroy(),this routine is the main procedure
                to destroy a slab cache.
                finally,unlock mutex,put online CPUs,and return to caller.
                /**
                 * before invoke __kmem_cache_destroy(),have to check if SLAB_DESTROY_BY_RCU flag is enabled in @cachep,
                 * if it is,then function have to wait on a RCU barrier for synchronization.
                 */

      Interfacing the Slab Allocator with the Zoned Page Frame Allocator :
        when the slab allocator creates a new slab,it relies on the zoned page frame allocator to obtain
        a group of free contiguous page frames.function kmem_getpages() is used to take charge of this.
                                                     /* get page frames for a slab cache from page allocator */

        /**
         * kmem_getpages - interface to system's page allocator
         * @cachep:        which slab cache needs additional page frames
         *                   @order == @cachep->gfporder
         * @flags:         GFP flags,these flags will combined with the specific cache
         *                 allocation flags stored in the @gfpflags field
         * @nodeid:        NUMA node idx
         * return:         address of the first page frame(not page descriptor)
         *                 BAD ADDRESS => failed
         * # this function need not to hold the cache lock
         */
        static void *kmem_getpages(struct kmem_cache *cachep, gfp_t flags, int nodeid);

        what kmem_getpages() does :
          1> #ifndef CONFIG_MMU,then @flags |= __GFP_COMP.
             combine @flags with @cachep->gfpflags.
             if SLAB_RECLAIM_ACCOUNT is enabled in @cachep.flags,then have to enable it in @flags.
          2> try to get pages on an exact node @nodeid through function alloc_pages_exact_node().
             /* @order is @cachep->gfporder */
             
             /**
              * if the slab cache has been created with the SLAB_RECLAIM_ACCOUNT flag set,the page frames
              * assigned to the slabs are accounted for as reclaimable pages when the kernel checks
              * whether there is enough memory to satisfy some User Mode Request.
              */

             if failed to get page frames,then return NULL.
          3> we have got page frames,then set local variable @nr_pages to 1 << @gfporder.
             check if SLAB_RECLAIM_ACCOUNT is enabled,then have to update zone statistics,
             add @nr_pages to zone statistics data NR_SLAB_RECLAIMABLE;otherwise add @nr_pages to
             zone statistics data NR_SLAB_UNRECLAIMABLE.
          4> for each page frame have got previously,set PG_slab flag for them.
          5> check if kernel memory checking is enabled AND
                      @cachep->flags disabled SLAB_NOTRACK
             T =>
                     call to kmemcheck_alloc_shadow()
                     check if @cachep->ctor is not NULL
                     then
                             call to kmemcheck_mark_uninitialized_pages()
                     else
                             call to kmemchecK_mark_unallocated_page()
          6> return page_address(page)

        /**
         * kmem_freepages - do reversal operations what kmem_getpages() did
         *                  that is free page frames for a slab cache
         * @cachep:         which slab cache need to free page frames
         * @addr:           start address of the page frames
         */
        static void kmem_freepages(struct kmem_cache *cachep, void *addr);

        brief description for kmem_freepages() :
          convert @addr to page descriptor.
          do kernel memory checking,and then update zone statistics data NR_SLAB_RECLAIMABLE
          or NR_SLAB_UNRECLAIMABLE.
          for each page frame in the group,clear its PG_slab flag.
          if @current task is in reclaim state,then update @reclaimed_slab += @nr_freed.
                                 /**
                                  * this task is in reclaim state that is it suspended on
                                  * page frame allocation because there is not enough pages to
                                  * be allocated by Buddy System(slow path).
                                  * @nr_freed is initialized to 1 << @cachep->gfporder.
                                  */
          call to free_pages() to free page frames.

      Allocating a Slab to a Cache :
        a newly created slab cache does not contain any slab,therefore does not contain any
        free objects.
        new slabs are assigned to a slab cache only when both of the following are true :
          1> a request has been issued to allocate a new object
          2> the cache does not include a free object

        function cache_grow() is used by slab alloctor for allocate a new slab to a slab cache.

        <mm/slab.c>
          /**
           * cache_grow - allocate a new slab for a given slab cache
           * @cachep:     the slab cache
           * @flags:      GFP flags used by Buddy System
           * @nodeid:     NUMA node id used by Buddy System
           * @objp:       the objects,it usually is the address of page frames contains objects
           *              # if pass NULL as argument of this parameter,cache_grow() will call to
           *                kmem_getpages() to allocate new page frames for store the objects
           * return:      0 => failed
           *              1 => succeed
           */
          static int cache_grow(struct kmem_cache *cachep, gfp_t flags, int nodeid, void *objp);

          brief description of cache_grow() :
            cache_grow() -> kmem_getpages() /* get a group of page frames needed to store single slab */
            cache_grow() -> alloc_slabmgmt()
                            /**
                             * allocate a new slab descriptor.
                             * if CFLGS_OFF_SLAB is enabled in the slab cache @flags,then
                             * the new slab descriptor is allocated from the general cache
                             * pointed to by the @slabp_cache field of the slab cache;otherwise
                             * the slab descriptor is allocated in the first page frame of the slab.
                             */
            functions page_set_cache() and page_set_slab() is called by function slab_map_pages(),and this
            function is invoked after alloc_slabmgmt() allocate a slab descriptor successfully by cache_grow().
            slab_map_pages() traverse all page frames assigned to "this" slab and call to the two functions on
            them.
                            @page->lru.next := @the-cache @page->lru.prev := @the-slab
            /**
             * @lru of a page frame is used by the Buddy System if only if the page is free.when attempt to 
             * allocate a page frame,the page would be unlinked from the LRU list.
             * therefore,PG_slab is set in the page frame if it is assigned to a slab.(not free)
             */
    
            @s_mem field of a slab and @gfporder of the slab cache which contains this slab can be used together
            to determine the page frames implement this slab.
    
            function cache_init_objs() is called by cache_grow() after slab_map_pages() accomplished,this function
            will call constructor(if existed) on all the objects contained in the new slab.
            finally,cache_grow() call to list_add_tail() to link the new slab to @slabs_free list,and updates the
            number of current free objects.

      Releasing a Slab from a Cache :
        slab can be destroyed in two cases :
          1> there are too many free objects in the slab cache
          2> a timer function invoked periodically determines that
             there are fully unused slabs that can be released

        function slab_destroy() used to release a slab :
          <mm/slab.c>
            /**
             * slab_destroy - destroy and release all objects in a slab
             * @cachep:       cache pointer being destroyed
             * @slab:         slab pointer being destroyed
             * # @slabp must be unlinked from @cachep before call to this function
             */
            static void slab_destroy(struct kmem_cache *cachep, struct slab @slabp);

            ! the page frames will be released and back to Buddy System.

            brief description for slab_destroy() :
              it get the virtual address of the page frames had assigned to this @slabp.
              /* @slabp->s_mem - slabp->colouroff */

              function slab_destroy_debugcheck() is called at next,if "DEBUG" is not defined,
              this function will do nothing.but if "DEBUG" is defined,slab_destroy_debugcheck()
              will processes the following steps :
                start a for-cycle
                for each objects in the slab >
                  get virtual address of current object
                  check if @cachep->flags enabled SLAB_POISON
                  T =>
                          #ifdef CONFIG_DEBUG_PAGEALLOC
                          check if @cachep->buffer_size % PAGE_SIZE is zero AND
                                   off-slab is enabled
                          T =>
                                  call to kernel_map_pages(),this will unpoison
                                  "@buffer_size / PAGE_SIZE" page frames start from
                                  current object
                                  /* if page frame poisoned,then unpoison it. */
                                  /* unpoison a page frame might dump_stack(). */
                          F =>
                                  call to check_poison_obj()
                                  /* check slab corruption */
                          #else
                                  call to check_poison_obj()
                          #endif
                  check if @cachep->flags enabled SLAB_RED_ZONE
                  T =>
                          /**
                           * redzone1 :
                           *         (unsigned long long *)(@objp + obj_offset(@cachep) -
                           *                                sizeof(unsigned long long))
                           * redzone2 :
                           *         (unsigned long long *)(@objp + @cachep->buffer_size -
                           *                                sizeof(unsigned long long))
                           *   if @cachep->flags enabled SLAB_STORE_USER
                           *         (unsigned long long *)(@objp + @cachep->buffer_size - 
                           *                                sizeof(unsigned long long) -
                           *                                REDZONE_ALIGN)
                           */
                          check if redzone1 of @cachep for current object != RED_INACTIVE
                          T =>
                                  slab-error : start of a freed object was overwritten
                          check if redzone2 of @cachep for current object != RED_INACTIVE
                          T =>
                                  slab-error : end of a freed object was overwritten

              check if @cachep->flags enabled SLAB_DESTROY_BY_RCU,then release slab through
              RCU mechanism.
              /**
               * convert @slabp to pointer of struct slab_rcu
               * assign @cachep to @slab_rcu->cachep
               * assign @addr to @slab_rcu->addr
               * call_rcu(&@slab_rcu->head, kmem_rcu_free)
               * # kmem_rcu_free() is defined in <mm/slab.c>,
               *   which call to kmem_feepages(),and it call to kmem_cache_free() to release
               *   slab descriptor if off-slab is enabled.
               * # struct slab_rcu is defined in the same file and has these members :
               *           struct rcu_head head
               *           struct kmem_cache *cachep
               *           void *addr
               * # force type converting struct slab to struct slab_rcu is OK.becasue
               *   we have saved virtual address of page frames,and now we is going to
               *   release this @slabp,thus re-use the memory it had occupied is OK,we
               *   no longer need the data of this slab object.
               */
              otherwise,release slab through kmem_freepages() to release page frames had assigned
              to this slab.next,check if off-slab is enabled,then call to kmem_cache_free() to
              free this slab.
              /**
               * kmem_cache_destroy() is different to kmem_cache_free(),
               * kmem_cache_free() free an object.
               */

      Object Descriptor :
        each object has a short descriptor of type "kmem_bufctl_t".
        <mm/slab.c>
          /**
           * kmem_bufctl_t - type definition for unsigned int to kmem_bufctl_t,
           *                 bufctl's are used for linking objs within a slab
           *                 linked offsets
           */
          typedef unsigned int kmem_bufctl_t;

          /**
           * BUFCTL_END - object descriptor of the last element in the free object list
           */
          #define BUFCTL_END  (((kmem_bufctl_t)(~0U)) - 0)

          /* BUFCTL_FREE - for DEBUG */
          #define BUFCTL_FREE (((kmem_bufctl_t)(~0U)) - 1)

          #define BUFCTL_ACTIVE  (((kmem_bufctl_t)(~0U)) - 2)
          #define SLAB_LIMIT  (((kmem_bufctl_t)(~0U)) - 3)

          !! this implementation relies on "struct page" for locating the cache AND
             slab an object belongs to.
             this allows the bufctl structure to be small(one int),but limits the
             number of objects a slab can contain when off-slab bufctls are sued.
             the limit is the size of the largest general cache that does not use
             off-slab slabs.
             # note :
                 this limit can be raised by introducing a general cache whose size is
                 less than 512 (PAGE_SIZE << 3),but greater than 256.

          the linked offsets right after the slab descriptor,they are append to the end
          of a slab descriptor in the memory location.
          if off-slab is enabled,then the object size for slab descriptor in a general
          cache is
            ALIGN(sizeof(slab descriptor) + slab cache @num * sizeof(kmem_bufctl_t))

          off-slab disabled :

            /* slab cache */
            +--------------------------------------------------------------------+
            |      |         |         |     |         |         |     |         |
            | Slab | offset0 | offset1 | ... | offsetN | object0 | ... | objectN |
            |      |         |         |     |         |         |     |         |
            +--------------------------------------------------------------------+
                                                         @s_mem

          off-slab enabled :

            /* general cache */
            +------------------------------------------+
            |      |         |         |     |         |
            | Slab | offset0 | offset1 | ... | offsetN |
            |      |         |         |     |         |
            +------------------------------------------+

            /* slab cache for objects */
            +-----------------------------------+
            |         |         |     |         |
            | object0 | object1 | ... | objectN |
            |         |         |     |         |
            +-----------------------------------+
              @s_mem

          /* <mm/slab.c> */

          static inline kmem_bufctl_t *slab_bufctl(struct slab *slabp)
          {
                  /**
                   * @slabp + 1 == (char *)@slabp + sizeof(char) * sizeof(struct slab)
                   * returns the start address of the linked offsets as an array.
                   *                                  # right after slab descriptor
                   */
                  return (kmem_bufctl_t *)(slabp + 1);
          }

          static inline void *index_to_obj(struct kmem_cache *cache, struct slab *slab,
                                           unsigned int idx)
          {
                  /**
                   * objects in a slab is start from @s_mem,and sizeof(object) is represented
                   * by slab cache @buffer_size.
                   * virtual addresss of @idx obj = @s_mem + @buffer_size * @idx
                   */
                  return slab->s_mem + cache->buffer_size * idx;
          }

          static inline unsigned int obj_to_index(const struct kmem_cache *cache,
                                                  const struct slab *slab, void *obj)
          {
                  /**
                   * offset of a specified object is
                   *         offset = virtual address of object - @s_mem
                   * index of "this" object is
                   *         (u32)(((u64)offset * @reciprocal_buffer_size) >> 32)
                   */
                  u32 offset = (obj - slab->s_mem);
                  return reciprocal_divide(offset, cache->reciprocal_buffer_size);
          }

          ! an object descriptor is meaningfull only when the object is free.
            slab_get_obj() :
              obj = index_to_obj(slab cache, slab, slab->free)
              inuse++

              /* array start from index _zero_ */
              /**
               * suppose @free = 3 [0][1][2][3]...[N]
               * [3] => the fourth element in object descriptor array
               * @next = [3] => 4
               * @free = @next = [3] => 4 -> [4] the fifth element
               * # cache_init_objs(): slab_bufctl(slabp)[i] = i + 1
               */
              kmem_bufctl_t next = slab_bufctl(slab)[slab->free]
              slab->free = next

              /* the initialized value of @free is 0 */ 

      Aligning Objects in Memory :
        objects managed by the slab allocator are aligned in memory,they are stored in memory cells
        whose initial physical addresses are multiples of a given constant(alignment factor),it usually
        is power of 2.
        /**
         * the largest alignment factor is allowed in slab is 4096,objs can be referred by their  
         * physical addresses or linear addresses
         * in both cases,only the 12 least significant bits of the address may be altered by the
         * alignment
         */

        microcomputers access to memory unit which is aligned to the word size will more
        quickly,thus kmem_cache_create() try to align objects with the alignment factor defined
        by macro BYTES_PER_WORD.
        /**
         * 80x86 a word is 32bits
         * Linux 2.6.34.1,BYTES_PER_WORD is defined in <mm/slab.c> with value of "sizeof(void *)"
         */
        /**
         * kmem_cache_create() alignment calculating :
         *   size => object size ; align => caller mandated alignment factor
         *   
         *   # this is needed to avoid unaligned accesses for some archs when redzoning is used
         *     and makes sure any on-slab bufctl's are also correctly aligned
         *   if size & (BYTES_PER_WORD - 1)
         *     size += (BYTES_PER_WORD - 1)
         *     size &= ~(BYTES_PER_WORD - 1)
         *
         *   # calculate the final buffer alignment
         *   if @flags & SLAB_HWCACHE_ALIGN # align in the first-level hardware cache
         *     ralign = cache_line_size() /* hardware cache line size */
         *     # if size is too small,then decrease ralign to prevent obj aligning in
         *       hardware cache line span across two cache lines
         *       otherwise,obj is aligned in RAM to a multiple of (default)32 OR 64(x86_64)
         *     while size <= ralign / 2 { ralign /= 2 }
         *   else
         *     ralign = BYTES_PER_WORD
         *
         *   if @flags & SLAB_STORE_USER
         *     ralign = BYTES_PER_WORD
         *
         *   # adjust object size accordingly for the second redzone is suitably aligned
         *   if @flags & SLAB_RED_ZONE 
         *     ralign = REDZONE_ALIGN
         *     size += REDZONE_ALIGN - 1
         *     size &= ~(REDZONE_ALIGN - 1)
         *
         *   ## if-branch SLAB_STORE_USER and SLAB_RED_ZONE
         *      redzoing and user store require word alignment or possibly larger,but this
         *      will be overriden by arch or caller mandated alignment if either is
         *      greater than BYTES_PER_WORD
         *   
         *   if ralign < ARCH_SLAB_MINALIGN # arch mandated alignment
         *     ralign = ARCH_SLAB_MINALIGN
         *   if ralign < align # caller mandated alignment
         *     ralign = align
         *
         *   if ralign > __alignof__(unsigned long long) # disable debug if necessary
         *     flags &= ~(SLAB_RED_ZONE | SLAB_STORE_USER)
         *
         *   align = ralign
         */

        the trade happened at there is trading memory space for access time,it get better
        cache performance by artificially increasing the object size,thus causing additional
        internal fragmentation.

      Slab Coloring :
        objects of the same size being stored at the same offset within a cache.
          slab cache {
                  slab1 { offset0 : obj0, offset1 : obj1 }
                  slab2 { offset0 : obj0, offset1 : obj1 }
          }

        objects have the same offset within different slabs will,with a relatively high probability,
        mapped in the same cache line.
          hardware cache {
                  cache line0 { slab1 : offset0 : obj0, slab2 : offset0 : obj0 }
                  ...
          }
          /**
           * the cache hardware might therefore waste memory cycles transfering two objs from the
           * same cache line back and forth to different RAM locations.
           * cache lines except to cache line mapped slab objects are go underutilized.
           */

        goal fo slab coloring strategy : reduce cache line underutilized.
        different arbitraty values called colors are assigned to the slab by slab allocator.

        how to place objs in a slab cache to the memory unit when alignment required :
          choices depend on decisions made for the following variables >
            @num
              number of objs that can be stored in a slab.
            @osize
              obj size,including alignment bytes.
            @dsize
              slab descriptor size plus all object descriptors size,rounded up to the smallest
              multiple of the hardware cache line size.its value is equal to 0 if the slab and
              object descriptors are stored outside of the slab.
            @free
              number of unused types inside the slab.
              @free is always samller than @osize but could be greater than a multiple of alignment factor.

          slab size := @num * @osize + @dsize + @free

          slab allocator takes advantage of the @free to color the slab.
          "color" is used simply to subdivide the slabs and allow the memory allocator to spread objs
          out among different linear addresses.
          slabs having different colors store the first object of the slab in different memory locations,
          while satisfying the alignment constraint.

          number of available colors := @free / align(alignment factor) /* stored in @colour field */
          /**
           * colour_0 => 0
           * ...
           * colour_N => @free / align(alignment factor) - 1
           * even the division might results zero,but colour_0 is 0,thus @colour is equal to 1.
           */

          slab colored with the colour @col >
            offsetof(obj0, &slab) := @col * align(alignment factor) + @dsize (in bytes)

          colouring essentially leads to moving some of the free area of the slab from the end to
          the beginning.

          +---------------------------------------------------------------------------+
          |            |                 |                 |      |      |     |      |
          | colour_off | slab descriptor | obj descriptors | obj0 | obj1 | ... | objN |
          |            |                 |                 |      |      |     |      |
          +---------------------------------------------------------------------------+
          |            |                 |                 |                          |
          |            |                 |                 |                          +----> end of slab
          |            |                 |                 +----> objs @s_mem
          |            |                 +----> kmem_bufctl_t objs
          |            +----> slab
          +----> virtual address of page frames for this slab

        !! coloring works only when @free is large enough.if no alignment is required for the objects
           or if the number of unused bytes inside the slab is smaller than the required alignment,
           the only possible slab coloring is the one that has the colour_0.

        various colors are distributed equally among slabs of a given object type by storing the
        current color in a field of the struct kmem_list3 called @colour_next.

        cache_grow() use @colour_next as the colour off for the new created slab,and then increase
        it for next slab node creating.                                              /* @colour_next++ */
        /**
         * colour off of "this" slab is different to previous slab,
         * if it reached @colour,then @colour_next wraps around again to 0.
         */

        the field @colouroff of slab descriptor is calculated by alloc_slabmgmt() through the expression
                @col * align(alignment factor) + @dsize
                /* if off-slab is disabled */

      Local Caches of Free Slab Objects :
        implementation of slab allocator for multiprocessor system in Linux 2.6 is differs from the original
        Solaris 2.4.
        
        for reduce spin lock racing and make better use of the hardware caches,slab cache includes a per-CPU
        data structure consisting of a small array of pointers to freed objects called the "slab local cache".
        
        <mm/slab.c>
          /**
           * array_cache - structure array_cache represent a slab local cache
           * @avail:       available freed objects,this field also acts as the index of
           *               the first free slot in the cache
           * @limit:       maximum number of the freed objects in slab local cache
           * @batchcount:  chunk size of local cache refill or emptying
           * @touched:     is this slab local cache touched by someone recently?
           *               this member would be reset to zero if drain_array() is called
           *               on it under the conditions that @touched is TRUE AND non-force drain
           *               is requested 
           * @lock:        concurrent protector
           * @entry:       array of pointers for _freed_ objects in this slab local cache
           * # purpose of introducing this structure :
           *     LIFO ordering,to hand out cache-warm objects from _alloc
           *     reduce the number of linked list operations
           *     reduce spinlock operations
           * # local cache descriptor does not include the address of the local cache itself.
           */
          struct array_cache {
                  unsigned int avail;
                  unsigned int limit;
                  unsigned int batchcount;
                  unsigned int touched;
                  spinlock_t lock;
                  void *entry[];
          };
          /* struct kmem_cache.array[CPUIDX] => slab local cache */

          /**
           * array_cache LIFO :
           *   [0] [1] [2] [3] [4] ... [avail] [inuse] [inuse] ... [inuse]
           *   allocat new object at @avail,then decrease @avail,
           *   the number of freed objects has decreased,and the idx for the last freed object
           *   is @avail.
           */

          ! elements in @entry of a slab local cache is possible to be transfered to another
            slab local cache by the way memcpy(),that is they are stay valid in current slab local
            cache even transferring accomplished.

          /**
           * alloc_arraycache - allocate a new slab local cache descriptor
           * @node:             which NUMA node is used by kmalloc_node() to allocate memory
           * @entries:          size of @entries
           *                    the memory to allocate of new descriptor is
           *                      sizeof(void *) * entries + sizeof(struct array_cache)
           *                    and the value of @entries also is used to init @limit field
           * @batchcount:       batch count
           * @gfp:              GFP flags
           * return:            pointer of new descriptor
           *                    NULL to failed
           */
          static struct array_cache *alloc_arraycache(int node, int entries, int batchcount, gfp_t gfp);

          function __cpuinit cpuup_prepare() will initialize slab local cache for each slab cache on
          @cache_chain.during this procedure,struct kmem_cache.nodelists[CURRENT_NODE] is going to be
          initialized,too.
          for each slab cache
            if @shared is equal to 1,then a shard slab local cache will be created
            and assigned to kmem_list3.@shared.
            if @use_alien_caches is equal to 1,then an alien slab local cache will be created through
            alloc_alien_cache(),and the pointer of pointer of the newly created alien slab local cache
            is assigned to kmem_list3.alien.
            a new slab local cache for current CPU will be created and assign to
            @cachep->array[THIS_CPU].

          when create a new slab cache,function setup_cpu_cache() is invoked for setup @array field
          of the new slab cache corresponding to current smp cpu.
            if @g_cpucache_up is neither equal to FULL nor NOE
                    @limit of the local cache is set to
                    BOOT_CPUCACHE_ENTRIES(expand to 1),@batchcount is set to 1.
                    /**
                     * structure arraycache_init is used as a helper for determine how many memory
                     * to be allocated for slab local cache of the newly created slab cache by
                     * kmem_cache_create().
                     * # allocated from general cache.
                     */
            /**
             * if @g_cpucache_up equal to FULL,then enable_cpucache() will be called.
             * @g_cpucache_up is set to FULL by kmem_cache_init_late() after enable_cpucache() is called
             * and returned.
             */

          slab cache can not work without slab local cache anymore,but the initial slab local cache is not
          useful,thus function do_tune_cpucache() is used to adjust size of slab local cache of the specified
          slab cache pointed by parameter @cachep for each online CPU.
          <mm/slab.c>
            /**
             * ccupdate_struct - structure as a helper to be used by do_tune_cpucache()
             * @cachep:          slab cache pointer
             * @new:             array of pointers used to store new allocated slab local cache in future
             */
            struct ccupdate_struct {
                    struct kmem_cache *cachep;
                    struct array_cache *new[NR_CPUS];
            };

          for each online cpu,allocate new array_cache through alloc_arraycache().
          call do_ccupdate_local() on each cpu with @new of struct ccupdate_struct.do_ccupdate_local() just
          simply exchange the old slab local cache and the new one in @new[CPUIDX].

          /* cache tunable */
                                       /* parameters of do_tune_cpucache() */
          reset @cachep->batchcount to @batchcount
          reset @cachep->limist to @limit
          reset @cachep->shared to @shared

          for each online cpu,free entries of the slab local cache in @new[CPUIDX],then free the slab local
          descriptor.

          !! function do_tune_cpucache() is invoked by function enable_cpucache() with a specified limit
             from {1, 8, 24, 54, 120} and the batch count is @limit / 2.
                  /**
                   * 1 => @buffer_size > 131072
                   * 8 => @buffer_size > PAGE_SIZE
                   * 24 => @buffer_size > 1024
                   * 54 => @buffer_size > 256
                   * _else_ 120
                   */

      Allocating a Slab Object :
        a slab object is created through function kmem_cache_alloc().

        <mm/slab.c>
          /**
           * kmem_cache_alloc - allocate a new slab object from a specified slab cache
           * @cachep:           slab cache
           * @flags:            GFP flags
           * return:            virtual address of the object
           *                    NULL => failed
           * # this function actually call to __cache_alloc().
           */
          void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags);

          /**
           * __cache_alloc - allocate an object from slab cache
           * @cachep:        slab cache
           * @flags:         GFP flags
           * @caller:        who called to this routine
           * return:         virtual address for the object
           *                 NULL => failed
           */
          static __always_inline
          void *__cache_alloc(struct kmem_cache *cachep, gfp_t flags, void *caller);

          brief description for __cache_alloc() :
            kmem_cache_alloc() call to this function with @caller = __builtin_return_address(0),
            the builtin feature used to obtain the ip register for current statement.
            __cache_alloc() detach gfp allowed flags from @flags,then check if slab should
            failslab? it is likely result in false.
            next,process kernel debug checking for object allocating.
            the primary work to allocate new object will be done by __do_cache_alloc(),
            __cache_alloc() call to it with local interrupt disabled.
            function __do_cache_alloc() is defined in the same file and actually it call
            to ____cache_alloc().if CPU slab local cache for "this" slab cache is available,
            then try to allocate object from the slab local cache;
            otherwise,_refill_ "slab local cache" from @cachep and retry allocate object.
                                       /* refill attempt order: shared -> partial -> free */
                                       /**
                                        * for each online CPU,a slab cache has an array_cache
                                        * corresponding to it,which stores the freed objs
                                        * in this slab cache.
                                        */
            use prefetch primitive to cache @objp to hardware cache.
            /**
             * __cache_alloc() would call to kmemleak_alloc_recursive to create kmemleak object
             * which is metadata for allocated memory block.
             */

            ! cache_alloc_refill() selects refill slab local cache as the first method,if no
              freed object is available in "this" slab cache,then it attempt to grow this slab
              cache through cache_grow().
              NULL would be returned if cache_grow() was failed,in the case,kmem_cache_alloc()
              allocate new object failed.

      Freeing a Slab Object :
        function kmem_cache_free() is used to free an object in a slab cache.

        <mm/slab.c>
          /**
           * kmem_cache_free - free an allocated object to its slab cache
           * @cachep:          slab cache
           * @objp:            address of object
           * # this function call to __cache_free() for object releasing,and
           *   during __cache_free() is processing,the local interrupt must be
           *   disabled
           */
          void kmem_cache_free(struct kmem_cache *cachep, void *objp);

          /**
           * __cache_free - the main routine for free a slab object
           * @cachep:       slab cache of the object
           * @objp:         address of object
           * # must call to this routine with local interrupt disabled
           */
          static inline void __cache_free(struct kmem_cache *cachep, void *objp);

          brief descriptor for __cache_free() :
            this function will skip calling cache_free_alien() when the platform is not
            NUMA to avoid cache misses that happen while accessing slabp to get nodeid!
            get slab local cache,if @avail < @limit,then just place the object into this
            slab local cache as well;otherwise,call to cache_flusharray to get space to
            place this object.

          /**
           * cache_flusharray - flush slab local cache for a specified slab cache
           * @cachep:           slab cache
           * @ac:               slab local cache for current CPU
           */
          static void cache_flusharray(struct kmem_cache *cachep, struct array_cache *ac);

          brief description for cache_flusharray() :
            retrieve @batchcount from @ac,retrieve kmem_list3 object for current NUMA node from
            @cachep.
            before operating to @l3,must acquire spin lock.
            if @l3 has a shared slab local cache,then attempt to flush elements in @ac to
            the shared slab local cache of @l3.
            the maximum number of elements can be moved up is min(@batchcount, shared(@limit - @avail)),
            after transfering accomplished,the @shared->avail += transferred elements.
            if @l3 has not a shared slab,then call to free_block() to give back freed object in @ac to
            slab allocator.                           /* with @nr_objects = @batchcount */
            finally,update @ac->avail to @ac->avail - @batchcount,and move the remained free objects(valid)
            in @ac to the beginning of @ac->entries.
            /* the transferring is start at idx0 to idxN */

          /**
           * free_block - free a block is group of slab objects back to slab allocator
           * @cachep:     where the objects come from
           * @objpp:      array of pointers
           * @nr_objects: how many objs to be freed
           * @node:       NUMA node
           * # require kmem_list's list_lock
           */
          static void free_block(struct kmem_cache *cachep, void **objpp, int nr_objects, int node);

          brief description for free_block() :
            for each object in @objpp,do
              get @slabp of current obj /* struct page.@lru.prev */
              call to slab_put_obj(),this function will get the idx of current object and then
              set object descriptor array[obj-idx] to @slabp->free,set @slabp->free to obj-idx,
              decrease @inuse of the slab descriptor.
              /* get object index from object address through obj_to_index() */
              increase @free_objects field of @l3 becasue slab_put_obj() was called.
              if @inuse of @slabp became zero due to slab_put_obj(),then have to determine
              whether should destroy the slab descriptor.
              @l3->free_objects > @l3->free_limit,no more freed objects are allowed be
              existed in this @l3,then must destroy the @slabp;otherwise,should add this
              slab descriptor to the freed slabs list of @l3.
              if @inuse is not equal to _zero_,that is there must be some objects is allocated
              and have not released,should chain this slab descriptor to partial list of @l3.

            ! by the way free @entries(objs) back to the slab descriptor they belong to for
              get space to stores the object which is going to be released by kmem_cache_free().

      General Purpose Objects :
        kernel create 13 geometrically distributed lists for free memory areas whose size between
        [32, 131072](bytes) as the general slab cache.
        function kmalloc() handles such memory allocating request.

        <linux/slab_def.h>
          /**
           * kmalloc - allocate specified size for memory area from general slab cache
           * @size:    size of memory area
           * @flags:   GFP flags
           * return:   pointer to start of the memory area
           *           NULL => failed
           */
          static __always_inline void *kmalloc(size_t size, gfp_t flags);

          brief description for kmalloc() :
            this function used a GCC builtin function __builtin_constant_p() to check
            whether @size is a constant(for optimization),if it is,then use a 
            local variable @i as the idx of 13 lists,and if @size is less than or
            equal to pre-defined malloc size,allocate memory area from predefined
            @malloc_sizes[i].cs_cachep or @malloc_sizes[i].cs_dmacachep through
            kmem_cache_alloc_notrace().                    /* GFP_DMA */

            otherwise,call to __kmalloc() to allocate memory area at runtime,and which
            just call to __do_kmalloc() with @size, @flags, NULL(caller address) do
            primary memory allocating.

        <mm/slab.c>
          /**
           * __do_kmalloc - the primary routine to allocate memory from general slab cache
           * @size:         size of memory area
           * @flags:        GFP flags
           * @caller:       code address of caller for debug tracking
           * return:        pointer to the start of allocated memory area
           *                NULL => failed
           */
          static __always_inline void *__do_kmalloc(size_t size, gfp_t flags, void *caller);

          brief description for __do_kmalloc() :
            this function use @size and @flags to attempt find out an appropriate general slab
            cache.if unfound,return ZERO_OR_NULL_PTR(@cachep).
            if such slab cache is existed,then call to __cache_alloc() on it.

        function kfree() handles memory area free which is acquired through kmalloc().
        
        <linux/slab.h> <mm/slab.c>
          /**
           * kfree - free an object acquired through kmalloc() previously
           * @objp:  virtual address of the object
           * # this function call to __cache_free() to release object
           * # the slab cache descriptor is get from struct page.@lru.@next
           */
          void kfree(const void *objp);

        ! some common kmalloc functions provided by all allocators :
            krealloc - kfree /* resize allocated memory area */
                             /* if new-size less than current,then no realloc */
                             /* if reallocated,then copy memory and kfree the old one */
            kzalloc - kzfree /* __GFP_ZERO */
            ksize : get the actual amount of memory allocated for a given object
                            /* struct kmem_cache.@obj_size request CONFIG_DEBUG_SLAB */

      Memory Pools :
        new feature introduced in Linux 2.6,allow kernel component to allocate some dynamic
        memory to be used only in "low-on-memory" emergencies.
        memory pools are not reserved page frames!
                             /* for atomic page request */
        /**
         * reserved dynamic memory can be used only by a specific kernel component in a special
         * circumstance that all usual memory allocation requests are doomed to fail.
         */

        the memory unit handled by memory pool is referred to "memory element".
        often,a memory pool is stacked over the slab allocator-that is,it is used to keep a
        reserve of slab objects.howevery,a memory pool can be used to allocate every kind of
        dynamic memory,from whole page frames to small memory areas allocated with kmalloc().

        <linux/mempool.h>
          typedef void *(mempool_alloc_t)(gfp_t gfp_mask, void *pool_data);
          typedef void (mempool_free_t)(void *element, void *pool_data);

          /**
           * mempool_t - structure mempool_s represent a memory pool
           * @lock:      concurrent protector
           * @min_nr:    number of elements at *@elements
           * @curr_nr:   current number of elements at *@elements
           * @elements:  element pointer array
           * @pool_data: memory pool specified data used by specific component
           * @alloc:     function to allocate dynamic memory from mempool
           * @free:      function to free dynamic memory back to mempool
           * @wait:      wait queue used when the memory pool is empty
           */
          typedef struct mempool_t {
                  spinlock_t lock;
                  int min_nr;
                  int curr_nr;
                  void **elements;

                  void *pool_data;
                  mempool_alloc_t *alloc;
                  mempool_free_t *free;
                  wait_queue_head_t wait;
          };

          ! @min_nr stores the initial number of memory elements in the memory pool,that is the
            owner of this memory pool is sure to obtain @min_nr memory elements from the memory
            allocator.
            @curr_nr is always lower than or equal to @min_nr.
          ! @alloc and @free interface with the underlying memory allocator to get and release
            a memory element,respectively.
          !! if the memory elements are slab objects,then @alloc and @free generally are set to
             the functions mempool_alloc_slab() and mempool_free_slab(),respectively.
             in this case,@pool_data is the slab cache descriptor.
             /* alloc slab : kmem_cache_alloc ; free slab : kmem_cache_free */

          /**
           * mempool_create - create a memory pool
           * @min_nr:         initial number of memory elements would in this mempool
           * @alloc_fn:       allocate function
           *                  the default gfp mask is GFP_KERNEL specified by
           *                  mempool_create(),but @alloc_fn can modifies gfp mask
           * @free_fn:        free function
           * @pool_data:      kernel component specific data
           * return:          pointer to memory pool
           *                  NULL => failed
           * # this function actually call to _node_ version with node -1
           * # mempool_create_node() will pre-allocate the guaranteed number of memory elements+
           *   through @pool->@alloc()
           */
          extern mempool_t *mempool_create(int min_nr, mempool_alloc_t *alloc_fn,
                                           mempool_free_t *free_fn, void *pool_data);

          ! creating steps :
              first,use kmalloc_node() to allocate memory pool descriptor.
              second,use kmalloc_node() to allocate memory element pointer array.
              third,while until @curr_nr equal to @min_nr,pre-allocate memory elements.
              /* through @pool->alloc(GFP_KERNEL, @pool->pool_data) */

          /**
           * mempool_resize - resize a mempool
           * @pool:           memory pool descriptor
           * @new_min_nr:     new @min_nr
           * @gfp_mask:       GFP mask
           * return:          0 => succeed (return 0 maybe @alloc returned NULL)
           *                  -ENOMEM => failed
           * # during this function is processing,must no mempool_destory() is called
           * # this function release spin lock before call to @pool->@alloc()
           */
          extern int mempool_resize(mempool_t *pool, int new_min_nr, gfp_t gfp_mask);

          /**
           * mempool_destroy - destroy a memory pool
           * @pool:            memory pool descriptor
           * # this function call to free_pool() to process primary works
           * # this function only sleeps if the @free_fn function sleeps
           * ! caller has to guarantee that all elements have been returned to the pool
           */
          extern void *mempool_destroy(mempool_t *pool);

          ! free_pool() release all current memory elements and the invoke kfree() twice to
            free @pool->@elements and @pool,respectively.

          /**
           * mempool_alloc - allocate a memory element from memory pool
           * @pool:          memory pool descriptor
           * @gfp_mask:      GFP mask
           * return:         virtual address of the obtained element
           *                 NULL => failed
           * # this function attempt to allocate underlying memory through
           *   @alloc method of @pool,if no memory is available,then allocate
           *   memory from @pool through remove_element()
           *   if the two strategies are failed,then wait(TASK_UNINTERRUPTIBLE)
           *   until memory available under the circumstance that __GFP_WAIT is
           *   enabled and retry allocating after "this" kernel control path is
           *   woken up(retry allocate from underlying memory allocator)
           * # the schedule entry-point is io_schedule_timeout()
           */
          void *mempool_alloc(mempool_t *pool, gfp_t gfp_mask);
          
          ! THIS FUNCTION WOULD ENABLE __GFP_NOMEMALLOC __GFP_NORETRY __GFP_NOWARN
          ! BEFORE ENTERY WAITING STATE,FUNCTION WOULD CHECK @pool->@curr_nr IS
            EQUAL TO _ZERO_ AGAIN,THAT IS MAYBE SOME ONE GIVE BACK MEMORY ELEMENT
            TO MEMORY POOL(NOT ZERO),OTHERWISE,CALL TO io_schedule_timeout(5 * HZ)

          /**
           * mempool_free - free a memory element
           * @element:      the memory element
           * @pool:         the memory pool @element belongs to
           * # if @curr_nr is less than @min_nr,then give back @element to @pool
           *   through function add_element(),
           *   otherwise,free @element through @free method give it back to 
           *   underlying memory allocator
           */
          void mempool_free(void *element, mempool_t *pool);

          ! add_element() and remove_element() : @curr_nr++ OR @curr_nr--

      Noncontiguous Memory Area Management :
        noncontiguous page frames accessed through contiguous linear addresses,the main
        advantage of this schema is to avoid external fragmentation,while the disadvantage
        is that it is necessary to fiddle with the kernel Page Tables.
        /* sizeof(noncontiguous memory area) must be a multiple of 4096 */

        Linear Addresses of Noncontiguous Memory Areas :
          normally,for to find out available contiguous linear addresses can look in the
          area starting from PAGE_OFFSET(usually 0xc0000000,the fourth gigabyte)
            how fourth gigabyte is used :
              1> beginning of the area includes the linear addresses that map the first
                 896MB of RAM.
                 /**
                  * the linear address that corresponds to the end of the directly mapped
                  * physical memory is stored in the @high_memory variable.
                  */
              2> the end of the area contains the fix-mapped linear addresses.
              3> linear addresses start from PKMAP_BASE are used by kernel to map high-memory
                 page frames.
              4> the remaining linear addresses can be used for noncontiguous memory areas,
                 between 1> and 4>,a 8MB safety interval(VMALLOC_OFFSET) is inserted into there.
                 /* VMALLOC_OFFSET used for capture out-of-bounds memory accesses */
                 a safety interval of 4kB is inserted between each noncontiguous memory areas.

          3GB                           Linear Addresses                                                   4GB
          +---------------------------------------------------------------------------------------------------+
          |                  |     |               |     |              |     |     |            |            |
          | physical mapping | 8MB | vmalloc areas | 4kB | vmalloc area | ... | 8kB | PKMAP_BASE | fix-mapped |
          |                  |     |               |     |              |     |     |            |            |
          +---------------------------------------------------------------------------------------------------+
          |                  |     |               |                                |            +--> fix-mapping
          |                  |     |               |                                +--> high memory page mapping
          |                  |     |               +--> safety interval
          |                  |     +--> noncontiguous memory area
          |                  +--> safety interval
          +--> PAGE_OFFSET

            macro VMALLOC_START defines the starting address of the linear space reserved for
            noncontiguous memmory areas,while VMALLOC_END defines its ending address.
            /**
             * <arch/x86/include/asm/pgtable_32_typs.h>
             *   #define VMALLOC_OFFSET (8 * 1024 * 1024)
             *   #define VMALLOC_START ((unsigned long)high_memory + VMALLOC_OFFSET)
             *   #define VMALLOC_END (PKMAP_BASE - 2 * PAGE_SIZE)
             *   /* #define VMALLOC_END (FIXADDR_START - 2 * PAGE_SIZE) no CONFIG_HIGHMEM */
             */
        !! WHAT IS struct vm_struct DO THAT IS CONSTRUCT A MAPPING BETWEEN LINEAR ADDRESS AND
           NONCONTIGUOUS MEMORY.

        Descriptors of Noncontiguous Memory Areas :
          structure vm_struct is used to present a noncontiguous memory area.

          <linux/vmalloc.h>
            #define VM_IOREMAP 0x00000001  /* ioremap() and friends */
            #define VM_ALLOC   0x00000002  /* vmalloc() */
            #define VM_MAP     0x00000004  /* vmap()ed pages */
            #define VM_USERMAP 0x00000008  /* suitable for remap_vmalloc_range */
            #define VM_VPAGES  0x00000010  /* buffer for pages was vmalloc'ed */

            /**
             * vm_struct - noncontiguous memory area descriptor
             * @next:      next vm_struct structure(chained together)
             * @addr:      linear address of the first memory cell
             * @size:      size of area plus PAGE_SIZE(safety-interval)
             * @flags:     noncontiguous memory area flags,identifies
             *             the type of memory mapped by the area
             * @pages:     page frame array for this memory area
             * @nr_pages:  number of page frames in @pages
             * @phys_addr: phsical address,set to 0 unless the area
             *             has been created to map the I/O shared memory
             *             of a hardware device
             * @caller:    who created this memory area
             */
            struct vm_struct {
                    struct vm_struct *next;
                    void *addr;
                    unsigned long size;
                    unsigned long flags;
                    struct page **pages;
                    unsigned int nr_pages;
                    unsigned long phys_addr;
                    void *caller;
            };

            /* vmlist_lock - concurrent protector */
            extern rwlock_t vmlist_lock;

            /* vmlist - the first vm_struct structure */
            extern struct vm_struct *vmlist;
        
            /**
             * get_vm_area - look for a free range of linear addresses between VMALLOC_START
             *               and VMALLOC_END,and attempt to create a new vm_area to map these
             *               linear addresses
             * @size:        size of memory area
             * @flags:       flags of memory area
             * return:       pointer to the vm_area object
             *               NULL => failed
             * # this function actually call to __get_vm_area_node() with arguments
             *     @size, 1, @flags, VMALLOC_START, VMALLOC_END, -1, GFP_KERNEL,
             *     __builtin_return_address(0)
             */
            extern struct vm_struct *get_vm_area(unsigned long size, unsigned long flags);

          <mm/vmalloc.c>
            /**
             * __get_vm_area_node - attempt to create a new vm_struct which map to a specified
             *                      range of linear addresses from a NUMA node
             * @size:               sizeof memory area
             * @align:              alignment
             * @flags:              memory area flags
             * @start:              searching start address
             * @end:                searching end address
             * @node:               NUMA node
             * @gfp_mask:           GFP flag mask
             * @caller:             who called to this routine
             * return:              poiner to vm_struct
             *                      NULL => failed
             */
            static struct vm_struct *__get_vm_area_node(unsigned long size, unsigned long align,
                                                        unsigned long flags, unsigned long start,
                                                        unsigned long end, int node, gfp_t gfp_mask,
                                                        void *caller);

            brief description for __get_vm_area_node() :
              check @flags at first,if enabled VM_IOREMAP,then set local variable @bit to result of fls(@size).
              then
                @bit := @bit > IOREMAP_MAX_ORDER ? IOREMAP_MAX_ORDER : @bit < PAGE_SHIFT ? PAGE_SHIFT : @bit
                reset @align to 1ul << @bit
              next,align @size on PAGE_SIZE,if @size is _zero_ then return NULL.
              attempt to allocate "vm_struct" object from general purpose slab cache through kzalloc_node(),
              if failed,then return NULL.
              update @size to @size plus PAGE_SIZE for safety interval.
              call to alloc_vmap_area() to allocate a vmap_area,this should be the noncontiguous memory area
              information structure that vm_struct associated with.
              if succeed to got vmap_area,then call to insert_vmalloc_vm(),which initializes vm_struct and
              associate vmap_area with the vm_struct,chained vm_struct into @vmlist.
              finally,return the vm_struct.
              /* if failed to get vmap_area,then kfree vm_struct and return NULL! */

          structure vmap_area is used to represent virtual area mapping.
          <mm/vmalloc.c>
            /**
             * vmap_area - structure used to represent mapping betwenn virtual area and linear address
             * @va_start:  start linear address of the virtual area
             * @va_end:    end linear address of the virtual area
             * @flags:     flags =>
             *                     VM_LAZY_FREE 0x01
             *                     VM_LAZY_FREEING 0x02
             *                     VM_VM_AREA 0x04
             * @rb_node:   address sorted rbtree
             * @list:      address sorted list
             * @purge_list: "lazy purge" list
             * @private:   private data,in normally,it is the struct vm_struct object's address
             * @rcu_head:  RCU
             */
            struct vmap_area {
                    unsigned long va_start;
                    unsigned long va_end;
                    unsigned long flags;
                    struct rb_node rb_node;
                    struct list_head list;
                    struct list_head purge_list;
                    void *private;
                    struct rcu_head rcu_head;
            };

            # [@va_start, @va_end] <=====> @private -> pointer of an object is type of struct vm_struct
                               struct vmap_area
                              /* like a bridge */

            !! function alloc_vmap_area() allocate a region of KVA of the specified size
               and alignment within the @va_start and @va_end.the function is defined in
               <mm/vmalloc.c>
               struct vmap_area object is allocated by kmalloc_node().
               the searching is processed by walk @vmap_area_root a red-black tree.
               function free_vmap_area() is used to release a vmap_area object,which is
               defined in the same file.the deleting is process by rb_erase() and RCU
               routine rcu_free_va() to update the vmap_area object.
                       /* kfree that vmap_area object */

      Allocating a Noncontiguous Memory Area :
        function vmalloc() is used to allocate virtually contiguous memory,that is,
        it is contiguous in linear addresses.

        <mm/vmalloc.c>
          /**
           * vmalloc - allocate virtually contiguous memory
           * @size:    size to allocate
           * return:   linear address for the start of memory area
           *           NULL => failed
           * # this function actually call to __vmalloc_node() with arguments 
           *     @size, 1, GFP_KERNEL | __GFP_HIGHMEM, PAGE_KERNEL, -1, caller-address
           */
          void *vmalloc(unsigned long size);

          /**
           * __vmalloc_node - allocate virtual memory area from a specified NUMA node
           * @size:           size of memory area
           * @align:          alignment
           * @gfp_mask:       GFP flag mask
           * @prot:           protection mask for the allocated pages
           *                  @it always set to 0x63,Present,Accessed,Read/Write,Dirty
           * @node:           NUMA node
           * @caller:         caller's return address
           * return:          linear address for the start of memory area
           *                  NULL => failed
           * # this function allocate page frames from page allocator to cover @size memory area,
           *   and then map them in kernel linear address space,using a pagetable protection of @prot
           */
          static void *__vmalloc_node(unsigned long size, unsigned long align, gfp_t gfp_mask,
                                      pgprot_t prot, int node, void *caller);

          brief description for __vmalloc_node() :
            align @size at first,return NULL if @size is _zero_ OR (@size >> PAGE_SHIFT) > totalram_pages.
            use __get_vm_area_node() attempt to create a vm_struct object which would be used for establish
            mapping,return NULL if failed to create.
            function __vmalloc_area_node() is used to allocate memory area with @size and map them in kernel
            linear address space.
            finally,it returns what __vmalloc_area_node() returned.

          /**
           * __vmalloc_area_node - allocate virtual memory area and map it to kernel linear address space
           * @area:                the memory area descriptor
           * @gfp_mask:            GFP flag mask
           * @prot:                protection flags for allocated pages
           * @node:                NUMA node
           * @caller:              who called me?
           * return:               linear address OR NULL(failed)
           */
          static void *__vmalloc_area_node(struct vm_struct *area, gfp_t gfp_mask, pgprot_t prot, int node,
                                           void *caller);

          brief description for __vmalloc_area_node() :
            the page frames would be allocated by page-level allocator is calculated through
                    (@size - PAGE_SIZE) >> PAGE_SHIFT (=> @nr_pages)
            local variable @array_size is equal to the result of "@nr_pages * sizeof(struct page *)".
            @array_size indicates the array of page frame pointers hold how much page frames used to cover the
            memory area.
            if @array_size > PAGE_SIZE(32-bit 4096),use __vmalloc_node() to allocate pages array,and enable
            VM_VPAGES in @area->flags;otherwise,use kmalloc_node() to allocate pages array.
            @area->pages points to the allocate pages array,and @caller member is set to parameter @caller.
            but function would return NULL and kfree @area if we failed to allocate pages array!
            now,array of page frames has been allocated,next,have to allocate element for this array.
            for each element,call to alloc_page() to allocate page frame if @node < 0;otherwise,use
            alloc_pages_node() as instead.
            any NULL page descriptor is got will cause procedure fail,in this case,have to vfree() the
            been allocated page frames in the pages array.
            function map_vm_area() is the main routine to map vm_struct to kernel linear address space.if
            failed to establish mapping,we have to vfree the page frames and return NULL.
            if mapping has been established,then @addr member of @area must point to the linear address,
            then return it as well.

          /**
           * map_vm_area - establish mapping between virtual memory area with kernel linear address
           * @area:        virtual memory area
           * @prot:        page protection flag
           * @pages:       pointer to an array of page frame pointers
           * return:       0 => succeed
           *               error code => failed
           * # this function call to vmap_page_range() to do the primary work
           *   if this function is failed,then *@pages += error code
           */
          int map_vm_area(struct vm_struct *area, pgprot_t prot, struct page ***pages);

          ! start from @area->addr(it is struct vmap_area.va_start)
            end at @addr + @area->size - PAGE_SIZE

          /**
           * vmap_page_range - establish mapping between a range of kernel linear address and 
           *                   given page frames
           * @start:           start address
           * @end:             end address
           * @prot:            page protection flag
           * @pages:           page frame pointer array
           * return:           # this function actually call to vmap_page_range_noflush(),thus,
           *                     if returns what that function returend
           * # this function will call to flush_cache_vmap() to flush hardware cache for virtual
           *   mapping(the linear addresses have been point to another,the olds been invalid)
           */
          static int vmap_page_range(unsigned long start, unsigned long end, pgprot_t prot,
                                     struct page **pages);

          /**
           * vmap_page_range_noflush - routine called by vmap_page_range() to establish mapping
           * @start:                   start address
           * @end:                     end address
           * @prot:                    page protection flag
           * @pages:                   page frame pointer array
           * return:                   number of setup page table entries
           *                           -ENOMEM => failed
           */
          static int vmap_page_range_noflush(unsigned long start, unsigned long end, pgprot_t prot,
                                             struct page **pages);

          brief description for vmap_page_range_noflush() :
            get kernel Page Global Directory through pgd_offset_k(addr),which is architecture depended
            macro function,it will be expanded to "@init_mm.pgd + pgd_index(@start)".
            enter a do-while cycle,endup when @start == @end.
            in each cycle,get the address of next boundary through macro pgd_addr_end() which is defined
            in <asm-generic/pgtable.h>,if no next boundary,@end would be returned.
            call to function vmap_pud_range() to setup Page Upper Directory,and which will call to
            vmap_pmd_range() to setup Page Middle Directory,then is vmap_pte_range() to setup
            Page Table Entries(through set_pte_at()).
            /**
             * Hierarchy of Page Table of Linux has been descripted in "Paging in Linux".
             * vmap_pud_range() attempt to get pud_t through pud_alloc() from @init_mm in @pgd,
             * vmap_pmd_range() attempt to get pmd_t through pmd_alloc() from @init_mm in @pud,
             * vmap_pte_range() attempt to get pte_t through pte_alloc_kernel() from @pmd
             *                                  # new Page Table
             */

         !! BEFORE map_vm_area() IS CALLED AND SUCCEED TO SETUP PAGE TABLE,ALL LINEAR ADDRESSES
            IN struct vmap_area ARE NO SENSE,WHICH ARE NOT MAPPED IN KERNEL LINEARA ADDRESS SPACE.
            /**
             * c3 switch in,but no such linear addresses can not accessed at the kernel code address
             * segment.
             */

         !! NOTICE THAT,FUNCTION vmalloc() FIDDLES KERNEL Page Global Directory THAT IS KERNEL
            "Page Table Entries",IF A PROCESS IN KERNEL MODE ATTEMPT TO ACCESS SUCH LINEAR ADDRESSES
            WILL CAUSE "Page Fault".BUT "Page Fault" EXCEPTION HANDLER WILL CAPTURE THIS EXCEPTION
            AND TRY TO FIND OUT SUCH "Page Table Entries" CORRESPONDING TO THE LINEAR ADDRESSES FROM
            KERNEL "Page Table",IF FOUND OUT,THEN COPIES THEM INTO PROCESS "Page Table",AND RESUME 
            EXECUTION OF THE PROCESS.

         !! FUNCTION vmalloc_32() ATTEMPT TO ALLOCATE PAGE FRAMES FROM ZONE_DMA AND ZONE_NORMAL.

      Releasing a Noncontiguous Memory Area :
        function vfree() is used to do the reverse to what vmalloc() did,that is,it releases
        noncontiguous memory areas created by vmalloc() or vmalloc_32().
        function vunmap() releases memory areas created by vmap().

        <mm/vmalloc.c>
          /**
           * vfree - release memory allocated by vmalloc()
           * @addr:  memory base address
           * # this function free the virtually contiguous memory area starting at @addr,
           *   if @addr is NULL,no operation is performed
           * # this function actually call to __vunmap(@addr, 1)
           */
          void vfree(const void *addr);

          /**
           * vunmap - release virtual mapping obtained by vmap()
           * @addr:   memory base address
           * # this function free the virtually contiguous memory area starting at @addr,
           *   which was created from the page array passed to vmap()
           * # this function actually call to __vunmap(@addr, 0)
           */
          void vunmap(const void *addr);

          !! BOTH THE TWO FUNCTIONS ABOVE CAN NOT BE CALLED WITH IN _INTERRUPT_ CONTEXT.

          /**
           * __vunmap - routine for release virtual memory area
           * @addr:     base address
           * @deallocate_pages: whether deallocate page frames mapped to this memory area
           */
          static void __vunmap(const void *addr, int deallocate_pages);

          what __vunmap() does :
            first,check @addr if valid through "(PAGE_SIZE - 1) & (unsigned long)@addr",if
            @addr is not valid,then report error and return to caller.
            call to remove_vm_area() to find out the struct vm_struct corresponding to @addr,
            the function will unmap linear addresses for the @addr in kernel linear address space,
            that is,it manipulate Page Table.
            /**
             * # this function is NOT safe on SMP machines.
             * remove_vm_area() call to find_vmap_area() attempt to find out the vmap_area object
             * for @addr.
             *
             * if _FOUND_ AND VM_VM_AREA flag is enabled in @va->flags,then unlink the vm_struct
             * from @vmlist.next,call to vmap_debug_free_range(),this function will unmap page tables
             * and force a TLB flush immediately if CONFIG_DEBUG_PAGEALLOC is set.
             * call to free_unmap_vmp_area(@va) to free and unmap a vmap area,this function will
             * increase @vmap_lazy_nr by result of "(va_end - va_start) / PAGE_SIZE",if @vmap_lazy_nr
             * has exceeded maximum limit value,then call to try_purge_vmap_area_lazy() to purges all
             * lazily-freed vmap areas.(that is,all kernel linear addresses will be recycled)
             * !! in this stage,function vunmap_page_range() would be called to manipulate kernel
             *    Page Table.
             * !! the vmap_area object for these linear addresses would be freed by __free_vmap_area()
             *    later called by __purge_vmap_area_lazy().
             *
             * modify @size member of vm_struct object,subtract PAGE_SIZE(because it had been unlinked
             * from @vmlist),return @vm.
             *
             * if _UNFOUND_,return NULL.
             */
            if @deallocate_pages is TRUE,then __vunmap() must deallocate all page frames they have been
            mapped in the virtual memory area.
            /**
             * call __free_page() for each page pointer in struct vm_struct.@area.
             * if VM_VPAGES is enabled in struct vm_struct.@flags,then call to vfree() to release the
             * page pointer array,otherwise,call to kfree() to release the page pointer array.
             */
            finally,function call to kfree() to free the struct vm_struct object which is found out
            previously.

          !! vunmap_page_range() -> vunmap_pud_range() -> vunmap_pmd_range() -> vunmap_pte_range ->
             ptep_get_and_clear() for each Page Table Entry
             /**
              * static inline function ptep_get_and_clear() is defined in <arch/x86/include/asm/pgtable.h>.
              * it call to native_ptep_get_and_clear(),which is defined in <arch/x86/include/asm/pgtable_64.h>,
              * this function call to native_pte_clear() make a _zero_ pte and use it to instead the old,and
              * return old one.
              * # if CONFIG_SMP is defined,just call to native_make_pte() directly.
              */
          !! kernel leaves unchanged the entries of the process page tables mapping the fourth gigabyte,
             because the kernel never reclaims PUD PMD PT rooted at the master kernel Page Global Directory.
             ONLY _CLEAR_ PTE,NOT RECLAIMING.(ACCESS TO _ZERO_ ENTRY,Page Fault Handler WILL CONSIDER SUCH
             ACCESSES A BUG,BECAUSE THE MASTER KERNEL PAGE TABLES DO NOT INCLUDE VALID ENTRIES.)


/* END OF CHAPTER8 */


Chapter 9 : Process Address Space
    Memory allocating behaviors in Kernel Mode and in User Mode :
      > if a kernel function makes a request for dynamic memory,it must have a valid reason to issue that
        request,and there is no point in trying to defer it.
      > kernel trust itself,all kernel functions are assumed to be error-free,so the kernel does not need
        to insert any protection against programming errors.
      > kernel will try to defer dynamic memory allocating request issued by a user process from User Mode,
        all requests are considered non-urgent.
      > user programs cannot be trusted,the kernel must be prepared to catch all addressing errors caused
        by processes in User Mode.

      ! "interval" - "memory region"
        when process attempt to get additional page frames,instead,it will gets the right to
        use a new range of linear addresses,which become part of its address space,that is the interval.

    The Process's Address Space :
      the address space of a process consists of all linear addresses that the process is allowed to use,
      each process sees a different set of linear addresses.
      kernel use new resource named "memory region" to represents process's address space,which is characterized
      by an initial linear address,a length,and some access rights.both initial address and length must be 
      multiples of 4096,this requirement is made for efficiency.
            /* data identified by each memory region completely fills up the page frames allocated to it */

      situations in which a process gets new memory regions :
        1> process fork /* child get new memory regions,copy-on-write */
        2> process does exec syscall
        3> process does memory mapping on a file
        4> kernel decide to expand the size of process User Mode stack
           /* all stack spaces had been used */
        5> process create IPC Shared Memory
        6> process expand its heap

      syscalls related to the process's memory regions changing :
        brk                   changes the heap size of the process
        execve                loads a new executable file,thus changing the process address space
        _exit                 terminates the current process and destroys its address space
        fork                  creates a new process,and thus a new address space
        mmap, mmap2           creates a memory mapping for a file,thus enlarging the process address space
        mremap                expands or shrinks a memory region
        remap_file_pages      creates a non-linear mapping for a file        [[deprecated]]
        munmap                destroys a memory mapping for a file,thus contracting the process address space
        shmat                 attaches a shared memory region
        shmdt                 detaches a shared memory region

      Page Fault Exception Handler is essential for the kernel to identify the memory regions currently owned
      by a process which cause Page Fault.
      this allows the Page Fault Exception Handler to efficiently distinguish between two types of invalid
      linear addresses that cause it to be invoked :
        1> caused by programming errors
        2> caused by a missing page
           /**
            * the linear address belongs to the process address space,but corresponding page frames are
            * not existed.(for process,the linear addresses which no page frames mapped to it are invalid)
            * "demand paging" : kernel provides the missing page frame and lets the process continue.
            */

    The Memory Descriptor :
      linux introduced a structure mm_struct as the descriptor of address space of the process.
      @mm member of process descriptor is the memory descriptor pointer of that process.

      <linux/mm_types.h>
        /**
         * structure vm_area_struct is defined in the same header,which is a part of process
         * virtual memory space that has a special rule for Page Fault handlers.
         * it defines a memory "virtual memory mapping" memory area.
         * it is differ from vm_struct,the structure vm_struct defines noncontiguous memory area,
         * which is a part of memory allocator.
         */

        /**
         * mm_struct - structure collected all information related to the process address space,
         *             mm_struct is the memory descriptor of a process
         * @mmap:      list of VMAs,that is memory region objects
         * @mm_rb:     root of the red-black tree of memory region objects
         * @mmap_cache: last find_vma() result,that is the last referenced memory region object
         * @get_unmapped_area: routine used to searches an available linear address interval
         *                     in the process address space
         * @unmap_area: routine used to release a linear address interval
         * @mmap_base: base of mmap area,identifies the linear address of the first allocated
         *             anonymous memory region or file memory mapping
         * @task_size: size of task vm(virtual memory) space
         * @cached_hole_size: if non-zero,the largest hole below @free_area_cache
         *                    # when seaching encoutered a hole,this member would be updated
         * @free_area_cache:  first hole of size @cached_hole_size or larger,address from there
         *                    the kernel will look for a free interval of linear addresses in the
         *                    process address space
         *                    # this member always be updated on the last successed memory area
         *                      searching
         *                    # this member usually is initialized to one-third of User Mode
         *                      linear address space,that is 1 GiB (total is 3 GiB)
         *                    # linear address start from @free_area_cache may be a memory hole,
         *                      but it just possibility,if @cached_hole_size is not zero,then
         *                      there must be a memory hole which is below @free_area_cache
         * @pgd:       process Page Global Directory
         * @mm_users:  how many users with user space,secondary usage counter
         *             the number of lightweight processes that share the mm_struct data structure
         * @mm_count:  how many references to "struct mm_struct",main usage counter
         *             all users counted by @mm_users are treated as one unit in @mm_count
         *             kernel deallocate this mm_struct when detected this member becomes _ZERO_
         * @map_count: number of VMAs
         *             # default value is 65536,but system administrator can modify this limit
         * @mmap_sem:  memory region read/write semaphore
         * @page_table_lock: protects page tables and some counters
         * @mmlist:    list of maybe swapped mm's,these are globally strung together off @init_mm.mmlist,
         *             and protected by @mmlist_lock
         *             this member is used to chain all memory descriptors
         * @hiwater_rss: high-watermask of RSS usage,maximum number of page frames ever owned by the process
         * @hiwater_vm:  high-water virtual memory usage,maximum number of page frames ever included in the 
         *               memory regions of the process
         * @total_vm -- @env_end: the meanings are consist to their names
         *                        except @def_flags and @nr_ptes,all are linear addresses in the process
         *                        address space
         *                        @def_flags: the default access rights for the memory regions
         *                        @nr_ptes: how much Page Table Entry the process address space occupied
         *                                  this member is increased by function __pte_alloc(),as
         *                                  described in earily Chapter,a new Page Table Entry is necessay
         *                                  when building mapping between linear addresses and page frames
         *                                  # this member is initialized to _ZERO_ by mm_init()
         *                                    (Copy-On-Write)
         * @saved_auxv: used when starting the execution of an ELF program,for /proc/<pid>/auxv
         * @rss_stat:  special counter for RSS statistics,protected by @page_table_lock
         * @binfmt:    linux binary file format
         * @cpu_vm_mask: bit mask for lazy TLB switches
         * @context:   architecture-specific MM context
         * @faultstamp: last value of global fault stamp as seen by this process,
         *              how long it has been since this task got the token
         * @token_priority: token priority
         * @last_interval: last interval
         * @flags:     swap flags,must use atomic bitops to access the bits
         * @core_state: coredump support
         * @ioctx_lock: lock used to protect the list of asynchronous I/O contexts
         * @ioctx_list: asynchronous I/O context hash list
         * @owner:     who has this mm_struct
         * @exe_file:  store reference to file /proc/<pid>/exe symlink points to
         * @num_exe_file_vmas: number of virtuam memory areas that exe file has
         * @mmu_notifier_mm: memory management unit notifier
         */
        struct mm_struct {
                struct vm_area_struct *mmap;
                struct rb_root mm_rb;
                struct vm_area_struct *mmap_cache;
        #ifdef CONFIG_MMU
                unsigned long (*get_unmapped_area)(struct file *filp, unsigned long addr,
                                                   unsigned long len, unsigned long pgoff,
                                                   unsigned long flags);
                void (*unmap_area)(struct mm_struct *mm, unsigned long addr);
        #endif
                unsigned long mmap_base;
                unsigned long task_size;
                unsigned long cached_hole_size;
                unsigned long free_area_cache;
                pgd_t *pgd;
                atomic_t mm_users;
                atomic_t mm_count;
                int map_count;
                struct rw_semaphore mmap_sem;
                spinlock_t page_table_lock;

                struct list_head mmlist;

                unsigned long hiwater_rss;
                unsigned long hiwater_vm;

                unsigned long total_vm, locked_vm, shared_vm, exec_vm;
                unsigned long stack_vm, reserved_vm, def_flags, nr_ptes;
                unsigned long start_code, end_code, start_data, end_data;
                unsigned long start_brk, brk, start_stack;
                unsigned long arg_start, arg_end, env_start, env_end;

                unsigned long saved_auxv[AT_VECTOR_SIZE];

                struct mm_rss_stat rss_stat;

                struct linux_binfmt *binfmt;

                cpumask_t cpu_vm_mask;
               
                mm_context_t context;

                unsigned int faultstamp;
                unsigned int token_priority;
                unsigned int last_interval;
                
                unsigned long flags;

                struct core_state *core_state;

        #ifdef CONFIG_AIO
                spinlock_t ioctx_lock;
                struct hlist_head ioctx_list;
        #endif
        #ifdef CONFIG_MM_OWNER
                struct task_struct *owner;
        #endif
        #ifdef CONFIG_PROC_FS
                struct file *exe_file;
                unsigned long num_exe_file_vmas;
        #endif
        #ifdef CONFIG_MMU_NOTIFIER
                struct mmu_notifier_mm *mmu_notifier_mm;
        #endif
        };

        ! about @mm_users and @mm_count,the mm_struct object maybe lent to kernel thread,
          and kernel thread might increase @mm_users to keep @mm_count do not become zero
          in side-effect;or increase @mm_count directly.
          if @mm_count become zero,then kernel deallocate it,but if it is using by a kernel
          thread now,so it should not to be destroyed.

      function mm_alloc() is used to allocate a new memory descriptor from slab cache.
      function mmput() used to decrease @mm_users.
      function mmdrop() used to decrease @mm_count.

      <linux/sched.h>
        /**
         * mm_alloc - allocate a new memory descriptor and initializes it
         * return:    pointer to the memory descriptor allocated
         *            NULL => failed to allocate
         * # this function call to kmem_cache_alloc() attempt to allocate a memory descriptor
         *   from special purpose slab cache "mm_cachep"
         * # this function will sets the @mm_count and @mm_users to 1
         * # the implementation is introduced in <kernel/fork.c>
         */
        extern struct mm_struct *mm_alloc(void);

         
        /**
         * mmput - gets rid of the mappings and all user-space,it decreases
         *         the use count and release all resources for an mm
         * @mm:    memory descriptor
         * # this function will decrease @mm_users,if @mm_users become _zero_,
         *   it releases the Local Descriptor Table,thus the memory region descriptors
         *   and the Page Tables referenced by the memory descriptor,and then call to
         *   mmdrop()
         * # the implementation is introduced in <kernel/fork.c>
         */
        extern void mmput(struct mm_struct *mm);

        /**
         * mmdrop - drop an memory descriptor
         * @mm:     memory descriptor
         * # this function actually call to __mmdrop(@mm) if @mm_count becomes _zero_,
         *   in the case,@mm will be deallocated
         * # the implementation of function __mmdrop() is introduced in <kernel/fork.c>
         */
        static inline void mmdrop(struct mm_struct *mm);

    Memory Descriptor of Kernel Threads :
      kernel threads run only in Kernel Mode,so they are never access linear addresses below
      TASK_SIZE(defined in <arch/x86/include/asm/processor.h>, expanded to PAGE_OFFSET),
      and kernel threads do not use memory regions,thus,most of the fields of a memory
      descriptor are meaningless for them.

      Page Table entries for linear addresses above TASK_SIZE should always be identical,
      and kernel thread never access to linear addresses above TASK_SIZE,thus what set of
      Page Table entries the kernel thread uses is not matter.
      ! but for performance(TLB useless and cache flushes),kernel thread uses the set of
        Page Tables of the last previously running regular process.
        /**
         * So,structure task_struct contains two members of type poiner to memory descriptor
         * are named @mm and @active_mm,respectively.
         * For regular process,@mm and @active_mm are point to the same memory descriptor;
         * but for kernel thread,@mm is NULL,and when the kernel thread is going to runs,
         * @active_mm will be initialized with the value of @active_mm of the previously
         * running process.
         */

      ! if a process in Kernel Mode has modified the Page Table entry for "high" linear
        address(above TASK_SIZE),then it have to update the corresponding entry in the 
        sets of Page Tables of all processes in the system.
        /* once set,the entry should be effective for all other processes in Kernel Mode */

        BUT,updating for all processes in the system is costly operation,therefore,linux
        adopts a deferred approach.
        /**
         * the approach has been described in "Noncontiguous Memory Area".
         * PAGE_OFFSET => 0xc0000000 (3 GiB linear address)
         * make up linear address mapping above 3 GiB :
         *   vmalloc {
         *           allocate linear addresses
         *           allocate page frames
         *           get master Kernel Page Global Directory
         *           create new PUD PMD PTE entries
         *           TLB cache flush and reload
         *   }
         *   # kernel updates a canonical set of Page Tables rooted at the
         *     @swapper_pg_dir,master Kernel Page Global Directory.
         *   # master Kernel Page Global Directory is pointed by @init_mm.@pgd.
         */        

    Memory Regions :
      Process Address Space =>
        +----> Memory Descriptor @mm OR @active_mm 
        |
        +-------------------------------------------------------------+
        |     |     |     |                         |     |     |     |
        | MR1 | MR2 | ... | unused linear addresses | MRk | ... | MRn |
        |     |     |     |                         |     |     |     |
        +-------------------------------------------------------------+
          |            |
          |            +----> Last accessed MR => struct mm_struct.@mmap_cache
          +----> Memory Region => struct mm_struct.@mmap
      
      Linux implemented a special structure named vm_area_struct to represents the memory region
      in process's address space.
        <linux/mm_types.h>
          /**
           * vm_area_struct - structure used to represent memory region,
           *                  it identifies a linear address interval
           * @vm_mm:          the address space this structure belong to
           * @vm_start:       start linear address within @vm_mm,count from _zero_
           * @vm_end:         end linear address within @vm_mm,real end is @vm_end - 1
           * @vm_next:        linked list of virtual memory areas per task,sorted by address
           * @vm_page_prot:   access permissions of this VMA
           * @vm_flags:       virtual memory area flags
           * @vm_rb:          data for the red-black tree
           * @shared:         links to the data structures used for reverse mapping
           *                  for areas with an address space and backing store : (OR)
           *                    linkage into the address_space->i_mmap prio tree
           *                    linkage to the list of like vmas hanging off its node
           *                    linkage of vma in the address_space->i_mmap_nonlinear
           * @vm_set:         set of vmas are shared file page
           * @list:           doubly linked list for the vmas have same radix_index and heap_index
           * @parent:         aligns with @prio_tree_node parent
           * @head:           head of the linked vmas
           * @prio_tree_node: raw priority tree node - PST associated with this VMA
           * @anon_vma_chain: chain of anonymous vmas
           *                  # serialized by @mmap_sem AND @page_table_lock
           * @anon_vma:       pointer to the struct anon_vma data structure
           *                  # serialized by @page_table_lock
           * @vm_ops:         methods of the memory region
           * @vm_pgoff:       offset in PAGE_SIZE units
           * @vm_file:        the file this memory region map to(can be NULL)
           *                  # can be regular file or block device file
           * @vm_private_data:
           *                  VMA private data,it was vm_pte(shared mm)
           * @vm_truncate_count:
           *                  used when releasing a linear address interval in a 
           *                  non-linear file memory mapping
           *                  # truncate_count or restart_addr
           * @vm_region:      NOMMU mapping region
           *                  # CONFIG_MMU did not selected
           * @vm_policy:      NUMA policy for the VMA
           *                  # require CONFIG_NUMA
           */
          struct vm_area_struct {
                  struct mm_struct *vm_mm;
                  unsigned long vm_start;
                  unsigned long vm_end;
                  struct vm_area_struct *vm_next;
                  pgprot_t vm_page_prot;
                  unsigned rb_node vm_rb;

                  union {
                          struct {
                                  struct list_head list;
                                  void *parent;
                                  struct vm_area_struct *head;
                          } vm_set;
                          
                          struct raw_prio_tree_node prio_tree_node;
                  } shared;                    
                  
                  struct list_head anon_vma_chain;
                  struct anon_vma *anon_vma;

                  const struct vm_operations_struct *vm_ops;
                  unsigned long vm_pgoff;
                  struct file *vm_file;
                  void *vm_private_data;
                  unsigned long vm_truncate_count;

          #ifndef CONFIG_MMU
                  struct vm_region *vm_region;
          #endif
          #ifdef CONFIG_NUMA
                  struct mempolicy *vm_policy;
          #endif
          };

      ! Memory regions owned by a process never overlap.
        kernel attempts to merge memory regions when a new one is allocated right next to an existed
        one.(but the access rights of memory regions must be the same!)
        # MR1 => existed, MRnew => new allocated, MRnew.vm_start == MR1.vm_end => MRa := MR1 + MRnew
          /* MR1.vm_page_prot == MRnew.vm_page_prot */

      When a new memory region is allocated to the process address space :
        a> kernel check if there is one memory region which the new one can be merged into it,
           then enlarge the old one
        b> no such memory region is existed which can be combined with the new one,then create the
           new memory region

      When a range of linear addresses is removed from the process address space :
        a> if a memory region which identified linear address interval is just right equal to the
           range of linear addresses,then remove the memory region
        b> if the memory region is longer than the range of linear addresses,then resize the memory
           region,and if necessary,split it to two memory regions
                                   /**
                                    * the range of linear addresses is included by the memory region,
                                    * and right at the middle position
                                    */
           ! REMOVES A LINEAR ADDRESS INTERVAL MAY THEORETICALLY FAIL BECAUSE NO FREE MEMORY IS 
             AVAILABLE FOR A NEW MEMORY DESCRIPTOR /* THE NEW ONE IS SPLITED FROM THE OLD */

      member @vma_ops is type of const struct vm_operations_struct pointer,and structure vm_operations_struct
      collected some methods to be used by kernel control path to manipulate the memory region.
        <linux/mm.h>
          /**
           * vm_operations_struct - collection of the virtual MM functions
           * @open:                 routine to open an area
           *                        # invoked when the memory region is added to the set of regions
           *                          owned by a process
           * @close:                routine to close an area and unmap it
           *                        # invoked when the memory region is removed from the set of
           *                          regions owned by a process
           *                        # needed to keep files on disk up-to-date etc
           * @fault:                routine for no-page or wp-page exections
           *                        # invoked by Page Fault exception handler
           * @page_mkwrite:         routine for notification that a previously read-only page
           *                        is about to become writable,if an error is returned,it will cause
           *                        a SIGBUS
           * @access:               routine called by access_process_vm() when get_user_pages() fails,
           *                        typically for use by special VMAs that can switch between memory
           *                        and hardware
           * @set_policy:           set up NUMA policy for the memory region,it must add a reference to
           *                        any non-NULL @new mempolicy to hold the policy upon return
           *                        # caller can provides NULL @new to remove a policy and fall back to
           *                          surrounding context(neither MPOL_DEFAULT,nor the task or system default)
           * @get_policy:           get the NUMA policy of this memory region,it must add reference [mpol_get()]
           *                        to any policy at (@vma, @addr) marked as MPOL_SHARED,if no MPOL_SHARED marked,
           *                        then must not add a reference
           *                        # if no [shared/vma] mempolicy exists at the @addr,this routine must
           *                          return NULL(no fallback)
           * @migrate:              routine for migrate this VMA from the NUMA nodes specified by @from to
           *                        the NUMA nodes specified by @to with new flags @flags
           */
          struct vm_operations_struct {
                  void (*open)(struct vm_area_struct *area);
                  void (*close)(struct vm_area_struct *area);
                  void (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf);
                  int (*page_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf);
                  int (*access)(struct vm_area_struct *vma, unsigned long addr, void *buf, int len, int write);

          #ifdef CONFIG_NUMA
                  int (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new);
                  struct mempolicy *(*get_policy)(struct vm_area_struct *vma, unsigned long addr);
                  int (*migrate)(struct vm_area_struct *vma, const nodemask_t *from,
                                 const nodemask_t *to, unsigned long flags);
          #endif
          };

          ! header <linux/mm.h> extern declared a special slab cache object *@vm_area_cachep,which is
            used by allocating memory region descriptor.

      Memory Region Data Structures :
        memory regions of a process are sorted in ascending order by the memory addresses,
        successive regions can be separated by an area of unused memory addresses.

        struct mm_struct.@mmap is the pointer to the first memory region in the address space.
        
        ! kernel access to @mmap for find out a specified memory region which contains a request
          linear address only if @map_count is small,less than a few tens of elements in the list.
          if @map_count is too great,then search on list is inefficient.

        struct mm_struct.@mm_rb is the root of red-black tree in which each node is corresponding to a
        memory region in the address space.
        struct vm_area_struct.@vm_rb is the red-black tree node object,there also used embedded data
        structure,because the offset of struct member is fixed during compile-time.macro container_of()
        can be used to retrieve the memory region which contains a specified red-black tree node.
        /**
         * red-black is auto-balancing binary search tree.
         * red-black tree rules :
         *   1> every node must be either red or black
         *   2> the root of the tree must be black
         *   3> the children of a red node must be black -> no succssive red nodes
         *   4> every path from a node to a descendant leaf must contain the same number of black nodes
         *   !  null node is counted as black node
         *
         * each new node to be inserted must as a leaf and color red.
         *
         * a red-black tree has N nodes must has a height of at most 2 * log(N + 1)
         * operations on balanced binary search tree => O(logN) N is nodes in the tree
         */

        ! struct mm_struct.@mm_rb often used to find out a specified linear address from all memory regions
          struct mm_struct.@mmap often used to scanning all memory regions in the address space

      Memory Region Access Rights :
        there are three kinds flags for pages :
          1> hardware flags stored in Page Table Entry
          2> flags stored in page descriptor used by linux kernel
          3> flags stored in @vm_flags of struct vm_area_struct

        the third flags associated with the pages of a memory region.
        <linux/mm.h>
          /* currently active flags */
          #define VM_READ 0x00000001
          #define VM_WRITE 0x00000002
          #define VM_EXEC 0x00000004
          #define VM_SHARED 0x00000008

          /* mprotect() hardcodes VM_MAYREAD >> 4 == VM_READ,and so on */
          #define VM_MAYREAD 0x00000010
          #define VM_MAYWRITE 0x00000020
          #define VM_MAYEXEC 0x00000040
          #define VM_MAYSHARE 0x00000080

          

          #define VM_GROWSDOWN 0x00000100 /* general info on the segment */
          #define VM_GROWSUP 0x00000200
                                          /**
                                           * If one of these two flags is set,
                                           * that means this VMA contains a stack
                                           *                 # mapping a process stack
                                           * in this case,@vm_start may only decrease/increase
                                           * but @vm_end always fixed up
                                           *                        # even 'pop' instruction
                                           *                          processed
                                           * the size of VMA stay unchanged even @vm_start
                                           * changed
                                           */

          #define VM_PFNMAP 0x00000400 /* Page-ranges managed without "struct page",just pure PFN */
          #define VM_DENYWRITE 0x00000800 /* ETXTBSY on write attempts.. */

          #define VM_EXECUTABLE 0x00001000
          #define VM_LOCKED 0x00002000
          #define VM_IO 0x00004000 /* Memory mapped I/O or similar */

          /* used by sys_madvise() */                        
          #define VM_SEQ_READ 0x00008000 /* Application will access data sequentially */
          #define VM_RAND_READ 0x00010000 /* Application will not benefit from clustered reads */

          #define VM_DONTCOPY 0x00020000 /* no copy on fork */
          #define VM_DONTEXPAND 0x00040000 /* cannot expand with mremap() */
          #define VM_RESERVED 0x00080000 /* count as reserved_vm like IO */
          #define VM_ACCOUNT 0x00100000 /* is a VM accounted object */
          #define VM_NORESERVE 0x00200000 /* should the VM suppress accounting */
          #define VM_HUGETLB 0x00400000 /* huge TLB page VM */
          #define VM_NONLINEAR 0x00800000 /* is non-linear (remap_file_pages) */
          #define VM_MAPPED_COPY 0x01000000 /* T if mapped copy of data(nommu mmap) */
          #define VM_INSERTPAGE 0x02000000 /* the VMA has had "vm_insert_page()" done on it */
          #define VM_ALWAYSDUMP 0x04000000 /* always include in core dumps */

          #define VM_CAN_NONLINEAR 0x08000000 /* has ->fault & does nonlinear pages */
          #define VM_MIXEDMAP 0x10000000 /* can contain "struct page" and pure PFN pages */
          #define VM_SAO 0x20000000 /* Strong Access Ordering(powerpc) */
          #define VM_PFN_AT_MAP 0x40000000 /* PFNMAP VMA that is fully mapped at mmap time */
          #define VM_MERGEABLE 0x80000000 /* KSM may merge identical pages */

          #ifndef VM_STACK_DEFAULT_FLAGS
          #define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS /* stack default flags,arch can override this */
          #endif

          /* stack flags */
          #ifdef CONFIG_STACK_GROWSUP
          #define VM_STACK_FLAGS (VM_GROWSUP | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)
          #else
          #define VM_STACK_FLAGS (VM_GROWSDOWN | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)
          #endif

        Page Frame access rights for a memory region can be combined arbitrarily.
        Access rights can be implemented efficiently by duplicate page access rights for a memory region
        to the Page Table Entries,then the checking will only be processed by Paging Unit circuitry.
        @vm_page_prot is the Page Table Entry flags(protection bits) used to initialize the rights of
        page frames which are assigned to this memory region.

        ! the translating from access rights for pages of memory region is not straightforward to
          Page Table Entry protection bits.
          the reasons :
            1> in some cases,a page access should generate a Page Fault exception even when its access
               type is granted by the page access rights specified in the @vm_flags member of the
               corresponding memory region.
               /* Copy-On-Write mechanism */
            2> 80x86 Page Table have just Read/Write bit and User/Supervisor bit,and the User/Supervisor
               for a page frame in a memory region must be always setup because have to let the page frame
               can be accessed from User Mode.
            3> PAE(NX,No eXecute flag).
               /**
                * if CONFIG_PAE and CPU has NX flag 
                * then
                *         rule1 :
                *                 the Execute access right always implies the Read access right
                *         rule2 :
                *                 the Write access right always implies the Read access right
                * else if no CONFIG_PAE
                *         rule1 :
                *                 the Read access right always implies the Execute access right
                *                 and vice versa
                *         rule2 :
                *                 the Write access right always implies the Read access right
                */

            !! THE PAGE FRAME IS WRITE-PROTECTED WHENEVER THE CORRESPONDING PAGE MUST NOT BE SHARED BY
               SEVERAL PROCESSES.
               /* if no VM_SHARED */

        rules for the 16 possible combinations of Read,Write,Execute,Share access rights :
          > Write AND Share -> Read/Write bit is set in Page Table Entry
          > Read AND Execute AND NOT(Write OR Share) -> Read/Write is cleared in Page Table Entry
          > Nx AND NOT(Execute) -> CPU NX flag is set
          > NOT(Read OR Write OR Execute OR Share) -> Present is cleared in Page Table Entry
            /**
             * Linux use Page size bit to distinguish that the page frame is real non-present from
             * NOT(Read AND Write AND Execute AND Share).
             * Present disabled AND Page size enabled -> The Page Frame is real non-present
             */
        ! THE 16 POSSIBLE COMBINATIONS FOR ACCESS RIGHTS ARE STORED IN ARRAY NAMED @protection_map
          WHICH IS TYPE OF pgprot_t AND extern DECLARED IN HEADER <linux/mm.h>.
          /* EACH ELEMENT IS A COMBINATION FOR PROTECTION BITS OF THE PAGE TABLE ENTRY */
        /**
         * Associating between Map type and Page Protection in Linux 2.6.34.1 :
         *   map_type        prot
         *                   PROT_NONE        PROT_READ        PROT_WRITE        PROT_EXEC
         *   MAP_SHARED      r (no) no        r (yes) yes      r (no) yes        r (no) yes
         *                   w (no) no        w (no) no        w (yes) yes       w (no) no
         *                   x (no) no        x (no) yes       x (no) yes        x (yes) yes
         *
         *   MAP_PRIVATE     r (no) no        r (yes) yes      r (no) yes        r (no) yes
         *                   w (no) no        w (no) no        w (copy) copy     w (no) no
         *                   x (no) no        x (no) yes       x (no) yes        x (yes) yes
         */

      Memory Region Handling :
        the primary procedures used to enlarge and shrink address space of a process are
        do_mmap() and do_munmap(),respectively.
        before the main procedures,there are some auxiliary methods used to simplify these two
        functions' works.

        Finding the closest region to a given address - find_vma() :
          for find out the most closest memory region to a given address,function find_vma()
          is introduced and take charge of this work.

          <mm/mmap.c> /* CONFIG_MMU;if no CONFIG_MMU,then which is defined in <mm/nommu.c> */
            /**
             * find_vma - look up a VMA which satisfies a specifial condition that is
             *            which may includes the given linear address
             *            (in other words,VMA->@vm_end > @the_given_linear_address)
             * @mm:       memory descriptor,and it is where we look for
             * @addr:     the linear address
             * return:    NULL => unfound OR NULL @mm
             *            pointer to VMA descriptor
             */
            struct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr);

            brief description for find_vma() :
              it checks memory region cache of the memory descriptor @mm at first.
              if the cache is the appropriate VMA,then it must satisfied the condition :
                      cache AND cache->@vm_end > @addr AND cache->@vm_start <= @addr
              if no cache hitting,then scanning the memory regions in the memory try to
              find out a proper one.
              as described previously,for optimize to scanning procedure,red-black tree
              must be considered.
              because the key of left node is less than current and less than right node,
              thus we can quickly find out it in O(logN) times.
              /* through binary search tree FIND() procedure */
              if we finally find the memory region,then modify cache to this one,and return it;
              otherwise,return NULL.

              ! have to pay attention about that @addr may not be included in the memory region
                selected by this function.
                because,find_vma() returns the most closest memory region to @addr,if it is
                lie outside of any memory region,then it is one memory region is 
                selected probably yet.
                /**
                 * @vm_end > @addr BUT @vm_start also > @addr then should goto left branch.
                 * before goto left branch,current VMA identifier have been updated to "this" VMA.
                 * thus if @left is NULL,the last accessed VMA will as the result.
                 */

            /**
             * find_vma_prev - similar to find_vma(),but writes the pointer of previous memory
             *                 region of the memory region by this function to an additional
             *                 parameter
             * @mm:            memory descriptor
             * @addr:          linear address
             * @pprev:         where address of the prevous memory region to be stored
             * return:         NULL => unfound OR NULL @mm
             *                 pointer to the memory region
             * # note that,the previous memory region is not the parent node,its @vm_end
             *   must greater than or equal to @addr
             * # if result is the first memory region in the memory,then *@pprev should be
             *   set with value NULL
             */
            struct vm_area_struct *find_vma_prev(struct mm_struct *mm, unsigned long addr,
                                                 struct vm_area_struct **pprev);

            /**
             * find_vma_prepare - routine used to find the appropriate position where will be
             *                    used to insert a new leaf contains a specified linear address as
             *                    its key value to red-black tree,this routine will returns some
             *                    necessary informations in function parameters
             * @mm:               memory descriptor
             * @addr:             linear address
             * @pprev:            pointer to the pointer of previous memory region
             *                    previous memory region's @vm_end is always less than or equal to
             *                    this memory region's @vm_start
             * @rb_link:          pointer to pointer to pointer to the red-black node which is
             *                    selected at last proceduring
             *                    **@rb_link := pointer to pointer to the last accessed one
             * @rb_parent:        pointer to pointer to the parent node of @rb_link
             *                    *@rb_parent := pointer to parent of ***@rb_link
             * return:            NULL => no such node is existed,thus it is able to insert @addr
             *                    pointer to the memory region => the most closest VMA to @addr
             *                                                    its @vm_end > @addr but @vm_start <= @addr
             * # this function will ends up with several situations :
             *     1> VMA && *@rb_link != @rb_parent => @addr is not existed,can be inserted
             *     2> VMA && *@rb_link == @rb_parent => @addr is existed
             *     3> !VMA && *@rb_link == NULL && @rb_parent == NULL => @addr is not existed,can be inserted
             */
            static struct vm_area_struct *find_vma_prepare(struct mm_struct *mm, unsigned long addr,
                                                           struct vm_area_struct **pprev,
                                                           struct rb_node ***rb_link, struct rb_node **rb_parent);

            !! WHY IS POINTER TO POINTER BECAUSE MEMBERS @rb_left and @rb_right OF struct rb_node OBJECT BOTH
               ARE TYPE OF struct rb_node * .

        Finding a memory region that overlaps a given interval - find_vma_intersection() :
          as the meaning of its name,we can find out the first memory region which overlaps
          a given linear address interval.
          /* interval means a range of linear addresses */

          <linux/mm.h>
            /**
             * find_vma_intersection - look up the first VMA which intersects the given interval
             * @mm:                    memory descriptor
             * @start_addr:            start address of the interval
             * @end_addr:              end address of the interval
             * return:                 pointer to the VMA
             *                         NULL => unfound OR @mm NULL
             * # this function actual call to find_vma(@mm, @start_addr),if the returned VMA
             *   does not overlap [@start_addr, @end_addr],then this function returns NULL
             */
            static inline struct vma_area_struct *find_vma_intersection(struct mm_struct *mm,
                                                                        unsigned long start_addr,
                                                                        unsigned long end_addr);

            about overlap :

              VMA && @end_addr <= VMA->@start_addr
                      return NULL

                                          +----> @vm_start
                                          |
                                          +----------------+
                                          |                |
                                          |       VMA      | => overlap
                                          |                |
                                          +----------------+
              +-------------------------------------------+
              |     |                               |     |
              | ... |             | -------- |      | ... | => process address space <- @mm
              |     |             |          |      |     |
              +-------------------|----------|------------+
                                  |          |
                 @start_addr <----+          +----> @end_start
                                                 +----------------+
                                                 |                |
                                                 |       VMA      | => non overlap
                                                 |                |
                                                 +----------------+
                                                 |
                                                 +----> @vm_start

        Finding a free interval - get_unmapped_area() :
          function get_unmapped_area() is used to find out an available linear address interval
          in a given process address space.

          <mm/mmap.c>
            /**
             * get_unmapped_area - a wrapper for low-level routine get_unmapped_area()
             * @file:              file descriptor
             * @addr:              expected address
             * @len:               length of the interval
             * @pgoff:             file offset corresponding to a page belonging to a 
             *                     non-linear file memory mapping
             *                     exact file offset = @pgoff * PAGE_SIZE
             * @flags:             flags for the area
             * return:             linear address for the unmapped area
             *                     -ENOMEM   => no memory
             *                     -EINVAL   => invalid linear address interval
             *                     ERROR PTR => error ptr,can be checked by IS_ERR_VALUE()
             */
            unsigned long get_unmapped_area(struct file *file, unsigned long addr, unsigned long len,
                                            unsigned long pgoff, unsigned long flags);

            brief description for get_unmmaped_area() :
              as the description,it just a wrapper for low-level get_unmapped_area() routine.
              this function does arch_mmap_check() at first to @addr,@len,@flags.if any error
              is encountered,it immediately returns the error code.
              if @len is greater than TASK_SIZE,that means the request linear address interval can
              not be contained in a process address space,this is worst case,so have to return -ENOMEM.
              there are two methods may be invoked by this wrapper :
                1> @file->f_op->get_unmmaped_area
                   /* if all components of the member-accessing chain are TRUE */
                2> @current->mm->get_unmapped_area
                   /* this is the default behavior */
              @file->f_op->get_unmapped_area is called only for file memory mapping.
              if the wrapped routine returned ERROR VALUE,then propagate it to the caller.
              it returns -ENOMEM if the returned linear address is greater than TASK_SIZE - @len.
              it returns -EINVAL if the returned linear address is invalid.
              finally,wrapper invokes arch_rebalance_pgtables(@addr, @len),this function is architecture
              depended function used to rebalance elements in Page Global Directory,and there is Kernel Mode,
              thus it rebalance master Kernel Page Global Directory.
              wrapper returns what arch_rebalance_pgtables() returned,but if no such function is defined,
              then a macro named arch_rebalance_pgtables() is introduced in the same file and returns
              @addr.

              ! it is not all file-systems are provided get_unmapped_area() implementation.
              ! the wrapper may call to arch_get_unmapped_area_topdown(),this is depend on memory region layout.

          <arch/x86/kernel/sys_x86_64.c>
            /* arch_get_unmapped_area - architecture depened routine for x86 */
            unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr, unsigned long len,
                                                 unsigned long pgoff, unsigned long flags);

            what arch_get_unmapped_area() does :
              1> retrieve memory descriptor of current process.
                 declares some local variables.
              2> if MAP_FIXED was enabled in @flags,then returns @addr,because mapping is fixed,no adjustment
                 can take place.
              3> call to static function find_start_end(@flags, @begin, @end),which try to find out a range
                 where the required linear address interval will be picked up between the range.
                 if !TIF_IA32 AND MAP_32BIT was enabled in @flags
                 then
                         start at 0x40000000,end at 0x80000000
                         if defined PF_RANDOMIZE
                         then
                                 new start at randomize_range(start, start + 0x02000000, 0)
                                 if no new start is available,use the old start
                 else
                         start at TASK_UNMAPPED_BASE,end at TASK_SIZE
              4> if @len > @end then return -ENOMEM,because the required interval is too long.
              5> if @addr is not _zero_,then try to align it in a multiple of  PAGE SIZE at first,
                 then call to find_vma() to find out a closest VMA from the process address space.
                 if aligned @addr is less than or equal to @end - @len AND
                    VMA is NULL OR @addr + @len is less than or equal to VMA->@vm_start
                 then
                         returns @addr,it had satisfied the request
                         /**
                          * @addr <= @end - @len => the required length is satisfied
                          * @addr + @len <= VMA->@vm_start => no overlapping
                          */
              6> more adjustment is necessary.
                 if MAP_32BIT was enabled OR TIF_IA32 was enabled
                    AND @len is less than or equal to @mm->cached_hole_size
                 then
                         reset @cached_hole_size to _zero_
                         reset @free_area_cache to local variable @begin
                         /**
                          * struct mm_struct.@cached_hole_size != 0
                          * the first memory hole start at @free_area_cache which in size
                          * @cached_hole_size.
                          * @len <= @cached_hole_size,so we can use the hole for the
                          * required linear address interval.
                          * if we will start searching from @begin,then @cached_hole_size
                          * is useless,thus reset it to _zero_.
                          * # because we do not know the size of a hole between @begin to
                          *   the next allocated VMA.
                          */
              7> set @addr to @free_area_cache,and if it is greater than @begin,then
                 reset it to @begin. /* limit range */
              8> set local variable @start_addr to @addr
                 enter a for-cycle to search whole chain of memory regions start from
                 find_vma(@mm, @addr) /* start from @begin or @free_area_cache */
                 /* these memory regions are chained by @next member of struct vm_area_struct */
              full_search:
              9> for-cycle {
                   if @end - @len < @addr /* @addr is set to @free_area_cache before enter cycle */
                           if @start_addr == @begin
                           then
                                   return -ENOMEM
                                   /**
                                    * the allowed range is [@begin, @end),the length of the
                                    * interval is @end - @begin.
                                    * but @addr is before @end - @len and the searching is
                                    * start at @begin,that means the required length of 
                                    * linear address interval is longer than @end - @begin,
                                    * thus required size can not be satisfied.
                                    */
                           else
                                   reset @start_addr and @addr to @begin
                                   reset @cached_hole_size to _zero_
                                   goto full_search to try again
                                   /**
                                    * @start_addr != @begin,search the range of memory area
                                    * between @begin and @free_area_cache
                                    * process second searching if previous start from
                                    * @free_area_cache
                                    */
                   if VMA is NULL OR @addr + @len is less than or equal to VMA->@vm_start
                   then
                           we find out a proper linear address interval without overlapping
                           update @free_area_cache and returns @addr
                           /**
                            * VMA NULL => no more memory region is existed,we can safely 
                            * claim an linear address interval at the end.
                            * <= VMA->@vm_start => we can safely to claim an linear address
                            * interval before this VMA and withou overlapping
                            */
                           
                   if @addr + @cached_hole_size is less than VMA->@vm_start
                   then
                           update @cached_hole_size to VMA->@vm_start - @addr
                           /**
                            * VMA is not NULL and a hole has encountered,thus
                            * updates information(size) of the hole.
                            * # we start from @free_area_cache or @begin
                            */

                   update @addr to VMA->@vm_end and enter next cycle stage
                   /* should skip up this VMA,and restart searching at its end */
                 }

            !! IF WE CAN FIND OUT AN APPROPRIATE LINEAR ADDRESS INTERVAL QUICKLY,THEN NO
               FULL SEARCHING CAN TAKE PLACE.
               IF MAP_FIXED IS DEFINED,THEN SHOULD NOT START SEARCHING,BECOUSE NO ADJUSTMENT
               CAN TAKE PLACE.

            !! about @cached_hole_size and @free_area_cache

               +----> @begin
               +------------------------------------+
               |     |      |     |     |     |     |
               | ... | HOLE | ... | VMA |  |  | ... |
               |     |      |     |     |  |  |     |
               +------------------------------------+
                       |                   |
                       |                   +----> @free_area_cache
                       +----> @cached_hole_size

               if @len <= @cached_hole_size,a hole is available,so start searching from @begin
               otherwise,start from @free_area_cache
               /* both these two cases,@cached_hole_size is useless */

               ! if @free_area_cache is less than @begin,then have to start searching from @begin,
                 so even a hole(if it is existed) is greater than @len,we can not use it.

               +----> @begin
               +--------------------------------------------+
               |     |      |     |     |       |     |     |
               | ... | ...  | ... | VMA |  |HOLE| VMA | ... |
               |     |      |     |     |  |    |     |     |
               +--------------------------------------------+
                                           | |
                                           | +----> @cached_hole_size
                                           +----> @free_area_cache

               this appears only when searching start from @free_area_cache and the first VMA is not
               NULL but @addr + @len >= VMA->@vm_start AND @addr + @cached_hole_size < VMA->@vm_start
               ! when searching from @free_area_cache,it would not be updated in each cycle stage if 
                 the first searching-cycle finally resulted in fail,but @cached_hole_size would be
                 updated whenever a hole has encountered.

        Inserting a memory region into the memory descriptor - insert_vm_struct() :
          routine insert_vm_struct() is used to insert a memory region into the memory regions
          list and the red-black tree of a given memory descriptor.

          <mm/mmap.c>
            /**
             * insert_vm_struct - insert a VMA to a given memory descriptor
             * @mm:               memory descriptor
             * @vma:              the VMA
             * return:            0 => succeed
             *                    -ENOMEM => 
             *                      > no appropriate leaf is available to be inserted
             *                      > VMA has specified VM_ACCOUNT && @current is deny to
             *                        allocate new virtual memory area
             * # this function call to find_vma_prepare() attempt to find out an appropriate position
             *   in the red-black tree of @mm,if no such leaf is existed,then there must be a
             *   suitable position in the red-black tree where @vma can be placed on
             * # this function call to static function vma_link() to link @vma into @mm
             */
            int insert_vm_struct(struct mm_struct *mm, struct vm_area_struct *vma);

            ! routine vma_link() has three parts,the first is link the VMA into the list
              of memory regions;the second is insert the VMA into the red-black tree;
              the third is link the file if it existed.
              /**
               * before link VMA,anonymous VMA lock must be acquired,and release it later after
               * link have accomplished.
               * # VMA->anon_vma->lock => this would be handled by vma_link()
               */

              chain VMA and insert VMA is very simple :
                chain -> assign @next member
                insert -> insert as a leaf and adjust the order of red-black tree
              link the file : /* __vma_link_file() */
                routine __vma_link_file() is used to deal with this.
                if the @vm_file is not NULL,then retrieve the address space which corresponding
                to the file,that is the member @f_mapping of this file.
                if @vm_flags enabled VM_DENYWRITE,then decrease
                        @file.f_path.dentry->d_inode->@i_writecount /* write deny */
                        /* @dentry member is a directory entry */
                if @vm_flags enabled VM_SHARED,then increase
                        @i_mmap_writable member of the address space of the file,that is
                        @f_mapping
                        /* because it is shared by other,thus allow writing */
                if @vm_flags enabled VM_NONLINEAR,then insert
                        @vma into tail of @f_mapping->i_mmap_nonlinear /* connector is @vma->shared.vm_set.list */
                        AND
                        set @vma->shared.vm_set.parent to NULL
                        /* the file is not mapping in linear */
                flush dcache and return to caller.
                      /* dentry cache */
                
                ! if @file is NULL,then this routine do nothing.

      Allocating a Linear Address Interval :
        function do_mmap() used to create and initialize a new memory region for current process.
        if there is a memory region have existed and which could to be merged with this new one,
        then merge them.
        
        <linux/mm.h>
          /**
           * do_mmap - create and initialize a new memory region,insert it into @current process
           *           address space
           * @file:    the file to be mapped
           * @addr:    expected linear address
           * @len:     length of the linear address interval
           * @prot:    Page Table Entry protection flags
           * @flag:    virtual memory region flags
           * @offset:  offset of file
           * return:   start linear address of the new memory region
           *           -EINVAL => invalid @offset or invalid @len
           *           other kinds of error codes are returned by do_mmap_pgoff()
           * # this function actually call to do_mmap_pgoff() to process primary works,
           *   and which routine is defined in <mm/mmap.c>
           * # because do_mmap_pgoff() requires pgoff value,thus this function product
           *   pgoff through "offset >> PAGE_SHIFT"
           */
          static inline unsigned long do_mmap(struct file *file, unsigned long addr,
                                              unsigned long len, unsigned long prot,
                                              unsigned long flag, unsigned long offset);

          ! if (@offset + PAGE_ALIGN(len)) < @offset,then either @offset or @len is invalid.
            PAGE_ALIGN() => defined in the same file,expands to ALIGN(@len, PAGE_SIZE)
                                                                /* align @len to next page boundary */
            @len <= PAGE_SIZE => @len := PAGE_SIZE
            @len > PAGE_SIZE => @len := next page boundary

            < @offset => overflow

          ! this routine call to do_mmap_pgoff() only when @offset & ~0xfffff000 is false,that is,
            if @offset is not a valid offset value,then should not call to do_mmap_pgoff().

          ! if no file mapping,then @file is NULL and @offset is _zero_.

          ! @flag values :
                    MAP_GROWSDOWN                grows from top to down
                    MAP_LOCKED                   mapping locked
                    MAP_DENYWRITE                deny write permission
                    MAP_EXECUTABLE               executable

                    MAP_SHARED                   shared with other
                    MAP_PRIVATE                  process only

                    MAP_FIXED                    fix mapping
                    
                    MAP_ANONYMOUS                anonymous mapping,no file associating
                    
                    MAP_NORESERVE                no preliminary check on the number of free page frames

                    MAP_POPULATE                 should pre-allocate the page frames required for the
                                                 mapping established by the memory region
                                                 /* significant only for file mapping */

                    MAP_NONBLOCK                 when pre-allocating the page frames,this routine must not
                                                 block

        <mm/mmap.c>
          /**
           * do_mmap_pgoff - do memory mapping with pgoff
           * @file:          file descriptor
           * @addr:          expected linear address
           * @len:           length
           * @prot:          Page Table Entry protection flags
           * @flags:         virtual memory region flags
           * @pgoff:         pgoff value
           * return:         start linear address of the memory region
           *                 -EINVAL => invalid @len -> _zero_ OR overflow
           *                            MAP_TYPE unknown
           *                 -EOVERFLOW => @offset overflow
           *                 -ENOMEM => reached maximum memory region mapping count
           *                 -EPERM => disallowed MAP_LOCKED
           *                 -EAGAIN => reached memory locked limit
           *                            mandatory locks on the file
           *                 -EACCESS => file permission deny
           *                 -ENODEV => file operation undefined
           *
           *                 returns error code what security_file_mmap() returned
           *                 returns error code what mmap_region() returned
           * # this function does some adjustments to parameters and some checkings to
           *   them,the real memory map is handled by routine mmap_region()
           */
          unsigned long do_mmap_pgoff(struct file *file, unsigned long addr, unsigned long len,
                                      unsigned long prot, unsigned long flags, unsigned long pgoff);

          brief description for do_mmap_pgoff() :
            because this routine create a new memory region for current process,thus it have to
            retrieve the memory descriptor at first.
            checks page flags,if the application expect PROT_READ to imply PROT_EXEC,then enable
            PROT_EXEC.
            /**
             * if this is file mapping and enabled MNY_NOEXEC in @file->f_path.mnt->mnt_flags,
             * then should not enable PROT_EXEC.
             */
            refuse processing that @len == _zero_ or @len overflow after aligned to PAGE_SIZE.
            call to round_hint_to_min(@addr) and update @addr,if MAP_FIXED is not required.
            check @pgoff whether OVERFLOW,if it is,returns -EOVERFLOW.
            for create a new memory region,it needs a free(unmapped) linear address interval,this
            is handled by get_unmapped_area().
            but might an invalid address is returned by get_unmapped_area(),in this case,propagate it
            to caller. /* if @file defined get_unmapped_area(),then invoke the method
                        * otherwise,invoke the get_unmapped_area() that specified by memory descriptor
                        */
            calculate virtual memory region flags through @prot,@flags by the routines calc_vm_prot_bits(),
            and calc_vm_flag_bits() respectively.
            only these is not enough,have to bitOR @mm->def_flags,VM_MAYREAD,VM_MAYWRITE,VM_MAYEXEC.
            next,check mlock,if @vm_flags enabled VM_LOCKED,then compare rlimit(RLIMIT_MEMLOCK) with the
            new locked memory pages,if exceeded limit and no capable to CAP_IPC_LOCK,then returns -EAGAIN.
            /* @locked := @len >> PAGE_SHIFT         => calculate how many pages are going to be locked
             *                                          if @len is less than PAGE_SIZE,the result is 1
             * @locked += @mm->locked_vm             => total locked pages of @mm
             * @lock_limit := rlimit(RLIMIT_MEMLOCK) => system limit
             * @lock_limit >> PAGE_SHIFT             => the limited number pages can be locked
             */

            if we are in file mapping that @file is not NULL,then retrieve inode of the file,and process
            MAP_SHARED file mapping or MAP_PRIVATE file mapping.
              file mapping MAP_SHARED {
                      return -EACCESS if PROT_WRITE is TRUE but @file deny write
                      return -EACCESS if inode IS_APPEND() but @file permit write
                      return -EAGAIN  if there are mandatory locks on the @file
                      /* invoke locks_verify_locked() on @file's inode */
                      enable VM_SHARED and VM_MAYSHARE in virtual memory region flags
                      if @file deny write,then have to disable VM_MAYWRITE and VM_SHARED in VMA flags
              }
              file mapping MAP_PRIVATE {
                      return -EACCESS if @file deny read
                      return -EPERM if @file disallowed execute but VMA flags enabled execute
                      if @file disallowed execute,then disable VM_MAYEXEC in VMA flags
                      return -ENODEV if @file operations undefined(mmap() undefined)
              }

            if we are not in file mapping,then everything is simple.
              non-file-mapping MAP_SHARED {
                      ignore @pgoff
                      /* @pgoff is only for file mapping */
                      enable VM_SHARED and VM_MAYSHARE in VMA flags
              }
              non-file-mapping MAP_PRIVATE {
                      update @pgoff to @addr >> PAGE_SHIFT
                      /**
                       * set @pgoff according to @addr for anonymous VMA(no file associated)
                       * so,@pgoff := @addr / PAGE_SIZE,it identifies the number of start page frame
                       * of this linear address space in total memory.
                       * ! @addr had aligned to PAGE_SIZE,if no MAP_FIXED.
                       */
              }

            invoke security_file_mmap() to check capability,if we are not in file mapping,this routine
            should not returns any error.
            finally,call to mmap_region() to create and initialize a new memory region,link it into the
            process address space,and returns the start linear address of the memory region.
            /* maybe returns error code */

          /**
           * mmap_region - create new memory region,initialize it and insert it into the process
           *               address space
           * @file:        file descriptor for file mapping
           * @addr:        the expected address
           * @len:         length of the interval
           * @flags:       mapping flags
           * @vm_flags:    virtual memory area flags
           * @pgoff:       pgoff value
           * return:       the start linear address of memory region(may caused merging)
           *               error code => failed
           */
          unsigned long mmap_region(struct file *file, unsigned long addr, unsigned long len,
                                    unsigned long flags, unsigned int vm_flags, unsigned long pgoff);

          what mmap_region() does :
            1> retrieve @current's memory descriptor.
               initialize local variables both @correct_wcound and @charged to _zero_.
               retrieve file inode struct if @file is not NULL.
            munmap_back:
            2> call to find_vma_prepare() to find out the VMA where new memory region will roots at.
               if got VMA and no overlapping
               then
                       try to do_munmap() for @addr and @len in @mm
                       /**
                        * if the linear address interval have mapped,then unmap it at first
                        * generally,do_munmap() returns _zero_,returns -error_code if failed.
                        */
                       return -ENOMEM if do_munmap() failed,otherwise,goto munmap_back.
            3> if @len have reached limit on @mm address space,returns -ENOMEM.
            4> if enabled MAP_NORESERVE
               then
                       checks sysctl_overcommit_memory,if the value is not OVERCOMMIT_NEVER,
                       then enable VM_NORESERVE
                       checks file hugetlb,if it is,then enable VM_NORESERVE,too
            5> check memory availability,@charged := @len >> PAGE_SHIFT.
               if security_vm_enough_memory(@charged) returned TRUE,then returns -ENOMEM,
               otherwise,enable VM_ACCOUNT.
               /* this step would be processed if @file mapping is accountable */
            6> call to vma_merge(),if the new VMA is can be inserted by simply expand its @root,
               then just do expanding and returns the new linear address.
            7> call to kmem_cache_zalloc() to allocate a new VMA is we can not expand the @prev
               in 6>.returns -ENOMEM if VMA is NULL.
               /* in this case,have to unacctount the virtual memory before return,if @charged > 0 */
            8> initializes the new VMA {
                       @vm_mm := @mm
                       @vm_start := @addr
                       @vm_end := @addr + @len
                       @vm_flags := @vm_flags
                       @vm_page_prot := @vm_get_page_prot(@vm_flags)
                       @vm_pgoff := @pgoff
                       INIT_LIST_HEAD(VMA->@anon_vma_chain)
               }
            9> if we are doing file-mapping
               then
                       if enabled VM_GROWSDOWN AND VM_GROWSUP,then goto free_vma,error code is -EINVAL.
                       if enabled VM_DENYWRITE,then invoke deny_write_access() on @file to forbit writing;
                       goto free_vma and returns -ETXTBSY if routine did not return _zero_,otherwise,
                       set @correct_wcount to 1.           /* deny_wrtie_access() atomaticly decrease 
                                                            * @i_writecount of the file's inode,if
                                                            * it is not greater than 0;
                                                            * if its value is greater than 0,-ETXTBSY
                                                            * would be returned to caller.
                                                            * <fs/namei.c>
                                                            */
                       VMA->@vm_file := @file
                       increase @file's refcount through get_file()
                       invoke @file specified mmap() routine,goto unmap_and_free_vma if failed
                       if enabled VM_EXECUTABLE,add a vma mapping executable /* require @mmap_sem */

                       @addr := VMA->@vm_start

                       @pgoff := VMA->@vm_pgoff
                       @vm_flags := VMA->@vm_flags
                       /* because several device drivers can change address through @f_op->mmap() */

               if we are not doing file-mapping AND enabled VM_SHARED
               then
                       invoke shmem_zero_setup(@vma) to initialize the memory region for sharing to _zero_,
                       goto free_vma if failed,in this case,error code is the return value.

            10> either file-mapping or non-file-mapping have to continues from there.
            11> if write notify is wanted at VMA
                then
                        get VMA->@vm_page_prot and store it in @pprot(local variable)
                        /* as old data for later using */
                        /* @vm_page_prot may have been changed by drivers in their @f_op->mmap() */


                        VMA->@vm_page_prot := vm_get_page_prot(@vm_flags & ~VM_SHARED)
                        /* disable VM_SHARED */
                        /* re-construct @vm_page_prot from VMA flags */

                        /**
                         * macro pgprot_noncached(prot) is defined in <arch/x86/include/asm/pgtable.h>.
                         * it expands to 
                         *         @boot_cpu_data.x86 > 3 ? struct pgprot_t { @prot.pgprot | _PAGE_CACHE_UC_MINUS }
                         *         : @prot
                         *         # member @pgprot is type of pgprotval_t,the raw type is unsigned long
                         *         # member @x86 is CPU family
                         *           3 -> 80386 4 -> 80486 and so on
                         * # _PAGE_CACHE_MASK => _PAGE_PCD | _PAGE_PWT : page cache disabled and write through
                         *   _PAGE_CACHE_UC_MINUS => _PAGE_PCD : page cache disabled
                         * so,this macro means that,if "this" CPU architecture number is greater than 3,then
                         * disable page caching,otherwise,do nothing.
                         */

                        now checks @pprot,if @pprot.pgprot == pgprot_noncached(@pprot).pgprot
                        then update @vm_page_prot to pgprot_noncached(@vm_page_prot)
                        /**
                         * if page caching enabled before,then it should stay enabled in @vm_page_prot,
                         * otherwise,mark page uncached
                         */

            12> invoke vma_link()
                set @file to VMA->@vm_file
            13> once VMA denies write,undo temporary denial count, /* VM_DENYWRITE is set */
                that is @correct_wcount is TRUE => increase @file's indoe @i_writecount.
                /**
                 * if VM_DENYWRITE is set,vma_link() will decrease @i_writecount,too.so we need to
                 * undo the temporary denial count.
                 * actually,the recent kernel version have ignored MAP_DENYWRITE flag.
                 * long ago-linux 2.0 and earlier-it signaled that attempts to write to the underlying file
                 * should fail with ETXTBSY,but this was a source of denial-of-service attacks.
                 */
            out:
            14> raise performance event about memory map on VMA.
            15> @mm->total_vm += @len >> PAGE_SHIFT
                does virtual memory area statistics accounting
                if enabled VM_LOCKED
                then
                        if succeed to mlock the VMA,then update @mm->locked_vm += (@len >> PAGE_SHIFT)
                else if enabled MAP_POPULATE AND disabled MAP_NONBLOCK
                then
                        make pages of the VMA present /* make_pages_present() */
                        
                        /**
                         * <mm/memory.c>
                         *  ##
                         *  # make_pages_present - for a given interval,get user pages on it
                         *  # @addr:               starting linear address for the interval
                         *  # @end:                end boundary - usually come from "@addr + @len"
                         *  # return:              0 OR -EFAULT
                         *  # ! this routine make use of @addr to find_vma() on @current->mm,
                         *  #   then invoke get_user_pages() with arguments -
                         *  #     @current, @current->mm, @addr, @end/PAGE_SIZE - @addr/PAGE_SIZE, 
                         *  #     @vm_flags & VM_WRITE != 0, 0, NULL, NULL
                         *  ##
                         *  int make_pages_present(unsigned long addr, unsigned long end);
                         *
                         *  ##
                         *  # get_user_pages - pin user pages in memory
                         *  # @tsk:            target task
                         *  # @mm:             target mm
                         *  # @start:          starting user address
                         *  # @nr_pages:       number of pages from start to pin
                         *  # @write:          whether pages will be written to by the caller
                         *  # @force:          whether to force write access even if user mapping is
                         *  #                  readonly,this will result in the page being COWed even
                         *  #                  in MAP_SHARED mappings.
                         *  # @pages:          array that receives pointers to the pages pinned
                         *  #                  at least @nr_pages long OR null if caller only intends
                         *  #                  to ensure the pages are faulted in
                         *  # @vmas:           array of pointers to VMAs corresponding to each page,
                         *  #                  OR NULL if the caller does not require them
                         *  # return:          number of pages pinned OR error code
                         *  # ! EXPORT_SYMBOL
                         *  # ! this routine call to __get_user_pages() with specified flags,the default
                         *  #   flag is FOLL_TOUCH,@pages it not NULL - enable FOLL_GET,@write is TRUE - enable FOLL_WRITE,
                         *  #   @force is TRUE - enable FOLL_FORCE
                         *  # ! __get_user_pages(),for each page in the interval,it try find_extend_vma() to find out a VMA
                         *  #   which corresponding to the page's addres(if no such VMA,but has previous VMA,it expand_stack()
                         *  #   on previous VMA let it cover the address)
                         *  #     !! the interval is splited to several parts and size of each unit is PAGE_SIZE
                         *  #
                         *  #   if no such VMA(either no previous VMA,or expand_stack() faied),it checks the page's address,
                         *  #   if it is in gate area of @tsk,then make use of @tsk's @gate_vma(valid if and only if AT_SYS_INFO_EHDR defined)
                         *  #     for @gate_vam case,it makeups PTE for the page and continue to next page
                         *  #
                         *  #   VMA is NULL OR VM_IO | VM_PFNMAP enabled OR no required VM flags in VMA,return number of pinned pages or -EFAULT
                         *  #                                                                                                            !! nothing pinned
                         *  #   VMA associated hugetlb page,handle work over to follow_hugetlb_page()
                         *  #   
                         *  #   VMA is exist,and it is not hugetlb VMA,traverse current VMA's interval in PAGE_SIZE,
                         *  #   for each not present page(call to follow_page() on the VMA,if everything is OK,this function will return a
                         *  #   page descriptor,otherwise,return NULL(no such page start at the specified linear address) or BAD ADDRESS.the
                         *  #   traversing stopped when follow_page() returned a non-NULL address),invoke handle_mm_fault() for it - linear address
                         *  #   of the page,check the result,update accounting or report error
                         *  #
                         *  #   if we have encountered a unexpected error in the traversing,we have to early stop __get_user_pages() and return
                         *  #   error to caller,but if pinned pages number is not _zero_,we return it as instead
                         *  #   
                         *  #   finally,return number of pages been pinned to caller if no error occurred
                         *  #                            # after we handled a VMA(follow_page() returned a non-NULL page),if the pointer to
                         *  #                              that page is not PTR_ERR pointer,function increase number of pinned pages,decrease
                         *  #                              @nr_pages because we have pinned a page
                         *  #   !! actually,the follow_page() is called in a while-cycle(level-2),the stop condition is follow_page() returned
                         *  #      a non-NULL page.the cycle is nested in another do-while cycle,the stop condition is @nr_pages is 0 OR all
                         *  #      interval of current VMA has consumed(in PAGE_SIZE)
                         *  #      if follow_page() returned a BAD ADDRESS,then this will be detected in the do-while cycle(level-1)
                         *  #      if follow_page() returned a good page,then we increase pinned pages number,interval iterator
                         *  #      if follow_page() returned NULL,we handle it via handle_mm_fault()
                         *  #      note that,interval iterator is only updated in the do-while cycle,thus the next follow_page() in the
                         *  #      while-cycle should return the page we just handled OR BAD ADDRESS if error happened
                         *  #      (these two cycles are contained in another do-while cycle,it is the level-0 cycle,the stop condition
                         *  #       is @nr_pages equal to _zero_.its task is find out an appropriate VMA)
                         *  ##
                         *  int get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
                         *                     unsigned long start, int nr_pages, int write, int force,
                         *                     struct page **pages, struct vm_area_struct **vmas);
                         */

            16> return linear address @addr /* !! SUCCEED TO mmap_region() */
            unmap_and_free_vma:
            17> increase @file's inode @i_writecount if @correct_wcount > 0
                fput @file
                undo any partial mapping done by a device driver
                @charged := 0
            free_vma:
            18> kmem_cache_free VMA
            unacct_error:
            19> @charged is TRUE,unaccount memory through vm_unacct_memory()
            20> finally,return the error code /* !! FAILED TO mmap_region() */

      Releasing a Linear Address Interval :
        kernel use the routine named do_munmap() to delete an linear address interval from the address space
        of @current process.the deleting may split one memory region into two or more smaller memory regions
        than the old one.
        
        The do_munmap() function :
          <mm/mmap.c>
            /**
             * do_munmap - routine used by kernel to delete an linear address interval from
             *             a specified memory address space
             * @mm:        memory descriptor
             * @start:     startup linear address
             * @len:       length of the interval
             * return:     0 => succeed
             *             -EINVAL => @start is not a valid address
             *                        @start is greater than TASK_SIZE
             *                        @len is greater than TASK_SIZE - @start
             *                        @len is not a valid length after PAGE_ALIGN -> overflow
             *             -ENOMEM => @mm->map_count reached or exceeded its limit but there
             *                        would have spliting
             *             returns other errors returned by some help routines
             * # munmap is split into 2 main parts -- this part which finds what needs doing,and
             *   the areas themselves,which do the work
             *   this now handles partial unmappings
             * # returns _zero_ if no such VMA,or VMA does not overlap so have nothing to does
             *                                    VMA->@vm_start >= @start + @len
             */ 
            int do_munmap(struct mm_struct *mm, unsigned long start, size_t len);

            brief description for do_munmap() :
              this function does these works {
                find out VMA, split it if necessary,chain previous VMAs and the rest.
                munlock if necessary, detach VMA from address space, unmap page frames,
                remove VMA from memory descriptor
              }

              if @start is not a valid address or @len is not a valid length,returns -EINVAL.
              next,try to find out the VMA(@vma) which overlaps the interval,if unfound or no overlapping,
              just return _zero_.      /* find_vma_prev() */
              checks if the VMA have to be split for deleting the interval,if it is,then have to checks
              @mm->map_count;because,spliting would creates new memory regions,if reached limit,returns 
              -ENOMEM.
              help routine __split_vma(@mm, VMA, @start, 0) is invoked for split VMA,return error code
              if any error has encountered.
              after we succeed to split VMA,have to update @prev(a local variable used by find_vma_prev())
              to VMA -- the previous VMA of our target,it has overlapped our target.
              function have to checks it is spliting the last one through find_vma(@mm, @end).
              /* @end := @start + @len */
              if @last includes the interval but which does not match to it,then split @last through
              __split_vma(@mm, @last, @end, 1).
              /* @last = find_vma(@mm, end); @last AND @end > @last->vm_start */
              
              +--> last @vm_start
              +--------------------------------+
              |                                |
              |             |                  | --> the last
              |             |                  |
              +-------------|------------------+
                            |                  +--> last @vm_end
                            +--> @end
              ! @start -- @end have spanned several memory regions.
              ! if @end == @last->vm_start,that is the interval does not span @last

              update local variable @vma to @prev->vm_next or @mm->mmap.
              /* the real VMA or the first VMA in the memory if NULL @prev */

              munlock page frames of the vmas between @vma to the first VMA whose start address is
              equal to or greater than @end.
              /* if @mm->locked_vm is TRUE,and for each VMA whose @vm_flags enabled VM_LOCKED */

              call to detach_vmas_to_be_unmapped() to detach the VMAs between @prev and @end,these VMAs
              spanned [@start, @end). /* operates red-black tree and memory regions list
                                       * this routine also splice @prev and the rest
                                       */
              call to unmap_region() to unmap page frames of the VMAs between @prev and @end,these VMAs
              spanned [@start, @end).
              call to remove_vma_list() to remove the VMA from memory descriptor.
              finally,returns _zero_.

        The __split_vma() function :
          static routine __split_vma() is used to split memory region to several smaller memory regions.

          <mm/mmap.c>
            /**
             * __split_vma - split a VMA to several smaller VMAs
             * @mm:          memory descriptor
             * @vma:         VMA is going to be split
             * @addr:        linear address where start split,it identifies the intersection point
             *               between the linear address interval and the memory region
             * @new_below:   flag specifies whether the intersection occurs at the beginning or at
             *               the end of VMA
             * return:       0 => succeed
             *               error code => failed
             * # this function bypasses @sysctl_max_map_count checking,use this on the munmap path where
             *   it doesn't make sense to fail
             */
            static int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma, unsigned long addr,
                                   int new_below);

            brief description for __split_vma() :
              checks if @vma is hugetlb AND @addr is not huge page address,return -EINVAl.
              /* VMA and interval unmatched */
              because splitting will product several smaller virtual memory area,thus call to
              kmem_cache_alloc() on @vm_area_cachep to allocate kernel memory to store the coming in
              VMA descriptors;return -ENOMEM if failed on allocating.
              initializes @new_vma(local variable from allocating) to @vma,initializes its list_head @anon_vma_chain.
              @new_below is position indicator,thus,if it is TRUE,then @vm_end of @new_vma should be @addr,
              otherwise,its @vm_start should be @addr,and @vm_pgoff += (@addr - @vma->vm_start) >> PAGE_SHIFT.
                                                                       /* @new_below FALSE,thus @addr > @vm_start */
              next,duplicates memory policy(NUMA) by routine mpol_dup(),if it returns an error,then goto out_free_vma,by
              there,we have to deallocate @new_vma and returns error code.
              after succeed to duplicate memory policy,have to set the same policy for @new_vma,because it belongs to
              the same process address space.
              routine anon_vma_clone() is called to clone @anon_vma of @vma for @new_vma,if failed,goto out_free_mpol,
              have to mpol_put() the duplicated previously memory policy object.
              if @vma is file-mapping,then have to get_file() on @new_vma and added_exe_file_vma(@mm) if necessary.
                                                                              /* @vma->vm_flags enabled VM_EXECUTABLE */
              if @vm_ops->open is not NULL,then call to it to open this new virtual memory area.
              function vma_adjust() is used to adjust @vma,the arguments will be passed to it are determined by
              @new_below.

              /* *@new_vma = *@vma */
              TRUE

                  +--> file-mapping offset : @pgoff * PAGE_SIZE 
                  | 
                  |
                  +--> @vma->vm_start
                  +-------------------------+
                  |                         |
                  |   @new_vma              |
                  |                         |
                  +-------------------------+
                                            +--> @addr
                  +--> @vma->vm_start
                  +--------------------------------+
                  |                                |
                  |              @vma              |
                  |                                |
                  +--------------------------------+
                                                   +--> @vma->vm_end
                  L->R :
                    @vma, @addr, @vma->vm_end, @vma->vm_pgoff + ((@addr - @new_vma->vm_start) >> PAGE_SHIFT), @new_vma
                    /* target, start, end, pgoff, new to insert */
                
              FALSE
                        
                          +--> file-mapping offset : (@pgoff + ((@addr - @vma->vm_start) >> PAGE_SHIFT)) * PAGE_SIZE
                          |
                          |
                          +--> @addr
                          +------------------------+
                          |                        |
                          |      @new_vma          |
                          |                        |
                          +------------------------+
                                                   +--> @vma->vm_end
                  +--> @vma->vm_start
                  +--------------------------------+
                  |                                |
                  |              @vma              |
                  |                                |
                  +--------------------------------+
                                                   +--> @vma->vm_end
            
                  L->R :
                    @vma, @vma->vm_start, @addr, @vma->vm_pgoff, @new_vma

              /**
               * combine TRUE and FALSE will drive we into the situation that the linear address interval spanned two
               * memory regions
               */

              if vma_adjust() returned _zero_,then we returns _zero_,too.__split_vma() succeed.
              if we failed to split VMA,then have to close @new_vma(corresponding to @open() previous) if @close() is
              not NULL,fput() file and removed_exe_file_vma() for the memory,free memory policy,free @new_vma descriptor,
              finally,returns error code returned by vma_adjust().

        The vma_adjust() function :
          this function is used to adjust comfine for a specified memory region.

          <mm/mmap.c>
            /**
             * vma_adjust - adjust a specified VMA
             * @vma:        target
             * @start:      new start linear address
             * @end:        new end linear address
             * @pgoff:      new pgoff value
             * @insert:     new memory region to be inserted
             * return:      0 => succeed
             *              -ENOMEM => no enough memory
             * # we can not adjust vm_start, vm_end, vm_pgoff fileds of a VMA that is already present in
             *   an i_mmap tree without adjusting the tree
             */
            int vma_adjust(struct vm_area_struct *vma, unsigned long start, unsigned long end,
                           pgoff_t pgoff, struct vm_area_struct *insert);
            
            brief description for vma_adjust() :
              it retrieves @current's memory descriptor at first,and then retrieves the next VMA of @vma.
              if next VMA is existed and @insert is NULL,it have to handles some cases involved with
              the next VMA :
                /**
                 * @insert NULL => not another VMA will insert
                 * if an another VMA will be inserted,then can not adjust the next VMA,because the new one
                 * may occupies the hole in future.
                 */

                if @end is greater than or equal to next VMA's end,then have to remove the next VMA
                /* expand @vma to span the next VMA */
                /* @exporter := next VMA, @importer := @vma */
                /* should adjust @vma,should remove next VMA */

                if @end is greater than next VMA's start,then have to adjust the next VMA
                /* expand @vma to overlap the next VMA,and cut the length of the next VMA */
                /* @exporter := next VMA, @importer := @vma */
                /* should adjust @vma,should adjust next VMA */

                if @end is less than @vma's end,then adjust @vma only,do not touch the next VMA
                /* shrink @vma,and the next VMA expand */
                /* @exporter := @vma, @importer := next VMA */
                /* should adjust @vma,should adjust next VMA */

                !! calculation methods for remove next and adjust(or not adjust) next VMA are difference
                   remove next := 1 + (@end > @next->vm_end)
                   adjust next := (@end - @next->vm_start) >> PAGE_SHIFT
                   adjust next := - ((@vma->vm_end - end) >> PAGE_SHIFT)
            
              if we had encountered one of these three case,and @exporter->anon_vma is not NULL but
              @importer->anon_vma is NULL,then have to attempt clone @anon_vma of @exporter for
              @importer,return -ENOMEM if anon_vma_clone() was failed.
              if we succeed to clone anonymous vmas,then update @importer->anon_vma to @exporter->anon_vma.

              if we are handling file-mapping,then have to get the root of a tree about private and shared
              mappings in @file->f_mapping;if @insert is not NULL must invoke __vma_link_file() on it.
              /* struct address_space *f_mapping, struct prio_tree_root i_mmap */

              if @root is not NULL,then have to flush dentry cache on the @f_mapping,and remove @vma from
              the @root;if we should adjust the next VMA of @vma before,must remove the next,too.
              /* adjust the i_mmap tree before go to adjust the VMA */

              now,shrink or expand @vma,reset @vm_start,@vm_end,@vm_pgoff to @start,@end,@pgoff respectively.
              adjust next VMA's @vm_start and @vm_pgoff,if we should adjust it.
              if @root is not NULL,then re-insert the adjusted next and the adjusted @vma into the tree,again.
              unlink the next VMA,if we should remove it.if we are handling file-mapping,must remove it from
              the shared VMA.

              insert @insert to the memory descriptor,if @insert is not NULL.
              unlock @i_mmap_lock if we locked up it previously.

              if we have removed the next VMA at least once,then have to process the following actions :
                > put @file,call removed_exe_file_vma() on @mm,if the next VMA enabled VMA_EXECUTABLE
                > merge anonymous vma of @vma and the next VMA's
                > free memory policy of the next VMA
                > free descriptor of the next VMA
                > if we removed the next VMA twice,then have to reset @next to @vma->vm_next,and goto again
                  /**
                   * label again is the first case of the three cases described above within TRUE
                   * condition.
                   * and continues from there.
                   */
              validate @mm and returns _zero_.

        The unmap_region() function :
          helper unmap_region() is used to unmap a memory region in the specified memory.
         
          <mm/mmap.c>
            /**
             * unmap_region - get rid of page table information in the indicated region
             * @mm:           memory descriptor
             * @vma:          starting VMA of VMAs will be unmapped
             *                these VMAs covered the range of linear addresses [@start, @end)
             * @prev:         the previous VMA
             * @start:        linear address of start point
             * @end:          linear address of end point
             * # this function must called with the @mm semaphore held
             */
            static void unmap_region(struct mm_struct *mm, struct vm_area_struct *vma,
                                     struct vm_area_struct *prev, unsigned long start,
                                     unsigned long end);

            brief description for unmap_region() :
              get the next VMA,if @prev has given,then it is @prev->vm_next,otherwise,it is
              @mm->mmap.
              drain pages out of the CPU's pagevecs through lru_add_drain().
                                     /* per-CPU page vectors */
              retrieve TLB cache object,it is type of pointer to struct mmu_gather.
                                     /* from per-CPU variable named "mmu_gathers" */
                                     /**
                                      * it stores all information required for a successful updating
                                      * of the Page Table entries of a process
                                      */
              updates @mm's @hiwater_rss with the value get_mm_rss(@mm),if the value is greater
              than @hiwater_rss's.   /* processed by update_hiwater_rss() */
              function unmap_vmas() has defined in <mm/memory.c>,it is used to unmap a range of 
              memory covered by a list of a vma's.
              /**
               * list of a vma's => vma -> vma->next -> vma->next->next ...
               * this function requires mmu gather data of caller,it is TLB cache object has
               * introduced above.
               * @start -> start address, @end -> end address
               */
              unmap_region() call to this function(unmap_vmas()) with an argument named @nr_accounted,
              it is a local variable,which records number of unmapped pages in vm-accountable.
              vm_unacct_memory() is called at next with @nr_accounted to un-account memory.
              because there are some page frames have been unmapped,that is they will no longer
              associated with a set of linear addresses,thus have to invoke free_pgtables() to
              free the Page Table entries.
              finally,call to tlb_finish_mmu() to flush TLB cache for the MMU.
                                               /* the mmu_gather object acquired before */
                                               /**
                                                * call to free_pages_and_swap_cache() on
                                                * multiprocessor system to release the
                                                * page frames whose pointers have been
                                                * collected in the @mmu_gather object
                                                */

      Page Fault Exception Handler :
        Linux Page Fault Exception Handler must distinguish exceptions caused by programming errors from
        those caused by a reference to a page that legitimately belongs to the process address space but
        simply has not been allocated yet.
        function named do_page_fault() is the Page Fault interrupt service routine for 80x86 architecture.

        how do_page_fault() distinguish what the page fault come from?
          the linear address caused Page Fault -> @addr
          if @addr of @current->mm
          then
                  if access rights permitted
                  then
                          legal access -> allocate a new page frame
                          return
          else
                  if the exception come from Kernel Mode
                  then
                          Kernel bug -> kill the process
                          return
          Illegal access -> send a SIGSEGV signal
          return

        <arch/x86/mm/fault.c>
          /**
           * do_page_fault - Page Fault Exception service routine
           * @regs:          register contents of the microprocessor when the
           *                 exception occurred
           * @error_code:    3-bit error code,it is pushed on the stack by the
           *                 control unit when the exception occurred
           *                   bit-0:
           *                     0 -> the exception was caused by an access to a page
           *                          that is not present
           *                     1 -> the exception was caused by an invalid access right
           *                   bit-1:
           *                     0 -> the exception was caused by a read or execute access
           *                     1 -> the exception was caused by a write access
           *                   bit-2:
           *                     0 -> the exception occurred while the processor was in
           *                          Kernel Mode
           *                     1 -> the exception occurred while the processor was in
           *                          User Mode
           *                 !! @error_code ON x86 OCCUPIED FIVE BITS
           * # do_page_fault() must recognize several particular subcases
           * # do_page_fault() must distinguish several kinds of legal access
           */
          dotraplinkage void __kprobes
          do_page_fault(struct pt_regs *regs, unsigned long error_code);

          what do_page_fault() does :
            1> retrieve task struct of current process.
               retrieve the memory descriptor of current process. /* @current->mm */
            2> retrieve the linear address which caused the Page Fault exception from
               control register cr2.
               /* CPU control unit stores the address in cr2 when Page Fault occurred */
            3> execute kernel memory checking to detect and handle instructions that would 
               cause a page fault for both a tracked kernel page and a userspace page.
               /* kmemcheck_active(@regs) T => kmemcheck_hide(@regs) */
            4> prefetchw on the semaphore object of memory descriptor,this action can update
               cache line.
            5> call to static function kmmio_fault() to check whether mmiotrace is disabled or
               the fault is not handled by mmiotrace,if any condition has satisfied,this function
               returns _zero_;if kernel mmiotrace is active,and the fault had been handled by
               kmmio_handler(),then this function returns 1,in this case,do_page_fault() no
               more things can be done,just return to caller as well.
            6> if we fault in kernel space { /* address >= TASK_SIZE_MAX */
                       if @error_code disabled PF_PSVD, PF_USER, PF_PROT
                       then
                               have to check if this fault is vmalloc_fault,and if it is,then just
                               return to caller.
                               /* PF_RSVD -> bit 3 => use of reserved */
                               /* PF_USER -> bit 2 => userspace */
                               /* PF_PROT -> bit 0 => protection fault */
                               ! virtual memory allocating fault

                               if this fault is not vmalloc_fault,then invoke kmemcheck_fault() to does
                               kernel memory checking,if which results 1,do_page_fault() returns to caller.

                       if this fault is spurious_fault,then return to caller
                       if kprobes do not want to hook the spurious faults
                       then notify page fault and return to caller
                        
                       call to bad_area_nosemaphore() and return.
                       /* if the fault is not match to any cases above,it must be caused by a bad area access */
                       /* bad area : the area that can not be accessed by the process */
               }
            7> now,we can know that it failed in User Mode.
               User Mode registers count as a user access even for any potential system fault or CPU buglet.

               if kprobes do not want to hook the spurious faults,then just notify page fault and returns to
               caller.

               if we are in User Mode,then have to enable local interrupt and set PF_USER flag in @error_code;
               otherwise,to check if X86_EFLAGS_IF is set in @regs->flags,if it is,then enable local interrupt,too.
               /**
                * it is save to allow irq's after cr2 has been saved and the vmalloc fault has
                * been handled.
                */
            8> if PF_RSVD is set,then we can know that,the fault is caused when a kernel control path attempt to
               use reserved memory.                                             /* it delegate a user process */
               so,call to pgtable_bad().
            9> notify perfermance swtich event about PERF_COUNT_SW_PAGE_FAULTS.
           10> if we are in an interrupt,have no user context or are running in an atomic region then we must
               not take the fault.
               so,if in_atomic() OR !@mm,then call to bad_area_nosemaphore() and return to caller.
               /* interrupt handler never use of memory descriptor,thus it must be NULL @mm */
           11> down_read_trylock() on the memory semaphore.
               if we failed,then we can know there is another kernel path is holding it now.
               next,check if the fault is not from userspace AND check @regs->ip is not in well defined areas
               of code,then call to bad_area_nosemaphore() and return to caller.
               !! PF_USER closed => fault from kernel
                  @ip is not in well defined areas => the kernel validly references user space in a bad area

               otherwise,call to down_read() wait for the semaphore until another kernel control path release it.
                
               if we succeed to try down read semaphore,then call to might_sleep(),because in this case,we will
               have missed might_sleep() from down_read().
           12> find out the VMA from @mm by @address. /* invoke find_vma() function */
               if unfound,call to bad_area() and return.
               that is the linear address is outside to this process's address space.
           13> if @vm_start <= @address,goto good_area.
           14> /* did not goto good_area. */
               if @vm_flags disabled VM_GROWSDOWN
               then call to bad_area() and return.
               /**
                * because @address < @vm_start,and the VMA does not mapping to a stack,thus we can not to
                * grows it to include the fault @address
                * # stack growing
                */
           15> if the fault come from userspace
               then
                       check if @address + 65536 + 32 * sizeof(unsigned long) < @regs->sp
                       then call to bad_area() and return.
                       /**
                        * accessing the stack below %sp is always a bug.
                        * the large cushion allows instructions like "enter" and "pusha" to work.
                        */
           16> do_page_fault() attempt to expands stack through expand_stack(VMA, @address).
               if failed,then call to bad_area() and return.
               /* the case that @address is permitted only when the stack is allowed to be expanded to include it */
           good_area:
           17> set local variable @write to (@error_code & PF_WRITE).
               invoke access_error() on the VMA with @write and @error_code as the parameters.
               if the VMA is not allowed to be written /* it is access error */
               then call to bad_area_access_error() and return.
               /* permission deny */
           18> call to handle_mm_fault(),which handles this page fault.
               /* which function had defined in <mm/memory.c> */
               if it resulted in VM_FAULT_ERROR,then call to mm_fault_error() and return,we can not handles this
               Page Fault.
           19> if handle_mm_fault() is resulted in VM_FAULT_MAJOR
               then increase @maj_flt in task struct and notify perfermance sw event about PERF_COUNT_SW_PAGE_FAULTS_MAJ.
               otherwise,increase @min_flt in task struct and notify perfermance sw event about PERF_COUNT_SW_PAGE_FAULTS_MIN.
           20> call to function check_v8086_mode(),if the @address hit the DOS screen memory VA from vm86 mode,
               and "(@address - 0xA0000) >> PAGE_SHIFT" is less than 32,have to enable the bit "1 << bit" in @screen_bitmap of
               @thread member in task struct.
           21> up_read() to release memory semaphore.

          !! KERNEL NEVER ACCESS TO THE ADDRESS LOWER THAN OR EQUAL TO PAGE_OFFSET,IF IT HAVE ACCESSED TO,THEN THAT MUST BE
             A KERNEL BUG,THUS KERNEL MUST GENERATES AN OOPS.
             IF AN USER PROCESS ATTEMPT TO ACCESS AN LINERA ADDRESS IS NOT BELONGS TO IT,THEN KERNEL MUST SEND IT A SIGNAL SIGSEGV.

      Handling a Faulty Address Outside the Address Space :
        Case for User Mode >
          if the exception was caused by the process attempt to access a page frame is not existed in its address space,
          then find_vma() would returns NULL,so do_page_fault() will call to bad_area().
          the static routine bad_area() actually call to static routine __bad_area() with SEGV_MAPPER.
          SEGV_MAPPER => nonexisting page frame
          SEGV_ACCESS => existing page frame but permission deny
          as the result,__bad_area_nosemaphore() would be invoked,the memory semaphore had been acquired by __bad_area() before
          the invoking.
          if @error_code enabled PF_USER,then @current->thread.cr2 will be set to the @address,thread.error_code will be set
          to "error_code | (@address >= TASK_SIZE)",thread.trap_no will be set to 14(Page Fault Exception Number).
          finally,force_sig_info_fault() be called to send signal SIGSEGV to the @current process.
          routine force_sig_info_fault() make sure that the process does not ignore or block SIGSEGV signal.

        Case for Kernel Mode >
          there are two alternatives :
            1> the exception occurred while using some linear address that has been passed to the kernel
               as a parameter of a system call
            2> the exception is due to a real kernel bug

          in this case,only function no_context() would be called with the @regs, @error_code, @address .
          /* no_context() distinguishes these two alternatives and does something */

          no_context() :
            for the first case,it call to fixup_exception() with current register information.
            which function typically sends a SIGSEGV signal to @current or terminates a system call handler
            with a proper error code.

            for the second case,it print a complete dump of the CPU registers and of the Kernel Mode stack
            bothn on the console and on a system message buffer.
            next,send signal SIGKILL to the current process by oops_end()./* oops_begin() is called previously */
            this is the "Kernel oops".
            /**
             * the message format string is seem like "CR2: %0x161x@address\n".
             */

      Handling a Faulty Address Inside the Address Space :
        if @address is inside to the process address space,do_page_fault() proceeds to the statement labeled
        "good_area".
        if write is request but the flag in VMA is not enabled,then do_page_fault() jump to bad_area_access_error().
        which function is similar to bad_area(),but the signal is SEGV_ACCESS.
        /**
         * fault in writing {
         *         but VMA flag disabled write
         *         but Page Table Entry deny write permission
         *         but VMA flag disabled write, read, execute
         * }
         */

        if there is not an access error,then function handle_mm_fault() will be invoked with @write := 0 or
        @write := 1.
        /* @write => 0 -> not write request 1 -> write request */
        function handle_mm_fault() set @current's state to TASK_RUNNING at first,next,allocate PUD inside a
        specified PGD which is get by pgd_offset(),allocate PMD inside the PUD,allocate PTE inside the PMD.
        for allocate a PTE,function pte_alloc_map() will be called,which finally returns a pointer to the
        new created PTE(with type pte_t).
        if any allocating was failed,VM_FAULT_OOM would be returned to caller by handle_mm_fault().
        if no allocating was failed,function handle_pte_fault() will be called.
        /**                                  ^defained in <mm/memory.c>
         * if PMD is not present,pte_alloc_map() proceed to call __pte_alloc(),a new PTE would be allocated,
         * and set Present for the PMD let PTE populated on it.
         * # if PMD is present,then there must be another PTE is populating on it now,so have to free the
         *   new created PTE.
         *   that is the corresponding PTE for the @address has been existed in the PMD.
         * next,pte_alloc_map() invoke pte_offset_map(),which call to pte_offset_kernel(),finally,__va() will
         * be called to convert the physical address of the PMD to an linear address starting from PAGE_OFFSET,
         * pte_offset_kernel() end up by returns a pointer of PTE(pte_t),whose value is equal to
         *         (pte_t *)LINEAR_ADDRESS_OF_PMD + pte_index(@address)
         * so,a pointer to PTE is returned by pte_alloc_map().
         */

        ! function handle_pte_fault() is used to handle PTE fault,which inspect the Page Table Entry corresponding
          to @address and to determine how to allocate a new page frame for the process.
          /* pte_alloc_map() just populate PTE on a PMD,but does not set present for it */
          if PTE is not present
                  PTE is _zero_ value /* the PTE is filled with 0 */
                          if VMA operation is not NULL and routine fault() is setup
                          then end up by call to do_linear_fault()
                          else end up by call to do_anonymous_page()
                  it is used to mapping a file /* pte_file() : Present cleared, Dirty set */
                          end up by call to do_nonlinear_fault()
                  end up by call to do_swap_page()
          proceed checking for FAULT_FLAG_WRITE if setup in @flags,the parameter of handle_mm_fault(),
          if we had fault in writing,then have to check if write permission is allowed in the PTE.
          deny => end up by call to do_wp_page()
          permitted => update PTE to dirty state

          label "unlock" is existed at the end of handle_pte_fault(),which make sure that the PTE
          unmapped and unlocked after returned to caller.
          /* @mmap_sem still held,but pte unmapped and unlocked */

          ! strategy would be taken by handle_pte_fault() : 
              Demand Paging :
                the accessed page is not present,kernel setup the new created PTE properly and associated it
                with the @address,assign it to @current process
              Copy-On-Write :
                the accessed page is present but read-only,kernel make a copy of the old one and
                setup it properly,next associated the @address with the new copy and assign
                it to @current process

        the returned value from handle_mm_fault() is stored in a local variable named @fault.
        there has three kind of fault :
                VM_FAULT_ERROR <-> VM_FAULT_OOM | VM_FAULT_SIGBUS | VM_FAULT_HWPOISON
                /**
                 * function mm_fault_error() will be called,and then,do_page_fault() return
                 * to caller.
                 * mm_fault_error() :
                 *         if VM_FAULT_OOM => call to out_of_memory()
                 *         else if VM_FAULT_SIGBUS | VM_FAULT_HWPOISON => call to do_sigbus()
                 *         neither VM_FAULT_OOM nor VM_FAULT_SIGBUS | VM_FAULT_HWPOISON => BUG()
                 * do_sigbus() :
                 *         acquire memory semaphore
                 *         if fault in Kernel Mode => no_context()
                 *         # if fault occurred in userspace and because of prefetch => OK,return to caller
                 *         store exception address in @current->thread cr2
                 *         set trap number of @current->thread to Page Fault
                 *         set error code of @current->thread to @error_code
                 *         force_sig_info_fault()
                 */
                VM_FAULT_MAJOR  => @current->maj_flt++
                /**
                 * the Page Fault forced the current process to sleep
                 * # read from disk
                 */
                VM_FAULT_MINOR  => @current->min_flt++
                /**
                 * the Page Fault has been handled without blocking the current process
                 */

      Demand Paging :
        a dynamic memory allocation technique that consists of deferring page frame allocation until the
        last possible moment(until the process attempts to address a page that is not present in RAM - Page Fault).
        /* motivation : the process do not access to all the addresses inside its address space right after start */
        ! some addresses may never be accessed by the process.
        ! program locality principle(Hard Cache) ensures that,at each stage of program execution,only a small subset
          of process pages are really referenced,so the page frames containing the temporarily useless pages can be
          used by other processes.

        global allocation :
          assigning all page frames to the process right from the average number of free page frames in the system and
          therefore allows better use of the available free memory.
          Demand Paging prefer to this strategy.

        ! global allocation may cause system overhead - each Page Fault Exception induced by Demand Paging must be
          handled by kernel.
          Hard Cache(locality principle) reduce such Page Fault Exceptions.

        a page frame is not present in the main memory either because the page was never accessed by the process,or
        it has been reclaimed by the kernel.but in both case,kernel must handles the Page Fault Exception,assign a
        new page frame to the process.
        /**
         * how initialize the page frame is depend on what kind of page and on whether it was previously accessed by
         * the process.
         */

        particular cases :
          1> page was never accessed AND it does not map a disk file
             OR
             page maps a disk file
             => Page Table entry was filled with _zeros_ - pte_none() returns 1
          2> page belongs to a non-linear disk file mapping
             => Present == 0 AND Dirty == 0
          3> page was already accessed by the process,but its content is temporarily
             saved on disk
             => Page Table entry is not filled with _zeros_
                AND
                Present == 0 AND Dirty == 0

        SO handle_pte_fault() IS ABLE TO DISTINGUISH THE THREE CASES BY INSPECTING THE Page Table Entry
        THAT REFERES TO THE PARAMETER @address.

        in function handle_pte_fault(),if the PTE is not presented,then it check what the page frame is :
          case 1> => call to do_linear_fault()  /* it maps a disk file */
                     /* load the missing page from disk into RAM due to it mapped to a file */
                     /* @vma->vm_ops is not NULL AND @vma->vm_ops->fault is not NULL */

                     OR

                     call to do_anonymous_page() /* it does not map a disk file */
                     /* @vma->vm_ops is NULL OR @vma->vm_ops->fault is NULL */
                     /* anonymous mapping,get a new page frame */

          case 2> => call to do_nonlinear_fault() /* mapped a disk file but it is non-linear memory mappings */
          case 3> => call to do_swap_page()

        function __do_fault() :
          <mm/memory.c>
            /**
             * __do_fault - common interface for try to create a new page mapping
             * @mm:         memory descriptor
             * @vma:        VMA that need mapping
             * @address:    linear addres for the mapping
             * @pgoff:      page offset for @address
             * @flags:      Page Fault flags
             * @orig_pte:   original PTE for @address
             * return:      return value from @fault() method of VMA OR error code
             * # this routine only is called for linear fault and nonlinear fault
             */
            static int __do_fault(struct mm_struct *mm, struct vm_area_struct *vma, unsigned long address,
                                  pmd_t *pmd, pgoff_t pgoff, unsigned int flags, pte_t orig_pte);

            description for __do_fault() :
              create a local object named @vmf is type of struct vm_fault.
              set @vmf {
                      .virtual_address := (void __user *)(@address & PAGE_MASK)
                      .pgoff := @pgoff
                      .flags := @flags
                      .page := NULL
              }

              invoke @vma->vm_ops->fault() on @vmf,this method is used to handle Page Fault exception
              in that exact VMA,return value stored in local variable @ret.
              if VM_FAULT_ERROR | VM_FAULT_NOPAGE is set in @ret,return @ret to caller,that means failed.
              if @vmf.page set by @fault() enabled PG_hwpoison,then we can not use the page,unlock it if
              VM_FAULT_LOCKED is set in @ret,return VM_FAULT_HWPOISON.

              next,invoke lock_page() on @vmf.page,if no VM_FAULT_LOCKED is set in @ret,this is for consistency
              in subsequent calls,make the faulted page always locked.
              if VM_FAULT_LOCKED is set in @ret,then check whether PG_locked is set in the page;cleared
              means it is a VM_BUG.

              set local object @page to @vmf.page -- the page descriptor returned by fault() method of @vma.

              check early C-O-W break.
              if we are failed because FAULT_FLAG_WRITE -- write failed,then
                      @vm_flags no VM_SHARED /* get exclusive copy because PRIVATE MAPPING */
                              attempt to prepare anonymous VMA by call to anon_vma_prepare() on @vma.
                              set error code to VM_FAULT_OOM,goto "out" if failed.
                              /* attach an anon_vma to the memory region @vma */

                              invoke alloc_page_vma() with GFP_HIGHUSER_MOVABLE try to allocate a page
                              for @vma contain @address,goto "out" if failed,error code also is VM_FAULT_OOM.
                              /* the page stored in @page */

                              mem_cgroup_newpage_charge() this newly allocated page.
                              page_cache_release() it and goto "out" if failed,error code is VM_FAULT_OOM.

                              check VM_LOCKED flag of @vm_flags,if VMA been locked,we must clear memory lock
                              for page @vmf.page.
                              
                              call to copy_user_highpage() to copy data in @vmf.page to the newly allocated page.
                              __SetPageUptodate() on the newly allocated page @page.

                      @vm_flags has VM_SHARED
                              if @page_mkwrite() is defined in @vm_ops of @vma
                                      unlock the @page,in this case,it is @vmf.page.
                                      enable flags FAULT_FLAG_WRITE | FAULT_FLAG_MKWRITE in @vmf.flags.
                                      invoke @page_mkwrite() on @vma with @vmf.
                                      check status:
                                        VM_FAULT_ERROR | VM_FAULT_NOPAGE
                                          goto "unwritable_page",error code is the return value of page_mkwrite().

                                        !VM_FAULT_LOCKED
                                          lock_page()
                                          check its mapping,if the page is not cached,unlock it,goto "unwritable_page",
                                          return value is 0.
                                        VM_FAULT_LOCKED
                                          VM_BUG_ON() no PG_locked,usually,the page is locked by that method.

              invoke pte_offset_map_lock(),pass it @mm,@pmd,@address,&@ptl(local variable is type of spinlock_t) as
              parameters.the returned pointer to the newly created PTE is stored in local variable @page_table.
              /* makeup new PTE in PMD,and establish kernel atomic mapping */

              check @page_table and @orig_pte,if new PTE is same as @orig_pte /* same fields */
                      /* if we are from do_linear_fault(),the @orig_pte should been pte_unmap() in that function. */

                      invoke flush_icache_page() on @vma with the @page /* maybe the exclusive copy */
                      /* flush_icache_page() in x86 is NOP */

                      call mk_pte() to construct a new PTE{the @page, @vma->vm_page_prot}.
                      if we failed because FAULT_FLAG_WRITE,then have to
                              set Dirty flag for PTE
                              set Read/Write flag for PTE if @vma enabled VM_WRITE
                      next,check if we have did exclusive copy /* anonymous VMA prepare */
                              increase MM_ANONPAGES counter of @mm
                              invoke page_add_new_anon_rmap() to add PTE mapping to a new anonymous page -- the page
                      if we no exclusive copy,that means the VMA is shared,then
                              increase MM_FILEPAGES counter of @mm
                              invoke page_add_file_rmap() to add PTE mapping to a file page
                              if we failed FAULT_FLAG_WRITE,then record the page descriptor in local @dirty_page,and
                              get_page() on it.
                      set_pte_at(),set PTE at position @page_table.
                      invoke update_mmu_cache(),update MMU cache for entry @page_table.
              new PTE is not same as @orig_pte,some field been changed,then we cant use it,because it is not suit for
              current process
                      mem_cgroup_unchare_page() the page if we did mem cgroup charge.
                      page_cache_release() the page if we has exclusive copy.

              call to pte_unmap_unlock() on @page_table and the lock @ptl.
                      /* remove kernel atomic PTE mapping */

              "out": /* normal path */
                @dirty_page is TRUE,that means VMA is shared,and we have called page_mkwrite() method of @vma because
                FAULT_FLAG_WRITE.
                        set PG_dirty of @dirty_page.that is call to @a_ops of the mapping that cached @dirty_page.
                        unlock @dirty_page,PG_locked should been set by the member method of @vma.
                        put_page() on @dirty_page. /* we get it before */
                        if PG_dirty is set AND its mapping is not NULL -- cached,then call to
                        balance_dirty_pages_ratelimited().
                        if @vma->vm_file is TRUE,invoke file_update_time() for that file.
                @dirty_page is FALSE,that means VMA is not shared,we maybe have exclusive copy.
                        unlock @vmf.page,PG_locked is set by @fault() method.
                        if we have exclusive copy,invoke page_cache_release() on @vmf.page.
                finally,return @ret to caller.

              "unwritable_page": /* page_mkwrite() failed */
                page_cache_release() the page
                return @ret to caller.
    
        function do_nonlinear_fault() :
          <mm/memory.c>
            /**
             * do_nonlinear_fault - routine used to handle nonlinear file mapping Page Fault exception
             * @mm:                 memory descriptor
             * @vma:                VMA cause the exception
             * @address:            linear address
             * @page_table:         PTE corresponds to @address
             * @pmd:                PMD contains @page_table
             * @flags:              Page Fault flags
             * @orig_pte:           original PTE,generally,it is set to *@page_table
             * return:              0 OR error code
             * # this routine is end up call to __do_fault()
             *   first,it enable FAULT_FLAG_NONLINEAR in @flags
             *   then pte_unmap_same(), the function checks @page_table and @orig_pte whether
             *   they are same,and pte_unmap() @page_table,for non-smp and non-preempt platform,it always
             *   return TRUE(even they are not same,@page_table still be unmapped)
             *   if pte_unmap_same() returned false,do_nonlinear_fault() have to return 0,no __do_fault() is
             *   called
             *   next,if @vma has no VM_NONLINEAR,then we can not handle this Page Fault as a nonlinear fault,
             *   report bad pte -- Page table corrupted,return VM_FAULT_SIGBUS to caller -- must kill the process
             *   everything is OK,use pte_to_pgoff() to retrieve the pgoff related to the Page Fault from @orig_pte
             *   !! pgoff_to_pte() save it in @pte_high member
             *   return to caller by call to __do_fault()
             */
            static int do_nonlinear_fault(struct mm_struct *mm, struct vm_area_struct *vma,
                                          unsigned long address, pte_t *page_table, pmd_t *pmd,
                                          unsigned int flags, pte_t orig_pte);

        function do_linear_fault() :
          <mm/memory.c>
            /**
             * do_linear_fault - routine used to handle linear file mapping Page Fault exception
             * @mm:              process memory descriptor
             * @vma:             VMA cause the exception
             * @address:         linear address
             * @page_table:      PTE that corresponds to @address
             * @pmd:             PMD of the PTE
             * @flags:           Page Fault flags
             * @orig_pte:        original PTE,generally,it is set to *@page_table by handle_pte_fault()
             * return:           0 OR error code
             * # this routine is end up call to __do_fault()
             *   but before it does that,have to retrieve pgoff that contains @address,and invoke
             *   pte_unmap() on @page_table to unmap the PTE
             *   ! pte_unmap() is expanded to kunmap_atomic(),kernel will unmap the PTE in FIX_KMAP
             *   ! pgoff corresponds to @address is calculated through the expression -
             *       (((@address & PAGE_MASK) - @vma->vm_start) >> PAGE_SHIFT) + @vma->vm_pgoff
             *          |<----------------->|
             *          +--> Directory AND Table
             *          |<--------- length in VMA ----------->|
             *          |<---------------- number of pages ----------------->|
             *          |<--------------------- exact page offset --------------------------->|
             *     function invoke __do_fault() with the @pgoff
             */
            static int do_linear_fault(struct mm_struct *mm, struct vm_area_struct *vma,
                                       unsigned long address, pte_t *page_table, pmd_t *pmd,
                                       unsigned int flags, pte_t orig_pte);

        function do_anonymous_page() :
          <mm/memory.c>
            /**
             * do_anonymous_page - handle Page Fault Exception for anonymous page frame
             * @mm:                memory descriptor
             * @vma:               virtual memory area
             * @address:           the corresponding address for PTE which cause Page Fault
             * @page_table:        PTE
             * @pmd:               Page Middle Directory which @page_table residents on
             * @flags:             passed by do_page_fault(),it is FAULT_FLAG_WRITE or _zero_
             * return:             0 OR VM_FAULT_OOM
             * # this function return with @mmap_sem still held but PTE unmapped and unlocked
             */
            static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,
                                         unsigned long address, pte_t *page_table, pmd_t *pmd,
                                         unsigned int flags);

            brief description for do_anonymous_page() :
              at first,it does write accessing checking.
              if no FAULT_FLAG_WRITE,this function just check PTE none or PTE is not none.
              none -> /* read-only */
                      makeup a special PTE
                      /**
                       * there is no need to assign a new page frame filled with zeros to the process
                       * right away,because we might as well _give_ it an existing page called "zero page",
                       * @empty_zero_page,allocated statically and initialized with zeros
                       * during kernel initialization.
                       */
                     goto label unlock
              it is not none -> goto label setpte

              next,unmap PTE,attempt attach an anonymous vma to the memory region @vma.
              /* release kernel temporary mapping for the high-memory physical address */
              /* linear address is greater than PAGE_OFFSET */
              allocate a zeroed new page frame from high-memory,if failed,return OOM.
              /* may block current process,so have to release high-mem mapping at first */
              charge cgroup with the new allocated page,if failed,return OOM.
              create a new PTE accord with the @vm_page_prot member of @vma,
              if defined VM_WRITE in @vma,then enable flags writable and dirty for it.
              attempt to get PTE through pte_offset_map_lock(),which is lock version of
              pte_offset_map().the parameters are @mm, @pmd, @address &@ptl(a local variable).
              if the PTE is none,then goto label release,by there,the charged cgroup page frame
              would be unchanged,and page frame cache would be released,finally,unmap and unlock
              PTE,return 0 to caller./* label unlock */
              
              ! rmap - physical to virtual reverse mappings

              increase counter in @mm with kind MM_ANONPAGES,establish new anonymous rmap between
              the new zero @page, @vma, @address.

              about labels :
                setpte:
                  set_pte_at(),use the new created PTE to set the PTE corresponding to the @address,
                  and update mmu cache.
                  ! the new allocated page frame had been associated with the new created PTE.

                unlock:
                  unmap and unlock @page_table,then return 0 to caller.

                release:
                  uncharge memory cgroup,release page cache for the new created page frame,then goto unlock.

                oom_free_page:
                  release page cache for the new created page frame,and return VM_FAULT_OOM to caller.

                oom:
                  return VM_FAULT_OOM to caller.

            !! @page_table is the PTE which cause the Page Fault Exception by the interrupted process,
               in other word,the address that process want access to cause the Page Fault Exception,and
               the linear address associated with the PTE.

        !! if the page frame is marked nonwritable,a Page Fault Exception would be caused by the process
           who want to write a page frame marked read-only,then the Copy-On-Write mechanism is activated.

      Copy On Write :
        Copy On Write mechanism take place when fork() is called and a new child process is created.
        First-generation Unix system duplicated whole parent's process address space and assign the copy
        to child process.
        Linux adopt another way,it let child process shares address space with its parent,only makeup
        a copy when the child or the parent want to write a page frame in the address space.

        duplicate whole address space immediately requires :
          1> allocating page frames for the Page Tables of the child process
          2> allocating page frames for the pages of the child process
          3> initializing the Page Tables of the child process
          4> copying the pages of the parent process into the corresponding pages of the child process

          /* this way involved many memory accesses,used up many CPU cycles,completely spoiled cache contents */
          ! if the child process issues exec() system call,all things have done are nosense.

        Copy-On-Write(COW) :
          instead of duplicating page frames,they are shared between the parent and the child process.
          whenever the parent or the child process attempts to write into a shared page frame,an exception occurs,
          so kernel duplicates the page into a new page frame that it marks as writable.

          !! THE ORIGINAL PAGE FRAME REMAINS WRITE-PROTECTED,WHEN THE OTHER PROCESS TRIES TO WRITE INTO IT,KERNEL
             CHECKS WHETHER THE WRITING PROCESS IS THE ONLY OWNER OF THE PAGE FRAME;IN SUCH A CASE,IT MAKES THE
             PAGE FRAME WRITABLE FOR THE PROCESS.
                             
        About handle_pte_fault() when the page frame is present in RAM :
          first,have to check whether FAULT_FLAG_WRITE has enabled in @flags.
          if this Page Fault Exception is caused by wrting into the page frame,then have to
          check whether Write is permitted.
          if Write is deny,handle_pte_fault() return to caller by call to do_wp_page().
          /* Write is deny but we failed when writing into,so we have to consider if COW should be executed */
          else,use local variable @entry to re-build a PTE with Dirty,@entry is equal to *@pte generally.
          continue executing,pte_mkyoung() is called on @entry,this macro set _PAGE_ACCESSED on it.
          next,ptep_set_access_flags() is invoked,this function is defined in <arch/x86/mm/pgtable.c>.
          function ptep_set_access_flags() would check if @entry is same as @pte,if it was changed,
          this function just returns _zero_;if they are the same and the page frame is dirty(FAULT_FLAG_WRITE),
          then use @entry to setup @pte,update PTE which corresponding to the @address which cause Page Fault
          of the memory descriptor that @vma belongs to.finally,flush TLB cache and returns 1.
          handle_pte_fault() will call to update_mmu_cache() when ptep_set_access_flags() has returned 1.

        function do_wp_page() :
          <mm/memory.c>
            /**
             * do_wp_page - routine handles present pages,when users try to write into a shared page
             * @mm:         memory descriptor
             * @vma:        memory region
             * @address:    address cause Page Fault Exception
             * @page_table: Page Table Entry which corresponding to @address
             * @pmd:        Page Middle Directory where @page_table resident on
             * @ptl:        Page Table Spinlock
             * @orig_pte:   the original page frame which is shared between processes
             * return:      _zero_ OR error information code
             * # this routine is done by copying the page to a new address and decrementing the
             *   shared-page counter for the old page
             * # this routine assume that the protection checks have been done by caller
             * # this routine mark the page dirty at this point even though the page will change only
             *   once the write actually happens
             * # non-exclusive @mmap_sem
             */
            static int do_wp_page(struct mm_struct *mm, struct vm_area_struct *vma, unsigned long address,
                                  pte_t *page_table, pmd_t *pmd, spinlock_t *ptl, pte_t orig_pte);

            description for do_wp_page() :
              a local variable named @old_page(struct page *) would be retrieve through vm_normal_page(),
              it would the page frame that corresponding to @orig_pte.
              if there is no such page exsiting,there would be two cases.
              first> @vm_flags enabled VM_WRITE | VM_SHARED,jump to reuse label.
                     that is we can use the shared page with write or read without COW take place at there.
                     /* only one process owns the page frame,no COW */
              second> no such bits have setup,jump to gotten label.
                      /* need to copy */

              /**
               * cannot checks @_count if equal to 0(a single owner) only,because it would be increased
               * when the page frame has inserted into swap cache with PG_private flag is set in the page
               * descriptor.
               */

              /* @old_page is existing */
              take out anonymous pages,because anonymous shared vmas are not dirty accoutable.
              anonymous vma is existing AND @old_page is not ksm page
                                                             /**
                                                              * KSM => Kernel Same Page Merging
                                                              * A KSM Page => write-protected "shared page"
                                                              *               OR
                                                              *               merged page
                                                              */
              >
                      if we can not lock up @old_page,then we have to get its page cache,next unmap and unlock
                      @page_table.lock up @old_page,reset @page_table to the linear address of Page Table
                      entry which is corresponding to @address have residented on @pmd.
                      same @page_table and @orig_pte,then unlock @old_page,and release page cache for it,
                      jump to unlock lable.
                      they are not same,release page cache for @old_page,and continue executing.

                      continue from there:
                      
                      if we succeed to lock up @old_page,then set local variable @resue to the result
                      of reuse_swap_page(),which try to reuse a swap page frame for @old_page.
                      if we succeed to got a swap page to reuse,then move @old_page to @vma's anonymous VMA.
                      finally,unlock @old_page.
              @vma->vm_flags enabled VM_WRITE | VM_SHARED
              >
                      there,we handle write-faults on shared writable pages.
                      if virtual memory operations have been assigned AND page_mkwrite() routine is existing,too
                      then
                              construct a virtual memory area fault information structure @vmf is type of
                              struct vm_fault.
                              we initialize it with the informations about @old_page and the userspace address
                              which cause this fault(@address & PAGE_MASK)
                              next,get page cache for @old_page,unmap and unlock @page_table.
                              call to page_mkwrite(),which attempts to make a page frame in @vma is writable,the
                              information about the page frame is collected in @vmf
                              resulted in VM_FAULT_ERROR | VM_FAULT_NOPAGE,jump to unwritable_page.
                              resulted in no VM_FAULT_LOCKED,lock @old_page,if it is not mapped now,then unlock it
                              and jump to unwritable.
                              resulted in another cases,call to VM_BUG_ON() with parameter !PageLocked(@old_page).
                              
                              reset @page_table to pte_offset_map_lock().
                              if @page_table is not same as @orig_pte,then unlock @old_page and release page cache for
                              it,jump to unlock
                              otherwise,set local variable @page_mkwrite to 1,which records if make page writable is
                              succeed.

                      mark @old_page dirty,call to get_page(@dirty_page),set @resue to 1,which turn out we succeed
                      to reuse the page frame.
              if @reuse is TRUE,then jump to reuse label;otherwise,we get page cache for @old_page and jump to
              label "gotten".

              label resue:
                flush page cache for @orig_pte
                mark @orig_pte have been accessed,a new result is stored in @entry
                attempt to make @entry writable in @vma
                use @entry to set @page_table with _PAGE_ACCESSED enabled
                /* if succeed,have to update MMU cache */
                enable VM_FAULT_WRITE in @ret,which would be the result of this routine
                jump to unlock

              label gotten:
                unmap and unlock @page_table
                if @vma has an anonymous VMA,jump to oom
                if @orig_pte is the zero page frame(@empty_zero_page),then allocate a new zeroed page frame,if
                we failed,jump to oom
                if @orig_pte is not the first page frame,then allocate a new page frame with flag
                GFP_HIGHUSER_MOVABLE for the corresponding @address and @vma,if we failed,jump to
                oom;otherwise,do cow_user_page() /* Copy-On-Write for User Mode Page Frame */
                set @new_page Uptodate
                if @vma->vm_flags enabled VM_LOCKED and @old_page is existing
                        clear its memory lock
                        /* have to lock page at first,and release it later */
                if we failed to charge memory cgroup with the @new_page,jump to oom_free_new
                reset @page_table to pte_offset_map_lock() for @address
                if @page_table and @orig_pte are the same
                   /**
                    * allocating a new page frame maybe block @current
                    * thus have to checks whether Page Table Entry been modified by some other
                    * kernel control path when we sleeping
                    */
                        either @old_page is existing or is not,@mm's MM_ANONPAGES counter would be
                        increased
                        but if @old_page is not an anonymous page,have to decrease @mm's
                        MM_FILEPAGES counter

                        flush page cache for @orig_pte
                        makeup a new PTE through @orig_pte and @address,store it in @entry
                        attempts to make @entry is writable in @vma
                        clear @page_table and flush it first before updating the PTE with the new @entry
                        /* avoid a race condition,one thread doing SMC,another doing COW */
                        add PTE mapping toa new anonymous page @new_page in @vma at @address
                        notify MMU to map @new_page directly into the secondary page table
                        update MMU cache
                        if @old_page is existing,then take down pte mapping from a page
                        update @new_page to @old_page for free the old page later
                        enable VM_FAULT_WRITE in @ret
                if @page_table is not same as @orig_pte,then uncharge @new_page from memory cgroup
                release page cache for @new_page if it is existing
                /**
                 * decrease counter of @new_page
                 * PTE been modified by another kernel control path,then we should not do COW between
                 * @new_page and @old_page,so the counter of @new_page becomes to -1,it would be
                 * reclaimed later
                 * page_cache_release() -> put_page() -> free_hot_cold_page()
                 */
                release page cache for @old_page if it is existing
                /**
                 * we maybe decrease @old_page's counter twice,the first is the increasement we done
                 * before COW,the second is that @current is no longer use this @old_page because it
                 * has the exclusive page frame now
                 */

                !! special handling for the zero page would improves the system performance,because it
                   preserves the microprocessor hardware cache by making fewer address references.

              label unlock:
                unmap and unlock @page_table
                if @dirty_page is existing
                then checks
                        if @page_mkwrite is FALSE /* we failed to make the page frame at @address in @vma is writable */
                        then wait on @dirty_page until it locked up,next set page dirty balance on @dirty_page
                        put @dirty_page
                        if @page_mkwrite is TRUE
                        then
                                makeup a pointer of struct address_space,which will points to @dirty_page->mapping
                                                                                              /* mapping info */
                                set @dirty_page Dirty
                                unlock @dirty_page
                                release page cache for @dirty_page
                                if @mapping is existing,then balance dirty pages ratelimited on @mapping
                        if @vma is mapping a file,then call to file_update_time() on the file of @vma
                return @ret

              label oom_free_new:
                release page cache for @new_page
                jump to label oom

              label oom:
                if @old_page is existing,then release page cache for it,but must be done after unlock it;if @page_mkwrite
                have been succeed,we have to unlock @old_page and relese page cache for it,thus we may release page cache
                for it twice
                finally,return VM_FAULT_OOM

              label unwritable_page:
                release page cache for @old_page
                returns @ret to caller

        function cow_user_page() :
          <mm/memory.c>
            /**
             * cow_user_page - do Copy-On-Write on exact page frames
             * @dst:           copying of the original
             * @src:           original page frame
             * @va:            virtual linear address,starting from PAGE_OFFSET
             * @vma:           the VMA of @dst
             */
            static inline void cow_user_page(struct page *dst, struct page *src, unsigned long va,
                                             struct vm_area_struct *vma);

            brief description for cow_user_page() :
              there is a particular case that @src is NULL,in this case,function copy_user_highpage()
              would be invoked.and which would build two temporary kernel fix mappings for @src and @dst,
              next function copy_user_page() is called for make a copying for @src to @dst,finally,destroy
              the two fix mappings.
              the case @src is NULL that is unlikely,so when @src is not NULL,one kernel fix mapping was
              established for @dst with flag KM_USER0.next,@va is converted to userspace linear address,
              function __copy_from_user_inatomic() is called to copy the page frame starting at converted
              @va to the kernel temporary fix mapping.if we succeed to make a copying,memset() would be called
              to initializes the page frame.
              finally,unmap the fix mapping and flush dcache for the page frame @dst.

      Handling Noncontiguous Memory Area Access :
        a noncontiguous memory area is represented by a structure vm_area.
        function get_vm_area() will construct such descriptor and insert it into @vmlist,but the Page Table
        entries would not be updated.
        function vmalloc() and vfree() are used to control noncontiguous memory area,they would update
        Page Table entries,too.
        
        ! the master Kernel Page Tables(@init_mm.pgd) are not directly used by any process or kernel thread
          after kernel initialization phase have ended.

        the first time a process in Kernel Mode accesses a noncontiguous memory area the MMU would encountered a
        NULL Page Table entry,thus an Exception for Page Fault will be raised.because this is occurred in Kernel
        Mode,thus handler can recognizes this special case;and,the linear address is greater than TASK_SIZE.
        in this case,the Page Table is @init_mm.pgd.
        
        ! do_page_fault() call to vmalloc_fault() to handles noncontiguous memory area access fault.
          function vmalloc_fault() is defined in <arch/x86/mm/fault.c>.
          /**
           * static int vmalloc_fault(unsigned long address);
           * return 0 OR -1,@address is the faulty linear address.
           * brief description :
           *   if @address is not inside noncontiguous memory area,then return -1 to caller.
           *   get the master Kernel Page Table through pgd_offset_k(@address),that is the master Kernel Page Table
           *   Global Directory which corresponding to @address;the master Kernel Page Table Global Directory should
           *   not be none,if it was,return -1.
           *   get current process's PGD,if it is none,then copy the master Kernel Page Table Global Directory to it.
           *   # if it is not none but which is not equal to PGDK,then there is a BUG we encountered.
           *   function continue to processing,next to look at Upper Directory and at Middle Directory until find out
           *   such Page Table entry it corresponding @address;if Upper Directory or Middle Directory is NULL,then
           *   return -1 to caller.
           *   the PTE is get through pte_offset_kernel(),it is the kernel version for pte_offset().
           *   finally,setup this PTE and return _zero_ to caller.
           *   # if we failed on set Present on it or its pfn is not equal to the pfn in the master Kernel Page Table
           *     Directory,that means we encountered a BUG again.
           */

      Creating and Deleting a Process Address Space :
        in Linux,six cases will cause Kernel to creates a process address space,these have been mentioned in
        "The Process's Address Space".

        Creating a Process Address Space :
          routine copy_mm() would be called by copy_process() while creating a new process.the routine creates
          the process address space by setting up all Page Tables and memory descriptors of the new process.
          lightweight processes share the same address space of their creators(parents). /* CLONE_VM */
          because COW approach,traditional processes inherit the address space of their parent,pages are shared
          as long as they are only read;when a process attempts to write into a page frame,COW will take place
          at there,and the page frame would be duplicated.

          <kernel/fork.c>
            /**
             * copy_mm - copy address space
             * @clone_flags: behavior flags
             * @tsk:         the task that duplicated from @current process
             * return:       0 => succeed
             *               -ENOMEM => failed because no memory available
             */
            static int copy_mm(unsigned long clone_flags, struct task_struct *tsk);

            !! we can not clone a kernel thread,thus if @current->mm is NULL,copy_mm()
               will returns _zero_ to caller.

            about @clone_flags :
              CLONE_VM -> copy_mm() will increase @current's @mm_users member,then reset its
                          @token_priority and @last_interval of @mm member to _zero_,next,
                          assign @current's @mm to @tsk's @mm and @active_mm,that is shares
                          address space.
              !CLONE_VM -> copy_mm() will attempts to duplicate address space for @tsk,reset
                           duplicated @mm's @token_priority and @last_interval to _zero_,
                           finally,assign duplicated @mm to @tsk's @mm and @active_mm.
                           /**
                            * duplicating memory descriptor is done through dup_mm() routine,
                            * which will allocate a new memory descriptor and use @current's
                            * @mm to initialize it.
                            * if failed to allocating memory descriptor,function will returns
                            * -ENOMEM.
                            */

                           !! dup_mm() will call to
                                mm_init() -> initializes the address space of @tsk,and,a Page Global
                                             Directory for the new process @tsk will be created
                                init_new_context() -> architecture-dependent routine init_new_context()
                                                      is used to make a copy of customized LDT for @tsk
                                                      if @current has one
                                dup_mm_exe_file() -> duplicate executing file for @tsk,that is child
                                                     process will start running right after fork() calling.
                                dup_mmap() -> duplicate both the memory regions and the Page Tables of @current
                                              for @tsk.this routine inserts the new memory descriptor @tsk->mm
                                              in a global list of memory descriptor,then scans the list of regions
                                              owned by @current,starting from @current->mm->mmap,each encountered
                                              struct vm_area_struct object would be inserted into red-black tree
                                              of @tsk.
                                              right after each inserting of memory regions,routine copy_page_range()
                                              is called by dup_mmap().if necessary,the routine will create the Page
                                              Tables needed to map the group of pages included in the memory region
                                              and to initialize the new Page Table Entries.

              !! each page frame corresponding to a private,writable page(VM_SHARED 0,VM_MAYWRITE 1)
                 is marked as read-only for both the parent and the child,thus the Page Fault Exception
                 caused by write into a page frame is read-only marked can be handled by
                 COW mechanism.

        Deleting a Process Address Space :
          routine exit_mm() will be called while a process terminating,which release the address space owned by 
          this process.

          <kernel/exit.c>
            /**
             * exit_mm - release address space owned by the process
             * @tsk:     the process
             * # this routine will turn us into a lazy TLB process if
             *   we are not already
             */
            static void exit_mm(struct task_struct *tsk);

            brief description for exit_mm() :
              routine mm_release() would be called by exit_mm() on @tsk and @tsk's @mm.
              mm_release() will wake up all processes sleeping in @tsk->vfork_done completion,too.
              /* the wait queue is not empty,if this exiting process is created by means of the vfork() */
              if @mm is NULL,that this process is a kernel thread,then just returns to caller as well.
              if @tsk->mm->core_state is not NULL,then exit_mm() have to finish coredump,but before the checking,
              @mmap_sem must been acquired.
              /**
               * exit_mm() have to serialize any possible pending coredump,that is serialize any lightweight
               * process which is sharing memory address space with this process and attempts to create core file.
               * follow this way,it could avoid corruption in the core file.
               */
              next,increase memory usage counter @tsk->mm->mm_count.
              before reset @tsk->mm to NULL,have to check @mm and @active_mm at first.if they are not same,that
              is a kernel BUG.
              then,exit_mm() acquires task lock on @tsk,reset its @mm to NULL,release @mmap_sem,and put the
              processor that executing exit_mm() in lazy TLB mode,thus the Page Table entries would not be
              flushed immediately.
              proceed call to clear_freeze_flag() on @tsk,this routine would clear TIF_FREEZE flag,and next,
              release task lock,call to mm_update_next_owner() try to find out a process which needs this
              memory descriptor(adpot this address space);if unfound,set @mm's owner to NULL.
              /**
               * traverse all siblings,if any one is sharing this @mm,then set the owner of @mm to it.
               */
              finally,call to mmput() on @mm,which will release the LDT,the vm_area_struct objects,and the
              Page Table entries.
              /* because the processor in lazy TLB,thus the cache stay effect */

              !! @mm WOULD NOT BE DESTROYED NOW BECAUSE ITS USAGE COUNTER HAVE BEEN INCREASED.
                 @mm WOULD BE DESTROYED BY THE finish_task_switch() WHEN THE PROCESS BEING TERMINATED
                 WILL BE EFFECTIVELY REMOVED FROM THE LOCAL CPU.

      Managing the Heap :

        sections layout described by Linux ld : <arch/x86/kernel/vmlinux.lds.S>
          SECTIONS
          {
                  # if the program is not Linux Kernel
                    there would be ". = START"
                  . = __START_KERNEL /* x86_64 */
                  ...
                  text section {
                          _text = .
                          ...
                          _stext = .
                          ...
                          _etext = .
                  }
                  data section {
                          _sdata = .
                          ...
                          _edata = .
                  }
                  bss section {
                          __bss_start = .
                          ...
                          __bss_stop = .
                  }
                  program break {
                          __brk_base = .
                          ...
                          __brk_limit = .
                  }
                  _end = .
          }
          /* each program are linked and executing on Linux follow this memory layout */
          
        <arch/x86/kernel/setup.c> :
          _brk_end := __brk_base

          init_mm.start_code = _text
          init_mm.end_code = _etext
          init_mm.end_data = _edata
          init_mm.brk = _brk_end

          ! all other process are forked by init process,thus they would inherit the memory address space
            from init process.
          ! now we can know that how a C program's brk and start brk are arranged in memory
              [ text | data | bss |(start_brk) heap (brk)| ... unmapped ... | stack-start ] <- ESP
              <low address>                                                  <high address>
            heap : grows up
            stack : grows down
            <fs/binfmt_elf.c>: set_brk(@start, @end): current->mm->start_brk = current->mm->brk = end
            /**
             * function set_brk() calculate elf page aligned @start and @end,if @end is greater than @start,
             * acquire @current->mm_mmap_sem for write,do_brk(@start, @end - @start).
             * # do_brk() will establish memory region mapping [@start, @end - 1],new VMA will be allocated
             *   from kernel slab.
             * finally,set @start_brk and @brk of @current to the elf page aligned @end.
             * # this routine is called by load_elf_binary(),which invoke set_brk() with local variables
             *   @elf_bss and @elf_brk,they specified the range of bss section.
             *   after set_brk() mapped bss section,the @start_brk and @brk are set to the page aligned address
             *   that is greater than the address of bss section's boundary.
             */

        @start_brk and @brk members inside memory descriptor are used to delimit the starting and ending
        addresses,respectively,of the heap region.
        C library functions malloc(),calloc(),realloc(),free() are used to control dynamically allocating memory
        area.
        system call brk() is used to modifies the size of the heap directly,it receive a parameter is a
        linear address,which would be the new value of @current->mm->brk.
        /* program break -> the end of the process's data segment */
        /* it do not modify @start_brk because the process's @start_brk is initialized
         * when loading ELF binary file.
         */
        system call sbrk() is used to increase heap region.
        /**
         * on Linux,sbrk() is implemented as a library function which uses brk() system call
         * malloc(),calloc(),realloc(),free() are implemented through brk() or mmap()
         * realloc() can also make use of the mremap() system call
         */

        <mm/mmap.c>
          /**
           * sys_brk - Linux system call brk()
           * @brk:     new brk which is an linear address
           * return:   @current's @brk
           */
          SYSCALL_DEFINE1(brk, unsigned long, brk);
          => long int sys_brk(unsigned long brk);
          
          brief description for sys_brk() :
            before do something on memory descriptor,must acquires write lock on @mm->mmap_sem.
            if defined CONFIG_COMPAT_BRK,then the minmum brk is @end_code;otherwise is @start_brk.
            /* we can not let program break is fall into code segment */
            now check whether @brk is less than the minmum brk,if it is,then can not setup new brk.
            next to check resource limit,if @brk is not satisfy to the limitation,do not setup new brk.
            /**
             * rlimit(RLIMIT_DATA) < RLIMIT_INFINITY
             * AND
             * @brk - @mm->start_brk + @mm->end_data - @mm->start_data > rlimit(RLIMIT_DATA)
             * => un-satisfy
             */
            proceed executing PAGE_SIZE alignment on the new brk(@brk) and the old brk(@mm->brk),if aligned
            new brk is equal to the aligned old brk,then do not need to setup new brk.
            if @brk is less than or equal to @mm->brk,then have to shrink program break;call to do_munmap() for
            unmap the memory region starting from aligned new brk with length "aligned old brk - aligned new brk".
            do_munmap() returned zero,that means succeed to unmap the memory region,now set up new @brk,release
            semaphore and returns @mm->brk to caller.
            @brk is greater than @mm->brk,then must extend program break.
            first,check if there are memory region mappings existing intersects to new @brk,this will be done
            through find_vma_intersection(@mm, aligned old brk, aligned new brk + PAGE_SIZE).if found one,then
            can not setup new brk.
            second,call to do_brk() to extend program break.if the return value is not the aligned old brk,
            it turns out do_brk() was failed,do not setup new brk,release semaphore and returns the old brk.
            finally,release semaphore and returns @mm->brk to caller.
            /**
             * if we succeed on do_brk()(returned the aligned old brk),there would be a new VMA created through
             * kmem_cache_zalloc(),and its @vm_start is equal to aligned old brk,its @vm_end is equal to
             * aligned new brk.  
             * ! do_brk() will attempts to find out a VMA overlaps aligned old brk through find_vma_prepare(),
             *   if there is a VMA existing at aligned old brk,then have to unmap it at first;next try to
             *   expand the previous memory region for the aligned old brk,if failed,then create a new VMA right
             *   starting at aligned old brk,and insert it after the previous memory region.
             *   # if we can expand the previous memory region,then just adjust as well,no new VAM would be allocated.
             * # re-arrange the memory region which corresponding to the aligned old brk before setup new brk.
             * # the size to unmap is equal to "aligned new brk - aligned old brk".
             * # do_brk() routine is actually a simplified version of do_mmap() that handles only anonymous
             *   memory regions.(it assuming that the memory region does not map a file on disk)
             */


/* END OF CHAPTER9 */


Chapter 10 : System Calls
    POSIX APIs and System Calls :
      difference between an application programmer interface(API) and a system call >
        API => a function definition that specifies how to obtain a given service.
        System Call => an explicit request to the kernel made via a software interrupt.

      common APIs on Unix-like system >
        libc - standard C library,which refer to wrapper routines
               usually,each system call has a corresponding wrapper routine whose
               only purpose is to issue a system call,and programmers should to employ
               this routine for obtain the system service(if who wants the application is portable)
               # an API does not necessarily correspond to a specific system call
                 it can offer its services directly in User Mode withou system call
               # a single API function could make several system call
                 several API functions could make the same system call

      POSIX standard >
        it refers to APIs and not to system calls.
        A system can be certified as POSIX-compliant,if it offers the proper set of
        APIs to the application programs,no matter how the corresponding functions are
        implemented.

      error code >
        each system call retunrs negative value as the error code to indicates what kind of failure
        has encountered.
        the header <arch/x86/include/asm/errno.h> defined these error code as macro constants.
                                                          /* they are defined in positive value */
        in Linux 2.6.34.1,the header includes <include/asm-generic/errno.h>,and which includes
        <include/asm-generic/errno-base.h>.
        a header <errno.h> of a C library usually includes the Linux <asm-generic/errno.h> header.
        follow such way to allow portability of C programs among Unix / Unix-like systems.

    System Call Handler and Service Routines :
      when a User Mode process wants a system call,the CPU must switch to Kernel Mode,and start
      the execution of a kernel function.there are two ways for switch to Kernel Mode only;the
      first is executing the "int $0x80" assembly language instruction,the second is executing the
      "sysenter" assembly language(for x86_64,it is "syscall").both of them are jump to an assembly
      language function called the "system call handler".
      /**
       * 0x80 => 128 => Kernel Exception number 128 => associated with a System Gate
       */

      System Call Number :
        passed by User Mode process to Kernel to indicates what system call it wants,this is 
        mandatory because kernel have been implemented a lot of system-calls,thus we must tell
        kernel what system call should to be called.
        the system call number(NR_xyz) is used to retrieve the extra system call function address from
        system call table(@sys_call_table,an array contains NR_syscalls entries/* void pointer */).
        on x86_64,follow the ABI specification,register RAX is used to save the system call number.
        /**
         * NR_syscalls is a macro constant which just a static limit on the maximum number of
         * implementable system calls,it does not indicate the number of system calls actually
         * implemented by Kernel.
         * !! function sys_ni_syscall() means "nonimplemented" system call,it is a nop function usually,
         *    and it just returns error code -ENOSYS.
         */

      Return Value :
        system call returns a integer value,the conventions for these return values are different
        from those for wrapper routine.Linux Kernel use 0 or positive values denote a successful
        termination of the system call,while negative values denote an error condition,and the 
        negative value is the negation of error code which is used to describe what kind of error
        have been encountered.
        /**
         * wrapper routines may not returns the error code directly to caller(the userspace program),
         * global variable @errno is used to saves the error code for further description.
         */

      System Call Handler :
        system call handler has a structure similar to that of the other exception handlers.
        
        behaviors the system call handler will takes(in usual case) :
          > saves the contents of most registers in the Kernel Mode stack
            /* this behavior also be taken by interrupt handler */
          > handles the system call by invoking a corresponding C function called system call
            service routine
            /* sys_xyz(),such functions are defined by macro SYSCALL_DEFINEx() */
          > exits from the handler :
              loads the register contents saved in Kernel Mode stack previously
              CPU switch back to User Mode

        e.g.
          invoking flow scheme :
            
            User Mode process => Wrapper Routine => "int $0x80" OR "syscall" => ... =>
            Linux Exception Handler 128 => ... => Call to sys_call_table[System Call Number] =>
            ... => Ready to exit system call => ... => CPU switch back to User Mode =>
            User Mode process continues executing => ...
            /**
             * Yes,it would be interrupted at a moment until system call accomplished,
             * but in some cases,this User Mode process would enter TASK_INTERRUPTIBLE or 
             * TASK_UNINTERRUPTIBLE if the resource is not available now.
             */

    Entering and Exiting a System Call :
      native applications can invoke a system call in two different ways >
        1> int $0x80
        2> sysenter
      kernel can exit from a system call -- switching CPU back to User Mode >
        1> iret
        2> sysexit /* introduced in Intel Pentium II microprocessors together with "sysenter" */

      Kernel must supports these two ways to allows application is able to enter the kernel!
        > Kernel must supports both older libraries that only use "int $0x80" and more recent
          ones that also use "sysenter".
        > a standard library that makes use of "sysenter" must be able to cope with older kernels
          that support only "int $0x80".
        > Kernel and the standard library must be able to run both on older processors that do
          not include "sysenter" and on more recent ones that include it.
        /* these are compatiblity problems */

      Issuing a System Call via "int $0x80" instruction :
        the "traditional" way to invoke a system call makes use of the "int" assembly language
        instruction.
        interrupt vector 128 -- 0x80 is associated with the kernel entry point,it was setup by 
        trap_init() during kernel initialization.
        macro constant SYSCALL_VECTOR(0x80) is defined in <arch/x86/include/asm/irq_vectors.h>,
        and kernel routine trap_init() use it to setup a System Gate,and associated function
        system_call() with the it.
          "set_system_trap_gate(SYSCALL_VECTOR, &system_call);"

        <arch/x86/include/asm/desc.h>
          /**
           * _set_gate - write an entry into IDT at a specified gate
           * @gate:      interrupt vector as index for IDT
           * @type:      type of the gate
           * @addr:      entry point,corresponding to OFFSET field in gate descriptor
           * @dpl:       DPL
           * @ist:       Interrupt Stack Table
           * @set:       Segment Selector
           */
          static inline void _set_gate(int gate, unsigned type, void *addr, unsigned dpl,
                                       unsigned ist, unsigned seg);

        function trap_init() setup 0x80 by set_system_trap_gate(),and it just a wrapper
        call to _set_gate() with arguments (@n, GATE_TRAP, @addr, 0, 0, __KERNEL_CS).
        /**
         * GATE_TRAP => gate is trap,the corresponding handler does not disable maskable interrupts
         */

        The system_call() function :
          the function system_call() is defined in assembly language as an entry point,the 32bit version
          is defined in <arch/x86/kernel/entry_32.S>,and the 64bit version is defined in
          <arch/x86/kernel/entry_64.S>.

          <arch/x86/kernel/entry_64.S>
            ENTRY(system_call)
                    ... # this code segment does swap gs register
            ENTRY(system_call_after_swapgs)
                    ...
                    /**
                     * this code segment does the following works :
                     *   1> save RSP in a per-CPU variable named @old_rsp
                     *   2> switch current stack to the Kernel Mode stack,
                     *      which is stored in a per-CPU variable named @kernel_stack
                     *   3> enable interrupt
                     *      # CPU control unit will disable it before call to interrupt handler
                     *   4> save arguments
                     *   5> get thread info in RCX,and checks whether the system call invocations of
                     *      executed program are being traced by a debugger,if it is,then jump to
                     *      "tracesys" symbol
                     *   6> enter "system_call_fastpath"
                     */
            system_call_fastpath:
                    cmpq $__NR_syscall_max,%rax # can not exceed maximum limit
                    ja badsys                   # badsys:
                                                #   saves -ENOSYS on the stack position where corresponding
                                                #   to RAX register
                                                #   RAX-ARGOFFSET => 80 - 48 -> OFFSET OF RAX from stack top
                                                #                    RAX  R11
                                                #   next,jump to "ret_from_sys_call"
                    movq %r10,%rcx              # x86_64 system call ABI specification
                    call *sys_call_table(,%rax,8) # call to extra sys_xyz() handler /* segment jump */
                    movq %rax,RAX-ARGOFFSET(%rsp) # store return value from the sys_xyz() handler to stack
            ret_from_sys_call:
                    movl $_TIF_ALLWORK_MASK,%edi # flag _TIF_ALLWORK_MASK used to indicates 
                                                 # work to do on any return to user space
            sysret_check:
                    ...             # "sysret_check" will checks _TIF_ALLWORK_MASK(edi) if enabled in
                                    # struct thread_info @flags field(rcx),but before checking,it have to disable
                                    # local interrupt at first
                                    # if there is really some works have to be done before return to user space,
                                    # then jump to "sysret_careful"
                    USERGS_SYSRET64 # macro defined in <arch/x86/include/asm/irqflags.h>
                                    # expanded to "swapgs; sysretq;"
                                    # if current platform support to "sysret",then select this path
                                    # "sysret" will re-enable local interrupt
                                    # "sysret" is fast system call returning.if there would be some works have to
                                    # be done by "sysret_careful",then which finally end up at "irq_return"
                    ...
            sysret_careful:
                    ...  # if TIF_NEED_RESCHED,then TRACE_IRQS_ON and enable local interrupt,call to schedule
                         # after resumed from scheduling,jump to "sysret_check" to prepare exit system call
                         # if no TIF_NEED_RESCHED,then jump to "sysret_signal"
            sysret_signal:
                    ...  # TRACE_IRQS_ON,enable local interrupt
                         # if defined CONFIG_AUDITSYSCALL and TIF_SYSCALL_AUDIT is enabled,then jump to
                           "sysret_audit"
                         # otherwise,FIXUP_TOP_OF_STACK,and jump to "int_check_syscall_exit_work"
            ...
                    jmp int_check_syscall_exit_work
            ...
            irq_return:
                    INTERRUPT_RETURN  # iretq => macro defined in <arch/x86/include/asm/irqflags.h>
                                      # if current platform support to "iret" only,then select this path
                    ...

          !! an interrupt always be handled by "common_interrupt"(IDT descriptor) at first,next is do_IRQ(),
             thus function system_call() is invoked by do_IRQ().so,returning from system call will return to
             the caller that is do_IRQ(),finally,"common_interrupt" return to user space via "ret_from_intr".
          !! THINGS ARE SIMPLY,ASSEMBLY LANGUAGE INSTRUCTIONS "syscall","sysret","int","iret" DO NOT RETURN
             TO THE USERSPACE IMMEDIATELY,THEY RETURN TO THE CALLER BY RESTORE RIP/EIP REGISTER.

      Issuing a System Call via "sysenter" instruction :
        assembly language instruction "sysenter" is more fast than "int $0x80",because "int $0x80" must to
        finish several consistency and security checks.
        "sysenter" => fast system call

        The "sysenter" instruction :
          it makes use of three special registers that must be loaded with the following informations >
            1> SYSENTER_CS_MSR
               the segment selector of the kernel code segment
            2> SYSENTER_EIP_MSR
               the linear address of the kernel entry point
            3> SYSENTER_ESP_MSR
               the kernel stack pointer
            /* "MSR" => an acronym for "Model-Specific Register" */

          CPU control unit would does the following things when "sysenter" is executed :
            1> copies the content of SYSENTER_CS_MSR into cs
            2> copies the content of SYSENTER_EIP_MSR into eip
            3> copies the content of SYSENTER_ESP_MSR into esp
            4> adds 8 to the value of SYSENTER_CS_MSR,and loads this value into ss
            /* after accomplished these,the code of kernel entry point would be executed */
            /* 4> SYSENTER_CS_MSR + 8 => Kernel Data Segment Descriptor in GDT */

          function enable_sep_cpu() is used to initializes MSRs.
          <arch/x86/vdso/vdso32-setup.c>
            /**
             * enable_sep_cpu - initializes MSRs for later "sysenter" instruction
             * # if boot_cpu_has(X86_FEATURE_SEP) returns FALSE,then this function does nothing
             * # SEP => SYSENTER/SYSEXIT present
             */
            void enable_sep_cpu(void);

            brief description for enable_sep_cpu() :
              each CPU on the system would processes this function once during initialization.
              if boot_cpu_has(X86_FEATURE_SEP) returned TRUE,that is SYSENTER/SYSEXIT presented,
              then setup
                      tss->x86_tss.ss1 = __KERNEL_CS
                      tss->x86_tss.sp1 = sizeof(struct tss_struct) + (unsigned long)tss
                      /**
                       * computes the linear address of the end of the local TSS
                       */
                      MSR_IA32_SYSENTER_CS := __KERNEL_CS
                      MSR_IA32_SYSENTER_ESP := tss->x86_tss.sp1
                      /* the register should point to a real stack */
                      MSR_IA32_SYSENTER_EIP := (unsigned long) ia32_sysenter_target
                                               /* sysenter entry point */

              When a system call starts,the kernel stack is empty,thus the esp register should
              point to the end of the 4- or 8-KB memory area.([thread_info, kernel mode stack])
              wrapper routine can not set this register properly because it does not know the
              address of this memory area.

        The vsyscall page :
          libc wrapper routine can make use of the "sysenter" instruction only if both the CPU and
          the Linux Kernel support it.
          function sysenter_setup() is used to build a page frame called vsyscall page during
          system initialization.the page frame containing a small ELF shared object,when a process
          issue a system call,the ELF would be linked into its address space automatically,the
          code in the vsyscall page makes use of the best avaiable instruction to issue the
          system call.

          vDSO : virtual dynamic shared object

          <arch/x86/vdso/vdso32-setup.c>
            /**
             * sysenter_setup - setup a vsyscall page frame for further system call
             * return:          always return _zero_             
             */
            int __init sysenter_setup(void);

            brief description for sysenter_setup() :
              this function attempts to get a zeroed page frame with GFP_ATOMIC flag.
              get the page frame descriptor via virt_to_page(),and use the descriptor to
              set @vdso32_pages[0] element,construct a reference.
              next,select the best system call method follow the order
                syscall > sysenter > int80
                /* vdso32_syscall_start(), vdso32_sysenter_start(), vdso32_int80_start() */
              the code contents of the system call method would be written into vsyscall
              page frame through memcpy().
              relocate vdso and return _zero_.
              /**
               * global symbols vdso32_syscall_start,vdso32_sysenter_start,vdso32_int80_start
               * are introduced in <arch/x86/vdso/vdso32.S>
               * in the file vdso32.S,GAS direct ".incbin" is used to include dynamic library files,
               * and the files are
               *   vdso32-int80.so
               *   vdso32-syscall.so
               *   vdso32-sysenter.so
               * the corresponding assembly file is under the directory "arch/x86/vdso/vdso32".
               */

          if the libc wrapper routine wants to issue a system call,it can call to "__kernel_vsyscall".
          the symbol is defined in files int80.S,syscall.S,sysenter.S.because the vsyscall page frame
          have initialized by sysenter_setup() with the best system call method,the wrapper can make
          use the best method in simply call to the symbol "__kernel_vsyscall" as well.
          /**
           * assembly code segment of __kernel_vsyscall for "sysenter" :
           *   __kernel_vsyscall:
           *         push %ecx
           *         push %edx
           *         push %ebp
           *   .Lenter_kernel:
           *         movl %esp,%ebp
           *         sysenter
           *         .space 7,0x90
           *         jmp .Lenter_kernel
           *   VDSO32_SYSENTER_RETURN:
           *         pop %ebp   # these registers have been stored in User Mode stack previously
           *         pop %edx
           *         pop %ecx
           *         ret
           */

          !! IN THE OLD VERSION OF KERNEL,NO vsyscall page frame IS BUILT,THUS "__kernel_vsyscall"
             DOES NOT EXIST,THE libc WRAPPER MUST USE OF ASSEMBLY LANGUAGE INSTRUCTION "int $0x80" TO
             ISSUE A SYSTEM CALL.

        Entering the system call :
          MSR_IA32_SYSENTER_EIP is set to ia32_sysenter_target(),the function is written by assembly
          language which is introduced in <arch/x86/kernel/entry_32.S>.

          <arch/x86/kernel/entry_32.S>
            /**
             * ia32_sysenter_target - Interl 32bit architecture system call function for sysenter
             * # this function is similar to system_call() assembly function
             */
            ENTRY(ia32_sysenter_target)
                    movl TSS_sysenter_sp0(%esp),%esp # esp stored TSS pointer previously
                                                     # this statement setup kernel stack pointer
                                                     # of current process
            sysenter_past_esp:
                    push USER DATA SEGMENT
                    push ebp register                # at there,the content of ebp register is
                                                     # is the content of esp that set by
                                                     # __kernel_vsyscall(),that is User Mode
                                                     # stack pointer
                    push eflag register
                    orl $X86_EFLAGS_IF,(%esp)
                    push USER CODE SEGMENT
                    /**
                     * push current_thread_info()->sysenter_return to the stack
                     * a tiny bit of offset fixup is necessary
                     *   4 * 4 -> 4 words pushed above
                     *   + 8 -> corresponds to copy_thread's esp0 setting
                     */
                    push TI_sysenter_return-THREAD_SIZE+8+4*4)(%esp)
                    push eax register   # system call number
                    SAVE_ALL
                    ENABLE_INTERRUPTS(CLBR_NONE) # enable local interrupt during system call
                    /**
                     * load the potential sixth argument from user stack
                     */
                    cmpl $__PAGE_OFFSET-3,%ebp # __PAGE_OFFSET => 0xc0000000
                                               # ebp => User Mode stack pointer
                    jae syscall_fault
                    /**
                     * assembly function :
                     *   "enter" => pushl %ebp
                     *              movl %esp,%ebp
                     *   "leave" => movl %ebp,%esp
                     *              popl %ebp
                     * EBP => current user stack pointer 
                     * __kernel_vsyscall() saved ebp on the User Mode Stack before
                     * process "sysenter" instruction
                     */
            1:      movl (%ebp),%ebp
                    movl %ebp,PT_EBP(%esp)
                    GET_THREAD_INFO(%ebp)
                    testl $_TIF_WORK_SYSCALL_ENTRY,TI_flags(%ebp)
                    jnz sysenter_audit
            sysenter_do_call:
                    compares system call number in eax with @nr_syscalls
                    jump to syscall_badsys if exceeded maximum limit
                    call *sys_call_table(,%eax,4)
                    movl %eax,PT_EAX(%esp) # save system call return value
                    LOCKDEP_SYS_EXIT
                    DISABLE_INTERRUPTS(CLBR_ANY)    # before switch back to User Mode
                                                    # have to check thread_info flags at first,
                                                    # if there remains some works have to be done,
                                                    # must deal with them at first
                                                    # must disable local interrupt during checking
                                                    # "sysexit_audit" will enable local interrupt
                                                    # later
                    TRACE_IRQS_OFF
                    movl TI_flags(%ebp),%ecx
                    testl $_TIF_ALLWORK_MASK,%ecx
                    jne sysexit_audit
            sysenter_exit:
                    ...

        Exiting from the system call :
          a symbol named "sysenter_exit" is existing in assembly function ia32_sysenter_target(),
          when a system call is terminated,the return value would be stored in eax,and processor
          continue to execute the code segment of "sysenter_exit".

          <arch/x86/kernel/entry_32.S>
            ENTRY(ia32_sysenter_target)
                    ...
            sysenter_exit:
                    movl PT_EIP(%esp),%edx      # restore return point in edx
                                                # the pointer is point to
                                                # current_thread_info()->sysenter_return
                                                # which have been stored on Kernel Mode stack
                                                # by "sysenter_past_esp",and @sysenter_return
                                                # usually is the symbol VDSO32_SYSENTER_RETURN
                    movl PT_OLDESP(%esp),%ecx   # restore the old esp in ecx
                    xorl %ebp,%ebp
                    TRACE_IRQS_ON
            1:      mov PT_FS(%esp),%fs         
                    PTGS_TO_GS
                    ENABLE_INTERRUPTS_SYSEXIT # aka "sti; sysexit"
            /**
             * system call return value have been stored in Kernel Mode stack by
             * "sysenter_do_call" via "movl %eax,PT_EAX(%esp)"
             * if there remains some works have to be done,that would be handled by
             * "work_pending".
             */

        The sysexit instruction :
          sysexit is the companion of sysenter,it allows a fast switch from Kernel Mode to User Mode.
          CPU control unit does these when sysexit is executed :
            1> SYSENTER_CS_MSR += 16
               mov SYSENTER_CS_MSR,%cs # restore user code segment
            2> movl %edx,%eip          # VDSO32_SYSENTER_RETURN
            3> SYSENTER_CS_MSR += 24   # restore user stack segment
               mov SYSENTER_CS_MSR,%ss
                                       # have to understand that,register esp
                                       # is stack pointer,which always point to the
                                       # top of stack
            4> movl %ecx,%esp

    Parameter Passing :
      On the older architecture such Intel 80386,function's arguments usually passed via active program stack,
      either the User Mode stack or the Kernel Mode stack.
      /**
       * if passing via stack,the right-most argument must be push on the stack at the first
       * the return value of a function usually stored in eax regular register
       */
      But on the recent architecture such x86_64(amd64),the arguments are passed via CPU registers.
      /**
       * User Mode Program function calling :
       * 1   2   3   4   5  6
       * rdi rsi rdx rcx r8 r9
       * rax => 1st return value
       * rdx => 2nd return value
       * 
       * passing or return floating arguments
       * xmm0 - xmm1
       * passing floating arguments
       * xmm2 - xmm7
       * return long double arguments
       * st0 st1
       */

      Which registers to be used is defined by architecture ABI(Application Binary Interface).
      /**
       * x86_64 system call :
       * 1   2   3   4   5  6
       * rdi rsi rdx r10 r8 r9
       * rax => return value
       * all kernel system calls are not require a floating argument,in other word,no floating
       * value is appearences in kernel.
       * callee-clobered : rdi rsi rdx rcx r8 r9
       * callee-saved : rbx rbp r12 r13 r14 r15
       * caller-saved [callee-clobered] : r10 r11
       */

      Linux kernel would copies the arguments stored in the CPU registers on the Kernel Mode stack
      before call to a extra system call service routine,because the latter is an ordinary C function.
      But for passing arguments through CPU registers,there are two conditions must be satisfied :
        1> the length of each parameter cannot exceed the length of a register(32 bits on 8086)
           /**
            * always true,POSIX standard,large parameters that cannot be stored in a 32-bit register
            * must be passed by reference(such argument pointer)
            */
        2> the number of parameters must not exceed six,besides the system call number passed in eax,
           because 8086 processors have a very limited number of registers
           /**
            * if the number of parameters have exceeded six,then an additional register is used to
            * stores a pointer points to a memory area in the process address space that contains
            * the parameter _values_
            */

      !! BEFORE A FUNCTION CALLING,IF SOME CPU REGISTERS WOULD BE USED DURING THE FUNCTION IS PROCESSING,
         MUST SAVE THEIR CONTENTS ON THE STACK,AND RESTORE THEM LATER IF NECESSARY.
         SOME REGISTERS ARE NOT REQUIRE KEEP UNCHANGED ACROSS FUNCTION CALLING.
         /* SUCH REGISTER USUALLY IS A REGULAR REGISTER AND USED FOR FUNCTION PARAMETER PASSING,i.e. rdi,etc. */

      8086 architecture kernel system call use the registers :
        /* these informations can be found in <arch/x86/include/asm/calling.h> */

        eax => system call number / 1st return value
        ebx => 1st parameter
        ecx => 2nd parameter
        edx => 3rd parameter / 2nd return value
        esi => 4th parameter
        edi => 5th parameter
        ebp => stack frame pointer
               /**
                * if more than six parameters have passed,then the remain parameters include 6th would
                * be stored on the User Mode stack,and ebp as the stack frame pointer.
                */

        callee-clobered : eax edx ecx
        callee-saved : ebx edi esi ebp

      ! callee saved => callee should takes care of the contents of the registers if it wants to use
                        them,caller does not care about whether the contents of them has changed
        callee clobered => callee would uses these registers,thus the contents should be saved
                           by caller before invocation
        caller saved => caller should take care of the contents of the registers

      Verifying the Parameters :
        all system call parameters must be carefully checked before the kernel attempts to satisfy a
        user request,the type of check depends both on the system call and on the specific parameter.

        address checking : common to all system calls
                           1> verify that the linear address belongs to the process address space and,
                              if so,that the memory region including it has the proper access rights
                              /* adopted by early Linux Kernel */
                           2> verify just that the linear address is lower than PAGE_OFFSET
                              /* adopted by recent Linux Kernel(include Linux 2.6.34.1) */
                              /* necessary but not sufficient condition */

        ! defer real checking until the last possible moment.
        ! the coarse checking is actually crucial to preserve both process address spaces and the
          kernel address space from illegal accesses.

        <arch/x86/include/asm/uaccess.h>
          /**
           * access_ok - architecture depent accessing checking for address
           * @type:      type of access
           *             => VERIFY_READ || VERIFY_WRITE
           * @addr:      user space pointer to start of block to check
           * @size:      size of block to check
           * return:     true => is OK
           *             false => is definitely invalid
           * # this macro function may sleep,it can only running within User context
           * # depending on architecture,this function probably just checks that the
           *   pointer is in the user space range,after calling this routine,memory
           *   access functions may still return -EFAULT
           * # VERIFY_WRITE is a superset of VERIFY_READ - if it is safe to write to a block,
           *   it is always safe to read from it
           */
          #define access_ok(type, addr, size) (likely(__range_not_ok(addr, size) == 0))

          /**
           * __range_not_ok - test whether a block of memory is a valid user space address
           * @addr:           address
           * @size:           size of memory block
           * return:          0 => valid
           *                  nonzero => invalid
           */
          #define __range_not_ok(addr, size)

          ! __range_not_ok() is equivalent to the following test :
                    (u33)addr + (u33)size >= (u33)current->addr_limit.seg
            /* x86_32 => 33-bit x86_64 => 65-bit */
            /* the additional bit for carry(CF flag in eflags) */

            this routine would call to __chk_usr_ptr() on @addr,the function use the compiler
            feature to check whether @addr is a user space pointer.
            /* do the test during compile-time */

            this routine has an inline assembly piece :
              # @flag => local variable is type of unsigned long
              # @roksum => local variable is type of unsigned long

              add @size,@addr
              sbb @flag,@flag # @flag := @flag - @flag - CF
              cmp @roksum,current_thread_info()->addr_limit.seg # @roksum := @size + @addr
              sbb @flag,@flag # @flag := @flag - @flag - CF
              # finally,returns @flag
            
              # because @flag is output location,thus the register which would be used as
                output register for @flag has the value _zero_ or _nonzero_
              # current_thread_info()->addr_limit.seg usually has the value PAGE_OFFSET for
                normal processes,and the value 0xffffffff for kernel threads
                /**
                 * get_fs() and set_fs() can change the field dynamically.
                 * kernel can use them to bypass the security checks made by access_ok(),
                 * so that it can invoke system call service routines directly passing to
                 * them addresses in the kernel data segment.
                 */

          ! function verify_area() is similar to access_ok(),but it been obsolete.

    Accessing the Process Address Space :
      Linux includes a set of macros that make the accessing to process address space easier.
      
      !! these macros can only be called in User context.      

      <arch/x86/include/asm/uaccess.h>
        /**
         * get_user - get a variable from user space and store it in a specified position
         * @x:        where the result to be stored
         * @ptr:      pointer points to the data source address
         * return:    0 => succeed
         *            -EFAULT => failed,in this case,@x set to _zero_
         */
        #define get_user(x, ptr)

        how get_user() works :
          it computes the size of *@ptr at first,and then call to macro __get_user_x() to does
          retrieving.
          the routine supports to retrieves data from user space with size 1B,2B,4B,8B.

        /**
         * put_user - put a variable which stored in a special postion to
         *            the user space position
         * @x:        where the data storing
         * @ptr:      user space position for store the data
         * return:    0 => succeed
         *            -EFAULT => failed
         * # @ptr must have pointer-to-simple-variable type
         *   @x must be assignable to the result of dereferencing @ptr
         * # this routine is similar to get_user() and also supports 1B,2B,4B,8B data
         */
        #define put_user(x, ptr)

      __get_user_@size() :
        the routine iscalled by __get_user_x(),and which is called by get_user().
        for different @size(1,2,4,8),the different __get_user_@size() will be invoked.
        these assembly functions are defined in <arch/x86/lib/getuser.S>.

        ! __get_user_x() stores @ptr in eax register

        e.g.
          ENTRY(__get_user_2)
                  CFI_STARTPROC
                  add $1,%_ASM_AX   # eax += 1
                  jc bad_get_user   # CF == 1,"carry", overflow occurred "access_ok()"
                  GET_THREAD_INFO(%_ASM_DX)
                  cmp TI_addr_limit(%_ASM_DX),%_ASM_AX # address of memory block must is a User Mode address
                                                       # "access_ok()"
                  jae bad_get_user                    
          2:      movzwl -1(%_ASM_AX),%edx # copy 2B data from the pointer eax points to to the register eax
                  xor %eax,%eax # set eax to _zero_,means succeed
                  ret
                  CFI_ENDPROC
          ENDPROC(__get_user_2)

      __put_user_@size() :
        it is similar to __get_user_@size().these routines are defined in <arch/x86/lib/putuser.S>.
        these function would be called by __put_user_x().

        ! __put_user_x() stores @x in eax,and stores @ptr in ecx.

        e.g.
          ENTRY(__put_user_2)
                  ENTER
                  mov TI_addr_limit(%_ASM_BX),%_ASM_BX # current_thread_info()->addr_limit
                  sub $1,%_ASM_BX          # may be underflow
                                           # 0xffffffff - 0x01 = 0xfffffffe
                  cmp %_ASM_BX,%_ASM_CX    # user space address should less than 0xc000000
                                           # and kernel thread usually has addr_limit on
                                           # 0xffffffff
                                           # ebx should has the value less than 0xc0000000,
                                           # because current is user context
                                           # if ebx underflow,then its value must be 0xffffffff
                  jae bad_put_user
          2:      movw %ax,(%_ASM_CX)      # move the data in eax to the position @ptr points to
                  xor %eax,%eax
                  exit
          ENDPROC(__put_user_2)
                                           ! if the thread's addr_limit is USER_DS,the value is
                                             0xc0000000,if the thread is kernel thread,its addr_limit
                                             is 0xffffffff

      another macros :
        get_user
        __get_user
        put_user
        __put_user
        copy_from_user          -                copies a block of arbitrary size from user space
        __copy_from_user        
        copy_to_user            _                copies a block of arbitrary size to user space
        __copy_to_user
        strncpy_from_user       -                copies a null-terminated string from user space
        __strncpy_from_user
        strlen_user             -                returns the length of a null-terminated string in user space
        strnlen_user
        clear_user              -                fills a memory area in user space with _zeros_
        __clear_user

        no "__" prefixed => check validity of the linear address
        with "__" prefixed => bypass the linear address checking

    Dynamic Address Checking - The Fix-up Code :
      access_ok() only does coarse checking on an linear address to ensures that linear address is less than
      PAGE_OFFSET(0xc0000000),but does not ensures that linear address whether belongs to another User Mode
      process's address space.
      a Page Fault Exception would be raised when kernel try to use the bad linear address.

      four cases that Page Fault exception may occur in Kernel Mode :
        1> the page frame does not exist or the kernel attempt to write a read-only page frame.
           in these cases,kernel must allocates and initializes a new page frame.
           /* page frame does not exist => allocate and initialize; read-only => Copy-On-Write */
        2> Page Table entry has not yet been initialized.
           kernel must properly set up some entries in the Page Tables of the current process.
        3> kernel programming BUG.
           transient hardware error.
           in these cases,kernel must report an oops.
        4> a system call service routine attempts to read or write into a memory area whose address has been
           passed as a system call parameter,but that address does not belong to the process address space.
                                                                                 /* @current */

      The Exception Tables :
        the key to determining the source of a Page Fault lies in the narrow range of calls that the kernel
        uses to access the process address space.
        only a small group of functions and macros are used to access process address space,so if the exception
        is caused by an invalid parameter,the instruction that caused it must be included in one of the functions
        or be generated by expanding one of the macros.

        "Exception Table" => a table recorded the addresses of instructions that access to process address space.
        when a Page Fault have been raised,do_page_fault() is able to destinguish what kind of the Page Fault
        exception is by examine "Exception Table" with the instruction caused this exception.

        ! Linux defines several exception tables.the main exception table is automatically generated by the
          C compiler when building the kernel program image.

        <arch/x86/include/asm/uaccess.h>
          /**
           * exception_table_entry - kernel exception table entry,the exception table consists of pairs of
           *                         addresses,the first is the address of an instruction that is allowed to
           *                         fault,and the second is the address at which the program should continue
           * @insn:                  instruction that access the process address space
           * @fixup:                 assembly language code to be invoked when a Page Fault exception triggered
           *                         by the instruction located at @insn occurs
           */
          struct exception_table_entry {
                  unsigned long insn, fixup;
          };

        section "__ex_table" => code segment contains the main exception table.

        <kernel/extable.c> : __start___ex_table => struct exception_table_entry pointer points to the starting of
                                                   section "__ex_table"
        <kernel/extable.c> : __stop___ex_table => struct exception_table_entry pointer points to the ending of
                                                  section "__ex_table"

        !! EACH DYNAMICALLY LOADED MODULE OF THE KERNEL INCLUDES ITS OWN LOCAL EXCEPTION TABLE,AND THE TABLE
           IS AUTOMATICALLY GENERATED BY THE C COMPILER WHEN BUILDING THE MODULE IMAGE,AND IT IS LOADED INTO
           MEMORY WHEN THE MODULE IS INSERTED IN THE RUNNING KERNEL.

        generally,when CPU jump to @fixup,a sequence of instructions would be processed,and forces the service
        routine to return an error code to the User Mode process.
        these instructions are usually defined in the same macro or function that access the process address space,
        they are placed by the C compiler into a separate section of the kernel code segment called ".fixup".

        <kernel/extable.c>
          /**
           * search_exception_tables - see if a given linear address  matched any one of the exception_table_entry's
           *                           @insn field
           * @addr:                    the linear address to matching
           * return:                   pointer points to an object is type of struct exception_table_entry whose
           *                           @insn matched @addr
           *                           NULL => if unfound
           */
          const struct exception_table_entry *search_exception_tables(unsigned long addr);

          brief description for search_exception_tables() :
            traverse each struct exception_table_entry objects in section "__ex_table"([__start___ex_table, __stop___ex_table))
            attempts to find out such object matched @addr.
            thus do_page_fault() is able to attempts retrieve the fixup routine by call to search_exception_tables(),
            if it succeed to find one,then @eip field of struct pt_regs will be set to the @fixup.
            the object is type of struct pt_regs have been stored on Kernel Mode stack by CPU control unit,it would be
            restore to CPU registers later when back to User Mode.
            after setup eip,do_page_fault() returns,and CPU continues from new eip => @fixup after switch back to User Mode.

      Generating the Exception Tables and the Fixup Code :
        Linux generates an exception table entry usually in assembly language.

          .section __ex_table, "a"
                  .long <faulty_instruction_address>, <fixup_code_address> # an exception table entry

        section "__ex_table" stores the exception table entries.
        section "fixup" stores the assembly code @fixup will be executed when @insn caused a Page Fault exception.

        gnu/as - the GNU assembler
                .section [directive] => define a section with an optional attribute,if the section @name have been
                                        defined,this directive will switch current section to the section @name.

                                        usage : .section name[, "flags"[, @type]]
                                                  flags for ELF target :
                                                    a: allocatable
                                                    w: writable
                                                    x: executable
                                                  @type :
                                                    @progbits => section contains data
                                                    @nobits   => section does not contain data

                                        e.g.
                                          .section __ex_table,"a"
                                                                +--> "a" : the section must be loaded into memory
                                                                           together with the Kernel image

                .previous => switch back to the previous section

        assembly code for __get_user_2() :
                  e.g.
          ENTRY(__get_user_2)
                  CFI_STARTPROC
                  add $1,%_ASM_AX   # eax += 1
                  jc bad_get_user   # CF == 1,"carry", overflow occurred "access_ok()"
                  GET_THREAD_INFO(%_ASM_DX)
                  cmp TI_addr_limit(%_ASM_DX),%_ASM_AX # address of memory block must is a User Mode address
                                                       # "access_ok()"
                  jae bad_get_user                    
          2:      movzwl -1(%_ASM_AX),%edx # copy 2B data from the pointer eax points to to the register eax
                  xor %eax,%eax # set eax to _zero_,means succeed
                  ret
                  CFI_ENDPROC
          ENDPROC(__get_user_2)

          ...

          bad_get_user:
                  CFI_STARTPROC
                  xor %edx,%edx
                  mov $(-EFAULT),%_ASM_AX   # just returns -EFAULT to the User Mode process
                  ret
                  CFI_ENDPROC
          END(bad_get_user)

          .section __ex_table,"a"
                  _ASM_PTR 1b,bad_get_user
                  _ASM_PTR 2b,bad_get_user
                  _ASM_PTR 3b,bad_get_user
          #ifdef CONFIG_x86_64
                  _ASM_PTR 4b,bad_get_user
          #endif

          assembly code at label "2" does the copy from user,that is the code will access to user space.
          and the assembly code "_ASM_PTR 2b,bad_get_user",it is placed in section "__ex_table",thus it
          represents an exception table entry.
          symbol "2b" as @insn,and symbol "bad_get_user" as @fixup.
          if Page Fault exception is occurred while process code at "2b",do_page_fault() will try to
          get @fixup from the __ex_table,so @fixup is points to "bad_get_user".
          !! @fixup will be called after do_page_fault() returned and CPU is progress switch back to User Mode.

        strlen_user() => strnlen_user() :
          strlen_user() is a macro which is expanded to strnlen_user().
          strnlen_user() is defined in <arch/x86/lib/usercopy_32.c>
            long strnlen_user(const char __user *s, long n);

          and it has an inline assembly statement.
          which is expanded to the following assembly code :
                    movl $0,%eax            # initialize eax to _zero_
                    movl $0x7fffffff,%ecx   # initialize eax,which will as the counter
                    movl @n,%ebx            # load maximum count in ebx
                    testl %ebx,%ebx         # if @n is _zero_,then do not scan the string
                    jz 3f
                    andl %ebx,%ecx          # set ecx to the maximum count
            0:      repne                   # repeat the string scanning instruction until
                                            # encountered _zero_ or ecx become _zero_
                                            # and symbol "0" will as @insn in an exception
                                            # table entry
                    scasb                   # scanning the string its address in esi
                    setne %al               # use ZF flag to set al
                    subl %ecx,%ebx          # get the number of scanned characters in ebx
                    addl %ebx,%eax          # if @n is less than real length,then al is _zero_,
                                            # otherwise,al is _one_ that means encountered the
                                            # null-termiate
                                            # now eax stored the length of the string
            1:
            .section .fixup,"ax"            # note,from there,the section have switched to "fixup",
                                            # thus there is no more code would be executed in this function
                                            # generally,the main executing path is under ".text" section
                                            # "ax" => section is allocatable and executable for ELF target
            2:      xorl %eax,%eax          # set eax to _zero_,thus 0 will as the error code when Page
                                            # Fault exception occurred
                    jmp 1b                  # jump to "1b",but the next ".previous" will switch back to
                                            # ".text",and there is no more statement to be executed further,
                                            # thus this function will returns _zero_

            3:      movb $1,%al             # symbol "3" is belong to section "fixup"
                    jmp 1b
            .previous
            .section __ex_table,"a"
                    .align 4
                    .long 0b,2b             # exception table entry (0b, 2b)
                                            # 0b => "repne; scasb"
                                            # 2b => "xorl %eax,%eax"
            .previous

            
    Kernel Wrapper Routines :
      !! have to pay attention about that Linux 2.6.34.1 does not provide system call wrapper
         routines such write(), read(), ... they are like to C library wrappers and builded through
         macros _syscall0, _syscall1, and so on.thus,these macros also have been removed,too.

      Linux defined six macros for simplify the declarations of system calls.

      <linux/syscalls.h>
        /* macros for system call routine definition */
        #define SYSCALL_DEFINE0(name)  asmlinkage long sys_##name(void)
        #define SYSCALL_DEFINE1(namem, ...) SYSCALL_DEFINEx(1, _##name, __VA_ARGS__)
        #define SYSCALL_DEFINE2(namem, ...) SYSCALL_DEFINEx(2, _##name, __VA_ARGS__)
        #define SYSCALL_DEFINE3(namem, ...) SYSCALL_DEFINEx(3, _##name, __VA_ARGS__)                                                   
        #define SYSCALL_DEFINE4(namem, ...) SYSCALL_DEFINEx(4, _##name, __VA_ARGS__)
        #define SYSCALL_DEFINE5(namem, ...) SYSCALL_DEFINEx(5, _##name, __VA_ARGS__)
        #define SYSCALL_DEFINE6(namem, ...) SYSCALL_DEFINEx(6, _##name, __VA_ARGS__)

        #define __SYSCALL_DEFINEx(x, name, ...)  asmlinkage long sys##name(__SC_DECL##x(__VA_ARGS__))

        /* recursive definitions,these are similar to C++ template */
        #define __SC_DECL1(t1, a1)  t1 a1
        #define __SC_DECL2(t2, a2, ...) t2 a2, __SC_DECL1(__VA_ARGS__)
        ...
        #define __SC_DECL6(t6, a6, ...) t6 a6, __SC_DECL5(__VA_ARGS__)

      e.g.
        /* the real definition of sys_write() is included in <linux/syscalls.h> */
        SYSCALL_DEFINE3(write, unsigned int, fd, const char __user *, buf, size_t, count);

      the standard system call always returns a value is type of long.

      if we want to call a system call in Kernel Mode,just use sys_NAME() as well.
      if the system call will do access_ok() the parameters passed-by pointer(or it will access to
      process address space),must call to set_fs(KERNEL_DS) at first to bypass address validity checking.
      /**
       * <arch/x86/include/asm/uaccess.h>
       *   #define MAKE_MM_SEG(s) ((mm_segment_t) { (s) })
       *   #define KERNEL_DS MAKE_MM_SEG(-1UL) /* -1 => 0xffffffff */
       *   #define USER_DS MAKE_MM_SEG(TASK_SIZE_MAX) /* TASK_SIZE_MAX => TASK_SIZE => PAGE_OFFSET */
       *   #define set_fs(x) (current_thread_info()->addr_limit = (x))
       *   #define get_fs() (current_thread_info()->addr_limit)
       *   !! thread_info.@addr_limit => type of mm_segment_t,it has a member named @seg is type of unsigned long
       *   !! struct mm_segment_t { unsigned long seg; }; # defined in <arch/x86/include/asm/processor.h>
       */

      !! if it is not necessary,do not invoke system call in Kernel Mode,use other kernel function to instead.
         for example,if we want to open a file and write some thing into it,do not call to open() and write()
         system calls,we should call to kernel functions filp_open() and vfs_write().these two function are
         declared in <linux/fs.h> they are belong to VFS,and their function definition is placed in
         <fs/namei.c> and <fs/read_write.c>,respectively.


/* END OF CHAPTER10 */


Chapter 11 : Signals
    The Role of Signals :
      a signal is a very short message that may be sent to a process or a group of processes,the only information
      given to the process is usually a number identifying the signal.

      Linux have defined 31 standard signals and 31 real-time signals,they are all POSIX signal.
      and these signals each of all is represented by a constant macro with prefix "SIG" and get the style SIGXX.
      /* the constant macro definitions can be find in <asm-generic/signal.h> */

      two main purpose of signal :
        1> to make a process aware that a specific event has occurred
        2> to cause a process to execute a signal handler function included in its code
        /* these two are not mutually exclusive */

    Linux signals :
      <asm-generic/signal.h>
        SIGHUP                1                POSIX                terminate
        SIGINT                2                POSIX                terminate
        SIGQUIT               3                POSIX                coredump
        SIGKILL               4                POSIX                coredump
        SIGTRAP               5                POSIX                coredump
        SIGABRT               6                POSIX                coredump
        SIGIOT                6                POSIX                coredump
        SIGBUS                7                POSIX                coredump
        SIGFPE                8                POSIX                coredump
        SIGKILL               9                POSIX                terminate(+)
        SIGUSR1               10               POSIX                terminate
        SIGSEGV               11               POSIX                coredump
        SIGUSR2               12               POSIX                terminate
        SIGPIPE               13               POSIX                terminate
        SIGALRM               14               POSIX                terminate
        SIGTERM               15               POSIX                terminate
        SIGSTKFLT             16               POSIX                terminate
        SIGCHLD               17               POSIX                ignore
        SIGCONT               18               POSIX                ignore(*)
        SIGSTOP               19               POSIX                stop(*)(+)
        SIGTSTP               20               POSIX                stop(*)
        SIGTTIN               21               POSIX                stop(*)
        SIGTTOU               22               POSIX                stop(*)
        SIGURG                23               POSIX                ignore
        SIGXCPU               24               POSIX                coredump
        SIGXFSZ               25               POSIX                coredump
        SIGVTALRM             26               POSIX                terminate
        SIGPROF               27               POSIX                terminate
        SIGWINCH              28               POSIX                ignore
        SIGIO                 29               POSIX                terminate
        SIGPOLL               SIGIO            POSIX                terminate
        SIGLOST               29
        SIGPWR                30               POSIX                terminate
        SIGSYS                31               /* bad sytem call */ coredump
        SIGUNUSED             31

        _NSIG                 64
        SIGRTMIN              32
        #ifndef SIGRTMAX
        #define SIGRTMAX      _NSIG
        #endif
        SIGRTMIN -- SIGRTMAX                   POSIX                terminate

        SIGEMT /* emulator trap */                                  coredump

        (+) => for SIGKILL and SIGSTOP,the action is _always_,not just _default_.
        (*) => special job control effects :
                 when SIGCONT is sent,it resumes the process(all thread in the group)
                 from TASK_STOPPED state,and also clears any pending/queued stop signals(
                 any of those marked with "stop(*)").
                 this happens regardless of blocking,catching,or ignoring SIGCONT.
                 when any stop signal is sent,it clears any pending/queued SIGCONT
                 signals,this hapeens regardless of blocking,catching,or ignored the
                 stop signal,though(EXCEPT FOR SIGSTOP) the default action of stopping
                 the process may happen later or never.

      POSIX real-time signals :
        signal numbers between [SIGRTMIN, SIGRTMAX] are real-time signal introduced by POSIX
        standard.
        real-time signals always queued so that multiple signals sent will be received.
        regular signals of the same kind are not queued :
          if a regular signal is sent many times in a rwo,just one of them is delivered to
          the receiving process.
        /* Linux Kernel does not use real-time signals,but support them by several specific system calls */

      Kernel system call(POSIX standard) related to Signal :
        kill
        tkill
        tgkill
        sigaction
        signal              /* obsolete */
        sigpending
        sigprocmask
        sigsuspend

        rt_sigaction
        rt_sigpending
        rt_sigprocmask
        rt_sigqueueinfo
        rt_sigsuspend
        rt_sigtimedwait

    !! SIGNAL IS SENT TO RECEIVING PROCESS ASYNCHRONOUSLY.
    !! signal may be sent at any time to a process whose state is usually unpredictable.
       if the process is not currently executing,then the signal must be saved by the kernel
       until that process resumes execution.
    !! block a kind of signal will not cancel all pending signals,but be held off until such kind of
       signal is later unblocked.

    Two different phases related to Signal transmission :
      1> Signal generation
           kernel updates a data structure of the destination process to represent that a new signal
           has been sent
      2> Signal delivery
           kernel forces the destination process to react to the signal by changing its execution state,
           by starting the execution of a specified signal handler,or both

      ! each signal generated can be delivered once at most.
        signals are consumable resources :
          once they have been delivered,all process descriptor information that refers to their previous
          existence is canceled.
      
      pending signal => generated but yet delivered
      ! at any time,only one pending signal of a given type may exist for a process(noqueued),additional
        pending signals will be discarded.

    Signal delivery :
      factors about signal may remain pending :
        > signals are usually delivered only to the currently running process
        > signals of a given type may be selectively blocked by a process
        > signals may be blocked when the process is executing signal-handler(signal process mask)
      
      kernel-side works :
        1> kernel have to remember which signals are blocked by each process
        2> when switching from Kernel Mode to User Mode,check whether a signal for a process has arrived,
           this happens at almost every timer interrupt
        3> determine whether the signal can be ignored
           /**
            * signal can be ignored must satisfy :
            *   1> the destination process is not traced by another process(PT_PTRACED cleared)
            *   2> the signal is not blocked by the destination process
            *   3> the signal is being ignored by the destination process
            */
        4> handle the signal,which may require switching the process to a handler function at any point
           during its execution and restoring the original execution context after the function returns

    Actions Performed upon Delivering a Signal :
      for a specified signal,the process can :
        1> ignore it
        2> catch it
        3> do nothing,in this case,the default action will be taken
           /**
            * default actions - depend on the type of signal
            *   terminate - just terminate
            *   dump - terminate and make coredump
            *   ignore - do not care
            *   stop - TASK_STOPPED
            *   continue - TASK_RUNNING
            */
        /**
         * some signals can not be ignored or be catched or be blocked
         *   !(ignore | block | catch) => SIGKILL SIGSTOP
         * SIGKILL has no effect on process 0(swapper) and process 1(init)
         *
         * blocking is different to ignoring
         *   ignoring - signal still be sent to the process,but process do not care about it 
         *   blocking - signal will not be sent to the process until the process unblocked it
         */

      a signal is fatal if the kernel will terminates(kill) the specified process.
      a signal is not fatal if the process catched signal and selected to terminates itself in signal-handler.

    POSIX Signals and Multithreaded Applications :
      POSIX 1003.1 stringent requirements for signal handing of multithreaded applications :
        1> signal handlers must be shared among all threads of a multithreaded application,however,each
           thread must have its own mask of pending and blocked signals
        2> the kill() and sigqueue() POSIX library functions must send signals to whole multithreaded applications,
           not to a specific thread,the same holds for all signals generated by the kernel
        3> each signal sent to a multithreaded application will be delivered to just one thread,which is arbitrarily
           chosen by the kernel among the threads that are not blocking that signal
        4> if a fatal signal is sent to a multithreaded application,the kernel will kill all threads of the application,
           not just the thread to which the signal has been delivered

      POSIX multithread in Linux Kernel :
        kernel implements a multithreaded application as a set of lightweight processes belonging to the same
        thread group

    Data Structures Associated with Signals :
      kernel must keep track of what signals are currently pending or msked
      kernel must keep track of how every thread group is supposed to handle every signal

      <asm-generic/signal.h>
        #define _NSIG 64
        #define _NSIG_BPW __BITS_PER_LONG
        #define _NSIG_WORDS (_NSIG / _NSIG_BPW)

        #ifndef __ASSEMBLY__
        typedef struct {
                unsigned long sig[_NSIG_WORDS];
        } sigset_t; /* signal set type */
        /**
         * because no signal's number is _zero_,thus the corresponding signal
         * in the bitmask is the bit index plus 1
         * that is bit 0 => 0 + 1 => SIGHUP
         * and so on
         * bit 32 => SIGRTMIN
         * ...
         * bit 63 => SIGRTMAX
         * ! IF _NSIG_WORDS IS 2,THEN [0] MEANS NON-REAL-TIME SIGNALS,AND [1] MEANS REAL-TIME SIGNALS.
         *   ON x86_64 ARCHITECTURE,_NSIG_WORDS IS 1.
         */    
        ...
        #endif
      <linux/sched.h>
        struct task_struct {
                ...
  
                  [member]  [type]
                @pending  => struct sigpending
                  > represents the pending signals
  
                *@signal  => struct signal_struct
                  > signal descriptor
  
                *@sighand => struct sighand_struct
                  > signal handler descriptor
  
                @blocked  => sigset_t
                  > mask of blocked signals
  
                @real_blocked => sigset_t
                  > temporary mask of blocked signals,used by rt_sigtimewait() system call
  
                @saved_sigmask => sigset_t
                  > saved sigmask,restored if set_restore_sigmask() was used
  
                @sas_ss_sp => unsigned long
                  > address of alternative signal handler stack(that is signal stack)
  
                @sas_ss_size => size_t
                  > size of alternative signal handler stack
                    sp register will be set to @sas_ss_sp + @sas_ss_size by __setup_rt_frame()
  
                @notifier => int (*)(void *priv)
                  > function pointer used by a device driver to block some signals of the process
  
                *@notifier_data => void
                  > data that might be used by the @notifier
  
                *@notifier_mask => sigset_t
                  > mask of signals blocked by a device driver through a notifier function
              
                ...
        }

      The signal descriptor and the signal handler descriptor :
        signal descriptor - struct signal_struct
        ! the signal descriptor is shared by all processes belonging to the same thread group,
          thus the signal descriptor includes the fields that must be identical for every process
          in the same thread group.
        <linux/sched.h>
          /**
           * signal_struct - signal descriptor,kernel use this structure to keeps track of the
           *                 shared pending signals,but it is not all fields are related to
           *                 signal handling
           * @count:         usage counter of the signal descriptor
           * @live:          number of live processes in the thread group
           * @wait_chldexit: waitqueue for the processes sleeping in a wait4() system call
           * @curr_target:   current thread group signal load-balancing target,it usually
           *                 is the last process in the thread group that received a signal
           * @shared_pending: shared pending signals
           * @group_exit_code: thread group exit support,process termination code for the
           *                   thread group
           * @notify_count:  used when killing a whole thread group
           *                 # overloaded :
           *                     - notify @gourp_exit_task when @count is equal to @notify_count
           *                     - everyone except @group_exit_task is stopped during signal delivery
           *                       of fatal signals,@group_exit_task processes the signal
           * @group_exit_task: used when killing a whole thread group
           * @group_stop_count: thread group stop support,overloads @group_exit_code too,
           *                    it is used when stopping a whole thread group
           * @flags:         flags used when delivering signals that modify the status of
           *                 the process
           *                 # flags are with prefix "SIGNAL_"
           */
          struct signal_struct {
                  atomic_t count;
                  atomic_t live;
                  wait_queue_head_t wait_chldexit;
                  struct task_struct *curr_target;
                  struct sigpending shared_pending;
                  int group_exit_code;
                  int notify_count;
                  struct task_struct *group_exit_task;
                  int group_stop_count;
                  unsigned int flags;
                  ...
          };

        signal handler descriptor - struct sighand_struct
        ! signal handler descriptor may be shared by several processes by invoking the clone()
          system call with the CLONE_SIGHAND flag set.
          threads in the thread group sharing the same signal descriptor and the same signal
          handler descriptor.
        <linux/sched.h>
          /**
           * sighand_struct - signal handler descriptor used to describes how
           *                  signal events will be handled
           * @count:          usage counter of the signal handler descriptor(sharing)
           * @action:         array of structures specifying the actions to be performed
           *                  upon delivering the signals
           * @siglock:        concurrent protector
           * @signalfd_wqh:   signal file descriptor wait queue head
           */
          struct sighand_struct {
                  atomic_t count;
                  struct k_sigaction action[_NSIG];
                  spinlock_t siglock;
                  wait_queue_head_t signalfd_wqh;
          };

        <asm-generic/signal.h>
          /**
           * k_sigaction - sigaction in Kernel Space
           * # some architectures assign properties to a signal that are visible only to
           *   the kernel,so the properties of a signal are stored in an object is type of
           *   struct k_sigaction
           *   for x86 platform,the definitions of struct k_sigaction are the same,but
           *   the definitions of struct sigaction are different between User Space and
           *   Kernel Space,however,the definitions are the same if the platform is not "i386"
           * # 80x86,all properties are visible to User Mode processes(80x86 is not i386)
           */
          struct k_sigaction {
                  struct sigaction sa;
          };

      The sigaction data structure :
        signal action - struct sigaction

        <asm-generic/signal-defs.h>
          typedef void __signalfn_t(int);
          typedef __signalfn_t __user *_sighandler_t;
          typedef void __restorefn_t(void);
          typedef __restorefn_t __user *__sigrestore_t;

        <arch/x86/include/asm/signal.h>
          #ifdef __i386__
          #ifdef __KERNEL__ /* i386 Kernel Space */
          struct sigaction {
                  __sighandler_t sa_handler;
                  unsigned long sa_flags;
                  __sigretore_t sa_restorer;
                  sigset_t sa_mask;
          };
          #else /* i386 User Space */
          struct sigaction {
                  union {
                          __sighandler_t _sa_handler;
                          void (*_sa_sigaction)(int, struct siginfo *, void *);
                  } _u;
                  sigset_t sa_mask;
                  unsigned long sa_flags;
                  void (*sa_restorer)(void);
          };
          #define sa_handler _u._sa_handler
          #define sa_sigaction _u._sa_sigaction

          #endif
          #else /* !i386 */
          struct sigaction {
                  __sighandler_t sa_handler;
                  unsigned long sa_flags;
                  __sigretore_t sa_restorer;
                  sigset_t sa_mask;
          };
          #endif

          @sa_handler:  specifies the type of action to be performed,its value can be a
                        pointer to the signal handler
                        SIG_DFL => value is _zero_,default action
                        SIG_IGN => value is _one_,ignoring
                        # can not carry additional information
          @sa_sigaction: signal handler can carry additional information
          @sa_flags:    specifies how the signal must be handled
                          /* <arch/x86/include/asm/signal.h> || <asm-generic/signal.h> */
                          SA_NOCLDSTOP        0x00000001u    => applies only to SIGCHLD,do not
                                                                send SIGCHLD to the parent when
                                                                the process is stopped
                          SA_NOCLDWAIT        0x00000002u    => applies only to SIGCHLD,do not
                                                                create a zombie when the process
                                                                terminates
                          SA_SIGINFO          0x00000004u    => provide additional information to
                                                                the signal handler
                          SA_ONSTACK          0x08000000u    => use an alternative stack for
                                                                the signal handler
                          SA_RESTART          0x10000000u    => interrupted system call are automatically
                                                                restarted
                                                                used to get restarting signals
                          SA_NODEFER          0x40000000u    => do not mask the current signal while
                          SA_NOMASK           SA_NODEFER        executing the signal handler
                          SA_RESETHAND        0x80000000u    => reset to default action after
                          SA_ONESHOT          SA_RESETHAND      executing the signal handler
                          SA_RESTORER         0x04000000     => signal trampoline,Linux does not
                                                                use this flag
          @sa_mask:     specifies the signals to be masked when running the signal handler
          @sa_restorer: kernel does not use this field,but C library use this field to
                        contains the address of a "signal trampoline"

      The pending signal queues :
        in oder to keep track of what signals are currently pending,the kernel associated two pending signal
        queues to each process.
        the first is shared pending signal queue,which is named @shared_pending inside to struct signal_struct
        object,and the process descriptor holds such member named *@signal.
          - stores the pending signals of the whole thread group
        the second is private pending signal queue,which is rooted at the @pending field of process descriptor.
          - stores the pending signals of the specific (lightweight) process.

        <linux/signal.h>
          /**
           * sigqueue - signal queue element
           * @list:     pending signal queue
           * @flags:    flags for the signal queue
           * @info:     decribes the event that raised the signal
           * @user:     pointer to per-user data structure of the process's owner
           * # a slab cache named @sigqueue_cachep is used to handles memory allocation
           *   for objects of this type
           */
          struct sigqueue {
                  struct list_head list;
                  int flags;
                  siginfo_t info;
                  struct user_struct *user;
          };

          /**
           * sigpending - pending signal queue
           * @list:       embedded list
           *              when send a signal,a new struct sigqueue object will be
           *              created and inserted before a sigpending object from the
           *              task where the signal is sending to
           *              # @sigpending->list.prev => @list of the new sigqueue object
           *                thus the signal queue is like this :
           *                  (sigqueue0, sigqueue1, ... , sigpending)
           *                                               +--> @task->pending OR
           *                                                    @task->signal->shared_pending
           *              # when flushing a signal queue,the iteration will stopped until only
           *                the @sigpending object is existing in this queue
           * @signal:     signal bitmask specifying the pending signals
           */
          struct sigpending {
                  struct list_head list;
                  sigset_t signal;
          };

        the siginfo_t data structure is a 128-byte data structure that stores information about an
        occurrence of a specific signal.
        <asm-generic/siginfo.h>
          /**
           * siginfo - signal information
           * @si_signo: the signal number
           * @si_errno: the error code of the instruction that caused the signal to be raised,or
           *            0 if there was no error
           * @si_code:  a code identifying who raised the signal
           * @_sifields: a union storing information depending on the type of signal
           *             # e.g.
           *                     SIGKILL - the union records the PID and the UID of the sender process
           *                               @_sigfields.@_kill
           *                     SIGSEGV - the union records the memory address whose access caused the
           *                               signal to be raised
           *                               @_sifields.@_sigfault           
           */
          struct siginfo {
                  int si_signo;
                  int si_errno;
                  int si_code;
                  union {
                          ...
                  } _sifields;
          };
          
          typedef struct siginfo siginfo_t;

          #ifdef __KERNEL__
          ...
          #define __SI_TIMER (1 << 16)
          ...
          #define __SI_MESGQ (6 << 16)
          ...
          #define __SI_CODE(T,N) ((T) | ((N) & 0xffff))
          ...
          #else /* for user space,all code is _zero_,and __SI_CODE() returns N */
          ...
          #endif

          /* si_code values,digital reserves positive values for kernel-generated signals */
          #define SI_USER 0  /* sent by kill,sigsned,raise */
          #define SI_KERNEL 0x80  /* sent by the kernel from somewhere */
          #define SI_QUEUE -1  /* sent by sigqueue */
          #define SI_TIMER __SI_CODE(__SI_TIMER,-2)  /* sent by timer expiration */
          #define SI_MESGQ __SI_CODE(__SI_MESGQ,-3)  /* sent by real time mesq state change */
          #define SI_ASYNCIO -4  /* sent by AIO completion */
          #define SI_SIGIO -5  /* sent by queued SIGIO */
          #define SI_TKILL -6  /* sent by tkill system call */
          #define SI_DETHREAD -7  /* sent by execve() killing subsidiary threads */

    Operation on Signal Data Structures :
      sigset_t operations :
        <linux/signal.h>
          /* empty @set,set to _zero_ */
          static inline void sigemptyset(sigset_t *set);

          /* fill @set,set to _one_ */
          static inline void sigfillset(sigset_t *set);

          /* add @_sig into *@set */
          static inline void sigaddset(sigset_t *set, int _sig);
    
          /* delete @_sig from *@set */
          static inline void sigdelset(sigset_t *set, int _sig);

          /* open all bits in *@set corresponding to @mask /
          static inline void sigaddsetmask(sigset_t *set, unsigned long mask);

          /* close all bits in *@set corresponding to @mask */
          static inline void sigdelsetmask(sigset_t *set, unsigned long mask);
    
          /* return TRUE <-> @_sig in *@set;otherwise return FALSE */
          static inline int sigismember(sigset_t *set, int _sig);

          /* yields the bit index of @sig */
          #define sigmask(sig) (1UL << ((sig) - 1))

          /**
           * for these three routines,each of them is receives three parameters.
           * @r: the first parameter used to save result (sigset_t *)
           * @a: the second parameter as first operand   (const sigset_t *)
           * @b: the third parameter as second operand   (const sigset_t *)
           * # _SIG_SET_BINOP(name, op) :
           *     this macro defines a routine named @name,and the routine process
           *     operation @op between @a and @b,finally,stores the result in
           *     @r
           */
          /* @r := @a OR @b */
          #define _sig_or(x, y) ((x) | (y))
          _SIG_SET_BINOP(sigorsets, _sig_or)

          /* @r := @a AND @b */
          #define _sig_and(x, y) ((x) & (y))
          _SIG_SET_BINOP(sigandsets, _sig_and)

          /* @r := @a AND NOT(@b) */
          #define _sig_nand(x, y) ((x) & ~(y))
          _SIG_SET_BINOP(signandsets, _sig_nand)

          /* return TRUE <-> bits of @mask in *@set are opened;otherwise,return FALSE */
          static inline int sigtestsetmask(sigset_t *set, unsigned long mask);

          /* use @mask to set @set->sig[0],and set the remained @set->sig[1.._NSIG_WORDS - 1] to _zero_ */
          static inline void siginitset(sigset_t *set, unsigned long mask);

          /* use NOT(@mask) to set @set->sig[0],and set the remained @set->sig[1.._NSIG_WORDS - 1] to -1 */
          static inline void siginitsetnv(sigset_t *set, unsigned long mask);

      sigpending and sigqueue operations :
        <linux/sched.h>
          /* return TRUE if *@p has nonblocked pending signals;otherwise return FALSE */
          /* this routine is implemented by simply checking TIF_SIGPENDING flag */
          static inline int signal_pending(struct task_struct *p);

          /* equal to recalc_sigpending_tsk(current) */
          extern void recalc_sigpending(void);

          /* remove all pending signals in @queue,remove and destroy sigqueue objects */
          extern void flush_sigqueue(struct sigpending *queue);

          /**
           * remove all signals sent to process @t,
           * this is done by clearing TIF_SIGPENDING flag and call invoking twic flush_sigqueue()
           * on @t->pending and @t->signal->shared_pending
           */
          extern void flush_signals(struct task_struct *t);

        <kernel/signal.c>
          /**
           * return TRUE if either *@t has pending signals or
           * the thread group that @t belongs to has pending signals
           * return FLASE,otherwise
           * if the routine returned TRUE,thread flag TIF_SIGPENDING of @t have been
           * opended
           */
          static int recalc_sigpending_tsk(struct task_struct *t);
        
          /**
           * close bits of @s->signal corresponding to the bits of @mask
           * if no such bits have been opened,routine returns 0,otherwise return 1
           * this routine will traverse the signal queue which associated to @s,
           * if any sigqueue object's member @info holds a signal less than SIGRTMIN
           * and the signal is opened in @mask,the sigqueue object will be removed
           * from the signal queue,then destroyed
           */
          static int rm_from_queue(unsigned long mask, struct sigpending *s);

    Generating a Signal :
      many kernel functions generate signals by updating one or more process descriptors,but they are
      not take charge of signal delivering.the delivering depending on the type of signal and the state
      of the destination processes.some signals may wake up some processes(TASK_INTERRUPTIBLE) and force
      them to receive the signal.

      Kernel functions to generate signal :
        <linux/sched.h>
          /**
           * send_sig - send a signal to a given task
           * @sig:      signal number
           * @p:        the task
           * @priv:     this parameter used to tell to __send_signal() how the signal info
           *            should to be constructed
           *            1 => SEND_SIG_PRIV
           *                    @si_signo := @sig
           *                    @si_code := SI_KERNEL
           *                    other members are _zero_
           *            0 => SEND_SIG_NOINFO
           *                    @si_signo := @sig
           *                    @si_code := SI_USER
           *                    other members are _zero_
           * return:    0 => succeed
           *            -ESRCH => no such task
           *            -EINVAL => invalid signal
           */
          extern int send_sig(int sig, struct task_struct *p, int priv);

          /**
           * send_sig_info - send a signal to a given task with additional info
           * @sig:           signal number
           * @info:          signal info
           *                 special value : <linux/sched.h>
           *                   SEND_SIG_PRIV        =>  (by user process)  0
           *                   SEND_SIG_NOINFO      =>  (by kernel)  1
           *                   SEND_SIG_FORCE       =>  (by kernel) SIGSTOP/SIGKILL
           * @p:             task
           * return:         same return values as send_sig()
           * # send_sig() actually invokes this function,and this function call to do_send_sig_info()
           */
          extern int send_sig_info(int sig, struct siginfo *info, struct task_struct *p);

          /**
           * force_sig - send a signal that cannot be explicitly ignored or blocked by the process
           * @sig:       signal number
           * @p:         task
           * # this function actually call to force_sig_info() with SEND_SIG_PRIV
           */
          extern void force_sig(int sig, struct task_struct *p);

          /**
           * force_sig_info - send a signal that cannot be explicitly ignored or blocked by the process
           *                  with additional info
           * @sig:            signal number
           * @info:           signal info
           * @p:              task
           * return:          0 => succeed
           *                  -EAGAIN => error only occurs for real time signals
           * # if necessary,this routine will set signal action to SIG_DFL and unblock @sig
           * # if the signal action is SIG_DFL,then signal flag SIGNAL_UNKILLABLE will be cleared
           *   to prevent recursive SIGSEGV
           */
          extern int force_sig_info(int sig, struct siginfo *info, struct task_struct *p);

        <linux/syscalls.h>
          /**
           * sys_tkill - system call tkill() handler
           * @pid:       task pid
           * @sig:       signal number
           * return:     0 => succeed
           *             - ERROR CODE => failed
           * # this handler actually call to do_tkill() with tgid _zero_
           */
          asmlinkage long sys_tkill(int pid, int sig);

          /**
           * sys_tgkill - system call tgkill handler
           * @tgid:       thread group id
           * @pid:        task pid
           * @sig:        signal number
           * return:      0 => succeed
           *              - ERROR CODE => failed
           * # this handler actually call to do_tkill()
           */
          asmlinkage long sys_tgkill(int tgid, int pid, int sig);

        the routines described above are end up invoking the __send_signal() which is defined in
        <kernel/signal.c>.
        /* some routine may call to specific_send_sig_info(),but the function just call to send_signal() */

        when a signal is send to a whole thread group,either from the kernel or from another process,
        the kernel generates it by invoking one of the following routines :
          /* <kernel/signal.c> */
          
          /**
           * send a signal with additional info to a group,@p is the task of such process group
           * this routine will call to check_kill_permission() to does signal permission checkings,
           * such invalid signal number,signal info audit,user permission,etc
           * # user permission => the user who wants send the signal to the target process is allowed
           *                      # the user is administrator OR the processes are belong to it
           *                      the user has capability to send a signal(CAP_KILL)
           *                      the signal is SIGCONT and the target process in the same login session
           * ! if @sig is _zero_,then will not send any signal
           *   signal 0 is used by the sending process to check whether it has the required privileges to send
           *   a signal to the destination thread group
           *   # group_send_sig_info(0, NULL, target-task) => returned _zero_ -> has required privileges
           */
          int group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p);

          /* send a signal to all tasks in a process group which is identified by @pid */
          int kill_pgrp(struct pid *pid, int sig, int priv);

          /* send a signal with additional info to all tasks in a process group which is identified by @pid */
          int __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp);

          /**
           * send a signal to a process group which is identified by @pid,and the @shared_pending
           * will be fetched from @pid's @signal member
           */
          int kill_pid(struct pid *pid, int sig, int priv);

          /**
           * send a signal with additional info to a process group which is identified by @pid
           * this routine actually call to kill_pid_info()
           */
          int kill_proc_info(int sig, struct siginfo *info, pid_t pid);

          /* system call kill() handler */
          asmlinkage long sys_kill(int pid, int sig);

          /* system call rt_sigqueueinfo() handler for real time signals */
          asmlinkage long sys_rt_sigqueueinfo(int pid, int sig, siginfo_t __user *uinfo);

          ! the routines described above are end up invoking group_send_sig_info(),but that routine
            will call to do_send_sig_info(),thus finally call to __send_signal().
          ! do_send_sig_info() will lock up task's signal handler and disable local interrupt 
            before call to send_signal() 
            # task->sighand->siglock,before lock up this spinlock,rcu_read_lock() will be called
              at first for rcu_dereference(task->sighand)
          
      The signal_send() Function :
        function specific_send_sig_info() only one statement inside its body,that is
          "return send_signal(sig, info, p, 1);"
        @sig,@info and @p are the parameters of specific_send_sig_info().

        <kernel/signal.c>
          /**
           * send_signal - send a signal with additional info to a given process or to the process group if
           *               necessary
           * @sig:         signal
           * @info:        additional signal info
           * @t:           task
           * @group:       should send to process group ?
           * return:       returns what __send_signal() returned
           * # this routine declared a local variable named @from_ancestor_ns
           *   if defined CONFIG_PID_NS,then it will has the value
           *     si_fromuser(@info) && !task_pid_nr_ns(current, task_active_pid_ns(t)))
           *   else
           *     _zero_
           * # this function just call to __send_signal(@sig, @info, @t, @group, @from_ancestor_ns)
           */
          static int send_signal(int sig, struct siginfo *info, struct task_struct *t,
                                 int group);

      The __send_signal() Function :
        static routine __send_signal() plays the role that take charge of all signals' delivering.

        <kernel/signal.c>
          /**
           * __send_signal - send a signal with additional info
           * @sig:           signal
           * @info:          additional info
           * @t:             task
           * @group:         send to group?
           * @from_ancestor_ns: pid namespace of the process's ancestor
           * return:         0 => succeed
           *                 -EAGAIN => queue overflow,only occurs when delivering real time signal
           * # the control path invokes this function must been disabled local interrupt
           */
          static int __send_signal(int sig, struct siginfo *info, struct task_struct *t,
                                   int group, int from_ancestor_ns);

          description for __send_signal() :
            when entered this routine,kernel trace point for signal generating will be enabled.
            an assert will be processed that the spinlock @t->sighand->siglock has been locked up.
            /* when we delivering a signal,the signal handler should not change at this time */
            next,call to prepare_signal().
            /**
             * prepare_signal() handles magic process-wide effects of stop/continue signals.
             * it does the actual continuing for SIGCONT,bui not the actual stopping for
             * stop signals,process stop is done as a signal action for SIG_DFL
             * this routine returns TRUE if the signal should be actually delivered,otherwise,
             * it should be dropped.
             * if @p is dying,nothing to do (SIGNAL_GROUP_EXIT,the thread group is being killed)
             * if @sig is kernel stop signal,remove SIGCONT from all queues(@shared_pendiing and
             * @pending of each thread in the thread group)
             * if @sig is SIGCONT,remove all stop signals from all queues,and wake all thread
             * 
             *   # remove signal from @shared_pending and @pending of each thread,set task state
             *     to __TASK_STOPPED,if user decided to handles this continuing signal,then
             *     set TIF_SIGPENDING for the threads,and modify the state to
             *     __TASK_STOPPED | TASK_INTERRUPTIBLE
             *   # notify the parent with CLD_CONTINUED if the process were stopped,and now continued
             *     if the process were in the middle of a group stop,then pretend it was already finished,
             *     and then continued
             *     SIGCHLD does not queue,so report ony CLD_STOPPED,as if the next CLD_CONTINUED was dropped
             *   # prepare_signal() return to caller with the return value
             *             !sig_ignored(@t, @sig, @from_ancestor_ns)
             *     if the task ignored signal @sig,then this function will returns FALSE
             *     the signal is ignored if all of the following are satisfied : 
             *             1> it is not blocked AND it is not real blocked
             *             2> sig_task_ignored(@t, @sig, @from_ancestor_ns) returned 1
             *                the routine returns 1 if 
             *                  > @t stated in SIGNAL_UNKILLABLE && sighandler == SIG_DFL &&
             *                    !@from_ancestor_ns
             *                  OR
             *                  > sighandler == SIG_IGN OR (sighandler == SIGDFL && sig_kernel_ignore(@sig))
             *                    # sig_kernel_ignore() returns TRUE if @sig is a standard signal and
             *                      the bits of it corresponding to SIG_KERNEL_IGNORE_MASK are opened
             *             3> @t was not traced by a debugger
             */
            if prepare_signal() returned FALSE,then return to caller,because the signal should be dropped.
            if we can continuing,then retrieve the sigpending object either @shared_pending or @pending,this
            determined by @group parameter.                               # @group is TRUE or FALSE
            call to legacy_queue(),it is a short-circuit,if @sig is not a real time signal and it have been
            a member of the pending sigset,then return to caller,because non-real-time signal does not support
            queue.
            routine proceeds to check if @info equal to SEND_SIG_FORCED(value is 2) then goto out_set.
            this is a fast-path for kernel-internal things like SIGSTOP or SIGKILL.
            if @sig is not pending,and it is not kernel-internal things,then prepare to send this signal.
            set local variable @override_rlimit,if @sig is not real time signal,then its value is equal
            to "is_sig_special(@info) || @info->si_code >= 0",otherwise,it is _zero_.
            this variable will be used by __sigqueue_alloc() as the parameter @override_rlimit,only
            @override_rlimit is true OR user's pending signal is not exceed RLIMIT_SIGPENDING,a new
            sigqueue is able to be allocated.
            /**
             * __sigqueue_alloc() will retrieves the user struct of @t via get_uid() routine,if it succeed
             * to allocate a new sigqueue object,then increase user_struct.@sigpending
             */
            continue executing,try to allocate a new sigqueue object with GFP_ATOMIC | GFP_NOTRACK_FALSE_POSITIVE.
            if succed to allocate a sigqueue object
            then
                    insert the sigqueue object to the signal pending queue,it will be inserted before
                    the selected pending
                    switch @info
                            SEND_SIG_NOINFO:
                                    @q->info.si_signo = sig
                                    @q->info.si_code = SI_USER
                                    @q->info.si_pid = task_tgid_nr_ns(current, task_active_pid_ns(@t))
                                    @q->info.si_uid = current_uid()
                                    # other members are set to _zero_
                                    break
                            SEND_SIG_PRIV:
                                    similar to the actions that SEND_SIG_NOINFO will takes,but
                                    @si_code is set to SI_KERNEL,and @si_pid,@si_uid are _zero_
                                    break
                            default: /* have to send additional info
                                    copy @info to @q->info
                                    if @from_ancestor_ns is TRUE,then set @si_pid to _zero_
                                    break
            else if @info > SEND_SIG_FORCED /* !is_si_special(@info),not the kernel forced signal */
                    @sig is real time signal and @si_code is not SI_USER
                    => queue overflow,abort,and returns -EAGAIN
                       /**
                        * may abort if the signal was rt and sent by user using something
                        * other than kill()
                        */
                    @sig is not real time signal and @si_code is SI_USER
                    => silent loss of information,still send the signal but the *@info bits are lost
                       because we have failed on allocating memory for sigquque object
            out_set:
            signalfd_notify(@t, @sig)
              deliver the signal to listening signalfd.

            sigaddset(the pending, @sig)
              deliver the signal.

            complete_signal(@sig, @t, @group)
              if the thread @t wants this signal,then it will be the signal target
              if the thread group is empty and @group is FALSE,then return to caller
              otherwise,try to find a suitable thread as the signal target starting
              from the thread which received the last thread group signal(signal->@curr_target)
              # wants_signal() :
                  the process can not
                    blocked the signal
                    be exiting
                    be traced by a debuger
                  AND
                  the process is currently runnning on the CPU or there is no pending
                  signals on the process
                  (generally,awake a process up which has pending signal is done by the
                   kernel control path that set TIF_SIGPENDING)

                  if the signal is SIGKILL,it always be wanted by any task(can not be
                  ignored or be catched)

              /* if we have selected a thread to receive the signal */
              for fatal signal and the corresponding sighandler of @t is SIG_DFL
                if
                  @t is not in the signal state (SIGNAL_UNKILLABLE | SIGNAL_GROUP_EXIT)
                  AND
                  @sig is not a member in the signal target's @real_blocked
                  AND
                  @sig is SIGKILL or the signal target is not traced by a debugger
                then this signal will be fatal to the whole group
                proceed checks
                  if
                    @sig is not a standard signal or it will not procudes coredump
                      @t->signal->flags = SIGNAL_GROUP_EXIT
                      @t->signal->@group_exit_code = @sig
                      @t->signal->group_stop_count = 0
                      the signal target will be @t
                      send SIGKILL to each thread of the thread group
                      return to caller
              /**
               * normal returning path => the checking above for fatal signal resulted FALSE,
               *                          or the result is TRUE but the signal will produces coredump
               *                          AND it is a standard signal
               */
              normal returning path,signal_wake_up() is called with arguments signal target and
              @sig == SIGKILL(@resume,parameter of signal_wake_up()),if the signal is SIGKILL,
              we want to wake it up in the stopped/traced/killable case.
              /**
               * signal_wake_up() sets TIF_SIGPENDING in thread flags of @t,set local variable
               * @mask to TASK_INTERRUPTIBLE,if @resume is TRUE that @sig is SIGKILL,set @mask
               * to @mask |= TASK_WAKEKILL.
               * call to wake_up_state(@t, @mask).if the routine returned _zero_,then kick_process(@t),
               * since the process is already running on another CPU,so we send an interprocessor interrupt
               * to the CPU for let the process quickly notices the new pending signal.
               */

            finally,__send_signal() returns _zero_ to caller.

    Delivering a Signal :
      when kerenl delivering a signal to the process,the corresponding signal action routine will be called
      for the signal.but in case,the process is not running on the CPU,kerenl will defers the task of
      delivering the signal.
      
      when returning from interrupt or from exception,the kernel control path will call to the assembly function
      resume_userspace() to switch back to userspace,but if there are some works have to be done such signal,
      rescheduling,etc,assembly function work_pending() is be invoked to handles these cases.before return to
      User Mode,TIF_SIGPENDING will be checked,if it been enabled,the kernel control path have to notice the
      process signal delivering.

      work_pending() call to work_notifysig(),and work_notifysig() call to do_notify_resume(),and thus the
      architecture dependent function do_signal() will be called.
      do_signal() is used to handle the nonblocked pending signals.

      <arch/x86/kernel/signal.c>
        /**
         * do_signal - handle pending signals before return to User Mode
         * @regs:      User Mode register contents
         * # everytime to the invocation of this routine,only one signal will be delivered,
         *   this approach ensures that real-time signals will be dealt with in the proper order
         */
        static void do_signal(struct pt_regs *regs);

        brief description for do_signal() :
          first,this routine can not be called in interrupt context,thus if user_mode(@regs) returned FALSE,
          this routine just return to caller.
          if @current's thread status enabled TS_RESTORE_SIGMASK,then process signal mask @saved_sigmask must
          be used during handle a signal,otherwise,use the @blocked signal mask.
          routine get_signal_to_deliver() is be invoked to dequeue pending signals in a for-cycle.if a signal
          is not fatal returned by the routine,local variable @ka is kernel sigaction structure will be set to
          the signal action that @current has registered or SIG_DFL;local variable @info is an object type of
          siginfo_t,it also be set to the additional signal information that associated to the signal.
          now have to handles the non-fatal signal,routine handle_signal() will be called to does this task.
          if we handled the signal,then we can return caller simply,otherwise,we have to check if did we come from
          a system call.
          if come from system call =>
                  get syscall error
                  -ERESTARTNOHAND OR -ERESTARTSYS OR -ERESTARTNOINTR
                          @regs->ax := @regs->orig_ax   /* the system call number in User Mode AX */
                          @regs->ip -= 2        /* restart the interrupted system call */
                  -ERESTART_RESTARTBLOCK
                          @regs->ax = NR_restart_syscall /* __NR_restart_syscall,number of syscall "restart_syscall" */
                          @regs->ip -= 2
          proceed executing,before returning to caller,if TS_RESTORE_SIGMASK have been enabled in @current's thread
          status,then must disable the flag and set its sigmask to @saved_sigmask.

        /**
         * handle_signal - handle a given signal for current process which is running in User Mode previously and
         *                 been interrupted
         * @sig:           signal number
         * @info:          signal info
         * @ka:            kernel space signal action
         * @oldset:        signal mask used to restore process signal mask
         * @regs:          register contents
         * return:         0 => succeed
         *                 otherwise,returns an error code
         * # in a simply description,this function call to setup_rt_frame() to setup register contents for
         *   handling the signal.
         * # setup_rt_frame() might call to IA32 __setup_frame() or x86 __setup_rt_frame(),the different
         *   routines setup registers with different values :
         *     __setup_frame {
         *             SP: signal frame
         *             IP: @ka->sa.sa_handler
         *             AX: @sig
         *             DX: 0
         *             CX: 0
         *             DS: __USER_DS
         *             ES: __USER_DS
         *             SS: __USER_DS
         *             CS: __USER_CS
         *     }
         *     __setup_rt_frame { /* 32-bit */
         *             SP: signal frame
         *             IP: @ka->sa.sa_handler
         *             AX: @sig
         *             DX: address of signal info in signal frame
         *             CX: address of user space context in signal frame
         *             DS: __USER_DS
         *             ES: __USER_DS
         *             SS: __USER_DS
         *             CS: __USER_CS
         *     } 
         *     __setup_rt_frame { /* 64-bit */
         *             DI: @sig
         *             AX: 0
         *             SI: address of signal info in signal frame
         *             DX: address of user space context in signal frame
         *             IP: @ka->sa.sa_handler
         *             SP: signal frame
         *             CS: __USER_CS
         *     }
         */
        static int handle_signal(unsigned long sig, siginfo_t *info, struct k_sigaction *ka,
                                 sigset_t *oldset, struct pt_regs *regs);

        ! handle_signal() call to sigorsets() to does OR between @current->blocked and @ka->sa.sa_mask,
          the result is write into @current->blocked.
          before return to do_signal(),have to invoke recalc_sigpending() to check that if
          nonblocked signals are pending of current process.
          /**
           * recalc_sigpending() :
           *   set TIF_SIGPENDING of the task and return 1 if
           *     @group_stop_count < 0 ||
           *     signal in @pending is not blocked in @blocked ||
           *     signal in @shared_pending is not blocked in @blocked
           *   otherwise return _zero_
           */
        ! if @ka->sa.sa_flags disabled SA_NODEFER,then handle_signal() will place this signal @sig
          into the blocked signal set @blocked of the task struct.
          this will block the signal during signal handler is executing.

      <kernel/signal.c>
        /**
         * get_signal_to_deliver - deliver both process pending signals and group pending signals
         * @info:                  info object will be fill up
         * @return_ka:             signal action that associated to the delivering signal
         * @regs:                  register contents
         * @cookie:                cookie
         * return:                 0 OR signal number
         */
        int get_signal_to_deliver(siginfo_t *info, struct k_sigaction *return_ka, struct pt_regs *regs,
                                  void *cookie);

        description for get_signal_to_deliver() :
          try_to_freeze(),if @current is in freezing(TIF_FREEZE enabled),then call to refrigerator() to
          freeze it,and recalculate its pending signals.
          /*
           * routine thaw_process() in <kernel/freezer.c> is used to thaw the frozen process.
           * it clear TIF_FREEZE and PF_FROZEN,then wake up the proces - wake up from refrigerator().
           * flag PF_FREEZING is enabled and disabled in refrigerator().
           */
          next,acquires spinlock @current->sighand->siglock.
          if @current->signal->flags enabled SIGNAL_CLD_MASK,disable the flag,if the process is traced by
          a debugger,then do_notify_parent_cldstop() to tell the debugger the tracing process is stopped,
          otherwise,get back to the starting of this routine.
          enter an infinite for-cycle {
                  invoke tracehook_get_signal() to deliver synthetic signal to traced task,but this function
                  should always returns _zero_ in Linux 2.6.34.1.
                  so we just consider the case local variable @signr is _zer_.

                  /* threads in the thread group should be stopped AND stopped all threads in the thread group */
                  if @current->signal->group_stop_count > 0 AND do_signal_stop(0) is TRUE
                  then get back to the starting of this routine
                  
                                                   /* signal mask */
                  call to dequeue_signal(@current, &@current->blocked, @info)
                  /**
                   * this routine will try to dequeue a signal in @current->pending and then in
                   * @current->signal->shared_pending if no pending signals in @current->pending.
                   * this routine actually call to __dequeue_signal().
                   * __dequeue_signal() :
                   *   get next_signal(@pending, @mask)
                   *   @sig > 0 AND @current->notifier is not NULL
                   *     sigismember(@current->notifier_mask, @sig) is TRUE
                   *       let device driver to handles this signal
                   *       clear TIF_SIGPENDING
                   *       return 0
                   *   @sig > 0
                   *     collect_signal() => copy signal info or fill signal info for @sig
                   *   returns @sig
                   *
                   * if __dequeue_signal() is not returned _zero_,recalc_sigpending().
                   * if @signr is kernel stop signal,enable SIGNAL_STOP_DEQUEUED in @current->signal->flags
                   * if the signal is raised by a timer AND it is system private signal,unlock @siglock and
                   * call to do_schedule_next_time(@info).
                   * finally,return @signr(returned by __dequeue_signal()).
                   * ! @shared_pending handling is similar to @pending handling,but if @signr is SIGALRM,
                   *   then this routine have to deal with high-resolution timer of @current->signal->real_timer.
                   */

                  if @signr is not SIGKILL
                    then set @signr to the return value of ptrace_signal(),this routine
                    returns _zero_ either argument of the parameter @signr is _zero_ or @signr is blocked by the
                    @current process.
                    if @signr is _zero_,continue to next cycle.
                  otherwise,set local variable @ka to the signal handler &@current->sighand->action[@signr - 1].

                  @ka->sa.sa_handler is SIG_IGN,ignore this signal,continue to next cycle.
                  @ka->sa.sa_handler is not SIG_DFL
                    set @return_ka to @ka;if AS_ONESHOT flag is enabled in @ka->sa.sa_flags,then reset
                    @ka->sa.sa_handler to SIG_DFL
                    break the for-cycle,because we find out an action of the signal been resgitered to be
                    processed when signal is delivering.some fatal signal can not be ignored and be blocked,be
                    caught,thus this signal @signr must be a non-fatal signal.
                 
                  proceed cycle,the @signr is kernel ignored signal,do nothing,continue to next cycle.

                  @current->signal->flags enabled SIGNAL_UNKILLABLE AND @signr is not kernel only signal,
                  continue to next cycle.
                  /* signal flag SIGNAL_UNKILLABLE is used to protect "init" process */
                  
                  @signr is kernel stop signal
                    @signr is not SIGSTOP
                      unlock @siglock and enable local interrupt
                      if the current process group become orphaned group,then go to the staring of this
                      routine                             /**
                                                           * POSIX : a process group is not orphaned as long as
                                                           *         there is a process in the group that has a
                                                           *         parent in a different process group but in
                                                           *         the same session
                                                           */
                      acquire @siglock and disable local interrupt
                                                                       /* @group_stop_count FALSE */
                    try to do_signal_stop(@info->si_signo),if there is no group stop in progress,then a
                    group stop will be initiated,all threads in the thread group will be stopped,and
                    @group_stop_count is set to 1,increase it whenever a thread in the group is stopped.
                    /**                                                /* wake up each thread to response STOP */
                     * if there are no _other_ threads in the group or a group stop in progress and this
                     * task is the last one,then have to notify its parent about CLD_STOPPED.
                     * # set local variable @notify to 1,and call to do_notify_parent_cldstop() later if
                     *   @notify is TRUE.
                     */
                    if @group_stop_count is TRUE,the current task state will be set to TASK_STOPPED.
                    because @current been stopped,function schedule() will be called to relinquish CPU time,
                    it will be resumed when SIGCONT has came or SIGKILL is received.if we can continue
                    executing,do_signal_stop() finally returns 1.

                    
                    if do_signal_stop() returned TRUE,then go back to the starting of this routine.
                    else,continue to next for-cycle.                    

                  @signr is not kernel stop signal.
                  unlock @siglock and enable local interrupt,enable PF_SIGNALED in @current->flags.

                  if @signr produces kernel coredump signal,then print the fatal signal in kernel
                  message queue(if global variable @print_fatal_signals is TRUE),call to
                  do_coredump() for coredump producing.
                  /* coredump signal is fatal signal,thus the process will be killed */
                  /* routine do_coredump() is defined in <fs/exec.c> */

                  at the end of once for-cycle,call to do_group_exit(@info->si_signo).
                  /**
                   * if @signr cause coredump,do_group_exit() is processed right after the checking is
                   * TRUE;if it is not,we still call to do_group_exit(),because other signals been handled
                   * previously,and these remaining 18 signals terminates current process.
                   */
          } /* END OF for-cycle */

          unlock @siglock and enable local interrupt,returns @signr.

        !! MUST BE CLEARING THAT get_signal_to_deliver() ALWAYS RETURNED TO CALLER NEITHER @signr
           IS FATAL SIGNAL NOR WORK CONTROL SIGNAL,AND THE PENDING SIGNAL @signr NEITHER BEEN BLOECKED
           NOR BEEN IGNORED.

    Catching the Signal :
      as the description above,if the signal has a registered signal handler,then it will be returned
      by get_singal_to_deliver() in @return_ka.by this way,do_signal() can call to handle_signal() for
      enforce the execution of the signal handler.

      execution of signal handler is rather complex.the signal handler is defined in User Mode,so
      it is inside to User Mode code segment,but handle_signal() runs in Kernel Mode.that means,
      the kernel control path which executing handle_signal() must switch to User Mode before the
      invocation to signal handler,and then return to Kernel Mode proceed the executing.there is a
      problem,if kernel switch to User Mode,the Kernel Mode stack no longer contains the hardware context
      of the interrupted program,because the Kernel Mode stack is emptied at every transition from
      User Mode to Kernel Mode.
      /**
       * restore_all - restore hardware context.when a new switching occurs from User Mode to Kernel Mode,
       * the Kernel Mode stack will stores the hardware context for this once,not the previous.
       */
      another problem,if the signal handler call to system call,the control path will return to signal handler,
      not to the handle_signal().

      the solution that adopted in Linux :
        > copying the hardware context saved in the Kernel Mode stack onto the User Mode stack of the current
          process,when the signal handler terminates,a system call named sigreturn() will be invoked automatically,
          which copy the hardware context back on the Kernel Mode stack and restore the original content of the
          User Mode stack.
        !! IN LINUX 2.6.34.1 SIGNAL STACK IS USED TO STORES THE CONTEXT,IT IS NO LONGER BE STORED IN USER MODE
           STACK.

      the flow of executing signal handler :
        @current -> interrupted -> interrupt service routine -> do_signal() -> get_signal_to_deliver() ->
        handle_signal() -> setup_rt_frame() -> __setup_rt_frame() -> get_sigframe() -> return to __setup_rt_frame()
        -> put_user_try { ... } -> return to setup_rt_frame() -> return to handle_signal() -> return to do_signal() ->
        resume_userspace() -> registered signal handler -> retcode (sigreturn()) -> int $0x80 to retcode ->
        switch to Kernel Mode -> restores hardware context -> return from system call -> @current

      retcode :
        /* retcode - return code used to invoke sys_sigreturn() right after signal handler finished */
        <arch/x86/kernel/signal.c>
          static const struct {
                  u16 poplmovl;
                  u32 val;
                  u16 int80;
          } __attribute__((packed)) retcode = {
                  0xb858,           /* popl %eax; movl $...,%eax */
                                    /* popl %eax => pop up the return value and restore sp */
                  __NR_sigreturn,   /* sys_sigreturn() */
                  0x80cd            /* int 0x80 */
          };

          /* for signals that do require signal information */
          static const struct {
                  u8 movl;
                  u32 val;
                  u16 int80;
                  u8 pad;
          } __attribute__((packed)) rt_retcode = {
                  0xb8,             /* movl $...,%eax */
                  __NR_rt_sigreturn,
                  0x80cd            /* int $0x80 */
                  0
          };

      signal frame datastructure :
        <arch/x86/include/asm/sigframe.h>
          /**
           * sigframe_ia32 - signal frame for non-rt signal
           * @pretcode:      pointer to the signal trampoline or vDSO.sigreturn or
           *                 &this->retcode
           * @sig:           signal number
           * @sc:            object is type of struct sigcontext_ia32 which contains
           *                 the hardware context of the User Mode process right before
           *                 switching to Kernel Mode
           * @fpstate_unused: the object used to store the floating point registers of the
           *                  User Mode process
           * @extramask:     bit array that specifies the blocked real-time signals
           * @retcode:       machine code for the assembly code that issues a syscall to
           *                 sys_sigreturn()
           * # CONIFG_X86_32 || CONFIG_IA32_EMULATION
           */
          struct sigframe_ia32 {
                  u32 pretcode;
                  int sig;
                  struct sigcontext_ia32 sc;
                  struct _fpstate_ia32 fpstate_unused;
          #ifdef CONFIG_IA32_EMULATION
                  unsigned int extramask[_COMPAT_NSIG_WORDS - 1];
          #else
                  unsigned long extramask[_NSIG_WORDS - 1];
          #endif
                  char retcode[8];
                  /* fp state follows here */
          };
          /**
           * rt_sigframe_ia32 - signal frame used for real-time signal,the structure is
           *                    similar to sigframe_ia32
           * @pretcode:         signal trampoline or vDSO.rt_sigreturn
           * @sig:              signal number
           * @pinfo:            pointer to signal info
           * @puc:              pointer to struct ucontext object
           * @info:             IA32 compatibility
           * @uc:               user space context
           * @retcode:          machine code to issues sys_rt_sigreturn()
           * # CONFIG_X86_32 || CONFIG_IA32_EMULATION
           */
          struct rt_sigframe_ia32 {
                  u32 pretcode;
                  int sig;
                  u32 pinfo;
                  u32 puc;
          #ifdef CONFIG_IA32_EMULATION
                  compat_siginfo_t info;
          #else
                  struct siginfo info;                  
          #endif
                  struct ucontext_ia32 uc;
                  char retcode[8];
                  /* fp state follows here */
          };

          /**
           * rt_sigframe - real time signal frame
           * @pretcode:    pointer to the signal trampoline
           * @uc:          user space context
           * @info:        signal info
           * # CONFIG_X86_64
           * # x86-64 always use SA_RESTORER
           */
          struct rt_sigframe {
                  char __user *pretcode;
                  struct ucontext uc;
                  struct siginfo info;
                  /* fp state follows here */
          };

        !! @retcode in earlier Linux was effectively executed to return from the signal
           handler;since Linux 2.6,this field is used only as a signature,so that debuggers
           can recognize the signal stack frame.

      related routines :
        <arch/x86/kernel/signal.c>
          /**
           * get_sigframe - get the signal stack for the process
           * @ka:           Kernel Mode signal action
           * @regs:         User Mode register context
           * @frame_size:   size of signal stack
           * @fpstate:      floating point part state of Intel Processor
           * return:        User Mode signal stack
           *                -1L => invalid stack address,this will cause SIGSEGV
           * # if we on signal stack(@regs->sp => signal stack)
           *   signal stack := @regs->sp - (used_math ? : sig_xstate_size : 0)
           *
           *   if we are not
           *   signal stack := current->sas_ss_sp + current->sas_ss_size - (used_math ? sig_xstate_size : 0)
           *   /* @ka->sa.sa_flags enabled SA_ONSTACK */
           *
           *   signal stack := @ka->sa.sa_restorer - (used_math ? sig_xstate_size : 0)
           *   @regs->ss & 0xffff != __USER_DS AND NOT @ka->sa.sa_flags &SA_RESTORER AND @ka->sa.sa_restorer
           * # before return to caller,the signal stack sp must be aligned through align_sigframe(@sp - @frame_size)
           */
          static inline void __user *
          get_sigframe(struct k_sigaction *ka, struct pt_regs *regs, size_t frame_size, void __user **fpstate);

          /**
           * __setup_rt_frame - setup signal stack for the signal needs additional information
           * @sig:              the signal number
           * @ka:               Kernel Mode signal action
           * @info:             signal info
           * @set:              @saved_sigmask or @blocked,depends on do_signal()(TS_RESTORE_SIGMASK)
           * @regs:             User Moe register context
           * return:            0 => succeed
           *                    others => error
           */
          static int __setup_rt_frame(int sig, struct k_sigaction *ka, siginfo_t *info, sigset_t *set,
                                      struct pt_regs *regs);

          brief description for __setup_rt_frame() :
            this routine setup process signal stack.how to set up the registers of User Mode been described
            in the description of handle_signal() above.
            if access_ok() is failed on VERIFY_WRITE to signal frame,then -EFAULT will be returned to caller.
            the important part of the routine is put_user_try { ... }.
            put_user_try {
                    /* setup signal stack */
                    /* @frame got from get_sigframe() */

                    frame->sig = @sig
                    frame->pinfo = frame->info
                    frame->puc = frame->uc
                    copy_siginfo_to_user(frame->info, @info)

                    /* create the ucontext */
                    frame->uc.uc_flags = cpu_has_xsave ? UC_FP_XSTATE : 0
                    frame->uc.uc_link = 0
                    frame->uc.uc_stack.ss_sp = @current->sas_ss_sp
                    frame->uc.uc_stack.ss_flags = sas_ss_flags(@regs->sp)
                    frame->uc.uc_stack.ss_size = @current->sas_ss_size
                    setup_sigcontext(frame->uc.uc_mcontext, fpstate, @regs, @set->sig[0])
                    __copy_to_user(frame->uc.uc_sigmask, @set, sizeof(*@set))

                    /* setup to return from userspace */
                    /**
                     * for IA32,retcode is sigreturn(),and for x86,retcode is rt_sigreturn().
                     * handle_signal() just call to setup_rt_frame(),it do not care about the
                     * architecture,this will be take charged by setup_rt_frame().
                     * if architecture is x86,then whether signal info is required,__setup_rt_frame()
                     * will be invoked.
                     * # IA32 AND sa_flags & SA_SIGINFO -> ia32_setup_rt_frame
                     * # IA32 -> ia32_setup_frame
                     *   ! if defined CONFIG_X86_32,they are defined as __setup_rt_frame and __setup_frame
                     */
                    /* restorer - local variable is type of void __user * */
                    restorer = VDSO_32_SYMBOL(@current->mm->context.vdso, rt_sigreturn)
                    restorer = @ka->sa.sa_flags & SA_RESTORER ? @ka->sa.sa_restorer : restorer
                    frame->pretcode = restorer
                    frame->retcode = *((u64 *)&rt_retcode)
            }
            if any error was occurred during put to user,-EFAULT will be return to setup_rt_frame().

      !! after signal handler accomplished,the assembly instruction "ret" will pop up the top of signal
         frame into the eip register,the next instruction address is pointed by @pretcode which is setup by
         setup_rt_frame() previously in Kernel Mode called by handle_signal().
         +----------------+
         |    pretcode    | <= ESP /* @pretcode points to @retcode or vDSO.rt_sigreturn or signal trampoline */
         +----------------+
         |      sig       | => this element will be discarded frome stack because the "popl %eax" in @retcode
         +----------------+
         |     pinfo      |
         +----------------+
         |      puc       |
         +----------------+
         |     info       |
         +----------------+
         |      uc        |
         +----------------+
         |    retcode     | => machine code to issue syscall sys_rt_sigreturn()
         +----------------+
         |    fp state    |
         +----------------+
         |    ...         |

      sys_rt_sigreturn() :
        <arch/x86/kernel/signal.c>
          /**
           * sys_rt_sigreturn - syscall routine is issued after real-time signal handler accomplished
           * @regs:             User Mode registers context
           * return:            0 => failed
           *                    the previous value in ax => succeed
           */
          long sys_rt_sigreturn(struct pt_regs *regs);
          
          brief description for sys_rt_sigreturn() :
            this routine receives a parameter is the User Mode registers context.
            it compute the address of object type of struct rt_sigframe from @regs->sp.
            /**
             * [ pretcode ] [ sig ] [ ... ]
             *              esp
             * esp - sizeof(long)
             * @pretcode should been pop up by returning from signal handler.
             * @sig should been pop up by @retcode.
             * but the routine is called through rt_retcode,thus only one element been popped up.
             * ! stack starting from high address,grows to low address.
             *   push => -4 / (-8)
             *   pop  => +4 / (+8)
             */
            the address of the frame is @regs->sp - sizeof(long).
            if access_ok() on @frame with VERIFY_READ was failed,then goto badframe label.
            if failed to copy signal mask in @frame->uc.uc_sigmask,also goto badframe label.
            next,delete all _BLOCKABLE signals inside the @uc_sigmask,and use the new set to
            /* _BLOCKABLE => ~(sigmask(SIGKILL) | sigmask(SIGSTOP)) */
            setup @current->blocked the block signal mask during signal handler executing.
            call to recalc_sigpending(),if any pending signal has delivered,it will be handled
            at next interrupt.
            call to restore_sigcontext() to restore the User Mode context before signal handling,
            if failed,goto badframe label.
            /**
             * static int restore_sigcontext(@regs, @sc, @pax,used to store @sc->ax) :
             *   it is defined in the same file.
             *   the main task this routine does is get from user,and store into @regs.
             *     set_user_gs(@regs, GET_SEG(gs))
             *     COPY_SEG(fs)
             *     COPY_SEG(es)
             *     COPY_SEG(ds)
             *     COPY(di) COPY(si) COPY(bp) COPY(sp)
             *     COPY(bx) COPY(dx) COPY(cx) COPY(ip)
             *     COPY_SEG_CPL3(cs)
             *     COPY_SEG_CPL3(ss)
             *     @regs->flags = (@regs->flags & ~FIX_EFLAGS) | (@sc->flags & FIX_EFLAGS)
             *     @regs->orig_ax = -1
             *     restore_i387_xstate(@sc->fpstate)
             * macros :
             *   FIX_EFLAGS => (__FIX_EFLAGS | X86_EFLAGS_RF)
             *   __FIX_EFLAGS => X86_EFLAGS_AC | X86_EFLAGS_OF | X86_EFLAGS_DF | X86_EFLAGS_TF |
             *                   X86_EFLAGS_SF | X86_EFLAGS_ZF | X86_EFLAGS_AF | X86_EFLAGS_PF |
             *                   X86_EFLAGS_CS
             *   COPY(x) => get_user_ex(regs->x, &sc->x)
             *   GET_SEG(seg) => return tmp,get_user_ex(tmp, &sc->seg)
             *   COPY_SEG(seg) => regs->seg = GET_SEG(seg)
             *   COPY_SEG_CPL3(seg) => regs->seg = GET_SEG(seg) | 3  # User Mode CPL
             */
            proceed call to do_sigaltstack(),this routine is defined in <kernel/signal.c>,it alters
            the signal stack of @current process.it handles two task,the first is reset @sas_ss_sp
            and @sas_ss_size of current task if the first parameter is not NULL;the second is setup
            contents of @sas_ss_sp through the second parameter,if it is not NULL.
            /**
             * it requires two stack_t pointer and one unsigned long value @sp,return error code if
             * failed.
             */ 
            finally,returns local variable @ax,it should been setup by restore_sigcontext().
            badframe:
              signal_fault(@regs, @frame, "rt_sigreturn")
              return 0;

    Reexecution of System Calls :
      kernel puts the User Mode process issued a system call to TASK_INTERRUPTIBLE or TASK_UNINTERRUPTIBLE if
      this system call can not be satisfied immediately.
      in the case a signal is send to the process in TASK_INTERRUPTIBLE,then kernel will puts it to TASK_RUNNING,
      and proceed to handles the signal through signal handler,after signal handler accomplished,it switch back
      to Kernel Mode.
      the system call is been interrupted,and error code EINTR will be return back to User Mode,this is generallly.
      but of course,there is not only one error code existing,the remains are used internally by Kernel to
      specify whether the system call may be reexecuted automatically after the signal handler termination.
        Signal action   EINTR        ERESTARTSYS        ERESTARTNOHAND                ERESTARTNOINTR
                                                        ERESTART_RESTARTBLOCK
        default         terminate    reexecute          reexecute                     reexecute
        ignore          terminate    reexecute          reexecute                     reexecute
        catch           terminate    depends            terminate                     reexecute

        ! ERESTARTNOHAND and ERESTART_RESTARTBLOCK error codes differ on the mechanism used to restart the
          system call.
        ! terminate => no system call restart,-EINTR is returned to User Mode process
          reexecute => reexecute system call,this is invisible to User Mode process
          depends   => reexecute system call if and only if SA_RESTART flag of the delivered signal is set
        
      kernel must to determines that the process which the signal is delivered to is really issued a system call
      before attempts reexecute system call.the member named @orig_ax in struct pt_regs as indicator,if the process
      really issued a system call,@orig_ax must stored the system call number.
      
      @orig_ax :
              interrupt => IRQ number associated with the interrupt minus 256
                           /* common_interrupt() adjusted the vector value to [-256, -1] */
              0x80 exception => system call number
              other exceptions => -1

      so we can easy to know that @orig_ax >= 0 means the signal woken up a TASK_INTERRUPTIBLE process that was
      sleeping in a system call. /* system call interrupted,if we do not reexecute it,should return -EINTR in eax */

      Restarting a system call interrupted by a non-caught signal :
        if the signal is explicitly ignored or if its default action is enforced,do_signal() analyzes the error code
        of the system call to decide whether the unfinished system call must be automatically reexecuted.
        /**
         * as the description of do_signal() above,it retrieves the system call number through syscall_get_nr() routine,
         * if the number is greater than or equal to _zero_,then it means we come from a system call.
         * in this case,it get the error code through syscall_get_error() routine.
         * # "@regs->ip -= 2;" => instructions "int $0x80" and "sysenter" are 2bytes long
         */

        as we have knew that do_signal() set eax to the number of system call restart_syscall() when the error code is
        -ERESTART_RESTARTBLOCK.this error code is usually returned by timer-related system call,in this case,before
        restart the system call,must adjust its parameters at first.
        suppose nanosleep(20) been issued a timer will expired in 20 milliseconds,if the system call is interrupted at
        10 milliseconds,the argument to restarted nanosleep() should be 10 milliseconds,not 20 milliseconds.
        /**
         * we have knew that the system call nanosleep() is actually call to hrtimer_nanosleep(),and the routine will
         * fills the restart_block in the task's thread_info structure.
         * and the restart_syscall() is actually process the statement
         *         current_thread_info()->restart_block->fn(current_thread_info()->restart_block);
         * # restart_syscall() is defined in <kernel/signal.c> by SYSCALL_DEFINE0().
         */
      
      Restarting a system call for a caught signal :
        if the signal is caught by process,then handle_signal() deal with reexecuting system call.in this case,
        the signal number got through routine get_signal_to_deliver() is greater than _zero_,and do_signal()
        will return to caller after handle_signal() is accomplished.
        handle_signal() does the similar steps to deal with reexecute system call.
        handle_signal() :
          first,it get error code of the interrupted system call
          next check the error code {
                  -ERESTART_RESTARTBLOCK
                  -ERESTARTNOHAND
                          set ax to -EINTR
                          do not reexecute system call
                  -ERESTARTSYS
                          check if SA_RESTART been disabled in @ka->sa.sa_flags
                          in this case,set ax to -EINTR
                          do not reexecute system call
                  -ERESTARTNOINTR
                          set ax to @orig_ax
                          set ip to @ip -= 2
                          the system call will be restarted after signal handler accomplished
          }
          ... setup signal handler processing environment ...

    System Calls Related to Signal Handling :
      The kill() System Call :
        kill() => C lib wrapper
        sys_kill() => Linux Kernel syscall

        it recevies two parameters and returns a value is type of long(Kernel syscall definition).
        1st param @pid
          > 0 => the signal is sent to the thread group of the process whose PID is equal to @pid
          = 0 => the signal is sent to all thread groups of the processes in the same process group
                 as the calling process
          = -1 => the signal is sent to all processes,except swapper,init,and @current
          < -1 => the signal is sent to all thread groups of the processes in the process group whose
                  ID is -@pid
        2nd param @sig
          the signal number

        sys_kill() :
          setup minimal signal information,and call to kill_something_info(),returns to caller what the
          routine returned.
          /**
           * @si_code := SI_USER
           * @si_pid := current task's thread group id
           * @si_uid := current task's user id
           */
          kill_something_info() call to kill_pid_info() while @pid > 0;call to __kill_pgrp_info() while @pid != -1;
          call to group_send_sig_info() in a for-cycle for send signal to each process in the system while @pid == -1.
          /* of couse,if @pid < -1,it call to __kill_pgrp_info() with a -@pid. */

        ! sys_kill() can send standard signals and real-time signals,but it would not queue real-time signals,so that,
          we should not use this system call to send real-time signal.
        ! SystemV and BSD Unix variants also have a killpg() system call,it is used to send signal to a process group
          explicitly.however,Linux implemented it via library function that uses kill() system call.
          another variation is raise() system call,it send signal to this task,and Linux implemented also via library
          function.

      The tkill() and tgkill() System Calls :
        these two system calls are defined in <kernel/signal.c>,same file as the sys_kill().
        tgkill() requires three parameters,and tkill() requires two parameters.

        tgkill() :
          1st param => tgid,the thread group id
          2nd param => pid,the process id
          3rd param => signal number

          it is used to send signal to one specific thread,and the thread is a member of the thread group whose id
          is @tgid,and the thread has the PID @pid.
          if @pid <= 0 OR @tgid <= 0,it returns -EINVAL.
          if the thread group with id @tgid is not exist,returns -ESRCH even the task has id @pid is exist.
          /* of course,if the task is not exist,-ESRCH also be returned to caller */
          this routine is actually returns to caller by call to do_tkill(),and which call to do_send_specific(),
          and do_send_sig_info(),finally,end up to __send_signal().
          /**
           * the only thing in signal info different to sys_kill() is @si_code,it fills the field with SI_TKILL.
           */

        tkill() :
          1st param => pid,the process id
          2nd param => signal number

          this routine send a signal to only one task,even if it is a CLONE_THREAD task.

          if @pid <= 0,it returns -EINVAL to caller.
          this routine is also returns to caller by call to do_tkill(),but the parameter @tgid is _zero_.

        ! pthread_kill() function of every POSIX-compliant pthread library invokes either of them to send a
          signal to a specific lightweight process.

      Changing a Signal Action :
        system call sigaction() is used to change the signal action for standard signal.
        the routine sys_sigaction() is called by C lib wrapper sigaction().

        <arch/x86/kernel/signal.c>
          /**
           * sys_sigaction - system call service routine used to change signal action
           * @sig:           signal number
           * @act:           new action
           * @oact:          used to store the old action,if it is not NUL
           * return:         -EFAULT => failed to set new actioon or old action
           *                 -EINVAL => invalid signal or kernel only signal
           *                 0 => succeed
           * # structure old_sigaction has the same members to structure sigaction
           */
          asmlinkage int sys_sigaction(int sig, const struct old_sigaction __user *act,
                                       struct old_sigaction __user *oact);

          brief description for sys_sigaction() :
            use the parameter @act to setup a local variable to retrieves the signal action related
            info from user space. /* get_user_ex(),include setup @sa_mask */
            if any error encountered during retrieving,return -EFAULT. /* if deny VERIFY_READ,return -EFAULT */
            next,call to do_sigaction(),if @act is not NULL,then pass the local variable @new_ka;
            otherwise,pass NULL to it.and if @oact is not NULL,then pass the local variable @old_ka,
            otherwise,pass NULL to it.
            proceed to check if return value from do_sigaction() is not _zero_ AND @oact is not NULL,
            in this case,it fills @oact through @old_ka;if @oact deny VERIFY_WRITE,return -EFAULT,
            if any error is encountered during filling,return -EFAULT.
            finally,return _zero_.

        <kernel/signal.c>
          /**
           * do_sigaction - routine called by sys_sigaction() to setup signal action
           * @sig:          signal number
           * @act:          new action
           * @oact:         store old action
           * return:        -EINVAL => invalid signal
           *                0 => succeed
           * # POSIX 3.3.1.3 :
           *     setting a signal action to SIG_IGN for a signal that is pending shall cause
           *     the pending signal to be discarded,whether or not it is blocked
           *     setting a signal action to SIG_DFL for a signal that is pending and whose
           *     default action is to ignore the signal,shall cause the pending signal to be
           *     discarded,whether or not it is blocked
           */
          int do_sigaction(int sig, struct k_sigaction *act, struct k_sigaction *oact);

          brief description for do_sigaction() :
            this routine first to checks if @sig is invalid,it is,then return -EINVAL.
            get the kernel sigaction object of this process,@current->sighand->action[@sig - 1].
                                                            /* array index starting from _zero_ */
            acquire @siglock inside to @sighand,and disable local interrupt.
            fill @oact if it is not NULL,it will store the old sigaction.
            check if @act is TRUE,then prepare to setup new action.
            delete SIGKILL and SIGSTOP from @sa_mask,because these two signals neither can be caught nor
            can be ignored.
            next,use @act to setup the correspond action at the position @sig - 1 in @sighand->@action.
            proceed to do POSIX request checking,if the new action is SIG_IGN or is SIG_DFL but the default
            action is SIG_IGN,then discarded the pending signal.
            /* @handler == SIG_IGN || (@handler == SIG_DFL && sig_kernel_ignore(@sig)) */
            /**
             * add @sig to a local object is type of sigset_t
             * call to rm_from_queue_full(&mask, @shared_pending)
             * do-while cycle to rm_from_queue_full(&mask, @pending) for each thread of this process
             */
            finally,unlock the spinlock acquried previously,enable local interrupt,return _zero_.

      Examing the Pending Blocked Signals :
        system call sigpending() allows a process to examine the set of pending blocked signals,
        the corresponding sys_sigpending() is defined in <kernel/signal.c> through SYSCALL_DEFINE1().
        it require only one parameter is named @set that is type of old_sigset_t*.
                                                                    /* typedef unsigned long old_sigset_t */
        sys_sigpending() :
          it returned to caller actually by call to do_sigpending(@set, sizeof(*@set)).
          do_sigpending() must check if its second parameter @sigsetsize is less than or equal to sizeof(sigset_t),
          if it is not,return -EINVAL to caller.
          and then,acquire local @siglock in @sighand,disable local interrupt.
          combine @pending.signal and @shared_pending.signal,the result is stored into @pending.
          release spinlock,enable local interrupt.
          disable all blocked signals inside the @pending mask.
          attempts to copy @pending to @set which is inside to user space,if failed,return -EFAULT.
          finally,return _zero_.

      Modifying the Set of Blocked Signals :
        system call sigprocmask() is used to modify the set of blocked signals,but it only applies to regular siganls.
        the corresponding kernel routine is sys_sigprocmask().

        <kernel/signal.c>
          /**
           * sys_sigprocmask - system call allows process in User Mode to modify its blocked regular signals set
           * @how:             SIG_BLOCK => add @set into @blocked,except SIGKILL and SIGSTOP
           *                   SIG_UNBLOCK => delete the signals inside to @set from @blocked
           *                   SIG_SETMASK => use @set to reset @blocked,except SIGKILL and SIGSTOP
           * @set:             signal mask
           * @oset:            if not NULL,used to store the old set
           * return:           -EFAULT => failed to set signal mask
           *                   -EINVAL => invalid @how
           *                   0 => succeed
           * # because this routine only applies to regular signals,thus @current->blocked.sig[0] would be selected
           * # before take the action @how on @blocked,must acquire @siglock,disable local interrupt at first
           * # if @set is NULL,this routine will not modify current blocked signal mask
           *   if @oset is NULL,this routine will not return the old blocked signal mask
           * !! AFTER THE ACTION @how PROCESSED,ROUTINE recalc_sigpending() WILL BE CALLED TO RE-CALCULATE PENDING
           *    SIGNALS
           */
          SYSCALL_DEFINE3(sigprocmask, int, how, old_sigset_t __user *, set, old_sigset_t __user *, oset);
          => asmlinkage long sys_sigprocmask(...);        

      Suspending the Process :
        system call sigsuspend() puts the process in the TASK_INTERRUPTIBLE state,and it will wake up only
        when a nonignored,nonblocked signal is sent to it.
        the corresponding kernel routine is sys_sigsuspend().

        <arch/x86/kernel/signal.c>
          /**
           * sys_sigsuspend - suspend current process until a nonignored,nonblocked signal is delivered to it
           * @history0:       unused
           * @history1:       unused
           * @mask:           the signal mask is used to reset @blocked for preparing suspend
           * return:          -ERESTARTNOHAND
           * # only blockable signals can get appearence in @mask
           */
          asmlinkage int sys_sigsuspend(int history0, int history1, old_sigset_t mask);

          brief description for sys_sigsuspend() :
            disable unblockable signals in the @mask.
            acquire @siglock and disable local interrupt.
            temporary stores @blocked into @saved_sigmask.
            use @mask to reset @blocked.
            recalculate signals pending.
            release lock and enable local interrupt.
            put this process into TASK_INTERRUPTIBLE state.
            call to schedule().
            after switch to this process,restore @blocked and return -ERESTARTNOHAND.
            /**
             * if we have been woke up by a signal,then -ERESTARTNOHAND will be returned,so it is
             * stored in eax.
             * resume_userspace() will check signal delivering,thus do_signal() is invoked.
             * after the corresponding signal handler accomplished,-EINTR will be return to caller,
             * and the system call sys_sigsuspend() is finished.
             */

          ! sys_sigsuspend() swap @blocked atomatically,just combine sigprocmask() and sleep() can not
            ensure atomic operation.

      System Calls for Real-Time Signals :
        regular signals and real-time signals associated different system call routines.

        real-time signal system call :
          rt_sigaction
          rt_sigpending
          rt_sigprocmask
          rt_sigsuspend

          rt_sigqueueinfo => sends a real-time signal so that it is added to the shared pending signal
                             queue of the destination process.
                             /* C lib wrapper sigqueue() call to this routine */
          rt_sigtimedwait => dequeues a blocked pending signal without delivering it and returns the
                             signal number to the caller;if no blocked signal is pending,suspends the
                             current process for a fixed amount of time.
                             /* C lib wrapper sigwaitinfo() and sigtimedwait() call to this routine */


/* END OF CHAPTER11 */


Chapter 12 : The Virtual Filesystem
    Virtual Filesystem allows Linux to manages multiple filesystem types in the same way.

    The Role of the Virtual Filesystem(VFS) :
      Virtual Filesystem(Virtual Filesystem Switch OR VFS) is a kernel software layer that handles all
      system calls related to a standard Unix filesystem.it providing a common interface to several
      kinds of filesystem.

      [ user program ] <====> [ Linux VFS ] <====> [ real filesystem ]
      whatever the "real filesystem" it is,user program is able to access the files in the same way.

      The first Virtual Filesystem was inclued in Sun Microsystem's SunOS in 1986.

      Three groups of filesystems supported by the VFS :
        1> Disk-based filesystems
           filesystem is installed on the local disk device or disk emulating device.
             EXT2 EXT3 EXT4 /* Unix and Linux */
             SYSV UFS MINIX VERITAS VxFS /* Unix variants */
             MS-DOS VFAT NTFS /* Windows */
             ISO9660-CD-ROM UDF(Universal Disk Format) /* DVD */
             HPFS HFS AFFS ADFS /* IMB or Apple,etc */
             JFS XFS /* journaling filesystems */
        2> Network filesystems
           filesystem is installed in the different network computer.
             NFS
             Coda
             AFS /* Andrew filesystem */
             CIFS /* Common Internet File System */
             NCP /* Novell's NetWare Core Protocal */
        3> Special filesystems
           no manage disk space,either locally or remotely.such filesystem is installed in the memory.
             devtmpfs
             tmpfs 
             sysfs
             proc
             cgroup
             mqueue
             ...

      VFS can handles virtual block devices such /dev/loop0 .

    The Common File Model :
      The key idea behind the VFS consists of introducing a "common file model" capable of representing all
      supported filesystems.the model is strictly mirros the file model provided by the traditional Unix system.
      each specific filesystem implementation must translates its physical organaization into the VFS's common
      file model.

      if some object is necessary for translate file model,Linux would allocates them in the memory,deallocates
      them when the filesystem is unmounted.
      Linux use pointer-to-implementation to handles different filesystem operations,the functions are registered
      by the filesystem and will be called when access to the files on it.
      /* such operations are contained in @f_op member of struct file */

      The common file model consists of the following object types :
        The superblock object
          stores information concerning a mounted filesystem. /* i.e. filesystem control block on the disk */
        The inode object
          stores general information about a specific file. /* i.e. file control block on the disk */
          used to uniquely identifies the file within the filesystem.
        The file object
          stores information about the interaction between an open file and a process.
          this information exists only in kernel memory during the period when the file is opening.
        The dentry object
          stores information about the linking of a directory entry with the corresponding file.
          /* each disk-based filesystem stores this information in its own particular way on disk */

        e.g.
          +-----------+
          | disk file |<----+-------------+
          +-----------+     |             |
                            |             |
                        +------------+    +-------+
                        | superblock |<---| inode |
                        +------------+    +-------+
                                            ^   ^
                                            |   |   
                                       +----+   +---+
                                       |            |
                                    +--------+  +--------+
                                    | dentry |  | dentry |
                                    +--------+  +--------+
                                     ^          ^
                  +------+           |          |
                  | file |-----------+          |
                  +------+(hard link)|          |
                                     |          |
                  +------+           |          |
                  | file |-----------+          |
                  +------+                      |
                                                |
                  +------+                      |
                  | file |----------------------+ (hard link)
                  +------+

                  /* private file object */

      except common interface,VFS also provides a disk cache named dentry cache.which is used to
      speed up the translation from a file pathname to the inode of the last pathname component.
      it is a software mechanism that alows the kernel to keep in RAM some information that is
      normally stored on a disk.

    System Calls Handled by the VFS :
      there are three groups of system calls handled by the VFS.
      the first group related to filesystem,regular file,directory,and symbolic.
      the second group related to device file and pipe.
      the third group related to network implementation.

      list of system call primitives :
        mount umount umount2
        sysfs
        statfs fstatfs statfs64 fstatfs64 ustat
        chroot privot_root
        chdir fchdir getcwd
        mkdir rmdir
        getdents getdents64 readdir link unlink rename lookup_dcookie
        readlink symlink
        chown fchown lchown chown16 fchown16 lchown16
        chmod fchmod utime
        stat fstat lstat access oldstat oldfstat oldlstat stat64 lstat64 fstat64
        open close creat umask
        dup dup2 fcntl fcntl64
        select poll
        truncate ftruncate truncate64 ftruncate64
        lseek _llseek
        read write readv writev sendfile sendfile64 readahead
        io_setup io_submit io_getevents
        io_cancel io_destroy
        pread64 pwrite64
        mmap mmap2 munmap madvise mincore remap_file_pages
        fdatasync fsync sync msync
        flock
        setxattr lsetxattr fsetxattr getxattr lgetxattr fgetxattr listxattr \
        llistxattr flistxattr removexattr lremovexattr fremovexattr

    VFS Data Structures :
     Superblock Objects :
       structure super_block is used to stores the filesystem control informations.
       in other word,super block represents a filesystem,inodes must be linked into the super block.

       <linux/fs.h>
         /* the first super block object */
         extern struct list_head super_blocks;
        
         /* protection for concurent accessing to @super_blocks */
         extern spinlock_t sb_lock;

         /**
          * super_block - filesystem super block information
          * @s_list:      super block list,point to the adjacent super block object
          * @s_dev:       device identifier
          * @s_dirt:      number of modified inodes
          * @s_blocksize_bits: block size in number of bits
          * @s_blocksize: block size in bytes
          * @s_maxbytes:  maximum size of file
          * @s_type:      filesystem type
          * @s_op:        super block methods
          * @dq_op:       disk quota handling methods
          * @s_qcop:      disk quota administration methods
          * @s_export_op: export operations used by network filesystem
          * @s_flags:     mount flags
          * @s_magic:     filesystem magic number
          * @s_root:      dentry object of the filesystem's root directory
          * @s_umount:    umounting protection
          * @s_lock:      super block concurrent protection
          * @s_count:     reference counter
          * @s_need_sync: flag indicates filesystem needs synchronization
          * @s_active:    secondary reference counter
          * @s_security:  super block security structure
          * @s_xattr:     super block extended attribute structure
          * @s_inodes:    list of all inodes
          * @s_anon:      anonymous dentries for network filesystem exporting
          * @s_files:     list of "in use" file objects
          * @s_dentry_lru: dentry cache
          * @s_nr_dentry_unused: number of unused dentry
          * @s_bdev:      block device driver descriptor
          * @s_bdi:       backing device info of this super block
          * @s_mtd:       MTD info of this super block
          *               # Linux MTD subsystem is made for memory devices
          *                 MTD - Memory Technology Device
          * @s_instances: list of super block objects of a given filesystem type(the same type)
          * @s_dquot:     disk quota descriptor
          * @s_frozen:    flag used when freezing the filesystem(forcing it to a consistent state)
          * @s_wait_unfrozen: wait queue for processes that sleep until the filesystem is unfrozen
          * @s_id:        name of the block device containing this super block
          * @s_fs_info:   super block information of a specific filesystem(private information)
          * @s_time_gran: timestamp's granularity(ns)
          * @s_vfs_rename_mutex: concurrent protection for file renaming across directories
          * @s_subtype:   subtype,if it is not NULL,
          *               the filesystem type field in /proc/mounts will be "type.subtype"
          * @s_options:   saved mount options for lazy filesystem using generic_show_options()
          * # all super block objects are linked together through a circular doubly linked list
          */
         struct super_block {
                 struct list_head s_list;
                 dev_t s_dev;
                 unsigned char s_dirt;
                 unsigned char s_blocksize_bits;
                 unsigned long s_blocksize;
                 loff_t s_maxbytes;
                 struct file_system_type *s_type;
                 const struct super_operations *s_op;
                 const struct dquot_operations *dq_op;
                 const struct quotactl_ops *s_qcop;
                 const struct export_operations *s_export_op;
                 unsigned long s_flags;
                 unsigned long s_magic;
                 struct dentry *s_root;
                 struct rw_semaphore s_umount;
                 struct mutex s_lock;
                 int s_count;
                 int s_need_sync;
                 atomic_t s_active;
         #ifdef CONFIG_SECURITY
                 void *s_security;
         #endif
                 struct xattr_handler **s_xattr;
                 struct list_head s_inodes;
                 struct hlist_head s_anon;
                 struct list_head s_files;
                 struct list_head s_dentry_lru;
                 int s_nr_dentry_unused;
                 
                 struct block_device *s_bdev;
                 struct backing_dev_info *s_bdi;
                 struct mtd_info *s_mtd;
                 struct list_head s_instances;
                 struct quota_info s_dquot;
                 
                 int s_frozen;
                 wait_queue_head_t s_wait_unfrozen;
                 
                 char s_id[32];

                 void *s_fs_info;
                 ...
                 u32 s_time_gran;
                 struct mutex s_vfs_rename_mutex;
                 char *s_subtype;
                 char *s_options;
         };

         ! in general,data pointed to by the @s_fs_info field is information from the disk
           duplicated in memory for reasons of efficiency.
         ! Linux copying all dirty super blocks to disk periodically.

         /**
          * super_operations - super block common interfaces defined by VFS
          * # if a filesystem wants register itself to VFS,it must implements these super block
          *   operations;for un-supported operation,set it to NULL
          * # the feature of the function is corresponding to its name
          */
         struct super_operations {
                 struct inode *(*alloc_inode)(struct super_block *sb);
                 void (*destroy_inode)(struct inode *);
                 
                 void (*dirty_inode)(struct inode *);
                 int (*write_inode)(struct inode *, struct writeback_control *wbc);
                 void (*drop_inode)(struct inode *);
                 void (*delete_inode)(struct inode *);
                 void (*put_super)(struct supber_block *);
                 void (*write_super)(struct supber_block *);
                 int (*sync_fs)(struct super_block *sb, int waits);
                 int (*freeze_fs)(struct super_block *);
                 int (*unfreeze_fs)(struct super_block *);
                 int (*statfs)(struct dentry *, struct kstatfs *);
                 int (*remount_fs)(struct super_block *, int *, char *);
                 void (*clear_inode)(struct inode *);
                 void (*umount_begin)(struct super_block *);

                 int (*show_options)(struct seq_file *, struct vfsmount *);
                 int (*show_stats)(struct seq_file *, struct vfsmount *);
                 
         #ifdef CONFIG_QUOTA /* Disk quota system */
                 ssize_t (*quota_read)(struct super_block *, int, char *, size_t, loff_t);
                 ssize_t (*quota_write)(struct super_block *, int, const char *, size_t, loff_t);
         #endif

                 int (*bdev_try_to_free_page)(struct super_block *, struct page *, gfp_t);
         };

     Inode Objects :
       the information needed by the filesystem to handles a file is included in inode,each file has a
       unique inode number;the inode number would not changed as long as the file is exist on the filesystem.

       <linux/fs.h>
         /**
          * inode - inode object
          * @i_hash:      inode hash list,used to speed up the inode search when kernel
          *               have known the inode number and the super block it belongs to
          * @i_list:      backing dev IO list
          * @i_sb_list:   inode list of the super block,point to the adjacent inode object
          * @i_dentry:    in-use dentry objects list corresponding to this inode
          * @i_ino:       inode number
          * @i_count:     usage counter
          * @i_nlink:     number of hard links
          * @i_uid:       inode user ID
          * @i_gid:       inode group ID
          * @i_rdev:      real device identifier,used for device file,contains the
          *               major number and minor number of the device
          * @i_blkbits:   block size in number of bits
          *               # block is filesystem specific data unit,thus a page might
          *                 contains several blocks,the number of blocks in a page
          *                 is able to be calculated through "page size >> @i_blkbits"
          *                 that is - "page size / (2^@i_blkbits)"
          * @i_version:   nversion number,automatically increased after each use
          * @i_size:      inode size in bytes
          * @i_size_seqcount: 
          *               sequence counter used in SMP systems to get
          *               consistent values for @i_size
          * @i_atime:     inode access time stamp
          * @i_mtime:     inode modify time stamp
          * @i_ctime:     inode change time stamp
          * @i_blocks:    number of blocks of the inode
          * @i_bytes:     number of bytes of the last block
          * @i_mode:      file mode
          * @i_lock:      member accessing concurrent protection
          * @i_mutex:     inode concurrent protection
          *               this mutex would be locked up by some function such pipe_read()
          * @i_alloc_sem: direct I/O concurrent protection
          * @i_op:        inode operations
          * @i_fop:       file operations
          * @i_sb:        the super block this inode belongs to
          * @i_flock:     file lock supporting
          * @i_mapping:   an address space object used to indicates the range in memory
          *               where this file is mapped
          * @i_data:      saved inode mapping address space
          * @i_dquot:     quota system supporting
          * @i_devices:   list of inodes relate to a device
          *               # for example,the character device,this member is added into
          *                 the list cdev.list
          * @i_pipe:      pipe specific
          * @i_bdev:      block device driver
          * @i_cdev:      character device driver
          * @i_generation:
          *               inode version number used by some filesystems
          * @i_fsnotify_mask:
          *               filesystem notification mechanism
          * @i_fsnotify_mark_entries:
          *               filesystem notification mechanism
          * @inotify_watches:
          *               inotify mechanism
          * @inotify_mutex:
          *               inotify mechanism
          * @i_state:     inode state flag
          * @dirtied_when:
          *               jiffies of first dirtying
          * @i_flags:     inode flag
          * @i_writecount:
          *               usage counter for writing processes
          *               == 0 -> no writers,no VM_DENYWRITE mappings
          *                < 0 -> vm_area_structs with VM_DENYWRITE set exist
          *                > 0 -> users are writing to the file
          *               # get_write_access(): i_writecount += 1 <-> i_wrtiecount >= 0
          *                 put_write_access(): --i_writecount
          *                 deny_write_access(): i_writecount -= 1 <-> i_writecount <= 0
          *
          * @i_security:  security specific
          * @i_private:   private data
          */
         struct inode {
                 struct hlist_node i_hash;
                 struct list_head i_list;
                 struct list_head i_sb_list;
                 struct list_head i_dentry;
                 unsigned long i_ino;
                 atomic_t i_count;
                 unsigned int i_nlink;
                 uid_t i_uid;
                 gid_t i_gid;
                 dev_t i_rdev;
                 unsigned int i_blkbits;
                 u64 i_version;
                 loff_t i_size;

         #ifdef __NEED_I_SIZE_ORDERED
                 seqcount_t i_size_seqcount;
         #endif

                 struct timespec i_atime;
                 struct timespec i_mtime;
                 struct timespec i_ctime;
                 blkcnt_t i_blocks;
                 unsigned short i_bytes;
                 umode_t i_mode;
                 spinlock_t i_lock;
                 struct mutex i_mutex;
                 struct rw_semaphore i_alloc_sem;
                 const struct inode_operations *i_op;
                 const struct file_operations *i_fop;
                 struct super_block *i_sb;
                 struct file_lock *i_flock;
                 struct address_space *i_mapping;
                 struct address_space i_data;

         #ifdef CONFIG_QUOTA
                 struct dquot *i_dquot[MAXQUOTAs];
         #endif

                 struct list_head i_devices;
                 
                 union {
                         struct pipe_inode_info *i_pipe;
                         struct block_device *i_bdev;
                         struct cdev *i_cdev;
                 };

                 __u32 i_generation;

         #ifdef CONFIG_FSNOTIFY
                 __u32 i_fsnotify_mask;
                 struct hlist_head i_fsnotify_mark_entries;
         #endif 

         #ifdef CONFIG_INOTIFY
                 struct list_head inotify_watches;
                 struct mutex inotify_mutex;
         #endif

                 unsigned long i_state;
                 unsigned long dirtied_when;

                 unsigned int i_flags;
                 atomic_t i_writecount;
         
         #ifdef CONFIG_SECURITY
                 void *i_security;
         #endif

                 ...

                 void *i_private;
         };

       inode in the memory duplicated some information of the inode in the disk device.
       if @i_state is equal to I_DIRTY_SYNC,I_DIRTY_DATASYNC,I_DIRTY_PAGES,the inode is dirty;
       the corresponding inode in the disk must be updated.
       /* macro I_DIRTY combined the three flags,thus we can checks all of them at once */
       /**
        * Other states :
        *   I_NEW - new inodes set I_NEW,if two processes both create the same inode,one of
        *           them will release its inode and wait for I_NEW to be released before
        *           returning
        *   I_WILL_FREE - set when calling write_inode_now() if @i_count is _zero_,
        *                 I_FREEING must be set when I_WILL_FREE is cleared
        *   I_FREEING - the inode is about to be freed but still has dirty pages or buffers
        *               attached or the inode itself is still dirty
        *   I_CLEAR - the inode is clean and can be destroyed
        */

      header <linux/writeback.h> extern declared a variable named @inode_unused,it is type of
      struct list_head.the inodes are not dirty and their @i_count is _zero_,will be putting into
      the unused inodes list.member @i_list point to the adjacent inode object.

      header <linux/writeback.h> extern declared a variable named @inode_in_use,it also is type
      of struct list_head,all inodes they are in use will be putting into this list.the inodes
      are not dirty,but @i_count is positive,@i_list point to the adjacent inode object.

      !! THE BOOK MENTIONED A MEMBER NAMED @s_dirty INSIDE TO SUPER BLOCK OBJECT,BUT LINUX 2.6.31
         DOES NOT EXIST SUCH MEMBER IN THE STRUCTURE.
         FOR EXT2 FILESYSTEM,@write_node IS SET TO ext2_write_node(),WHICH ATTEMPTS TO GET A
         BUFFER AND MARKED IT DIRTY,THE BUFFER WILL BE WRITEBACK BY sync_dirty_buffer() LATER.

      <linux/fs.h>
        /**
         * inode_operations - common inode operations defined by VFS,if the filesystem wants to
         *                    registers itself on VFS,must provides implementations of these interfaces,
         *                    for un-supported feature,set to NULL
         */
        struct inode_operations {
                int (*create)(struct inode *, struct dentry *, int, struct nameidata *);
                struct dentry * (*lookup)(struct inode *, struct dentry *, struct nameidata *);
                int (*link)(struct dentry *, struct inode *, struct dentry *);
                int (*unlink)(struct inode *, struct dentry *);
                int (*symlink)(struct inode *, struct dentry *, const char *);
                int (*mkdir)(struct inode *, struct dentry *, int);
                int (*rmdir)(struct inode *, struct dentry *);
                int (*mknod)(struct inode *, struct dentry *, int, dev_t);
                int (*rename)(struct inode *, struct dentry *, struct inode *, struct dentry *);
                int (*readlink)(struct dentry *, char __user *, int);
                void * (*follow_link)(struct dentry *, struct nameidata *);
                void (*put_link)(struct dentry *, struct nameidata *, void *);
                void (*truncate)(struct inode *);
                int (*permission)(struct inode *, int);
                int (*check_acl)(struct inode *, int);
                int (*setattr)(struct dentry *, struct iattr *);
                int (*getattr)(struct vfsmount *mnt, struct dentry *, struct kstat *);
                int (*setxattr)(struct dentry *, const char *, const void *, size_t, int);
                ssize_t (*getxattr)(struct dentry *, const char *, void *, size_t);
                ssize_t (*listxattr)(struct dentry *, char *, size_t);
                int (*removexattr)(struct dentry *, const char *);
                void (*truncate_range)(struct inode *, loff_t, loff_t);
                long (*fallocate)(struct inode *inode, int mode, loff_t offset, loff_t len);
                int (*fiemap)(struct inode *, struct fiemap_extent_info *, u64 start, u64 len);
        };

     File Objects :
       a file object describes a opened file.structure file is used to represents such object.
       must pay attention about that file object has no corresponding image on the disk,it is
       only exists in memory.

       <linux/fs.h>
         /**
          * file_ra_state - track a single file's readahead state
          * @start:         where readahead started
          *                 # index of first page in the current window
          * @size:          size of readahead pages in the current window
          * @async_size:    size of asynchronous readahead pages
          * @ra_pages:      maximum readahead window
          * @mmap_miss:     cache miss stat for mmap accesses
          *                 # Read-Ahead miss counter,used for memory mapped files
          * @prev_pos:      cache last read() position
          *                 # index of the last page requested by the process
          * # this structure usually is initialized by routine file_ra_state_init(),
          *   the routine is defined in <mm/readahead.c> and invoked when opening the file,
          *   it is called by __dentry_open()
          */
         struct file_ra_state {
                 pgoff_t start;
                 unsigned int size;
                 unsigned int async_size;
                 
                 unsigned int ra_pages;
                 unsigned int mmap_miss;
                 loff_t prev_pos;
         };


         /**
          * file - file object used to represents an opened file
          * @fu_list:    file object list
          * @fu_rcuhead: file list updating protection
          * @f_path:     file path structure,contains some information about dentry
          * @f_op:       file operations
          * @f_lock:     concurent protection
          * @f_count:    reference counter
          *              CLONE_FILES flag let lightweight process share the table that
          *              identifies the open files,thus no reference counter increasing
          *              # this counter increase when the file is used by kernel itself
          * @f_flags:    flags specified when opening the file
          * @f_mode:     file permission
          * @f_pos:      file position,the next file operations such start from there
          * @f_owner:    file's owner
          * @f_cred:     security context,includes some identification info
          * @f_ra:       file's readahead state,includes some info such start,size,mmap_miss,etc
          * @f_version:  automatically increased after each use
          * @f_security: security mechanism related info
          * @private_data:    private data specified by filesystem or device driver
          * @f_ep_links:  used by fs/eventpoll.c to link all the hooks to this file
          * @f_mapping:  address space where the content of this file mapping to
          * @f_mnt_write_state:    write counter for debug
          */
         struct file {
           union {
                   struct list_head fu_list;
                   struct rcu_head fu_rcuhead;
           } f_u;

           struct path f_path;
         #define f_dentry f_path.dentry
         #define f_vfsmnt f_path.mnt

           const struct file_operations *f_op;
           spinlock_t f_lock;
           atomic_long_t f_count;
           unsigned int f_flags;
           fmode_t f_mode;
           loff_t f_pos;
           struct fown_struct f_owner;
           const struct cred *f_cred;
           struct file_ra_state f_ra;
           
           u64 f_version;

         #ifdef CONFIG_SECURITY
           void *f_security;
         #endif

           void *private_data;

         #ifdef CONFIG_EPOLL
           struct list_head f_ep_links;
         #endif
           struct address_space *f_mapping;
         #ifdef CONFIG_DEBUG_WRITECOUNT
           unsigned long f_mnt_write_state;
         #endif

         };

         /**
          * spinlock used to protect @fu_list member,that is all file objects are linked together,
          * if you want to modify @fu_list,must lock this spinlock at first,and release it later
          */
         extern spinlock_t files_lock;

         /* filp_open - open a file through given pathname,open flags,and mode,returns
          *             a pointer points to the file object,returns error pointer when failed
          * # this function creates a file object via call to get_empty_filp(),which is defined in
          *   <fs/file_table.c> and it use kmem_cache_zalloc() to allocates a new file object from
          *   filp slab cache,the new file object will be initialized by using current task's cred
          */
         extern struct file *filp_open(const char *pathname, int flags, int mode);
      
       a slab cache named filp is used to handles file object allocating,its descriptor address
       is stored in the filp_cachep variable.

       structure files_stat_struct is defined in the same header and it described file amount limits.
       @nr_files means number of files that are using;@nr_free_files means number of files that opened
       but not in using;@max_files means the maximum number of files that could be opened.(the maximum
       number of file objects that could be allocated)

       <linux/fs.h>
         /**
          * file_operations - common file interfaces defined by VFS,un-support feature should be set to
          *                   NULL
          * # member @owner is always initialized with the value THIS_MODULE,it is used to manage the module's
          *   usage count.when a file associated with the file_operations structure is opened,the kernel calls
          *   try_module_get() on the @owner to increases its usage count before invokes open() method,
          *   this mechanism prevents the module from being unloaded while its operations are acrively being
          *   used by open files
          * # the first time initialization of a file's @f_op member is initialized with the same value of
          *   its inode object's @i_fop
          *   when a inode is loaded from disk,the filesystem driver specifies the file operation implementations
          *   in @i_fop,but VFS is able to modify the file operations later
          *   (actually,when a filesystem is installed,the pre-defined file operation implementations is also 
          *    loaded into memory)
          */
         struct file_operations {
                 struct module *owner;
                
                 loff_t (*llseek)(struct file *, loff_t, int);
                 ssize_t (*read)(struct file *, char __user *, size_t, loff_t *);
                 ssize_t (*write)(struct file *, const char __user *, size_t, loff_t *);
                 ssize_t (*aio_read)(struct kiocb *, const struct iovec *, unsigned long, loff_t);
                 ssize_t (*aio_write)(struct kiocb *, const struct iovec *, unsigned long, loff_t);
                 int (*readdir)(struct file *, void *, filldir_t);
                 unsigned int (*poll)(struct file *, struct poll_table_struct *);
                 int (*ioctl)(struct inode *, struct file *, unsigned int, unsigned long);
                 long (*unlocked_ioctl)(struct file *, unsigned int, unsigned long);
                 long (*compat_ioctl)(struct file *,unsigned int, unsigned long);
                 int (*mmap)(struct file *, struct vm_area_struct *);
                 int (*open)(struct inode *, struct file *);
                 int (*flush)(struct file *, fl_owner_t id);
                 int (*release)(struct inode *, struct file *);
                 int (*fsync)(struct file *, struct dentry *, int datasync);
                 int (*aio_fsync)(struct kiocb *, int datasync);
                 int (*fasync)(int, struct file *, int);
                 int (*lock)(struct file *, int, struct file_lock *);
                 ssize_t (*sendpage)(struct file *, struct page *, int, size_t, loff_t *, int);
                 unsigned long (*get_unmapped_area)(struct file *, unsigned long, unsigned long,
                                                    unsigned long, unsigned long);
                 int (*check_flags)(int);
                 int (*flock)(struct file *, int, struct file_lock *);
                 ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, loff_t *, size_t, unsigned int);
                 ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int);
                 int (*setlease)(struct file *, long, struct file_lock **);
         };
       
     Dentry Objects :
       dentry object is the form of directory implemented by VFS,once a specific directory entry is read into memory,
       VFS transforms it to a dentry object.
       the kernel creates a dentry object for every component of a pathname that a process looks up,the dentry object
       associates the component to its coressponding inode.
       ! dentry object no corresponding image on disk,so it no the concept about dirty.
       ! slab cache named dentry_cache(a slab cache descriptor pointer) which is defined in <fs/dcache.c>,and function
         d_alloc() used to allocate a dentry object.

       <linux/dcache.h>
         /**
          * qstr - quick string,eases parameter passing,but more importantly saves "metadata" about
          *        the string
          * @hash: hash value
          * @len:  string length
          * @name: the name
          */
         struct qstr {
                 unsigned int hash;
                 unsigned int len;
                 const unsigned char *name;
         };

         #ifdef CONFIG_64IT
         #define DNAME_INLINE_LEN_MIN 32    /* 192B */
         #else
         #define DNAME_INLINE_LEN_MIN 40    /* 128B */
         #endif

         /**
          * dentry - VFS directory entry object,used to represents the components from pathname
          * @d_count:    usage counter
          * @d_flags:    dentry flags
          * @d_lock:     concurrent protection
          * @d_mounted:  for directories,counter for the number of filesystems mounted on this dentry
          * @d_inode:    the corresponding inode
          * @d_hash:     dentry hash list
          * @d_parent:   parent dentry of this one
          * @d_name:     filename
          * @d_lru:      dentry LRU list
          * @d_child:    for directories,pointers for the list of directory dentries in the same parent directory
          *              # child of parent list
          * @d_rcu:      RCU protection
          * @d_subdirs:  children(subdirectories) of this one
          * @d_alias:    inode alias list,that is the pointers for the list of dentries(in-use)
          *              associated with the same inode(alias)
          * @d_time:     used by d_revalidate() */
          * @d_op:       dentry operations
          * @d_sb:       the root of the dentry tree,the super block object of the file
          * @d_fsdata:   filesystem denpendent data
          * @d_iname:    short filename
          * # dentry can be a directory,or be a file;if it is a directory,then it might contains some other dentries
          */
         struct dentry {
                 atomic_t d_count;
                 unsigned int d_flags;
                 spinlock_t d_lock;
                 int d_mounted;
                 struct inode *d_inode;
                 struct hlist_node d_hash;
                 struct dentry *d_parent;
                 struct qstr d_name;
                 struct list_head d_lru;
                 
                 union {
                         struct list_head d_child;
                         struct rcu_head d_rcu;
                 } d_u;

                 struct list_head d_subdirs;
                 struct list_head d_alias;
                 unsigned long d_time;
                 const struct dentry_operations *d_op;
                 struct super_block *d_sb;
                 void *d_fsdata;
                
                 unsigned char d_iname[DNAME_INLINE_LEN_MIN];
         };

         four states of a dentry :
           Free
             dentry contains no valid info and it is not used by VFS
             the corresponding memory area is handled by the slab allocator
           Unused
             dentry is not currently used by the kernel,@d_count is _zero_,but @d_inode is not NULL
             the dentry contains valid info but its contents may be discarded if necessary in order to
             reclaim memory
           In use
             dentry is currently used by the kernel,@d_count is positive,@d_inode is not NULL
             dentry contains valid info and can not be discarded
           Negative
             the inode associated with the dentry does not exist,either because the corresponding disk
             inode has been deleted or because the dentry object was created by resolving a pathname of a
             nonexistent file
             @d_inode is NULL,but this dentry still remains in the dentry cache for speed up further same file
             pathname resolving

       <linux/dcache.h>
         /**
          * dentry_operations - common dentry interfaces defined by VFS,filesystem can provides its own
          *                     implementations,un-supported feature set to NULL;if filesystem does not
          *                     provide implementations,then VFS set them to default functions
          */
         struct dentry_operations {
                 int (*d_revalidate)(struct dentry *, struct nameidata *);
                 int (*d_hash)(struct dentry *, struct qstr *);
                 int (*d_compare)(struct dentry *, struct qstr *, struct qstr *);
                 int (*d_delete)(struct dentry *);
                 void (*d_release)(struct dentry *);
                 void (*d_iput)(struct dentry *, struct inode *);
                 char *(*d_name)(struct dentry *, char *, int);
         };

       dentry object locking rules for dentry operations :
                             big lock        dcache_lock        d_lock        may block
         d_revalidate        N               N                  N             Y
         d_hash              N               N                  N             Y
         d_compare           N               Y                  Y             N
         d_delete            N               Y                  N             N
         d_release           N               N                  N             Y
         d_iput              N               N                  N             Y

         /**
          * @dcache_lock object is type of spinlock_t which is used to protect dentry cache
          * concurrently accessing on SMP,it will be putting into hardware cache line for
          * speed up accessing.
          */

     The dentry Cache :
       Linux use dentry cache to speed up dentry object accessing which is constructed and will be
       accessed later.
       Linux dentry cache consists two kinds of data structures :
         1> a set of dentry objects in the in-use,unused,or negative state
         2> a hash table to derive the dentry object associated with a given filename and a given
            directory quickly
       ! because the dentries are in-use or are unused still valid and would not be destroyed by kernel,
         such dentry is associated with a inode,thus dentry cache can be used to speed up inode
         searching.

       All "unused" dentries are included in a doubly linked "Least Recently Used"(LRU) list sorted by time
       of insertion. /* last released dentry is put in front of the list */
       !! LINUX 2.6.31 DOEST NOT EXIST VARIABLE NAMED "dentry_unused",ALL UNUSED DENTRIES ARE CHAINED TOGETHER
          IN THE SUPER BLOCK OBJECT'S @s_dentry_lru.
          THE MEMBER @s_dentry_lru IS THE HEAD OF DENTRY LRU LIST.
       Each "in-use" dentry is included in the inode's @i_dentry,that is each inode could be associated with
       several hard links.
       An dentry becomes negative if the last hard link to the file been deleted,the dentry will be moved into
       dentry LRU list. /**
                         * negative dentries would be moved towards to the tail of LRU list when kernel shrinks
                         * the dentry cache,they are gradually freed.
                         */
       static variable named @dentry_hashtable is defined in <fs/dcache.c> which is type of struct hlist_head *.
       each dentry is placed into the hash table for efficiency.(@d_hash)
       /**
        * usually,d_hash() calculates the hash value of the given dentry with its parent and the value @d_name.hash
        *                                                                     #   parent directory     filename
        */

     Files Associated with a Process :
       each process has a member named @fs which is type of struct fs_struct *,and the structure is used to represents
       the interactions between this process and a filesystem.

       <linux/fs_struct.h>
         /**
          * fs_struct - interaction between a process and a filesystem
          * @users:     users sharing this filesystem information
          * @lock:      concurrent protect
          * @umask:     file umask
          * @in_exec:   indicates whether the process is in exec calling
          * @root:      root directory of this process
          * @pwd:       process work directory
          * # allocating for struct fs_struct object is handled by slab allocator through the slab cache named
          *   @fs_cachep which is type of struct kmem_cache *
          */
         struct fs_struct {
                 int users;
                 rwlock_t lock;
                 int umask;
                 int in_exec;
                 struct path root, pwd;
         };

         /**
          * path - represents a path such "/../xxxx"
          * @mnt:  VFS mount point,the structure contains all information of
          *        the mount point
          *        e.g.
          *                dentry mount point
          *                root of the mounted tree
          *                pointer to super block of the filesystem
          *                ...
          * @dentry:    dentry for this path
          */
         struct path {
                 struct vfsmount *mnt;
                 struct dentry *dentry;
         };

       <linux/fdtable.h>
         /**
          * fdtable - file descriptor table
          * @max_fds: current maximum number of file descriptor in this table
          * @fd:      current fd array(@fd_array)
          * @close_on_exec:    file descriptor set contains the file descriptors
          *                    they will be closed on exec
          * @open_fds:         file descriptor set contains the file descriptors
          *                    they been opened
          * @rcu:              RCU mechanism used to modify this table
          * @next:             next file descriptor table from fdtable_defer structure
          */
         struct fdtable {
                 unsigned int max_fds;
                 struct file **fd;
                 fd_set *close_on_exec;
                 fd_set *open_fds;
                 struct rcu_head rcu;
                 struct fdtable *next;
         };

         /**
          * embedded_fd_set - represents a small fd set
          */
         struct embedded_fd_set {
                 unsigned long fds_bits[1]; /* 32 OR 64 */
         };

         !! kernel defined a structure __kernel_fd_set which is a larger fd set,and it have been
            typedefed to fd_set in <linux/types.h>.
            the structure __kernel_fd_set is defined in <linux/posix_types.h>,it only contains
            one member @fds_bits.@fds_bits is an array of unsigned long,the maximum number of
            elements is "1024 / (8 * 4)" or "1024 / (8 * 8)".
                        /* 32bit            64bit */

         /**
          * files_struct - represents the files related to a given process,these informations
          *                are contained in @files member of struct task_struct
          * @count:        number of processes sharing this structure
          * @fdt:          pointer to @fdtab,if process have to opens more files,
          *                this pointer may points to a new allocated one
          *                (copy_files() invokes dup_fd() to creates a new fdtable,
          *                 this member of the new one will points to @fdtab when dup_fd()
          *                 is called,and may be points to another new one when expand_fdtable()
          *                 is called)
          * @fdtab:        initial file descriptor table
          * @file_lock:    concurrent protection
          * @next_fd:      next fd number of the open file
          * @close_on_exec_init:    initial files close on exec
          * @open_fds_init:         initial open file descriptor set
          * @fd_array:     file descriptor array used to stores the handlers of the open files
          *                if process needs to open more files,a new fdtable will be allocated and
          *                the old fdtable @fdt will be copied into the new fdtable,finally,@fdt
          *                points the new one,@fdt will be freed by RCU @fdt->rcu
          * # @close_on_exec and @open_fds of struct fdtable are points to @close_on_exec_init and
          *   @open_fds_init respectively,but the process can modifies these pointers later
          * # different fd can points to the same file,because the handlers are pointers,they are
          *   could points to the same file object
          */
         struct files_struct {
                 /* read mostly part */
                 atomic_t count;
                 struct fdtable *fdt;
                 struct fdtable fdtab;

                 /* written part on a separate cache line int SMP */
                 spinlock_t file_lock ____cacheline_aligned_in_smp;

                 int next_fd;
                 struct embedded_fd_set close_on_exec_init;
                 struct embedded_fd_set open_fds_init;
                 struct file *fd_array[NR_OPEN_DEFAULT];
                 /* NR_OPEN_DEFAULT := BITS_PER_LONG */
         };

       !! the fixed maximum number open files is defined by NR_OPEN,but kernel also enforces a dynamic bound on the
          maximum number of file descriptors in @signal->@rlim[RLIMIT_NOFILE] structure of the process descriptor,
          thus process can open more files than NR_OPEN if and only if the number does not exceeds hard limit.
       !! function alloc_fdtable() will invokes alloc_fdmem() to allocate memory for @open_fds and @close_on_exec.
          @open_fds points to the head of the memory area,and @close_on_exec points to "@mem += @nr / BITS_PER_BYTE",
          the real size of the memory area is max(2 * @nr / BITS_PER_BYTE, L1_CACHE_BYTES) bytes.
          so we can know that @open_fds points to front half area and @close_on_exec pointsto back half area.
          /**
           * @nr been aligned,and if the aligned @nr is greater than @sysctl_nr_open,then it will be set to
           *         (@sysctl_nr_open - 1) | (BITS_PER_LONG - 1) + 1
           */

       <fs/file_table.c>
         /**
          * fget - get the corresponding file object from current task's @files member
          * @fd:   file descriptor
          * return:    pointer to file object OR NULL
          * # EXPORT_SYMBOL
          * # @f_count will be increased
          */
         struct file *fget(unsigned int fd);

         /**
          * fput - put the given file object
          * @file: file object pointer
          * # EXPORT_SYMBOL
          * # @f_count will be decreased.if @f_count becomes _zero_,__fput() will be called with @file
          */
         void fput(struct file *file);

         !! __fput() will destroys the file.
            it calls to release() method,if it is not NULL;@i_writecount will be decreased if this file
            is opened for writing;the file will be removed from super block object's @s_files;the usage
            counter of corresponding dentry and of the filesystem descriptor will be decreased.
            finally,the memory will be deallocated and back to slab allocator.
        
         !! the light version functions fget_light() and fput_light() assume that the current process
            already owns the file object,that is the process has already previously increased the file
            object's reference counter.
            /* fget_light() is defined in <fs/file_table.c>,and fput_light() is defined in <linux/file.h>. */

     Filesystem Types :
       Before using a filesystem type,must process filesystem registration.

       Special Filesystems :
                            mount point                description
         bdev               none                       block devices
         binfmt_misc        any                        miscellaneous executable formats
         devpts             /dev/pts                   pseudo terminal support
         eventpollfs        none                       used by the efficient event polling mechanism
         futexfs            none                       used by the futex mechanism
         pipefs             none                       pipes
         proc               /proc                      general access point to kernel data structures
         rootfs             none                       provides an empty root directory for the bootstrap phase
         shm                none                       IPC-shared memory regions
         mqueue             any                        used to implement POSIX message queues
         sockfs             none                       sockets
         sysfs              /sys                       general access point to system data
         tmpfs              any                        temporary files
         usbfs              /proc/bus/usb              USB devices

         "none": this filesystem is not provided for user,it is used by kernel to reuse some VFS layer code
         "any": this filesystem have no fixed mount point,it can be freely mounted and used by the user

         ! special filesystems are not bound to physical block devices.
           but,the kernel assigns to each mounted special filesystem a fictitious block device that has
           _zero_ as major number and an arbitrary value as minor number.

         <fs/super.c>
           static DEFINE_IDA(unnamed_dev_ida);
           static DEFINE_SPINLOCK(unnamed_dev_lock);
           static int unnamed_dev_start = 0;

           !! Linux IDR layer provides small id to pointer translation service avoiding
              fixed sized tables.
              its purpose is to provide a robust and performant way to allocate and
              manage integer IDs.
              the header is <linux/idr.h>,and implementation in <lib/idr.c>.

           /**
            * set_anon_super - setup an anonymous super block object,such super block
            * @s:              super block object
            * @data:           filesystem private data
            * return:          0 OR error code
            * # EXPORT_SYMBOL
            */
           int set_anon_super(struct super_block *s, void *data);

           brief description for set_anon_super() :
             generally,kernel call to this function to setup an anonymous super block for special filesystems,
             that is the filesystem used by kernel to reuse VFS code.
             first,this routine call to ida_pre_get() on @unnamed_dev_ida with gfp flag GFP_ATOMIC,if failed,
             returns -ENOMEM to caller.
             if the prepare works been finished,then lock @unnamed_dev_lock and call to ida_get_new_above()
             with arguments &@unnamed_dev_ida,@unnamed_dev_start,&@dev /* @dev is a local variable */.
             the function allocates a new ID above or equal to @unnamed_dev_start and stores it in @dev.
             @unnamed_dev_start will be increased by set_anon_super() when no error was encountered in
             ida_get_new_above().but if any error was detected,it will release the lock and retry the phase
             from ida_pre_get().
             /**
              * @dev can not equal to "1 << MINORBITS(the value is 20)",otherwise,remove the connection between
              * @unnamed_dev_ida and @dev,then set @unnamed_dev_start to @dev if it is greater than @dev,
              * finally returns -EMFILE to caller.
              */
             after we got an appropriate minor number,set the super block object's @s_dev member to the
             device number (0, @dev & MINORMASK).@s_bdi is set to &@noop_backing_dev_info,that is no backing
             device.
             return _zero_ to caller means succeed to setup anonymous super block.
           
           /**
            * kill_anon_super - does the reversing operations to set_anon_super() on an anonymous super block object
            * @sb:              anonymous super block object
            * # EXPORT_SYMBOL
            * # this function will remove the connection between @unnamed_dev_ida and the minor number of @sb
            *   if its minor number is less than @unnamed_dev_start,set @unnamed_dev_start to the minor number
            */
           void kill_anon_super(struct super_block *sb);

         !! EVEN SPECIAL FILESYSTEM IT STILL HAS AN ANONYMOUS SUPER BLOCK USED TO REPRESENTS IT.

       Filesystem Type Registration :
         the VFS must keep track of all filesystem types whose code is currently included in the kernel,it
         does this by performing filesystem type registration.
         each registered filesystem is represented as a file_system_type object.
         /**
          * as showing above,super block object contains a member named @s_type which is
          * type of file_system_type *
          */
          
         <linux/fs.h>
           /**
            * file_system_type - represents the registered file system
            * @name:             filesystem name
            * @fs_flags:         filesystem type flags
            * @get_sb:           method for reading a super block
            *                    the routine is depends on the real filesystem,and it 
            *                    allocate and initialize(if necessary,reading from disk) a
            *                    new super block
            * @kill_sb:          method for removing a super block(for destroying)
            * @owner:            pointer to the module implementing the filesystem,
            *                    and it can be used to prevent unload filesystem when
            *                    someone is using it
            * @next:             pointer to the next element in the list of filesystem types
            * @fs_supers:        head of a list of super block objects having the same
            *                    filesystem type
            *                    # the head is dummy element
            * @s_lock_key:       runtime locking correctness validator for super block locking
            * @s_umount_key:                                           for super block umount
            * @i_lock_key:                                             for inode locking
            * @i_mutex_key:                                            for inode mutex
            * @i_mutex_dir_key:                                        for directory inode mutex
            * @i_alloc_sem_key:                                        for inode allocaing semaphore
            */
           struct file_system_type {
                   const char *name;
                   int fs_flags;

                   int (*get_sb)(struct file_system_type *, int, const char *, void *, struct vfsmount *);
                   void (*kill_sb)(struct super_block *);

                   struct module *owner;
                   struct file_system_type *next;
                   struct list_head fs_supers;

                   struct lock_class_key s_lock_key;
                   struct lock_class_key s_umount_key;

                   struct lock_class_key i_lock_key;
                   struct lock_class_key i_mutex_key;
                   struct lock_class_key i_mutex_dir_key;
                   struct lock_class_key i_alloc_sem_key;
           };

           /* publoc flags for file_system_type */

           /* every filesystem of this type must be located on a physical disk device */
           #define FS_REQUIRES_DEV 1

           /* the filesystem uses binary mount data */
           #define FS_BINARY_MOUNTDATA 2

           /* filesystem has subtype */
           #define FS_HAS_SUBTYPE 4

           /* always revalidate the "." and ".." paths in the dentry cache */
           #define FS_REVAL_DOT 16384

           /* filesystem will handle d_move() during rename() internally */
           #define FS_RENAME_DOES_D_MOVE 32768

         <fs/filesystems.c>
           /* the first element of the list for registered filesystems in the Linux kernel */
           static struct file_system_type *file_systems;
            
           /* lock for @file_systems */
           static DEFINE_RWLOCK(file_systems_lock);

         <linux/fs.h>
           /**
            * register_filesystem - register a given file system on VFS
            * @fs:                  filesystem type
            * return: 0 => succeed
            *         -EBUSY => only permit one file system registration in once calling
            *                   (@fs->next is TRUE)
            *                   OR
            *                   been registered
            * # EXPORT_SYMBOL
            * # during kernel initializing,all file system type specified at compile time will be
            *   registered on VFS,and can not be unregistered
            */
           extern int register_filesystem(struct file_system_type *fs);
          
           /**
            * unregister_filesystem - unregister a file system
            * @fs:                    the file system to be unregistered
            * return:                 0 => succeed
            *                         -EINVAL => invalid @fs
            *                                    OR
            *                                    no such filesystem been registered
            * # EXPORT_SYMBOL
            * # if the file system is implemented by a linux kernel module,
            *   then register_filesystem() will be called when the module is load,
            *   and unregister_filesystem() will be called when the module is unload
            */
           extern int unregister_filesystem(struct file_system_type *fs);

           /**
            * get_fs_type - lookup a specified filesystem from @file_systems
            * @name:        the file system name to be matched
            * return:       pointer to the file_system_type object OR NULL
            * # EXPORT_SYMBOL
            * # this function will attempts to find out dot character in the @name string,
            *   that is because the file system may be a subtype,in this case,its name contains
            *   dot character,but we have to get its primary type.
            *   static function __get_fs_type() is used to traverse list.
            *   if find out such file system but dot been detected in @name and the file system
            *   does not enable flag FS_HAS_SUBTYPE,then NULL will be return to caller.
            *   if no such file system but the file system is implemented by a module and kernel
            *   can load the module,then call to __get_fs_type() again after the module is loaded.
            *   finally,returns the pointer to caller.
            */
           extern file_system_type *get_fs_type(const char *name);

           !! these functions are defined in <fs/filesystems.c>.

         figure :
           /* relation between file system type and the super block objects */
        
           +----@file_systems
           |
           V
           +------+  +------+  +------+
           | fst0 |->| fst1 |->| fst2 |-> ...
           +------+  +------+  +------+
           | | |     |         | |
           | | |     +-sbX   | +-sbT
           | | |               +-sbK
           | | +-sbC
           | +-sbB
           +-sbA

           /* file system type and super block is associated by struct super_block.@s_type */
           
           /* linked together by struct super_block.@s_instances */
           sbA <-> sbB <-> sbC <-> sbA
           sbX
           sbK <-> sbT <-> sbK

     Filesystem Handling :
       traditional Unix and Linux make use of a system's root filesystem("/"),it is the filesystem that is directly
       mounted by the kernel during the booting phase and that holds the system initialization scripts and
       the most essential system programs.
       /* other filesystems can be mounted by initialization scripts or by users.
        * by users - mounted on the directories of already mounted filesystem
        */

       ! every filesystem has its own root directory.
         directory on which a filesystem is mounted is called mount point.
         mounted filesystem is a child of the mounted filesystem to which the mount point directory belongs.
         root of a mounted filesystem hides the content of the mount point directory of the parent filesystem.
         /* include whole subtree of the directory */

       Namespaces :
         traditional Unix has only one tree of mounted filesystems,that is starting from the system root directory.
         /* process can access every file in a mounted filesystem */
         Linux 2.6 refined this :
           every process might have its own tree of mounted filesystems,so called namespace of the process.

         generally,processes share the same namespace,that is system root tree,because process "init" used it.
         a process can have a new namespace through set CLONE_NEWNS flag of clone(). /* since Linux 2.4.19 */
         ! the new ns is initialized with a copy of parent's namespace.
           if no CLONE_NEWNS has set,child shares the namespace of its parent process.
           # can not specify CLONE_NEWNS and CLONE_FS in the same clone call.
         when a process mounts/unmounts a filesystem,it only modifies its namespace,the change is visible to
         all processes that share the same namespace.
         ! system call pivot_root() can change the root filesystem.

         a member named @nsproxy is type of struct nsproxy *,which contains pointers to all per-process namespaces.
         /**
          * uts_namespace - Unix Timesharing System(provides system identifiers)
          * ipc_namespace - SystemV IPC
          * mnt_namespace - filesystem mounting
          * pid_namespace - process identifier
          * net           - network namespace
          */

         <linux/nsproxy.h>
           /**
            * nsproxy - task namespace set
            * @count:   number of tasks holding a reference
            * # if tasks share all namespaces,then they share the same namespace proxy,
            *   and namespace proxy will be copied as soon as a single namespace is cloned or
            *   unshared
            */
           struct nsproxy {
                   atomic_t count;
                   struct uts_namespace *uts_ns;
                   struct ipc_namespace *ipc_ns;
                   struct mnt_namespace *mnt_ns;
                   struct pid_namespace *pid_ns;
                   struct net *net_ns;
           };

         <linux/mnt_namespace.h>
           /**
            * mnt_namespace - process filesystem namespace
            * @count:         reference counter
            *                 # new allocated mnt_ns.@count is 1
            *                 # its value increased by get_mnt_ns(),
            *                   and decreased by put_mnt_ns()
            * @root:          VFS mount point
            *                 # process root directory can be different to filesystem's
            *                   root directory,member @root of fs_struct corresponding to
            *                   the "/" pathname,but this can be changed by chroot() system
            *                   call
            *                   
            * @list:          head of the list for all mounted filesystems that belong to this
            *                 namespace
            * @poll:          for poll mechanism
            * @event:         poll event indicator
            */
           struct mnt_namespace {
                   atomic_t count;
                   struct vfsmount *root;
                   struct list_head list;
                   wait_queue_head_t poll;
                   int event;
           };

           About @list :
           
                     +---------------------+
                     | mnt_namespace.@list |
                     +---------------------+
                                |
                                V
                     +---------------------+     +---------------------+
             ... <-> | vfsmount0.@mnt_list | <-> | vfsmount1.@mnt_list | <-> ...
                     +---------------------+     +---------------------+

         <fs/namespace.c>
           /**
            * namespace_sem - rw semaphore for filesystem related namespace concorrect operations
            */
           static struct rw_semaphore namespace_sem;

           /**
            * alloc_mnt_ns - attempts to allocate a new mnt_ns from general slab cache with
            *                gfp flag GFP_KERNEL
            * return:        pointer to the new allocate mnt_ns object or NULL
            * # this routine initializes all members inside to structure mnt_namespace,and it
            *   set mnt_namespace.@count to 1,mnt_namespace.@event to 0
            */
           static struct mnt_namespace *alloc_mnt_ns(void);

       Filesystem Mounting :
         traditional Unix system can only mount a filesystem once,but Linux is different.it is possible
         to mount the same filesystem several times,and of course,several mount points would be created.
         if the filesystem have been mounted more than once,then all mount points will share the same
         super block object.

         Linux mounting :
                                                 +----> system root
                                                 /
                         bin    boot    dev    etc    home    lib    lib64    mnt    sbin    proc    ...
                         /      /       /      /      /       /      /        /      /       /
                       ...    ...     ...    ...    Subdir  ...     ...     ...     ...     ...
                              +                      +
                              |                      |
                              |                      V
                              |           VFS mount point for a Ext4 filesystem (device: /dev/sdb1)
                              |           /* mount point : /home/Subdir */
                              |                      /
                              |                    ForSDB2
                              |                      +
                              |                      |
                              |                      V
                              |           VFS mount point for a XFS filesystem (device: /dev/sdb2)
                              |           /* mount point : /home/Subdir/ForSDB2 */
                              |                      /
                              |                     ...
                              V
                         Boot partition (device: /dev/sda1)
                         /* mount point : /boot */

         
         Linux allows filesystem stacking,the newly mounted filesystem will hides the previous filesystem
         mounted on the same mount point.the processes already using the old mounting can continue to do so.
         Linux have to keep track of mounted filesystems and the relationship between the filesystem to be
         mounted and the other mounted filesystems,structure vfsmount used to stores the necessary infos.

         <linux/mount.h>

           /**
            * mount_hashtable - vfs mount point hash table indexed by the address of vfsmount descriptor
            *                   of the parent filesystem and the address of the dentry object of the mount
            *                   point directory
            * # each element is the head of the list for vfsmount objects their hash value is the same
            */
           static struct list_head *mount_hashtable __read_mostly;

           /* vfsmount_lock - spinlock used for vfsmount related operations,inplace of dcache_lock */
           __cacheline_aligned_in_smp DEFINE_SPINLOCK(vfsmount_lock);

           /* vfsmount initial identifiers */
           static int mnt_id_start = 0;
           static int mnt_group_start = 1;

           /* for vfsmount ID management */
           static DEFINE_IDA(mnt_id_ida);
           static DEFINE_IDA(mnt_group_ida);

           /**
            * mnt_cache - slab cache for vfsmount object,a new vfsmount object is allocated by
            *             call to alloc_vfsmnt() routine
            */
           static struct kmem_cache *mnt_cache __read_mostly;

           /**
            * vfsmount - VFS mount point,that is the mounted filesystem
            * @mnt_hash: mount point hash list stores the pointers to the vfsmounts they have the same
            *            hash value
            * @mnt_parent:    this mount point's parent,the fs this one mounted on
            * @mnt_mountpoint:    dentry of mount point
            * @mnt_root: root of the mounted tree
            * @mnt_sb:   super block of this fs
            *
            * @mnt_mounts:    list of children,anchored here
            * @mnt_child:     and going through their @mnt_child
            *                 # list @mnt_mounts of parent stored the address of @mnt_child of 
            *                   children's
            *                 # list @mnt_child is the real children list
            *
            * @mnt_flags:    mounting flags
            * @mnt_devname:    name of device
            * @mnt_list: list of VFS mount points belong to the same namespace
            * @mnt_expire:    link in fs-specific expiry list
            * @mnt_share:    list of shared mounts
            * @mnt_slave_list:    list of slave mounts
            * @mnt_slave:    slave list entry
            * @mnt_master:    slave is on master->@mnt_slave_list
            * @mnt_ns:   namespace this mount point belongs to
            * @mnt_id:   mount identifier
            * @mnt_group_id:    peer group identifier
            * @mnt_count:    usage counter
            * @mnt_expiry_mark:    expiry mark
            * @mnt_pinned:    pinned vfs mount point can not be removed immediately
            *                 # this will be checked in mntput_no_expire(),
            *                   routines mnt_pin() and mnt_unpin() pins and unpins
            *                   this vfs mount point,respectively
            * @mnt_ghosts:    number of mount points under the this VFS mount point
            *                 been unmounted but yet destoryed
            *                 # this field may be increased in umount_tree() and be
            *                   decreased in release_mounts()
            * @mnt_writers:    number of task got write access to this mount point
            *
            * # when a new vfsmount object is created,its @mnt_count set to 1,and
            *   an ID(Linux idr) distributed through mnt_alloc_id() will associated
            *   with it,memory for @mnt_devname will be allocated from generic slab
            *   cache and the content is copied from parameter @name which is type of
            *   const char *
            * # vfsmount object is allocated through alloc_vfsmnt(),destroyed through
            *   free_vfsmnt();routine lookup_mnt() attempts to find out a vfsmount object
            *   by a given path structure pointer,if such vfmount is existing,then returns
            *   it to the caller with increased reference count
            */
           struct vfsmount {
                   struct list_head mnt_hash;
                   struct vfsmount *mnt_parent;
                   struct dentry *mnt_mountpoint;
                   struct dentry *mnt_root;
                   struct super_block *mnt_sb;
                   struct list_head mnt_mounts;
                   struct list_head mnt_child;
                   int mnt_flags;
                   const char *mnt_devname;
                   struct list_head mnt_list;
                   struct list_head mnt_expire;
                   struct list_head mnt_share;
                   struct list_head mnt_slave_list;
                   struct list_head mnt_slave;
                   struct vfsmount *mnt_master;
                   struct mnt_namespace *mnt_ns;
                   int mnt_id;
                   int mnt_group_id;
                   atomic_t mnt_count;
                   int mnt_expiry_mark;
                   int mnt_pinned;
                   int mnt_ghosts;
           #ifdef CONFIG_SMP
                   int __percpu *mnt_writers;
           #else
                   int mnt_writers;
           #endif
           };

           /* VFS mount point flags */
           #define MNT_NOSUID 0x01
           #define MNT_NODEV 0x02
           #define MNT_NOEXEC 0x04
           #define MNT_NOATIME 0x08
           #define MNT_NODIRATIME 0x10
           #define MNT_RELATIME 0x20
           #define MNT_READONLY 0x40
           #define MNT_STRICTATIME 0x80
           #define MNT_SHRINKABLE 0x100
           #define MNT_WRITE_HOLD 0x200
           #define MNT_SHARED 0x1000
           #define MNT_UNBINDABLE 0x2000

     Mounting a Generic Filesystem :
       shell command "mount" is used to mount a filesystem,it is implemented through call to libc
       wrapper mount(),the wrapper is call to system call sys_mount(),and finally,do_mount() will
       be called.

       <fs/namespace.c>
         /**
          * sys_mount - system call service rountine for mount()
          * @dev_name:  device name on which the filesystem been installed,can be NULL for network fs
          * @dir_name:  mount point pathname
          * @type:      filesystem type,it must been registred in the kernel
          * @flags:     mounting flags
          * @data:      filesystem-dependent data
          * return:     0 OR error code
          * # the flags supported by sys_mount() can be found in manual page
          */
         SYSCALL_DEFINE5(mount, char __user *, dev_name, char __user *, dir_name, char __user *,
                         type, unsigned long, flags, void __user *, data);
         => asmlinkage long sys_mount(char __user *dev_name, char __user *dir_name, char __user *type,
                                      unsigned long flags, void __user *data);

           flags refered from manual page :
             MS_DIRSYNC                write operations on directories are immediate
             MS_LAZYTIME               lazy update timestamp of disk file
             (since Linux 4.0)
             MS_MANDLOCK               mandatory locking allowed
             MS_NOATIME                no atime
             MS_NODEV                  dont allow accesss to device files on this filesystem
             MS_NODIRATIME             no directory atime
             MS_NOEXEC                 disallow program execution
             MS_NOSUID                 forbid set uid and set gid
             MS_RDONLY                 read only
             MS_REC                    used in onjunction with MS_BIND to create a recrusive bind mount
             MS_RELATIME               update accessed file's atime only if the current value is less than
                                       or equal to mtime
             MS_SLIENT                 dont print warning messages in kernel log
             MS_STRICTATIME            always update atime when files on this filesystem are accessed
             MS_SYNCHRONOUS            make writes on this filesystem synchronous
             MS_NOSYMFOLLOW            dont follow symbol link when resolving path
             (since Linux 5.10)
             MS_REMOUNT                remount a mounted filesystem
             MS_BIND                   bind mount,a bind mount makes a file or a directory subtree visible
                                       at anoter point within the single directory hierarchy
             MS_SHARED                 make this mount shared
             MS_PRIVATE                make this mount private
             MS_SLAVE                  convert this one to a slave mount if it is a shared mount that is a
                                       member of a peer group
                                       that contains other members
             MS_UNBINDABLE             make this mount unbindable
             MS_MOVE                   move mount point(moving is atomic)

         brief description for sys_mount() :
           this routine's purpose it prepares environment for call to do_mount().
           it copies @dev_name,@dir_name,@data to the Kernel Mode address space,memory allocating is
           necessary.
           for @dir_name,memory is allocated from slab cache named @names_cachep;for @data,a page will
           be allocated;for others,memory is allocated from general slab cache with gfp flag GFP_KERNEL.
           if everything is OK,call to do_mount() with the new memory addresses and returns the result
           of do_mount() to caller.
           /**
            * whenever encountered errors or after do_mount() returned,the memory areas allocated
            * previously will be deallocated.
            */

         /**
          * do_mount - mount a speficied filesystem
          * @dev_name: string of device name
          * @dir_name: string of mount point
          * @type_page:    string of filesystem type
          * @flags:    mounting flags
          * @data_page:    address to a page where contains filesystem-dependent data
          * return:    0 OR error code
          */
         long do_mount(char *dev_name, char *dir_name, char *type_page,
                       unsigned long flags, void *data_page);

         brief description for do_mount() :
           do_mount() checks parapeters and flags to determines what next to do.
           first,it discard magic in @flags,if MS_MGC_VAL been matched.
           second,checks whether parameters are valid,returns -EINVAL when encountered invalid params.
           next,try to get an object is type of struct path through kern_path() routine from the parameter
           @dir_name.
           then performs security checking about super block mounting through security_sb_mount().
           and now,seperate the per-mountpoint flags.
           setup a local variable named @mnt_flags :
             set MNT_NOSUID <-> MS_NOSUID
             set MNT_NODEV  <-> MS_NODEV
             set MNT_NOEXEC <-> MS_NOEXEC
             set MNT_NOATIME <-> MS_NOATIME
             set MNT_NODIRATIME <-> MS_NODIRATIME
             disable MNT_RELATIME | MNT_NOATIME <-> MS_STRICTATIME
             set MNT_READONLY <-> MS_RDONLY

            disable MS_NOSUID,MS_NOEXEC,MS_NODEV,MS_ACTIVE,MS_NOATIME,MS_NODIRATIME,MS_RELATIME,MS_KERNMOUNT,
            MS_STRICTATIME in parameter @flags.
            after finished flags separating,we can determine what next action should to be executed,this is
            indicated by @flags.
            if MS_REMOUNT
                    do_remount(&@path, @flags & MS_REC, @mnt_flags, @data_page);
            else if MS_BIND
                    do_loopback(&@path, @dev_name, @flags & MS_REC);
            else if MS_SHARED | MS_PRIVATE | MS_SLAVE | MS_UNBINDABLE
                    do_change_type(&@path, @flags);
            else if MS_MOVE
                    do_move_mount(&@path, @dev_name);
            else
                    do_new_mount(&@path, @type_page, @flags, @mnt_flags, @dev_name, @data_page);
            finally,call to path_put() to put the dentry accquired previously by kern_path(),and returns
            the return value get from if-else action to caller.
            /**
             * path_put() is defined in <fs/namei.c>,which call to dput() to put the dentry object of the path,
             * and call to mntput() to put the vfsmount object of the path.
             */

         /**
          * do_new_mount - create a new mount for userspace and request it to be added into the namespace's tree
          * @path:         struct path object used to indicates mount point path
          * @type:         filesystem type string
          * @flags:        flags
          * @mnt_flags:    mounting flags
          * @name:         name string
          * @data:         filesystem-dependent data
          * return:        0 OR error code
          * # this function's body is very short,there are three steps it must to done,
          *   first,checks CAP_SYS_ADMIN,only administrator can mount a filesystem,
          *   second,lock kernel and call to do_kern_mount(),
          *   third,returned with call to do_add_mount(),
          *   ! this function locks the kernel through macro function lock_kernel(),and it will be expanded to
          *     _lock_kernel(__func__, __FILE__, __LINE__) /* these are C compiler macros */;function
          *     _lock_kenrel_() is defined in <lib/kernel_lock.c>,which will disable preemption at first then
          *     try to lock up spinlock
          *     @kernel_flag,function unlock_kernel() is used to release kernel lock
          *   ! function do_kern_mount() process the main mounting work through vfs_kern_mount()
          *   ! function do_add_mount() adds a mount into a namespace's mount tree
          *   ! @flags as argument for do_kern_mount(),@mnt_flags as argument for do_add_mount()
          */
         static int do_new_mount(struct path *path, char *type, int flags,
                                 int mnt_flags, char *name, void *data);

         /**
          * do_add_mount - add a mount into a namespace's mount tree
          * @newmnt:       newly allocated VFS mount point
          * @path:         the path @newmnt will be mounted on
          * @mnt_flags:    mounting flags
          * @fslist:       special expiration list,if this parameter is not NULL,
          *                &@newmnt->mnt_expire will be inserted into this list
          * return:        0 OR error code
          */
         int do_add_mount(struct vfsmount *newmnt, struct path *path, int mnt_flags,
                          struct list_head *fslist);

         brief description for do_add_mount() :
           first,it disables MNT_SHARED,MNT_WRITE_HOLD,MNT_INTERNAL in @mnt_flags.
           next,acquires @namespace_sem for writing.
           a while-cycle with condition "d_mountpoint(@path->dentry) AND follow_down(@path)"
           is used to get the path object of the last component in the path.
           mntput(@newmnt) and return to caller with -EINVAL,if @mnt_flags forbid
           MNT_SHRINKABLE AND the namespace of @mnt of the path is not same as current task's namespace.
           @mnt_sb of vfsmount of the path can not same as @newmnt->mnt_sb,@mnt_root of vfsmount of
           the path can not same as the path's dentry object,because the same filesystem on the same
           mount point is refused.in this case,returns -EBUSY to caller.
           @newmnt's root dentry can not be a symbol link,otherwise,returns -EINVAL to caller.
           now we prepare to add the mount,set @newmnt->@mnt_flags to @mnt_flags,add the mount
           into namespace's mount tree through routine graft_tree(@newmnt, @path).
           @mnt_parent and @mnt_mountpoint of @newmnt will be set to the @mnt of @path and
           the @dentry of @path,respectively.
           if @fslist is not NULL,then insert &@newmnt->mnt_expire to @fslist.
           finally,release writing for @namespace_sem,return _zero_ to caller.

       <fs/super.c>
         /**
          * do_kern_mount - this function gets the given filesystem type,invokes vfs_kern_mount() to
          *                 mount a filesystem on VFS,setup subtype for the VFS mount point if subtype
          *                 have existed,return the new VFS mount point object to caller
          * # error code:   -ENODEV => no such filesystem type
          */
         struct vfsmount *do_kern_mount(const char *fstype, int flags, const char *name, void *data);

         /**
          * vfs_kern_mount - mount a filesystem on Linux VFS
          * @type:           filesystem type
          * @flags:          flags
          * @name:           mounting name
          * @data:           filesystem-dependent data
          * return:          0 OR error code
          */
         struct vfsmount *vfs_kern_mount(struct file_system_type *type, int flags, const char *name,
                                         void *data);
         
         what vfs_kern_mount() does :
           1> checks @type,if it is NULL then returns -ENODEV to caller.
           2> allocate a new vfsmount object through alloc_vfsmnt(),@name is the device name.
           3> if failed to allocate new vfsmount object,then returns the error to caller.
           4> if enabled MS_KERNMOUNT in @flags,then sets @mnt_flags of the new vfsmount to MNT_INTERNAL.
           5> if fs-dependent data have given and no FS_BINARY_MOUNTDATA is enabled in filesystem type's
              @fs_flags,then prepare security checking data,a new zeroed page will be allocated through
              routine alloc_secdata().proceed copy @data to @secdata the new memory area,call to
              free_secdata() and deallocate the newly allocated vfsmount object,then to caller
              if failed to copying
           6> use the method get_sb() setup in filesystem type object to get a super block for newly
              allocated vfsmount object;if failed,then free secdata and the vfsmount object then
              return to caller with error code.
              /**
               * if @mnt_sb of the vfsmount object is NULL,then must a BUG encountered,kernel will
               * panic.
               * if the super block's backing devince is NULL,then kernel will print a warning.
               */
           7> do secuiry checking for super block mounting through secuirty_sb_kern_mount().
              if we failed on security checking,then we can not mount this filesystem,so we have to
              put the dcache of @mnt_root,put the super block,free secdata and vfsmount object,return
              to caller with the error code.
           8> now we passed security checking,so we can setup the newly allocated vfsmount object @mnt.
              @mnt->@mnt_mountpoint := @mnt->@mnt_root /* the root is our mount point */
              @mnt->@mnt_parent := @mnt                /**
                                                        * because we have not been added into namespace's
                                                        * mount tree
                                                        */
           9> finally,up_write() @s_umount of the super block object,free secdata,returns @mnt to caller.

         ! get_sb() method of the filesystem type object is depends on the filesystem implementation.
           but the real implementation often has only one statement which invokes to one VFS defined
           get_sb_XXX() routine.
           the routines :
             get_sb_ns          -        for filesystem no super block,such IPC mechanism
             get_sb_bdev        -        for filesystem installed in a disk device
             get_sb_nodev       -        for filesystem no disk device,such autofs,fuse,ramfs
             get_sb_single      -        for filesystem has single mount point,such devptfs,sysfs
             get_sb_pseudo      -        for filesystem with no mount point,such pipefs,libfs

           function get_sb_nodev() call to sget() with setup callback set_anon_super().

           function sget() is invoked by these get_sb_XXX() to get an appropriate super block object which been
           existed in @fs_supers of the filesystem type object or allocates a new super block object and inserts
           it into @fs_supers of the filesystem type object and the global super block list.
           if the super block object is not newly allocated,get_sb_XXX() just returns it as well.
           a callback function named @fill_super is required by these routines to fill the super block object.
           the callback function usually passed by filesystem implementation,for disk-based filesystem such ext2,
           this callback function should read the super block from disk device partition and use the data to fill the
           new super block object.

           function open_bdev_exclusive() is used to open a device,if no error encountered,open_bdev_exclusive() will
           returns a handler corresponding to the device which is type of struct block_device *;function
           close_bdev_exclusive() is used to close the device opended by open_bdev_exclusive() previously.

     Mounting the Root Filesystem :
       Linux allows the root filesystem is stored in a hard disk partition,a floppy disk,a remote filesystem shared via
       NFS,or even a ramdisk.generally,the root filesystem is stored in disk partition 1.
       global variable named @ROOT_DEV is used to stores the device info about where the root filesystem is stored.
       the rootfs can be specified as a device file in the "/dev" directory either when compiling the kernel or by
       passing a suitable "root" option to the initial bootstrap loader.
       /**
        * the grub bootloader command "linux" can select which kernel to be booted,and parameter "root" is used to
        * tell the kernel where the root filesystem stored.
        */
       the mounting flags of root filesystem is stored in global variable @root_mountflags,user can specifies the
       mounting flags via program "rdev" on a selected kernel image or through "root-flags" option of the bootloader.
       /* ROOT_DEV <linux/root_dev.h>, root_mountflags <linux/kernel.h> */

       Two stages for mounting the root filesystem :
         1> the kernel mounts the special rootfs filesystem,which simply provides an empty directory that
            serves as initial mount point.
         2> the kernel mounts the real root filesystem over the empty directory.
       /**
        * grub command "initrd" requires a pathname to the initramfs image for current kernel.
        * when booting the kernel,it installs the initramfs in the memory,then tell the kernel where to find the
        * components needed when initializes the system.if the kernel supports initramfs,function unpack_to_rootfs()
        * will be called,it unpacks the components into the initial rootfs filesystem,then kernel can access them
        * to initializes the system.
        * kernel image and initramfs image are stored in the directory "/boot",and it is stored in the disk
        * partition 0.
        * after bios self-checking completed,the bootloader will be read from disk to memory,ip register of CPU
        * will be set to the instruction of bootloader.
        * the kernel would be booted after the enviroment been setup by grub.
        */
        
       ! the kernel bother to mounts the initial rootfs before the real one can allows the kernel easily
         change the real root filesystem.

       Mounting the initial rootfs filesystem :
         function init_rootfs() is used to register @rootfs_fs_type on VFS,function init_mount_tree() is
         used to setup the vfsmount object of initial rootfs,the task "init" will use the vfsmount object.
         these two function are called by routine mnt_init().
         /* mnt_init() is called by vfs_caches_init(),and which is called by start_kernel() */

         <fs/ramfs/inode.c>
           /**
            * init_rootfs - register rootfs filesystem type on VFS during system initialization
            * return:       0 or error code
            * # this function invokes bdi_init() to initializes backing device info about ramfs,the
            *   object is named @ramfs_backing_dev_info;and next,call to register_filesystem()
            *   to register @root_fs_type on linux VFS
            */
           int __init init_rootfs(void);

         <fs/namespace.c>
           /**
            * init_mount_tree - initialize the mount tree,and the tree is come from the initial
            *                   rootfs filesystem
            */
           static void __init init_mount_tree(void);

           brief description for init_mount_tree() :
             call to do_kern_mount(),a new vfsmount object for "rootfs" is returned through do_kern_mount().
             this vfsmount represents the mounted initial rootfs filesystem.
             if the object IS_ERR,then kernel will be panic.
             next,call to create_mnt_ns() to create a namespace for this vfs mount point,and set the @mnt_ns
             of @nsproxy of @init_task.
             increase namespace reference counter,set @mnt and @dentry of a local variable named @root is type
             of struct path by @ns->root and @ns->root->mnt_root. /* @ns is the namespace of rootfs */
             setup filesystem process work directory and root directory of current task with @root the vfs mount point.
             /**
              * the get_sb() method of rootfs type will be called by vfs_kern_mount(),it is the routine ram_get_sb()
              * defined in <fs/ramfs/inode.c>.finally,get_sb_nodev() will be called to get super block object for
              * the inital rootfs.
              * the parameter @fill_super points to ramfs_fill_super().
              */

       Mounting the real root filesystem :
         function prepare_namespace() is used to mounts real root filesystem during system initialization,it is
         called by kernel_init() before invokes to init_post(). /* the next statement is "return 0;" */
         the name of root device is passed by bootloader through command line option "root=",and the calling
         __setup("root=", root_dev_setup) means call to root_dev_setup() in the section ".init.setup".
         function root_dev_setup() stores the parameter of "root=" to the array @saved_root_name,it will be
         used by prepare_namespace() to set @root_device_name.

         <init/do_mounts.c>
           /**
            * prepare_namespace - mounts the real rootfs filesystem and prepares namespace
            *                     it decides what/where to mount,load ramdisks, etc
            */
           void __init prepare_namespace(void);

           brief description for prepare_namespace() :
             after slept @root_delay(if it is not _zero_),the routine call to wait_for_device_probe() to
             waiting for the known devices to complete their probing.
             next call to md_run_setup() to setup memory disk environment.
             set @root_device_name and @ROOT_DEV through @saved_root_name[0],if the root device is mtd or
             ubi device,call to mount_block_root() and return to caller.
             if @saved_root_name[0] is NULL,then attempts initrd_load(),in this case,the rootfs come from
             a ram disk,it loads the filesystem and return to caller.
             the last case is the rootfs come from a hard disk partition,function mount_root() take care of
             rootfs mounting.
             if @ROOT_DEV is _zero_ AND @root_wait is true,before call to mount_root(),have to wait for any
             asynchronous scanning to the root device @saved_root_name,and wait for driver probe completed.
             if @ROOT_DEV's major number same as FLOPPY_MAJOR AND @rd_doload is true AND rd_load_disk(0)
             returned true,ROOT_DEV will be set to Root_RAM0.
             finally,call to mount_root(),then mount devtmpfs "dev",mount "/",chroot to ".".
                                               /* these is the common end-up for all cases described above */
           /**
            * mount_root - mount the real rootfs filesystem
            */
           void __init mount_root(void);

           brief description for mount_root() :
             #ifdef CONFIG_ROOT_NFS /* network booting */
                     ROOT_DEV's major number same as UNNAMED_MAJOR
                     call to mount_nfs_root(),return to caller if succed,set ROOT_DEV to Root_FD0 if failed.
                     /* also log message in kernel log */
             #endif
             #ifdef CONFIG_BLK_DEV_FD /* floppy root */
                     if ROOT_DEV's major number same as FLOPPY_MAJOR
                             if @rd_doload is 2 /* for a dual initrd/ramload setup */
                                     call to rd_load_disk(1),if succeed,then set ROOT_DEV to Root_RAM1,
                                     set @root_device_name to NULL
                     else call to change_floppy("root floppy")
             #endif
             #ifdef CONFIG_BLOCK
                     create "/dev/root" through create_dev() with ROOT_DEV
                     call to mount_block_root() attempts to mount "/dev/root" with @root_mountflags
             #endif
             /**
              * kernel will panic after mount_block_root() tried all do_mount_root() on available filesystem
              * names registered in the kernel(passed by bootloader or built when compiling kernel),but no one
              * can be mounted.
              * kernel log message such "VFS: Cannot open root device ...","VFS: Unableto mount root fs on ..."
              * will be printed.
              * if sys_mount() called by do_mount_root() try to mounts the filesystem on "/root",and do_mount_root()
              * is called by mount_block_root().
              * if succed to mount root filesystem,then do_mount_root() will chdir to "/root",thus the namespace
              * of init task will has a new root.
              * the initial rootfs is hidden by the real root filesystem.
              */

     Unmounting a Filesystem :
       system call umount() is used to issue a request for unmount a mounted filesystem,the corresponding system call
       service routine is sys_umount().

       <fs/namespace.c>
         /**
          * sys_umount - system call service routine for umount() the C lib wrapper
          * @name:       pathname to the mount point or the device file
          * @flags:      flags used to control behavior
          * return:      0 OR error code
          */
         SYSCALL_DEFINE2(umount, char __user *, name, int, flags);
         => asmlinkage long sys_umount(char __user *name, int flags);

         brief description for sys_umount() :
           @flags can not specified MNT_FORCE | MNT_DETACH | MNT_EXPIRE | UMOUNT_NOFOLLOW at
           the same time,if it was,-EINVAL will be returned.
           call to user_path_at() use @name and @lookup_flags(local variable, 0 or LOOKUP_FOLLOW)
           attempts find out the path object corresponding to @name.
           /* this will increase references to @dentry and @mnt that corresponding to the mount point */
           if unfound,then returns error code to caller.
           the path object @path's @dentry is not same as its @mnt_root,then put @dentry,
           mntput_no_expire() @mnt_root;that means the path is invalid,return -EINVAL to caller.
           otherwise,checks if the namespace of the @mnt of @path is same as current task's @mnt_ns,
           put @dentry and @mnt when they are not same,and return -EINVAL to caller.
           /**
            * each task can only modifies its own namespace,if they are not same,means the filesystem is not
            * contained in this namespace
            */  
           next,checks permission CAP_SYS_ADMIN,non root user can not modify namespace,so if this task
           have not root permission,-EPERM would be returned. /* of course,destroy @dentry and @mnt of @path */
           if everything is OK,call to do_umount(@path.mnt, @flags),which processes the main work.
           finally,put @dentry and @mnt of @path,returns the value what do_umount() returned to
           caller.

         /**
          * do_umount - umount a filesystem registered on Linux VFS,behavior is depends on flags
          * @mnt:       the vfs mount point
          * @flags:     flags related to routine behavior
          * return:     0 OR error code
          */
         static int do_umount(struct vfsmount *mnt, int flags);
          
         what do_umount() doest :
           1> get super block object for @mnt through @mnt->mnt_sb.
           2> initializes local list @umount_list,process security checking about sb umount,
              return to caller the error code if un-satisfied the conditions.
           3> check @flags :
                      enabled MNT_EXPIRE {
                              /**
                               * current task cannot unmount its root
                               * MNT_EXPIRE is exclusive flag
                               */
                              @mnt == @current->fs->root.mnt OR @flags enabled MNT_FORCE | MNT_DETACH
                                      return -EINVAL
                              @mnt->mnt_count != 2 /* this filesystem is using by other objects */
                                      return -EBUSY
                              /**
                               * xchg(a, b) => exchange @a and @b then returns @b
                               * if the vfs mount point is not marked expired previously,then
                               * the initial MNT_EXPIRE will mark the mount point expired and
                               * returns -EAGAIN;the second MNT_EXPIRE will umount the expired
                               * filesystem.
                               */
                              xchg @mnt->mnt_expiry_mark 1
                                      return -EAGAIN
                      }
           4> if enabled MNT_FORCE AND umount_begin() method of super block is implemented,
              call umount_begin() befor do realy unmounting.
           5> @mnt is the root's mount point of @current AND no MNT_DETACH
                      special case for "unmounting" root
                      we just try to remount it readonly
                      acquire @sb->s_umount for writing
                      if @sb->s_flags no MS_RDONLY enabled,that means this filesyste is not mounted
                      with readonly,then call to do_remount_sb() with MS_RDONLY on @sb
                      release lock and return 0 or value what do_remounted_sb() returned
           6> acquire @namespace_sem for writing.
              acquire @vfsmount_lock.
              increase @event the namespace event indicator which is defined in the same file with static
              linkage.
           7> if no MNT_DETACH
                      call to shrink_submounts() to process a list of expirable mountpoints with the intent
                      of discarding any submounts of @mnt
           8> enabled MNT_DETACH OR @mnt can be unmounted successfully(checked by propagate_mount_busy())
                      if @mnt->mnt_list is not empty /* more vfs mount points are contained in this namespace */
                              call to umount_tree(@mnt, 1, &@umount_list)
                              /* collects all submounts under this filesystem in @umount_list contains @mnt */
                      set return value to _zero_
           9> release @vfsmount_lock.
              if return value is not _zero_,do security checking for sb_umount_busy on @mnt
           10> release @namespace_sme
               release all mount points contained in @umount_list(by release_mounts())
               /**
                * for each element in the list
                *   delete the element from vfs mount point hash list
                *   # if @mnt->mnt_parent != @mnt
                *     lock @vfsmount_lock
                *     get mount point dentry
                *     get parent vfs mount point through @mnt_parent
                *     modify @mnt_mountpoint to @mnt_root
                *     modify @mnt_parent to mnt
                *     decrease @mnt_ghosts of parent
                *     unlock @vfsmount_lock
                *     dput() the dentry
                *     mntput() parent
                *   mntput() the element
                * # notice that,mntput() is a static function defined in <linux/mount.h>,if the parameter
                *   @mnt is TRUE,then @mnt_expiry_mark of @mnt will be set to _zero_,proceeds calling
                *   to mntput_no_expire(@mnt).
                *   and routien mntput_no_expire() will decrease usage counter of @mnt and test it,call to
                *   __mntput(@mnt) when usage counter becomes to _zero_.
                *   ! if counter of @mnt's parent becomes to _zero_,so the parent should be __mntput(),too.
                *     but the initial usage counter of a mounted vfs mount point usually equal to 2,
                *     one for parent vfsmount,another because sys_mount().
                */
               return the return value.

     Pathname Lookup :
       VFS pathname lookup actually is derive an inode from the corresponding file pathname.
       for a given pathname,VFS have to analyzes each component except the last filename in the pathname.
       except the last,each component must identifies a directory,thus the corresponding dentry object
       must exists.
       start with "/" => full pathname,the lookup start from root /* current->fs->root */
       start with "./" => lookup start from PWD                   /* current->fs->pwd  */
       start with "../" => lookup start from PWD's parent
       only filename is given => lookup start from PWD
       
       because,the dentry in state "Unused" OR "In-use" has a inode associated to it,the inode identified
       by @d_inode.
       so the lookup just is "get dentry via component name,get the corresponding inode via dentry,read the
       directory has that inode from disk".whenever kernel read a existed directory from the disk,a dentry
       object will be created and associated to its inode.for full pathname,the first component is "/",the
       filesystem's root,and the fs must be mounted during system initialization.now we got the first
       component,so we can find the second,the third,and so on.
       /* if the task has different namespace,then "/" points to different root */
       /* dcache can speed up the searching,the recently accessed dentry will not be destroyed immediately */

       ! VFS must checks acess rights for each component.
         if the filename is a symbolic link,the analysis must be extended to all components of that pathname.
         symbolic link may induce circular references,the kernel must pay attention to this.
         a filename can be the mount point of a munted filesystem,this situation must be detected,and the
         lookup operation must continue into the new filesystem.
         pathname lookup have to be done inside the namespace of the process that issued the system call.
         /* same pathname used by two procs with different ns may specify different files */

       path_lookup() :
         the routine used to process pathname lookup.

         <linux/namei.h>
           /**
            * open_intent - file accessing intent
            * @flags:       open flags
            * @create_mode: file mode
            * @file:        file object
            */
           struct open_intent {
                   int flags;
                   int create_mode;
                   struct file *file;
           };

           /* number of nested symbolic links can not exceeds 8 */
           enum { MAX_NESTED_LINKS = 8 };

           /* type of the last recorded component */
           enum { LAST_NORM, LAST_ROOT, LAST_DOT, LAST_DOTDOT, LAST_BIND };
                  |          |          |         |            |
                  |          |          |         |            +--> last component is a symbolic link
                  |          |          |         |                 into a special filesystem
                  |          |          |         |
                  |          |          |         +--> last component is ".."
                  |          |          +--> last component is "."
                  |          +--> last component is "/"
                  +--> last component is a regular filename

           /* bitmask for a lookup event */
           #define LOOKUP_FOLLOW 1              /* follow links at the end */
           #define LOOKUP_DIRECTORY 2           /* require a directory */
           #define LOOKUP_CONTINUE 4            /* ending slashes ok even for nonexistent files */
           #define LOOKUP_PARENT 16             /* internal "there are more path components" flag */
                                                /**
                                                 * lookup operation resolves next-to-last component
                                                 * rather than the last component,so the next-to-last
                                                 * component is the parent directory of the last component
                                                 */
           #define LOOKUP_REVAL 64              /**
                                                 * locked when lookup done with dcache_lock held detnry
                                                 * cache is untrusted;force a real lookup
                                                 */
           
           /* bitmask for intent data about file open flags */
           #define LOOKUP_OPEN 0x0100
           #define LOOKUP_CREATE 0x0200
           #define LOOKUP_EXCL 0x0400
           #define LOOKUP_RENAME_TARGET 0x0800

           /**
            * nameidata - pathname analysis data
            * @path:      record current status of path walk
            *             it start out referring to the starting point,
            *             and finally to the last component in the pathname
            *             if no error encountered during path walk
            * @last:      quick string for the last component,that is the next
            *             component
            * @root:      root of this task
            * @flags:     lookup flags
            * @last_type: type of last component of the pathname
            * @depth:     current level of symbolic link nesting
            * @saved_names:    array of pathnames associated with nested symbolic links
            * @intent:    specifying how the file will be accessed
            */
           struct nameidata {
                   struct path path;
                   struct qstr last;
                   struct path root;
                   unsigned int flags;
                   int last_type;
                   unsigned depth;
                   char *saved_names[MAX_NESTED_LINKS + 1];

                   union {
                           struct open_intent open;
                   } intent;
           };
         
         <fs/namei.c>
           /**
            * path_init - setup nameidata object through the given dfd and pathname
            * @dfd:       where to start
            * @name:      pathname
            * @flags:     lookup flags
            * @nd:        result where to be stored
            * return:     0 OR error code
            */
           static int path_init(int dfd, const char *name, unsigned int flags, struct nameidata *nd);

           brief description for path_init() :
             this routine setup @nd through @dfd,@name,@flags.
             first setup
                     @nd->last_type = LAST_ROOT
                     @nd->flags = flags
                     @nd->depth = 0
                     @nd->root.mnt =NULL
             next
                     if the pathname is full pathname,that is it started with "/"
                     then use the root of current task,and increase the counter of
                     the corresponding vfsmount object and dentry object(by set_root())
                     set @nd->path to @nd->root

                     else if @dfd is AT_FDCWD
                     then use current task's current work directory
                     set @nd->path to @current->fs->pwd

                     else
                             @dfd refers to a different directory,we have to start from the dir
                             use fget_light() on @dfd to get the corresponding file object of
                             the directory
                             check it with S_ISDIR(),and check file permission MAY_EXEC
                             if everything is OK,use it to setup @nd,and fput_light() the dir
             finally,returns 0 or error code to caller.

           /**
            * path_lookup - pathname analysis routine
            * @name:        pathname
            * @flags:       represent how the looked-up file is going to be accessed
            * @nd:          stores the result of the lookup operation
            * return:       0 OR error code
            * # EXPORT_SYMBOL
            * # this routine actually call to do_path_lookup() with @dfd AT_FDCWD
            */
           int path_lookup(const char *name, unsigned int flags, struct nameidata *nd);

           /**
            * do_path_lookup - do path lookup
            * @dfd:            directory file descriptor specifies where to start,a special
            *                  value is defined in <linux/fcntl.h>,AT_FDCWD means use the
            *                  current work directory
            *                  # same feature as in system call openat2 
            *                  # if pathname is the full pathname,this would be ignored
            *                  # this parameter is used by path_init() which is called by this
            *                    routine
            * @name:           pathname
            * @flags:          accessing flags
            * @nd:             stores lookup result
            * return:          0 OR error code
            */
           static int do_path_lookup(int dfd, const char *name, unsigned int flags, struct nameidata *nd);

           brief description for do_path_lookup() :
             call to path_init() to initializes @nd.
             next,process three if statements.
             the first,if we succeed to initializes @nd,then call to path_walk() for pathname lookup.
             the second,if path_walk() succeed,and audit_dummy_context() on current task is succceed,
             and both the dentry of the last component and the corresponding inode existed,then
             call to audit_inode() to audit the inode.
             the third,if @nd->root.mnt is not NULL,then path_put the root and reset it to NULL.
             finally,returns 0 or error code from path_init() or path_walk() to caller.

           /**
            * path_walk - walk through the pathname to process lookup
            * @name:      pathname
            * @nd:        lookup result recorder
            * return:     0 OR error code
            */
           static int path_walk(const char *name, struct nameidata *nd);

           brief description for path_walk() :
             the main work for lookup will be done by link_path_walk(),and which is called
             by this routine.
             path_walk() saves @nd->path in a local variable,then set this task's @total_link_count to _zero_.
             increase usage counter of @mnt and @dentry of @nd->path.
             call to link_path_walk().
             if the result is -ESTALE,that means the record been expired,it will repeat lookup again.
             /**
              * reset @total_link_count,save @nd->path,increase usage counter
              * and enable LOOKUP_REVAL in @nd->flags to force the next link_path_walk() process lookup.
              */
             if the result is _zero_,then decrease the usage counters and return the result to caller.

     The link_path_walk() function :
       link_path_walk() is the main routine processes lookup.
       it will has different behavior determined by nameidata object's @flags.
       
       <fs/namei.c>
         /**
          * link_path_walk - do pathname lookup
          * @name:           pathname
          * @nd:             resolving result recorder
          * return:          0 OR error code
          */
         static int link_path_walk(const char *name, struct nameidata *nd);

         !! "." in pathname has no effect

         what link_path_walk() does :
           1> updates @name to skip "/",if the remaining @name is '\0',goto label "return_reval".
              /**
               * that is the pathname is "/" or "//"
               */
           2> get inode of current dentry in @nd->path in local variable @inode(pointer to inode structure).
              reset lookup flags to LOOKUP_FOLLOW | (@nd->flags & LOOKUP_CONTINUE),if
              @nd->depth is not _zero_. /* otherwise,lookup flags is assigned to @nd->flags */
                                        /* @lookup_flags */
           3> enter a infinite for-cycle.
              as this point we know we have a real path component.
              for-cycle :
                      enables LOOKUP_CONTINUE in @nd->flags
                      check inode permission,break cycle when permission disallowed
                                             /**
                                              * inode execution permission,MAY_EXEC
                                              * the permission checking :
                                              *   call to @inode->i_op->permission() if implemented,
                                              *   return with security checking inode_permission if
                                              *   the method returned 0
                                              *   call to acl_permission_check,and return with security
                                              *   checking inode_permission if returned 0
                                              *   checks CAP_DAC_OVERRIDE AND CAP_DAC_READ_SEARCH,either
                                              *   of them is OK then returned with security checking
                                              *   inode_permission
                                              * # the checking returns positive value if condition un-satisfied
                                              */
                      an object is type of struct qstr named @this is declared
                      set @this.name to @name /* a const char * */
                      get the first character in @name,store it in local variable @c
                      enter a do-while cycle,it will ends when @c is '\0' OR @c is "/"
                      /**
                       * we can easily to know that,the do-while cycle calculates
                       * the hash value of current component of the pathname
                       */
                      do-while : /* until @c is '\0' OR @c is "/" */
                              updates @name
                              calculates hash for @c by call to partial_name_hash()
                              updates @c to the next character
                      calculates the length of the string of this component
                      set @this.hash to end_name_hash(calculated hash value)
                      if current component is the last component,then goto label "last_component"
                      enter a while cycle to updates @name to skip "/" before next component
                      goto label "last_with_slashes" when *@name is '\0'
                      check current component,"." and ".." are special cases :
                              "." => current work directory,for-cycle continue
                              ".." => parent directory of current work directory
                              in the case "..",have to follow_dotdot(@nd),this will updates
                              @nd to records the parent directory's info,then reset inode to
                              @nd->path.dentry->d_inode,and continue for-cycle to next component
                              /**
                               * if current dentry is the root,then can not change to its parent,
                               * thus follow_dotdot() will call to follow_mount() with &@nd->path.
                               * # @nd->path.dentry == @nd->root.dentry AND
                               *   @nd->path.mnt == @nd->root.mnt
                               * if current dentry is not the root of this vfsmount,then reset
                               * it to the parent dentry,and call to follow_mount(&@nd->path) to
                               * see whether it is a mount point.
                               * # @nd->path.dentry != @nd->path.mnt->mnt_root
                               * else,call to follow_up(),and call to follow_mount(&@nd->path) when the
                               * routine failed to follow up(current dentry is the top),otherwise,restart
                               * the checking 
                               *
                               * ! actually,the checkings are included in a infinite while-cycle,and calling
                               *   to follow_mount() is outside to the cycle.
                               * # follow_up(),get parent vfsmount of current mnt,returns _zero_ if
                               *   no parent vfsmount;returns 1 with reset the dentry to
                               *   { parent vfsmount, @path->mnt->mnt_mountpoin }.
                               *
                               * # follow_mount(),cycle until @path->dentry is not a mount point
                               *   use lookup_mnt() to find out the vfsmount object corresponding
                               *   to the @path
                               *   if unfound,then return to caller without modify @path.
                               *   else,dput() current @dentry,mntput() current @mnt,reset @path
                               *   to { mounted vfsmount, mounted vfsmount's @mnt_root } and start
                               *   next cycle
                               */
                      now we can process the actual lookups through do_lookup(@nd, &@this, &@next),
                      break for-cycle when any error was encountered in do_lookup()
                      /**
                       * @next is an object is type of struct path
                       * it is set to { @nd->path.mnt, __d_lookup(@nd->path.dentry, @this) }
                       * if @next is a vfs mount point(__follow_mount()) and some filesystems have mounted on it,
                       * then @next will stores the vfsmount object of the first or the last filesystem
                       * mounted on current dentry in @mnt,and stores the root dentry object of the mounted
                       * filesystem in @dentry.
                       * # __d_lookup() is defined in <fs/dcache.c>,which use a qstr object and a directory
                       *   as the starting point(the parent dir) to find out the corresponding dentry object
                       *   in dcache,if unfound,returns NULL,otherwise,returns the found dentry with increase
                       *   usage counter.
                       * !! IF NO SUCH DENTRY WAS FOUND IN dcache,do_lookup() WILL CALL TO d_alloc() WITH THE PARENT
                       *    DIRECTORY @nd->path.dentry AND @this.
                       *    THE ROUTINE d_alloc() WILL ALLOCATEs A NEW dentry OBJECT,INITIALIZES IT AND PLACES INTO dcache.
                       *    OF COURSE,AN INODE OBJECT ALSO BE CREATED(TRY TO FIND IT AT FIRST) TO ASSOCIAT WITH IT.
                       *    # THE INODE IS GET BY CALL TO @nd->path.dentry.d_inode->i_op->lookup(),THIS METHOD
                       *      IS CALLED BY do_lookup() AFTER d_alloc() SUCCEED.IF i_op->lookup() RETURNED A
                       *      DENTRY,THEN dput() THE NEWLY ALLOCATED DENTRY BY d_alloc();OTHERWISE,THE NEWLY
                       *      ALLOCATED DENTRY IS THE dentry OBJECT WE NEED.
                       *      ! for example,lookup() of ext2 filesystem is ext2_lookup(),which use ino to get
                       *        an inode from inode cache through iget_locked().if unfound such inode,then
                       *        allocated an new inode object and returns it to ext2_lookup().
                       *        finally,ext2_lookup() call to d_splice_alias() to associates the inode and the
                       *        dentry.
                       *        # the raw inode is read from disk by ext2 fs,by call to ext2_inode_by_name(),
                       *          which will call to ext2_find_entry().
                       */
                      updates @inode(the local variable) to @next.dentry->d_inode,goto lable "out_dput" with
                      error code -ENOENT if the @inode is NULL
                      if @inode->i_op->follow_link is TRUE
                      then
                              call to do_follow_link() on @next with @nd,goto "return_err" if do_follow_link()
                              detected an error
                              updates @inode to the new directory's inode,if the inode is NULL,then break for-cycle
                              with error code -ENOENT
                              /**
                               * do_follow_link() will process a recursive symlink following,the maximum is 8,and
                               * consecutive symlinks is 40
                               * if the @inode is not a symlink,then would not cause symlink following,that is
                               * method follow_link() is not implemented.
                               * it is recursive by call to link_path_walk().
                               * struct task.@link_count and @nd->depth will be increased before do_follow_link()
                               * call to the primary link resolving routine __do_follow_link(),decrease them after
                               * __do_follow_link() have returned.
                               * of course,member named @total_link_count of task structure also increased before
                               * call to __do_follow_link(),but it would not be decreased even __do_follow_link()
                               * returned.
                               * # cond_resched() is called by do_follow_link() before does security checking and
                               *   right after BUG_ON() checking completed.if necessary,a process switch will happens.
                               * !! ROUTINE do_follow_link() CHECKS @link_count,@total_link_count AND @nd->depth
                               *    RIGHT AFTER THE CONTROL ENTERED IT.
                               *    @link_count CAN NOT GREATER THAN OR EQUAL TO MAX_NESTED_LINKS
                               *    @total_link_count CAN NOT GREATER THAN OR EQUAL TO 40
                               *    BUG_ON() @nd->depth IS GREATHER THAN OR EQUAL TO MAX_NESTED_LINKS
                               *    IF @link_count AND @total_link_count EXCEEDED THE LIMIT,IT WILL GOTO LABEL "loop",
                               *    AT THERE,THE ROUTINE WILL path_put() @path AND @nd->path,RETURNS -ELOOP.
                               *             # BECAUSE IT HAVE TO DOES SECURITY CHECKING FOR inode_follow_link ON
                               *               @path->dentry AND @nd,SO MAY BE THE ERROR CODE IS NOT -ELOOP.
                               *    BUG_ON() WILL LET KERNEL PANIC.      /* the @next variable passed by link_path_walk() */
                               * # if @cookie set by __do_follow_link() is TRUE,AND put_link() method of the symblink inode
                               *   is implemented,do_follow_link() will call to the method for destory the temporary data
                               *   structure created by follow_link() after __do_follow_link() returned.
                               * # __do_follow_link() will set @nd->last_type to LAST_BIND.if follow_link() routine of
                               *   the inode of @path(the first parameter)->dentry returned a valid pointer,then it will
                               *   call to __vfs_follow_link() if nd_get_link(@nd) returned a valid pointer;otherwise,call
                               *   to force_reval_path() if @nd->last_type is LAST_BIND.
                               *     # no recursion happens,but force revalidate the dentry.
                               *     # nd_get_link() retrives the link through @nd->saved_names[@nd->depth],the path that symlink
                               *       points to will be placed at there by follow_link().
                               * # __vfs_follow_link(),which is defined in the same file.it reset root of @nd,@path of @nd,
                               *   if the argument of second parameter @link is "/".next,call to link_path_walk(@link, @nd).
                               *   it will returns @link if @link IS_ERR() pointer.
                               */
                      else
                              path_to_nameidata() &@next, @nd
                              /* this will updates @nd->path to @next */
                      checks if lookup() method been implemented in @inode's operation,break for-cycle with error code
                      -ENOTDIR when it is not implemented
                      /* if lookup() is not implemented,how can we lookup a file under a directory? */
                      continue to next iteration /* ends the main loop */

                      last_with_slashes:
                              enables LOOKUP_FOLLOW | LOOKUP_DIRECTORY in @lookup_flags

                      last_component:
                              /* there we are deal with the last component */
                              set @nd->flags to @lookup_flags | ~LOOKUP_CONTINUE
                              goto label "lookup_parent" if LOOKUP_PARENT is enabled in @lookup_flags
                              process the checking for special cases "." and "..",but goto label
                              "return_reval" when it is the first case;call to follow_dotdot(@nd) and reset
                              @inode,then goto label "return_reval" if it is the second case.
                              /* "." => @nd records the next-to-last component in pathname */

                              do_lookup() again on @nd, &@this, &@next,break for-cycel if error
                              updates @inode to @next.dentry->d_inode
                              call to follow_on_final() on @inode, @lookup_flags
                              /**
                               * it returns TRUE if 
                               *         @inode AND @inode->i_op->follow_link AND
                               *         (@lookup_flags & LOOKUP_FOLLOW OR S_ISDIR(@inode->i_mode))
                               */
                              if follow_on_final() returned TRUE
                              then
                                      repeat once do_follow_link()
                              else
                                      path_to_nameidata() &@next, @nd
                              checks if @inode is exist,break for-cycle with -ENOENT if it is FALSE
                              checks lookup() method whether implemented if LOOKUP_DIRECTORY is enabled
                              in @lookup_flags,break for-cycle with -ENOTDIR if it is not implemented
                              goto label "return_base"

                      lookup_parent:
                              /* in this case,we have to lookup parent directory */
                              set @last of @nd to @this,@last_type to LAST_NORM
                              goto label "return_base" if the first character in @this.name is not "."
                              if the length of @this is 1 and which is starting with ".",then the @last_type
                              of @nd must be set to LAST_DOT
                              else if length is 2 and which is starting with "..",then the @last_type must
                              be set to LAST_DOTDOT
                              else goto label "return_base"
                              /**
                               * for-cycle goto label "last_compoent" when no more characters in the pathname
                               * string.but when the goto happens,@nd still records the info of next-to-last
                               * component.that is,the parent directory which contains the last component.
                               */

                      return_reval:
                              /* in this case,we will checks the cached dentry for staleness */
                              if @nd->path.dentry AND @nd->path.dentry->d_sb AND 
                                 @nd->path.dentry->d_sb->s_type->fs_flags & FS_REVAL_DOT
                              then
                                      set error code to -ESTALE
                                      if we failed on
                                              @nd->path.dentry->d_op->d_revalidate(@nd->path.dentry, @nd)
                                      then
                                              break the for-cycle
                                              /* that is,we can not revalidate a expired dentry */
                              
                      return_base:
                              just returns _zero_ to caller as well /* no error */

                      out_dput:
                              path_put_conditional() &@next, @nd
                              break for-cycle

           4> path_put() @nd's @path member and returns error code to caller

     Implementations of VFS System Calls :
       The open() System Call :
         <fs/open.c>
           /**
            * sys_open - syscall open service routine is used to open a file
            * @filename: file pathname
            * @flags:    open flags
            * @mode:     access mode used when creating a new file
            * return:    file descriptor OR invalid pointer value
            * # this routine actually call to do_sys_open() with AT_FDCWD,
            *   if force_o_largefile() returned TRUE,the flag O_LARGEFILE will be enabled in @flags
            */
           SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, int, mode);
           => asmlinkage long sys_open(const char __user *filename, int flags, int mode);

           @flags :
             O_ACCMODE               0x00000003
             O_RDONLY                0x00000000
             O_WRONLY                0x00000001
             O_RDWR                  0x00000002
             O_CREAT                 0x00000100
             O_EXCL                  0x00000200
             O_NOCTTY                0x00000400     /* never consider the file as a controlling terminal */
             O_TRUNC                 0x00001000
             O_APPEND                0x00002000
             O_NONBLOCK              0x00004000
             O_DSYNC                 0x00010000
             FASYNC                  0x00020000     /* I/O event notification via signals */
             O_DIRECT                0x00040000
             O_LARGEFILE             0x00100000     /* large file,the size greater than 2GB */
             O_DIRECTORY             0x00200000
             O_NOFOLLOW              0x00400000
             O_NOATIME               0x01000000
             O_CLOEXEC               0x02000000
             O_SYNC                  0x04000000
             O_NDELAY                O_NONBLOCK

           /**
            * do_sys_open - do system call open
            * @dfd:         directory file descriptor,used as starting point
            * @filename:    file pathname
            * @flags:       open flags
            * @mode:        access mode used when creating a new file
            * return:       file descriptor OR invalid pointer value
            */
           long do_sys_open(int dfd, const char __user *filename, int flags, int mode);

           brief description for do_sys_open() :
             first,it call to getname(@filename),the routine allocates memory area with GFP_KENREL from
             slab cache named @names_cachep,and copy the filename from user space to kernel space.
             /* putname() recycle the allocated memory */
             if failed to allocate memory or failed to copy filename,an invalid pointer value will be returned
             in @tmp,then returns the error to caller.
             next,use get_unused_fd_flags(@flags) to get an unused file descriptor.
             call to do_filp_open(@dfd, @tmp, @flags, @mode, 0) try to open the file if we succeed to get a
             file descriptor
             associates the new fd and file object,fsnotify_open() notify the open event.
             returns the new fd to caller.

         <fs/namei.c>
           /**
            * do_filp_open - do filp_open
            * @dfd:          directory fd
            * @pathname:     file pathname
            * @open_flag:    open flag
            * @mode:         creating mode
            * @acc_mode:     access mode
            * return:        pointer to the file object OR invalid pointer value
            * # routine filp_open() actually call to this function
            * # if we wants kernel to open a file,should use this function to instead of sys_open() 
            *   in Kernel Mode
            */
           struct file *do_filp_open(int dfd, const char *pathname, int open_flag, int mode, int acc_mode);

           brief description for do_filp_open() :
             at the beginning,convert @open_flag to namei flags(@flag,local variable).checks O_CREAT flag
             in @open_flag,reset @mode to _zero_ if it is not enabled.
             convert O_SYNC or __O_SYNC to O_DSYNC in @open_flag.
             modify @acc_mode to MAY_OPEN | ACC_MODE(@open_flag) if @acc_mode is not _zero_.
             add MAY_WRITE in @acc_mode if O_TRUNC is enabled.
             add MAY_APPEND in @acc_mode if O_APPEND is enabled.

             "reval" label :

             invokes path_init() to prepare pathname lookup,the @flags parameter is LOOKUP_PARENT,return error
             to caller if failed.
             enable LOOKUP_REVAL in local variable @nd's flags if @force_reval is TRUE.
             set @total_link_count to _zero_.
             invokes link_path_walk() to lookup @pathname's parent,return error if failed;audit inode of the
             @nd.path.dentry with @pathname if necessary. /* we tried to find out the parent directory */
             use get_empty_filp() to makeup an empty file object,return error if have any.
             setup @nd.intent and @nd.flags,@filp->f_flags :
                     @nd.intent.open.file := @filp
                     @filp->f_flags := @open_flag
                     @nd.intent.open.flags := @flag
                     @nd.intent.open.create_mode := @mode
                     @nd.flags &= ~LOOKUP_PARENT            /* we do not need LOOKUP PARENT any more */
                     @nd.flags |= LOOKUP_OPEN
                     O_CREAT -> @nd.flags |= LOOKUP_CREATE
                     O_CREAT AND O_EXCL -> @nd.flags |= LOOKUP_EXCL
                     O_DIRECTORY -> @nd.flags |= LOOKUP_DIRECTORY
                     O_NOFOLLOW -> @nd.flags |= LOOKUP_FOLLOW
             call to do_last() to get the last component in @pathname,the previous link_path_walk(),we
             stopped at the next-to-last component,that is the parent directory.
             /* the returned pointer is saved in @flip the local variable */
             do_last() use @nd.last_type to update its @path member,if the file is not exist,then create
             it when O_CREAT is speficied.finally,it convert the namei data to file object and returns
             the pointer to caller,that is the do_filp_open().the converting will be done by nameidata_to_filp(),
             because we have set @nd.intent.open.file to the empty @filp,so @filp will represents the opened file.
             /* do_last() may directly invoke nameidata_to_filp() when need to create file;otherwise,the function
              * should be called by finish_open(),do_last() pass to it the nameidata,open flag,and access mode.
              */

             if @filp is NULL,that means the last component may be a symlink,so we have to follow the link
             and call do_last() again.
             at the end of this function,path_put() @nd.root if necessary,and returns @filp to caller.
             /**
              * if -ESTALE(expired) is specified in @filp and @force_reval is FALSE,then goto label "reval"
              * to repeat the work agian for revalidate the file object.
              */
             !! put dentry,intent open(put_filp() OR fput()),@nd.path when error detected.

       The read() and write() System Calls :
         system calls read() and write() are wrappers from C lib,the corresponding system call service routine
         is sys_read() and sys_write(),respectively.
        
         <fs/read_write.c>
           /**
            * sys_read - system call service routine for read()
            * @fd:       file descriptor
            * @buf:      data buffer
            * @count:    number of bytes to reading
            * return:    number of bytes been readed OR error code
            */
           SYSCALL_DEFINE3(read, unsigned int, fd, char __user *, buf, size_t, count);
           => asmlinkage long sys_read(unsigned int fd, char __user *buf, size_t count);

           brief description for sys_read() :
             get file object through @fd.
             if failed to get the file,returns -EBADF.
             otherwise,get current file position through file_pos_read(),then call to vfs_read()
             to process the real reading.updates file position.fput() the file and returns what
             vfs_read() returned to caller.


           /**
            * sys_write - system call service routine for write()
            * @fd:        file descriptor
            * @buf:       data buffer
            * @count:     number of bytes to write
            * return:     number of bytes been written OR error code
            * # this routine similar to the sys_read(),but it call to vfs_write() to write data
            *   to the file @fd
            */
           SYSCALL_DEFINE3(write, unsigned int, fd, const char __user *, buf, size_t, count);
           => asmlinkage long sys_write(unsigned int fd, const char __user *buf, size_t count);

           /**
            * vfs_read - VFS reading routine
            * @file:     file descriptor
            * @buf:      data buffer
            * @count:    number of bytes to read
            * @pos:      file position where at the starting point to read
            * return:    number of bytes been readed OR error code
            * # EXPORT_SYMBOL
            */
           ssize_t vfs_read(struct file *file, char __user *buf, size_t count, loff_t *pos);

           brief description for vfs_read() :
             first,checks file access mode,FMODE_READ must be allowed,otherwise returns -EBADF.
             second,checks whether either file operation read() method or aio_read() is implemented,
             if no one is implemented,then returns -EINVAL.
             third,access_ok() VERIFY_WRITE must be OK,otherwise returns -EFAULT to caller.
             then call to rw_verify_area() READ to verify the file whether it been locked by
             someone,and whether security checking is OK.besides,@count can not exceeds MAX_RW_COUNT.
             if the verification is failed,then returns the value returned by the routine to caller.
             now,everything is OK,call to @file->f_op->read() if the method is implemented;otherwise,
             call to do_sync_read().
             if we succeed to readed something,that is return value is greater than 0,call to
             fsnotify_access() to notify a filesystem access event,and call to add_rchar() and inc_syscr()
             to update this task's account data.
             finally,returns the number of bytes we have readed.
             /**
              * about do_sync_read() :
              *   ssize_t do_sync_read(struct file *filp, char __user *buf, size_t len, loff_t *ppos);
              *
              *   it is defined in the same file.
              *   this function will creates a iovec structure for reading,and a structure kiocb for
              *   I/O control in the function frame(as local variables).
              *   # kiocb object is initialized by macro function init_sync_kiocb()
              *   # kiocb => kernel I/O control block for synchronous/asynchronous I/O
              *   call to @file->f_op->aio_read() in a infinite for-cycle,call to wait_on_retry_sync_kiocb()
              *   at the end of each iteration for synchronization.
              *   # void wait_on_retry_sync_kiocb(*kiocb) - defined in the same file
              *       > set @current to TASK_UNINTERRUPTIBLE
              *       > if the kiocb descriptor is not kicked,then invoke schedule()
              *         else,invoke kiocbClearKicked() to kick this kiocb
              *       > finally,restore @current to TASK_RUNNING
              *   break the cycle if return value is not -EIOCBRETRY.
              *   outside to the cycle,call to wait_on_sync_kiocb() if -EIOCBQUEUED is returned,the routine
              *   set current task to TASK_UNINTERRUPTIBLE until the I/O event is completed.
              *   # ssize_t wait_on_sync_kiocb(*kiocb) - defined in <fs/aio.c>
              *     - waits on the given sync kiocb complete
              *       make use of a while-cycle until @ki_users become 0
              *       do
              *               set @current to TASK_UNINTERRUPTIBLE
              *               if @ki_users is _zero_,break cycle
              *               otherwise,invoke io_schedule()
              *       done
              *       set @current to TASK_RUNNING,return @ki_user_data
              *   finally,use kiocb.@ki_pos to update *@ppos,returns the number of bytes have readed.
              */

           /**
            * vfs_write - VFS write
            * @file:      file descriptor
            * @buf:       data buffer
            * @count:     number of bytes to write
            * @pos:       starting point
            * return:     number of bytes been written OR error code
            * # EXPORT_SYMBOL
            * # this routine is similar to vfs_read(),but it will checks FMODE_WRITE,VERIFY_READ,WRITE on
            *   the file
            *   if write() method,then call to it,otherwise,call to do_sync_write()
            *   do_sync_write() is similar to do_sync_read(),but it will call to @file->f_op->aio_write()
            */
           ssize_t vfs_write(struct file *file, const char __user *buf, size_t count, loff_t *pos);

           !! attention,the buffer pointer required by vfs_read() and vfs_write() is User Mode address,
              if we want use these two routines in Kernel Mode,must call to set_fs(KERNEL_DS) at first.that will
              bypass the User Mode address checking.

       The close() System Call :
         system call close() is used to close an opened file,the corresponding service routine is sys_close().
         
         <fs/open.c>
           /**
            * sys_close - system call service routine for close()
            * @fd:        file descriptor
            * return:     0 OR error code
            */
           SYSCALL_DEFINE1(close, unsigned int, fd);
           => asmlinkage long sys_close(unsigned int fd);
        
           brief description for sys_close() :
             the routine get file object through @fd from current task's fdtable.
             /* @file_lock is required, @filp := @fdt->fd[@fd] */
             before do the real closing,it must checks whether the file @filp is NULL.
             if @filp is NULL,then it do not call to filp_close(),this can prevents one
             clone task can not release an fd while another clone is opening it.
             after got the @filp,call to rcu_assign_pointer() on @fdt->fd[@fd] to set the
             element to NULL,call to FD_CLR() on the file to disable close-on-exec flag,
             __put_unused_fd() to release the @fd.
             then,release @file_lock and call to filp_close(),the routine is the primary
             routine for file closing.
             finally,returns the value got from filp_close() to caller.
             ! if the return value is
                 -ERESTARTSYS || -ERESTARTNOINTR || -ERESTARTNOHAND || -ERESTART_RESTARTBLOCK
               then returns -EINTR to caller

           /**
            * filp_close - close a file descriptor
            * @filp:       pointer to the file object
            * @id:         identification info,usually is the @files member of current task
            * return:      0 OR error code
            * # EXPORT_SYMBOL
            */
           int filp_close(struct file *filp, fl_owner_t id);

           brief description for filp_close() :
             at the beginning,this routine have to checks file usage counter of @filp,we
             can not close an file whose counter is _zero_.
             next,checks whether flush() method of the file operation is implemented.
             if flush() is implemented,call to it with @filp and @id.
             call to dnotify_flush() to makeup a directory notification.
             call to locks_remove_posix() to removes all locks associated to this file.
             fput() @filp,and returns the return value got from flush() method.

     File Locking :
       the POSIX standard requires file lock mechanism implemented through system call fcntl().
       fcntl command F_SETLK can either lock a region of file or whole file,and command F_GETLK
       can get current lock's information,F_SETLKW might block caller until acquired the lock.
       for release a file lock,still use the F_SETLK command.when call to fcntl() for acquire or
       release a file lock,the third parameter is a pointer to an object is type of struct flock.
       Traditional BSD variants implement file lock through flock() system call.
       SystemV provides lockf() library function which is an interface to fcntl().
       !! ALL THESE FILE LOCK ARE ADVISORY LOCKING.
       ! SystemV Release 3 provides mandatory file lock.when a mandatory lock have been placed
         on a file,the Kernel will checks every open(),read(),write() system calls to the file,
         if any conflict happened,the system call will failed.
         # even a file was locked by a mandatory lock,another process can still delete it by
           call to unlink() system call.because when unlink a hard link,must have write permission
           to its parent directory,not to the file.
       ! as usually,read lock is shared,and write lock is exclusive.write lock can not lock the
         region where been some read locks placed on,or read lock can not lock the region where a
         exclusive write lock have locked it.

     Linux File Locking :
       Linux supports all types of file locking : mandatory locking, advisory locking, fcntl(),
       flock(), lockf().
       every Unix-like OS,flock() always produce an advisory lock,without regard to mount flag
       MS_MANDLOCK.
       but in Linux,a special kind of flock()'s mandatory lock is used to support some proprietary
       network filesystems - it is called "share-mode mandatory lock".

       share-mode mandatory lock :
         when set this lock,no other process may open a file that would conflict with the access
         mode of the lock.
         ! do not use such lock for native Unix applications,this lock is nonportable.

       lease :
         fcntl()-based mandatory lock.
         when a process tries to open a file protected by a lease,it is blocked as usualy,
         the process owns the lease will be informed by a signal.that process should writeback
         the updating to the file,and then release the lease.because,if the process refused to
         do these steps,kernel will release the lease automatically in a pre-defined time interval.
         /* can be modified through /proc/sys/fs/lease-break-time,usually 45s */
         /**
          * fcntl() commands F_SETLEASE and F_GETLEASE are used acquire and release lease,respectively.
          * the process who issued the fcntl() system call must have CAP_LEASE capability.
          * lease actions :
          *   F_RDLOCK  - set read lock(shared)
          *               owner will be informed when file is opened for writing or is truncated
          *   F_WRLOCK  - set write lock(exclusive)
          *               owner will be informed when file is opened for reading,writing,or is truncated
          *   F_UNLCK   - unlock
          * signal : SIGIO(default),but fcntl() command F_SETSIG can change the signal type.
          *          # (0 means default sig).
          * # can not holding read lease and write lease at the same time.
          * # only regular files could be leased.
          * # lease do not support file region.
          */

       advisory locking :
         acquire or release an advisory lock by either call to flock() or issue fcntl() system call.
         ! flock() always locks whole file.
         ! both flock() and fcntl() can be used on the same file at the same time,but a file locked
           through fcntl() does not appear locked to flock(),and vice versa.
           # the purpose is to avoid deadlocks.

       mandatory locking :
         for set mandatory lock,must :
           >
             set flag MS_MANDLOCK when mounting a filesystem.the mount command default do not set
             the flag.the filesystem independent mount options "mand" and "nomand" enable MS_MANDLOCK
             and disable MS_MANDLOCK,respectively.
           >
             enable SGID bit on the file as candiates for mandatory locking,and the file must under
             the filesystem with MS_MANDLOCK been setup.
           >
             use system call fcntl() to acquire or release mandatory lock.(F_SETLK)

     File-Locking Data Structures :
       all type of Linux locks are represented by the same file_lock data structure.

       <linux/fs.h>
         /**
          * file_lock - structure used to represent file lock
          * @fl_next:   singly linked list of file locks for this inode(@i_flock)
          * @fl_link:   doubly linked list of all file locks
          * @fl_block:  circular list of blocked processes waiting on this lock
          * @fl_owner:  file lock owner information
          * @fl_flags:  file lock flags
          *             # available values :
          *                 FL_POSIX 1     
          *                 FL_FLOCK 2     
          *                 FL_ACCESS 8    not trying to lock,just looking
          *                 FL_EXISTS 16   when unlocking,test for existence
          *                 FL_LEASE 32    lease held on this file
          *                 FL_CLOSE 64    unlock on close
          *                 FL_SLEEP 128   a blocking lock
          * @fl_type:   file lock type
          *             # available values : (defined in <asm-generic/fcntl.h>)
          *                 F_RDLCK 0      read lock
          *                 F_WRLCK 1      write lock
          *                 F_UNLCK 2      unlock
          * @fl_pid:    pid of the process who owns to this lock
          * @fl_nspid:  pid structure get from the owner's task_tgid()
          * @fl_wait:   wait queue of blocked processes
          * @fl_file:   the file this lock associate to
          * @fl_start:  start offset of the file lock scope
          * @fl_end:    end offset of the file lock scope
          * @fl_fasync: lease break notifications
          * @fl_break_time:    nonblocking lease breaks
          * @fl_ops:    file lock operations
          * @fl_lmops:  lock manager operations
          * @fl_u:      network filesystem supporting
          */
         struct file_lock {
                 struct file_lock *fl_next;
                 struct list_head fl_link;
                 struct list_head fl_block;
                 fl_owner_t fl_owner;
                 unsigned char fl_flags;
                 unsigned char fl_type;
                 unsigned int fl_pid;
                 struct pid *fl_nspid;
                 wait_queue_head_t fl_wait;
                 struct file *fl_file;
                 loff_t fl_start;
                 loff_t fl_end;

                 struct fasync_struct *fl_fasync;
                 unsigned long fl_break_time;

                 const struct file_lock_operations *fl_ops;
                 const struct lock_manager_operations *fl_lmops;
                 union {
                         struct nfs_lock_info nfs_fl;
                         struct nfs4_lock_info nfs4_fl;
                         struct {
                                 struct list_head link;
                                 int state;
                         } afs;
                 } fl_u;
         };

         /* file_lock_operations - file lock methods */
         struct file_lock_operations {
                 void (*fl_copy_lock)(struct file_lock *, struct file_lock *);
                 void (*fl_release_private)(struct file_lock *);
         };

         /* lock_manager_operations - lock manager methods for file lock */
         struct lock_manager_operations {
                 int (*fl_compare_owner)(struct file_lock *, struct file_lock *);
                 void (*fl_notify)(struct file_lock *);
                 int (*fl_grant)(struct file_lock *, struct file_lock *, int);
                 void (*fl_copy_lock)(struct file_lock *, struct file_lock *);
                 void (*fl_release_private)(struct file_lock *);
                 void (*fl_break)(struct file_lock *);
                 int (*fl_mylease)(struct file_lock *, struct file_lock *);
                 int (*fl_change)(struct file_lock **, int);
         };

       <fs/locks.c>
         /**
          * file_lock_list - list linked @fl_link for all satisfied lock entities
          *                  with any type of lock
          */
         static LIST_HEAD(file_lock_list);

         /**
          * blocked_list - list linked @fl_link for all un-satisfied lock entities
          *                with type POSIX lock
          */
         static LIST_HEAD(blocked_list);

       ! if a process want to acquires a file lock,then it must makeup an object is
         type of struct file_lock,and then insert it into the file inode's @i_flock.
         the procedure will traverse @i_flock to checks whether a file lock entity
         is exist and which conflict to the request lock,if it is,then the request
         lock will be inserted into the @fl_block of the conflict lock;if no conflict,
         then the lock request can be satisfied.
         shared lock could have more than one entities,but exclusive lock can only have
         one entity.

     for_each_lock() macro :
       this routine is used to traverse the locks on a given inode.
       <fs/locks.c>
         #define for_each_lock(inode, lockp) \
                 for (lockp = &inode->i_flock; *lockp != NULL; lockp = &(*lockp)->fl_next)

     FL_FLOCK Locks :
       an FL_FLOCK lock is always associated with a file object and it is owned by the process that
       opened the file(or by all clone processes sharing the same opened file).
       When a process wants to change an already owned read lock into a write lock,or vice versa,
       the kernel will replace every other lock that the process is holding on the same file object
       with the new lock,
       when call to fput() to destory a file object,the locks on it will also be destroyed,but if 
       there are some processes set read locks on the file,they still remain active.

       system call flock() is used to place an advisory FL_FLOCK on a specified file,the system call
       service routine is sys_flock().          

       <fs/locks.c>
         /**
          * sys_flock - system call service routine for flock()
          * @fd:        file descriptor
          * @cmd:       the type of lock to apply
          *             # available values :
          *                 LOCK_SH        shared lock(reading)
          *                 LOCK_EX        exclusive lock(writing)
          *                 LOCK_UN        unlock
          *                 LOCK_MAND      mandatory flock
          *                 LOCK_NB        nonblocking
          * return:     0 OR error code
          */
         SYSCALL_DEFINE2(flock, unsigned int, fd, unsigned int, cmd);
         => asmlinkage long sys_flock(unsigned int fd, unsigned int cmd);

         brief description for sys_flock() :
           get file object through fget() on @fd,if failed,then return error code -EBADF.
           stores !LOCK_NB in local variable @can_sleep,disable LOCK_NB in @cmd.
           fput() file object and return error code -EBADF if !LOCK_UN AND !LOCK_MAND AND
           FMODE_READ | FMODE_WRITE disabled in file mode.
           now,we can attempts to acquire FL_FLOCK.
           call to flock_make_lock() to makeup an file lock structure in type FL_FLOCK,return
           error code if failed.
           /**
            * file_lock.fl_flags = FL_FLOCK
            * file_lock.fl_type = flock_translate_cmd(@cmd)
            * ! FL_FLOCK do not support lock on a part of file.
            * file_lock object is allocated by routine locks_alloc_lock(),which allocates
            * a file_lock object from slab cache named @filelock_cache with gfp flag GFP_KERNEL.
            */
           do security checking for file_lock,if un-satisfied,then free file_lock object and
           fput() file object,return error code to caller.
           sys_flock() may call to file method flock() of file's operation @f_op,if the method
           is implemented;otherwise,call to flock_lock_file_wait() to process common locking
           procedure.
           finally,free file_lock object,fput() file object,return error code to caller.if
           no error happened,return 0 to caller.

         /**
          * flock_lock_file_wait - place an FL_FLOCK on a given file,may sleep
          * @filp:                 file object
          * @fl:                   file lock entity
          * return:                0 OR error code
          * # EXPORT_SYMBOL
          * # this routine will enter an infinite for-cycle,in the cycle,it
          *   attempts lock file by call to flock_lock_file().if the return value is
          *   not FILE_LOCK_DEFERRED,then break cycle and return the value to caller;
          *   otherwise,call to wait_event_interruptible() on @fl->fl_wait until
          *   @fl->fl_next is NULL(that means we can acquire the lock).if wait_event()
          *   is interrupted,then delete @fl from block list and break cycle,returns
          *   -ERESTARTSYS to caller
          */
         int flock_lock_file_wait(struct file *filp, struct file_lock *fl);

         /**
          * flock_lock_file - main procedure for place an FL_FLOCK on a given file
          * @filp:            file object
          * @request:         file lock that request 
          * return:           0 OR error code
          */
         static int flock_lock_file(struct file *filp, struct file_lock *request);

         what flock_lock_file() doest :
           1> lock_kernel().
           2> if @request->fl_flags enabled FL_ACCESS,then goto label "find_conflict".
           3> if @request->fl_type is not F_UNLCK,then try to allocate a new file lock
              named @new_fl;goto out with error code -ENOMEN if failed to allocating.
           4> for_each_lock(@filp->f_path.dentry->d_inode, struct file_lock **before) {
                /* traverse @inode->i_flock,each element in the list will be represented by @before */
                if **before is POSIX lock,then break. /* the file been locked by a posix file lock */
                if **before is LEASE,then continue.   /* someone leased the file */
                if @filp != *before->fl_file,then continue. /* different file object */
                if @request->fl_type == *before->fl_type,then goto label "out".
                /**
                 * same type,LOCK_SH or LOCK_EX or LOCK_UN.
                 * such file lock been exsted,do nothing.
                 */
                set local variable @found to TRUE.
                locks_delete_lock(before) /* this will unlink *before from @fl_link,and free it */
                                          /* remove the older FL_FLOCK entity */
                break cycle.
                /**
                 * we finally find out an older FL_FLOCK whose type is different from @request,then we
                 * can use the newer @request to instead it later.
                 * current task can change its own file lock.
                 */
              }
           5> if @request->fl_type is F_UNLCK,then checks if @request->fl_flags enabled FL_EXISTS AND
              @found is FALSE.if condition satisfied,then set error code to -ENOENT.
              goto label "out". /* we are going to unlock a file lock */
           6> call to cond_resched() if @found is TRUE,we will place the lock at next schedule.

           find_conflict:
           7> for_each_lock(@filp->f_path.dentry->d_inode, struct file_lock **before) {
                      if **before is POSIX lock,then break. /* POSIX lock do not effect FL_FLOCK */
                      if **before is LEASE,then continue.
                      if !flock_locks_conflict(@request, *before),then continue.
                      /**
                       * flock_locks_conflict() returns :
                       *   TRUE - either @request or @before is F_WRLCK
                       *   FALSE - other cases
                       *           @before is not FL_FLOCK OR @request->fl_file == *@before->fl_file
                       *           @request->fl_type is LOCK_MAND OR *@before->fl_type is LOCK_MAND
                       */
                      set error code to -EAGAIN.
                      if FL_SLEEP disabled in @request->fl_flags,then goto label "out".
                      /* we find conflict,so we have to block current task,but LOCK_NB enabled. */
                      locks_insert_block(*before, @request);
                      /**
                       * insert @request->fl_block into @before->fl_block
                       * set @request->fl_next to @before
                       * add @request->fl_link to @blocked_list,if @request is a POSIX file lock
                       */
                      goto out.
              }
           8> if @request enabled FL_ACCESS,then goto label "out".
           9> copy @request into @new fl.
              insert @new_fl->fl_link to @file_lock_list,set @new_fl->fl_next to *before.
              reset @new_fl to NULL.
              set error code to _zero_(no error was encountered)
           
           out:
           10> unlock_kernel().
               locks_free_lock(@new_fl) if @new_fl is not NULL.
               return error code.

     FL_POSIX Locks :
       an FL_POSIX lock is always associated with a process and with an inode,the lock is
       automatically released either when the process dies or when a file descriptor is closed.
       /* even if the process opened the same file twice or duplicated a file descriptor */
       FL_POSIX lock never inherited by a child process across a fork().
       POSIX file lock is supported by fcntl() system call.

       <asm-generic/fcntl.h>
         /**
          * flock - POSIX file lock structure
          * @l_type: lock type
          *          # available values :
          *              F_RDLCK 0
          *              F_WRLCK 1
          *              F_UNLCK 2
          * @l_whence: file position seek
          * @l_len:  scope length in bytes
          * @l_pid:  owner pid
          */
         struct flock {
                 short l_type;
                 short l_whence;
                 __kernel_off_t l_start;
                 __kernel_off_t l_len;
                 __kernel_pid_t l_pid;
                 __ARCH_FLOCK_PAD /* __ARCH_FLOCK64_PAD for struct flock64 */
         };

         ! fcntl() cmd : F_SETLK, F_GETLK, F_SETLKW(blocking version)
                         F_SETLK64, F_GETLK64, F_SETLKW64

       <fs/fcntl.c>
         /**
          * sys_fcntl - system call service routine for fcntl()
          * @fd:        file descriptor
          * @cmd:       command
          * @arg:       pointer value to argument corresponding to @cmd
          * return:     0 OR error code
          * # this routine just get file object through @fd and checks security for file_fcntl,
          *   then call to do_fcntl()
          */
         SYSCALL_DEFINE3(fcntl, unsigned int, fd, unsigned int, cmd, unsigned long, arg);
         => asmlinkage long sys_fcntl(unsigned int fd, unsigned int cmd, unsigned long arg);

         /**
          * do_fcntl - do file control
          * @fd:       file descriptor
          * @cmd:      command
          * @arg:      argument pointer
          * @filp:     file object
          * return:    0 OR error code
          */
         static long do_fcntl(int fd, unsigned int cmd, unsigned long arg, struct file *filp);

         do_fcntl() on F_SETLK,F_GETLK,F_SETLKW :
           F_SETLK,F_SETLKW -> fcntl_setlk(@fd, @filp, @cmd, (struct flock __user *)@arg);
           F_GETLK -> fcntl_getlk(@filp, (struct flock __user *)@arg);

       <fs/locks.c>
         /**
          * fcntl_getlk - get POSIX file lock info and return to userspace
          * @filp:        file object
          * @l:           User Mode buffer
          * return:       0 OR error code
          * # this routine :
          *     copy @l from user to local @flock
          *     check @l_type of @flock,in the case that type neither is F_RDLCK nor F_WRLCK,error
          *     flock_to_posix_lock(@filp, &@file_lock(local file_lock object), &@flock)
          *     vfs_test_lock(@filp, &@file_lock)
          *     @flock.l_type := @file_lock.fl_type
          *     posix_lock_to_flock(&@flock, &@file_lock) if @file_lock.fl_type is not F_UNLCK
          *     copy @flock to user @l
          */
         int fcntl_getlk(struct file *filp, struct flock __user *l);

         /**
          * fcntl_setlk - set POSIX file lock on a given file
          * @fd:          file descriptor
          * @filp:        file object
          * @cmd:         fcntl cmd
          * @l:           User Mode variable stored lock info
          * return:       0 OR error code
          */
         int fcntl_setlk(unsigned int fd, struct file *filp, unsigned int cmd, struct flock __user *l);

         brief description for fcntl_setlk() :
           allocates file_lock object,return -ENOLCK if failed to allocating the object.
           copy @l from user to local @flock is type of struct flock,return -EFAULT if failed.
           get inode of @filp.
           free file_lock object allocated previously and return -EAGAIN,if mandatory_lock(@inode) AND
           mapping_writably_mapped(@filp->f_mapping).
           /* do not allow mandatory locks on files that may be memory mapped and shared */
           again:
           call to flock_to_posix_lock(@filp, @file_lock, &@flock) try to convert flock to posix file_lock
           with @filp,return error code if failed.
           enable FL_SLEEP in @file_lock->fl_flags,if @cmd is F_SETLKW.
           checks @flock.l_type,if it is
             F_RDLCK,checks file mode FMODE_READ
             F_WRLCK,checks file mode FMODE_WRITE
             F_UNLCK,nothing
             undefined,set free file_lock object and returns -EINVAL(invalid lock type)
           do_lock_file_wait(@filp, @cmd, @file_lock).
           acquire spinlock @current->files->file_lock.
           fcheck(@fd).
           release the spinlock.
           if the file object @f returned by fcheck() is not equal to @filp,and @flock.l_type is not F_UNLCK,
           and no error been encountered,then reset @flock.l_type to F_UNLCK,goto label "again".
           finally,locks_free_lock(@file_lock) and returns error code.if no error happened,it is _zero_.

         /**
          * do_lock_file_wait - place a file lock on a given file,may sleep
          * @filp:              file object
          * @cmd:               command
          * @fl:                file lock object
          * return:             0 OR error code
          * # the primary work will be done in an infinite for-cycle
          *   in the cycle :
          *     vfs_lock_file(@filp, @cmd, @fl, NULL)
          *     if the routine did not returned FILE_LOCK_DEFERRED,then break cycle
          *     call to wait_event_interruptible() until @fl->fl_next it NULL
          *     continue next cycle,if wait_event() did not returned -ERESTARTSYS
          *     if wait_event() have returned -ERESTARTSYS,then locks_delete_block(@fl),then
          *     break cycle(system call was interrupted)
          *   finally,return error code to caller
          * # vfs_lock_file() is defined in the same file,if method lock() of the file operation
          *   been implemented,then call to it;otherwise call to posix_lock_file()
          *   EXPORT_SYMBOL_GPL on vfs_lock_file
          */
         static int do_lock_file_wait(struct file *filp, unsigned int cmd, struct file_lock *fl);

         /**
          * posix_lock_file - place a POSIX file lock on a given file,if necessary,return the conflicting
          *                   lock(if found) in third parameter
          * @filp:            file object
          * @fl:              POSIX file lock
          * @conflock:        place to return a copy of the conflicting lock,if found
          * return:           0 OR error code
          * # EXPORT_SYMBOL
          * # this routine actually call to __posix_lock_file()
          * # if this routine called with FL_EXISTS,the caller may determine whether or not a lock was
          *   successfully freed by testing the return value for -ENOENT
          */
         int posix_lock_file(struct file *filp, struct file_lock *fl, struct file_lock *conflock);

         /**
          * __posix_lock_file - primary procedure to place a POSIX file lock on a given file inode
          * @inode:             file's inode
          * @request:           request for file lock
          * @conflock:          place to return a copy of the conflicting lock,if found
          *                     set to NULL means do not need the copying
          * return:             0 OR error code
          */
         static int __posix_lock_file(struct inode *inode, struct file_lock *request,
                                      struct file_lock *conflock);

         what __posix_lock_file() does :
           1> if FL_ACCESS unspecified AND (@fl_type of @request is not F_UNLCK OR
              @request->fl_start != 0 OR @request->fl_end != OFFSET_MAX)
              then
                      locks_alloc_lock() for local variable @new_fl
                      locks_alloc_lock() for local variable @new_fl2
              /**
               * we are going to place a file lock,not to look or unlock
               * but the locking scope have to be checked
               */
           2> lock_kernel().
           3> if @request->fl_type is not F_UNLCK
              then    /* conflicting checking */
                      /* we are going to place a file lock,thus have to
                       * checks lock conflicting at first.
                       * file locks owned by the same process would never conflict.
                       */
                      for_each_lock(@inode, struct file_lock **before) {
                              if **before is not POSIX lock,then continue.
                              if !posix_locks_conflict(@request, *@before),then continue.
                              /* this routine is similar to flock_locks_conflict() */
                              if @conflock is not NULL,then copy *@before into @conflock
                              set error code to -EGAIN
                              if FL_SLEEP unspecified in @request->fl_flags,then goto label "out"
                              /* we found out conflict lock,but nonblock been specified */
                              set error code to -EDEADLK
                              if posix_locks_deadlock(@request, *@before),then goto label "out"
                              /**
                               * deadlock checking
                               * owner of @before is same as @request
                               * and MAX_DEADLK_ITERATIONS been exceeded
                               */
                              set error code to FILE_LOCK_DEFERRED
                              locks_insert_block(*@before, @request)
                              /* @before is conflicting with @request */
                              goto label out
                      }
           4> we can place @request on the file.
              set error code to 0.
              if FL_ACCESS been specified,then goto label "out".
              set @before to &@inode->i_flock.

              skip locks owned by other processes via a while-cycle.
              while ((@fl(local file_lock pointer) = *@before) AND
                     (!IS_POSIX(@fl) OR !posix_same_owner(@request, @fl)))
                      @before = &@fl->fl_next
           5> process locks with this owner.
              while ((@fl = *@before) AND posix_same_owner(@request, fl)) {
                                                 /* F_RDLCK or F_WRLCK */
                      checks if @request has the same type with @fl,then {

                                    +-> fl->fl_end
                                    |
                              --------------------------------------------
                                      |
                                      +-> request->fl_start
                                
                              if @fl->fl_end < @request->fl_start - 1 then
                              set @before to &@fl->fl_next,and continue.

                                            +-> fl->fl_start
                                            |
                              --------------------------------------------
                                          |
                                          +-> request->fl_end

                              if @fl->fl_start - 1 > @request->fl_end,then
                              break. /* we can insert @request at there */

                              /* the new and old lock are of the same type and
                               * adjacent or overlapping.make one lock yielding
                               * from the lower start address of both locks
                               * to the higher end address.
                               */

                              /* merge two file lock to remove overlapping */
                                        +-> fl->fl_start
                                        |          +-> request->fl_start
                              --------------------------------------------
                                                  |                 +-> request->fl_end
                                                  +-> fl->fl_end

                              if @fl->fl_start > @request->fl_start,then
                              @fl->fl-start = @request->fl_start.
                              else,@request->fl_start = @fl->fl_start.

                              if @fl->fl_end < @request->fl_end,then
                              @fl->fl_end = @request->fl_end.
                              else,@request->fl_end = @fl->fl_end.

                              if @added(local variable,default is _zero_) is TRUE,
                              then,locks_delete_lock(@before) and continue.

                              set @request to @fl
                              set @added to 1
                      }
                      else {
                              /* processing for different lock types */

                                       +-> fl->fl_end
                                       |
                              --------------------------------------------
                                         |
                                         +-> request->fl_start

                              if @fl->fl_end < @request->fl_start,then goto label "next_lock".

                                                    +-> fl->fl_start
                                                    |
                              --------------------------------------------
                                                  |
                                                  +-> request->fl_end

                              if @fl->fl_start > @request->fl_end,then break cycle.
                              /* we can place @request at there */
                              
                              if @request->fl_type is F_UNLCK,then set @added to 1.

                              /**
                               * if new lock falls the region of old lock,then splite the old
                               * lock to three parts.
                               * [left, request, right]
                               * left and right have the same type,but request is different from
                               * them.
                               */

                                        (left)
                                        +-> fl->fl_start
                                        |         +-> request->fl_start
                              --------------------------------------------

                              if @fl->fl_start < @request->fl_start,then set @left(local variable) to @fl.

                                        (right)
                                        +-> fl->fl_end
                                        |
                              --------------------------------------------
                                       |
                                       +-> request->fl_end

                              if @fl->fl_end > @request->fl_end,then set @right(local variable) to @fl,break
                              cycle.

                              /* request lock covers the old one,then use new one to instead the old one,
                               * and of course,the file lock type will also be changed.
                               */
                                       +-> request->fl_start
                                       |  +-> fl->fl_start
                              --------------------------------------------
                                                     +-> fl->fl_end  |
                                                                     +-> request->fl_end

                              if @fl->fl_start >= @request->fl_start,then {
                                      /* the new lock completely replaces an old one */
                                      if @added is TRUE,then locks_delete_lock(@before),delete the old one,
                                      and continue.

                                      locks_wake_up_blocks(@fl),wake up waiters,because we are going to
                                      delete the old one.
                                      
                                      @fl->fl_start := @request->fl_start
                                      @fl->fl_end := @request->fl_end
                                      @fl->fl_type := @request->fl_type

                                      locks_release_private(@fl) /* release private info */
                                      locks_copy_private(@fl, @request) /* copy private info */
                                      
                                      @request := @fl (pointer assignment)
                                      set @added to 1                    
                              }
                      }
                      next_lock:
                              @before = &@fl->fl_next
              }
           6> /* new lock(s) need to be inserted */
              set error code to -ENOLCK
              if @right is not NULL AND @left is equal to @right AND @new_fl2 is NULL
                      goto label "out"
              set error code to 0
           7> if @added is FALSE,then
                      if @request->fl_type is F_UNLCK,then checks if @request->fl_flags enabled
                      FL_EXISTS,the condition is TRUE,then set error code to -ENOENT;else,goto
                      label "out".

                      if @new_fl is NUL,then set error code to -ENOLCK,and goto label "out".

                      locks_copy_lock(@new_fl, @request)  /* copy arg2 to arg1 */
                      locks_insert_lock(@before, @new_fl) /* @new_fl->next = *@before */
                                                          /* @new_fl->fl_link added into @file_lock_list */
                      set @new_fl to NULL
           8> if @right is not NULL,then
                      checks if @left is equal to @right,then
                              set @left to @new_fl2
                              set @new_fl2 to NULL
                              locks_copy_lock(@left, @right)
                              locks_insert_lock(@before, @left)
                      set @right->fl_start to @request->fl_end + 1
                      locks_wake_up_blocks(@right)
           9> if @left is not NULL,then
                      set @left->fl_end to @request->fl_start - 1
                      locks_wake_up_blocks(@left)
           out:
           10> unlock_kernel()
               locks_free_lock() @new_fl,if it is not NULL
               locks_free_lock() @new_fl2,if it is not NULL
               return error code to caller.


/* END OF CHAPTER12 */


Chapter 13 : I/O Architecture and Device Drivers
    I/O Architecture :
      buses >
        data paths let information flow between CPU(s),RAM,and the I/O devices that can be connected
        to the paths.
        such data paths are buses.
        bus act as the primary communication channels inside the computer.

      Any computer has a "system bus" that connects most of the internal hardware devices.
      PCI bus - Peripheral Component Interconnect bus,a typical system bus.
      Others - ISA bus,EISA bus,MCA bus, SCSI bus,USB bus,etc.
      ! typically,the same computer includes several buses of different types,linked together by
        hardware devices called bridges.
      ! any I/O devices is hosted by one,and only one,bus.the bus type affects the internal design
        of the I/O device,as well as how the device has to handled by the kernel.
      
      two high-speed buses :
        frontside bus - connects the CPUs to the RAM controller,used to transfers data from CPUs to memory
                        chips,and transfers data from memory chips to CPUs.
        backside bus - connects the CPUs directly to the external hardware cache.

      host-bridge :
        links together the system bus and the frontside bus.

      I/O bus :
        data path that connects a CPU to an I/O device is generically called an I/O bus.
        the I/O bus is connected to each I/O device by means of a hierarchy of hardware components including
        up to three elements: I/O ports, interfaces, device controllers.

      ! 80x86 microprocessors use 16 of their address pins to "address I/O devices",and 8,16,or 32 of
        their data pins to "transfer data".


                                      +-----+
                                      |     |
                                      | CPU |
                                      |     |
                                      +-----+
                                         |
                                         |
                         ------------------------------------- (I/O bus)
                                     |              |
                             +-------|--------------|---------+
                             | +-----|--------------|-------+ |
                             | | +----------+  +----------+ | |         +------------+
                             | | | I/O Port |  | I/O Port | | | <-----> | I/O Device |
                             | | +----------+  +----------+ | |         +------------+
                             | +----------------------------+ |
                             |       I/O Interface            |
                             +--------------------------------+
                                     I/O Controller

      I/O Ports :
        each device connected to the I/O bus has its own set of I/O addresses,that are the
        I/O Ports.

        I/O Ports in IBM PC architecture :
          the I/O address space provides up to 65536 8-bit I/O ports.
          16-bit I/O port - two consecutive 8-bit I/O ports,the starting address must be even address.
          32-bit I/O port - two consecutive 16-bit I/O ports,the starting address must be multiple of 4.

        Asssembly instructions for I/O in IA-32 architecture :
          in{bwl} imm8 / (%dx)
            imm - immediate port
            8/16/32 - 8-bit 16-bit 32-bit
            DX -> register used to store the immediate port number
                  when 8-bit immediate port is specified,the upper-eight bits will be 0
            in - transfers a byte,word,or long from the immediate port into the byte,word,or
                 long memory address pointed to by the AL,AX or EAX
                 transfers a byte,word,or long from the immediate port stored in DX into the
                 byte,word,or long memory address pointed by the AL,AX or EAX

          ins{bwl}
            /* DX to ES:(E)DI */
            DX - register used to store the immediate port number
            ES:(E)DI - section pointer:string operation destination pointer
            ins - transfers byte,word,or long from the immediate port into the byte,word,
                  or long memory address specified by ES:(E)DI
                  the counter of transfers is stored in CX,and flag DF used to control direction
                  of (E)DI
                  ins is used to transfers string.

          out{bwl} imm8/(%dx)
            out - AL / AX / EAX -> imm8 / imm16 / imm32 /* AL / AX / EAX to imm8 / imm16 / imm32 */
                  AL / AX / EAX -> DX                   /* AL / AX / EAX to DX */
          outs{bwl}
            outs - ES:(E)DI -> DX /* ES:(E)DI to DX */
                   CX as counter
                   DF control direction
            
        I/O ports may also be mapped into addresses of the physical address space,then CPU can communicate
        to the I/O device via the physical memory address.modern hardware devices are more suited to mapped
        I/O,because it is faster and can be combined with DMA(Directly Memory Access).

        I/O device registers :
          for offer a unified approach to I/O programming without down performance,the I/O ports of
          each device are structed into a set of specialized registers.

          device control register :
            this register used to command the device,CPU can writes the commands into this register for
            control the device.

          device status register :
            this register stores the current internal status of the I/O device,CPU can read from this
            register to knows what this I/O device is doing.

          device input register :
            this register is used by CPU to fetches data from the I/O device.

          device output register :
            this register is used by CPU to send data into the I/O device.

                                 | Control Register |

                                 | Status Register  |
                                 
                  CPU <----------|                  |---------> Device's I/O Interface

                                 | Input Register   |

                                 | Output Register  |

          ! to lower costs,the same I/O port is often used for different purpose.
            e.g.
              device control register is combined with device status register
              device input register can be used as device output register

        Accessing I/O ports :
          kernel has wrapped up the assembly instructions "in","out","ins","outs" into several 
          auxiliary functions.
          ! in the header for generic assembly I/O <asm-generic/io.h>,these functions are implemented
            by access to the memory address where mapped an I/O port.
            but in the header for architecture dependent assembly I/O  <arch/x86/include/asm/io.h>,
            these functions are implemented by inline assembly,and functions readX(),writeX() are 
            used to read/write a memory address where mapped an I/O port.    /* X => b, w, l */

          kernel defined :
            inb, inw, inl               =>  in[b/w/l]
            inb_p, inw_p, inl_p         =>  in[b/w/l] with an io delay
            outb, outw, outl            =>  out[b/w/l]
            outb_p, outw_p, outl_p      =>  out[b/w/l] with an io delay
            insb, insw, insl            =>  ins[b/w/l]
            outsb, outsw, outsl         =>  outs[b/w/l]
          
          <arch/x86/include/asm/io.h>
            #define BUILDIO(bwl, bw, type)                             \
            static inline void out##bwl(unsigned type value, int port) \
            {                                                          \
                    asm volatile("out" #bwl " %" #bw "0, %wl"          \
                                 : : "a"(value), "Nd"(port));          \
            }                                                          \
                                                                       \
            static inline unsigned type in##bwl(int port)              \
            {                                                          \
                    ...                                                \
            }                                                          \
                    ...                                                \
                    ...                                                \
                                                                       \
            static inline void outs##bwl(int port, const void *addr,   \
                                         unsigned long count)          \
            {                                                          \
                    asm volatile("rep; outs" #bwl                      \
                                 : "+S"(addr), "+c"(count) : "d"(port)); \
            }                                                          \
                    ...
            
            ! for the function execute a "dummy" instruction to introduce pause
              is implemented by call to slow_down_io() right after the auxiliary
              function.
            ! slow_down_io() is implemented by native_io_delay(),which is defined in
              <arch/x86/kernel/io_delay.c>.
              the kernel supports three io delay types :
                "0x80" -> outb %al, $0x80 /* transfer one byte in AL to I/O port 0x80 */
                "0xed" -> outb %al, $0xed
                "udelay" -> udelay(2)
                "none" -> none io delay
              ! the delay type can be selected by send kernel parameter io_delay="<type>".
              # I/O port 0x80 => POST - Power On Self Test
              # I/O port 0xed => an alternative for I/O port 0x80 while conflict with
                                 other hardware

          detecting which I/O ports have been assigned to I/O devices is more complex than accessing
          an I/O port,in paritcular,for systems based on an ISA bus.
          often a device driver must blindly write into some I/O ports to probe the hardware device,
          but in the case that I/O ports been assigned to another hardwar device,this writing may
          cause system crash.for prevent such case,kernel have to keep track of I/O ports assigned to
          each hardware device by means of "resources".

          the structure resource :
            <linux/types.h>
              typedef phys_addr_t resource_size_t;

            <linux/ioport.h>
              /**
               * ioport_resource - the root of I/O port tree of this system,
               *                   it spans the whole I/O address space from
               *                   port number 0 to 65535
               */
              extern struct resource ioport_resource;

              /**
               * resource - I/O port resource tree node
               * @start:    start of resource range
               *            # @start it is not the I/O port number,
               *              it is the starting address of memory area
               *              where mapped some device I/O ports
               * @end:      end of resource range
               * @name:     description of owner of the resource
               * @flags:    various flags
               * @parent:   parent of this node
               * @sibling:  sibling of this node
               * @child:    the first child of this node
               * # general rule :
               *     each node of the tree must correspond to a subrange of the
               *     range associated with the parent node
               *   follow this rule,we can split a range of I/O port to several
               *   subranges to satisfy device requirement
               *   # e.g.
               *       IDE disk interface - 0xf000 to 0xf00f as the I/O port addresses,
               *                            0xf000 to 0xf007 is used for the master disk
               *                            of the IDE chain
               *                            0xf008 to 0xf00f is used for the slave disk
               */   
              struct resource {
                      resource_size_t start;
                      resource_size_t end;
                      const char *name;
                      unsigned long flags;
                      struct resource *parent, *sibling, *child;
              };

              /**
               * request_region - shortcut for get a new resource object which spanned
               *                  subrange[@start, @start + n - 1] of resource of @ioport_resource,
               *                  returns the new allocated resource object
               * # kzalloc() with gfp flag GFP_KERNEL
               */
              #define request_region(start, n, name) __request_region(&ioport_resource, (start), (n), (name), 0)

              /**
               * release_region - release a resource which spanned subrange[@start, @start + n - 1] of
               *                  resource of @ioport_resource
               */
              #define release_region(start, n) __release_region(&ioport_resource, (start), (n))

              /**
               * request_resource - try to request a resource range in the resource
               *                    tree
               * @root:             the resource tree
               * @new:              new resource node spans a resource range
               * return:            NULL(succeed) OR address of a child of @root which
               *                    have spanned the new subrange
               * # EXPORT_SYMBOL
               */
              extern int request_resource(struct resource *root, struct resource *new);

              /**
               * allocate_resource - allocate empty slot in the resource tree given range
               *                     & alignment
               * @root:              root resource descriptor
               * @new:               resource descriptor desired by caller
               * @size:              requested resource region size
               * @min:               minimum size to allocate
               * @max:               maximum size to allocate
               * @align:             alignment requested,in bytes
               * @alignf:            alignment function,optional,called if not NULL
               * @alignf_data:       arbitrary data to pass to the @alignf function
               * return:             0 OR error code
               * # EXPORT_SYMBOL
               * # this function do not allocate memory for @new,the caller have to prepare
               *   a resource object before call to this function
               * # if all conditions been satisfied,then @new will spans the new subrange
               *   of @root,then __request_resource() will be called for insert @new
               * # this routine will write_lock @resource_lock before call to find_resource(),
               *   and release the lock before return to caller
               *   (@resource_lock - static DEFINE_RWLOCK(resource_lock) in <kernel/resource.c>)
               */
              extern int allocate_resource(struct resource *root, struct resource *new,
                                           resource_size_t size, resource_size_t min,
                                           resource_size_t (*alignf)(void *,
                                                                     const struct resource *,
                                                                     resource_size_t,
                                                                     resource_size_t),
                                           void *alignf_data);

      I/O Interfaces :
        an I/O Interface is a hardware circuit inserted between a group of I/O ports and the
        corresponding device controller.
                     
                      [I/O port]                               +----------------+
                      [I/O port]                               |                |
                      [I/O port]        [I/O Interface]        | I/O Controller |       
                         ...                   |               |                |
                      [I/O port]               |               +----------------+
                                               |
                                               | => IRQ Line
                                               |
                                               V
                                            [ PIC ]

        the I/O interface as an interpreter that translate the values in the I/O ports into
        commands and data for the device;in the opposite direction,it detects change in the
        device state and correspondingly updates the I/O port that plays the role of status
        register.
        use the IRQ Line,I/O Interface could be connected to PIC,then allows the CPU interactive
        with the device.

        type of I/O Interfaces :
          1> Custom I/O Interfaces
               Custom I/O Interface is used for one specific hardware device.In some cases,the
               device controller is located in the same "card" that contains the I/O interface.
               the devices attached to a custom I/O interface can be either internal devices or
               external devices.
               /**
                * card :
                *   each card must be inserted in one of the available free bus slots of the PC
                *   if the card can be connected to an external device through an external cable,
                *   the card supports a suitable connector in the rear panel of the PC.
                *   e.g. built-in network card
                */
               the common custom I/O interfaces :
                 Keyboard interface          -       connected to a keyboard controller
                 Graphic interface           -       packed together withe the graphic controller
                 Disk interface              -       connected by a cable to the disk controller
                 Bus mouse interface         -       connected by a cable to the mouse controller
                 Network interface           -       packed together with the network card controller
          2> General Purpose I/O Interfaces
               General Purpose I/O Interface is used to connect several different hardware device.
               devices attached to a general-purpose I/O interface are usually external devices.

               the most common general purpose I/O interfaces :
                 Parallel port
                   - tranditionally used to connect printers,removable disks,scanners,backup units
                     and other computers.the data transfered once on parallel port is 8-bit.
                 Serial port
                   - link the parallel port,but the data is transferred 1 bit at a time.
                     it includes a Universal Asynchronous Receiver and Transmitter(UART) chip to
                     string out the bytes to be sent into a sequence of bits and to reassemble the
                     received bits into bytes.
                     speed is lower than parallel port,usually used for external devices that do not
                     operate at a high speed.
                 PCMCIA interface
                   - included mostly on portable computers.the external device,which has the shape of
                     a credit card,can be inserted into and removed from a slot without rebooting the
                     system.
                     the most common PCMCIA devices are hard disks,moderns,network cards,and RAM expansions.
                 SCSI interface(Small Computer System Interface)
                   - a circuit that connects the main PC bus to a secondary bus called the SCSI bus.
                     SCSI-2 bus allows up to eight PCs and external devices.
                     Wide SCSI-2 and SCSI-3 interfaces allow connect to 16 devices or more if additional
                     interfaces are present.
                 Universal serial bus(USB)
                   - a general purpose I/O interface that operates as a high speed and may be used for
                     the external devices traditionally connected to the parallel port,the serial port,
                     and the SCSI interface.

      Device Controllers :
        a complex device may require a device controller to drive it.
        the roles that device controller plays :
          1> it interprets the high-level commands received from the I/O interface and forces
             the device to execute specific actions by sending proper sequences of electrical
             signals to it.
          2> it converts and properly interprets the electrical signals received from the device
             and modifies the value of the status register through the I/O interface.

        for example,the disk device controller,which interprets the command from I/O interface(from CPU),
        and then read from/write into the disk.before read and write,it have to move disk head on the
        right track,and then let disk process read/write action.everytime the action finished,an interrupt
        would be raised on the IRQ line.
        /**
         * modern disk controllers can keep the disk data in on-board fast disk caches,and can reorder the
         * CPU high-level requests to make an optimization for disk behavior.
         */

        the device no device controller,such PIT,the Programmable Interval Timer.

        I/O Shared Memory :
          the memory chip that is built-in the hardware device.such graphic card RAM,or network card RAM.

      The Device Driver Model :
        The device driver should typically take care of :
          1> Power management
               handling of different voltage levels on the device's power line
               # power management is performed globally by the kernel on every hardware device
                 in the system.the order of change power-level should follow a well defined order.
                 for example,when put the hard disk device into "standby"(low-power) state,must put
                 the disk at first,then is disk controller.
          2> Plug and play
               transparent allocation of resources when configuring the device
          3> Hot-pluging
               support for insertion and removal of the device while the system is running

          Linux provides some data structures and helper functions that offer a unifying view of
          all buses,devices,and device drivers in the system,the framework is called the
          "device driver model".

        The sysfs Filesystem :
          sysfs filesystem is similar to proc filesystem.the proc filesystem allows User Mode applications
          is able to access kernel internal data structures.and the sysfs filesystem,provides additional
          information on kernel data structures.
          
          goal of the sysfs filesystem :
            expose the hierarchy relationships among the components of the device driver model.

          top directories of sysfs :
            block
              the block devices,independently from the bus to which they are connected
            devices
              all hardware devices recognized by the kernel,organized according to the bus
              in which they are connected
            bus
              the buses in the system,which host the devices
            drivers
              the device drivers registered in the kernel
            class
              the types of devices in the system,the same class may include devices hosted by
              different buses and driven by different drivers
            power
              files to handle the power states of some hardware devices
            firmware
              files to handle the firmware of some hardware devices

            /**
             * sysfs in newer kernel version :
             *   block bus class dev devices firmware fs kernel module power
             *                    |                   |  |      |
             *                    |                   |  |      +-> registered kernel module
             *                    |                   |  +-> kernel related infos,such boot_params,irq,slab,etc
             *                    |                   +-> supported filesystem
             *                    +-> (block, char)
             */
            ! the relationships between components of the device driver models are expressed in the sysfs
              as symbolic links between directories and files.
              the main role of regular files in sysfs is to represent attributes of drivers and devices.

        Kobjects :
          the core data structure of device driver model is a generic data structure named kobject,each
          kobject corresponds to a directory in sysfs "/sys".
          ! kobject is embedded inside larger object(container),and it(the container) describes the components
            of the device driver model.
            /* e.g containers: device, bus, driver, ... */

          embedding a kobject inside a container allows the kernel to :
            1> keep a reference counter for the container
            2> maintain hierarchical lists or sets of containers
            3> provide a User Mode view for the attributes of the container

          Kobjects,ksets,and subsystems :
            <linux/sysfs.h>
              /**
               * attribute - attribute for the element in sysfs filesystem
               * @name:      the name
               * @owner:     who constructed this elemtn,and also used 
               *             as the eject protector
               * @mode:      file mode
               */
              struct attribute {
                      const char *name;
                      struct module *owner;
                      mode_t mode;
              #ifdef CONFIG_DEBUG_LOCK_ALLOCK
                      struct lock_class_key *key;
                      struct lock_class_key skey;
              #endif
              };
              
              /* sysfs_ops - sysfs filesystem operations */
              struct sysfs_ops {
                      ssize_t (*show)(struct kobject *, struct attribute *, char *);
                      ssize_t (*store)(struct kobject *, struct attribute *, const char *, size_t);
              };

            <fs/sysfs/file.c>
              /**
               * sysfs_open_dirent - represent opened regular file in sysfs
               * @refcnt: reference counter
               * @event:  event counter
               * @poll:   poll supporting
               * @buffers:    by use this list_head object,goes through sysfs_buffer.list
               */
              struct sysfs_open_dirent {
                      atomic_t refcnt;
                      atomic_t event;
                      wait_queue_head_t poll;
                      struct list_head buffers;
              };

              /**
               * sysfs_buffer - hold the contents of an opened regular file in sysfs
               * @count:        counter
               * @pos:          current po
               * @page:         pointer to the memory area storing the contents
               * @ops:          sysfs operation
               * @mutex:        protector
               * @needs_read_fill:    indicator for whether should fill the buffer by reading
               *                      file
               * @event:        event indicator
               * @list:         to another buffers of the opened file
               */
              struct sysfs_buffer {
                      size_t count;
                      loff_t pos;
                      char *page;
                      const struct sysfs_ops *ops;
                      struct mutex mutex;
                      int needs_read_fill;
                      int event;
                      struct list_head list;
              };

            <fs/sysfs/sysfs.h>
              /* sysfs_elem_dir - sysfs element for directory type */
              struct sysfs_elem_dir {
                      struct kobject *kobj;
                      struct sysfs_dirent *children;
              };

              /* sysfs_elem_dir - sysfs element for symlink type */
              struct sysfs_elem_symlink {
                      struct sysfs_dirent *target_sd;
              };

              /* sysfs_elem_attr - sysfs element for regular file type */
              struct sysfs_elem_attr {
                      struct attribute *attr;
                      struct sysfs_open_dirent *open;
              };

              /* sysfs_elem_bin_attr - sysfs element for binary file type */
              struct sysfs_elem_bin_attr {
                      struct bin_attribute *bin_attr;
                      struct hlist_head buffers; /* similar to the buffers of regualr file in sysfs */
              };

              /* sysfs_inode_attrs - sysfs inode attributes */
              struct sysfs_inode_attrs {
                      struct iattr ia_iattr;
                      void *ia_secdata;
                      u32 ia_secdata_len;
              };

              /**
               * sysfs_dirent - dentry in sysfs filesystem,it different from VFS dentry that
               *                is because of sysfs is in-memory filesystem
               * @s_count:      counter,recyle the dentry when this member becomes _zero_
               * @s_active:     usage counter
               * @s_parent:     parent sysfs dentry
               * @s_sibling:    sibling dentry in the hierarchy tree
               * @s_name:       name of the dentry
               * @s_dir:        directory implementation
               * @s_symlink:    symlink implementation
               * @s_attr:       regular file implementation
               * @s_bin_attr:   binary implementation
               * @s_flags:      flags
               * @s_mode:       file mode
               * @s_ino:        inode number
               * @s_iattr:      inode attribute
               */
              struct sysfs_dirent {
                      atomic_t s_count;
                      atomic_t s_active;
              #ifdef CONFIG_DEBUG_LOCK_ALLOC
                      struct lockdep_map dep_map;
              #endif
                      struct sysfs_dirent *s_parent;
                      struct sysfs_dirent *s_sibling;
                      const char *s_name;

                      union {
                              struct sysfs_elem_dir s_dir;
                              struct sysfs_elem_symlink s_symlink;
                              struct sysfs_elem_attr s_attr;
                              struct sysfs_elem_bin_attr s_bin_attr;
                      };

                      unsigned int s_flags;
                      unsigned short s_mode;
                      ino_t s_ino;
                      struct sysfs_inode_attrs *s_iattr;
              };

            <linux/kref.h>
              /**
               * kret - kobject reference counter,also could be the reference counter of
               *        the container in which the kobject is embedded
               * # method kobject_get() increase the counter,and method kobject_put() decrease
               *   the counter
               * # extern struct kobject *kobject_get(struct kobject *kobj); returns @kobj
               *   extern void kobject_put(struct kobject *kobj);
               * # these two methods are declared in <linux/kobject.h> and implemented in
               *   <lib/kobject.c>
               * # kobject will be recyled when kref becomes _zero_
               */
              struct kref {
                      atomic_t refcount;
              };
              
              /**
               * kset - a set of kobjects of a specific type,belonging to a specific subsystem
               * @list:         the list of all kobjects for this kset
               * @list_lock:    concurrently access protector
               * @kobj:         the embedded kobject for this set(recursion)
               * @uevent_ops:   the set of uevent operations for this set
               *                these are called whenever a kobject has something happen to it,
               *                so that the kset can add new environment variables,or filter
               *                out the uevents if so desired
               * # a kset difines a group of kobjects,they can be individually different "types",
               *   but overall these kobjects all want to be grouped together and operated on in
               *   the same manner
               * # ksets are used to define the attribute callbacks and other common events that happen
               *   to a kobjecj
               * # kset reference counter is represented by @kobj->kref,the methods kset_get() and kset_put()
               *   just call to kobject_get() and kobject_put() on @kobj as well
               * # kset could be a member of another kset by insert the kobject structure @kobj
               */
            <linux/kobject.h>
              struct kset {
                      struct list_head list;
                      spinlock_t list_lock;
                      struct kobject kobj;
                      const struct kset_uevent_ops *uevent_ops;
              };

              /**
               * kobj_type - kobject type,essentially,the type of the container that includes the kobject,
               *             some special methods and attributes are held by this structure 
               * @release:   method executed when the kobject is being freed
               * @sysfs_ops: sysfs operations
               * @default_attrs:    a list of default attributes for the sysfs filesystem
               * # about @release,the method usually defined only if the container of the kobject
               *   was allocated dynamically,frees the container itself
               */
              struct kobj_type {
                      void (*release)(struct kobject *kobj);
                      const struct sysfs_ops *sysfs_ops;
                      struct attribute **default_attrs;
              };

              /**
               * kobject - structure used to describe the kernel device driver model components
               * @name:    a string holding the name of the container,the large object this
               *           kobjec embedded
               * @entry:   pointers for the list in which the kobject is inserted
               * @parent:  parent kobjec,if any
               * @kset:    kobject group information,that is the group this kobjec belonging to
               * @ktype:  kobjec type descriptor
               * @sd:      sysfs dentry info
               * @kref:    reference counter
               * @state_initialized:    state initialization indicator
               * @state_in_sysfs:       sysfs export indicator
               * @state_add_uevent_sent:    state indicator for add uevent_sent
               * @state_remove_uevent_sent: state indicator for remove uevent_sent
               * @uevent_suppress:      uevent suppress indicator
               * # the kobjects have the same type in the kset,the @parent field of these
               *   kobjects is points to kset->@kobj if it has not a parent already
               */
              struct kobject {
                      const char *name;
                      struct list_head entry;
                      struct kobject *parent;
                      struct kset *kset;
                      struct kobj_type *ktype;
                      struct sysfs_dirent *sd;
                      struct kref kref;
                      unsigned int state_initialized:1;
                      unsigned int state_in_sysfs:1;
                      unsigned int state_add_uevent_sent:1;
                      unsigned int state_remove_uevent_sent:1;
                      unsigned int uevent_suppress:1;
              };

            /**
             * subsystems,collections of ksets called subsystems.a subsystem may include ksets of
             * different types,and it is represented by a subsystem data structure having just two
             * fields.
             *   - From "Understan Linux Kernel"
             * ! BUT LINUX 2.6.34.1 NO SUCH STRUCTURE IS EXISTED,BECAUSE THIS DATA STRUCTURE BEEN
             *   REMOVED SINCE LINUX 2.6.22,AND ALL APIs NEED SUCH ARGUMENT BEEN CHANGED.
             */
            Example for Device Driver Model Hierarchy in "/sys" :
              /sys/bus/pci/drivers/serial/new_id
               |   |   |   |       |      |
               |   |   |   |       |      |
               |   |   |   |       |      +-> attribute
               |   |   |   |       +-> kobject
               |   |   |   +-> kset
               |   |   +-> subsystem
               |   +-> subsystem
               +-> root

          Registering Kobjects,Ksets,and subsystems(been removed) :
            a general rule,if you want a kobject,kset to appear in the sysfs subtree,must first
            register it.for the directories of kobjects included in the same kset,appear in the
            directory of the kset itself.

            !! LINUX 2.6.34.1 HAVE REMOVED ROUTINES kobject_register() AND kobject_unregister(),
               BUT THE ROUTINE kset_register() AND kset_unregister() STILL AVAILABLE.

            The related routines :
              <linux/kobject.h>
                /**
                 * kobject_set_name - set the name of a kobject
                 * @kobj:             kobject
                 * @name:             format string used to build the name
                 * return:            0 OR -ENOMEM
                 * # EXPORT_SYMBOL
                 * # this routine actually call to kobject_set_name_vargs() with a packed
                 *   va_list variable
                 */
                extern int kobject_set_name(struct kobject *kobj, const char *fmt, ...)
                __attribute__((format(printf, 2, 3)));

                /**
                 * kobject_set_name_vargs - set the name of a kobject
                 * @kobj:                   kobject
                 * @fmt:                    format string used to build the name
                 * @vargs:                  vargs to format the string
                 * return:                  0 OR -ENOMEM
                 * # this routine call to kvasprintf() to build the name,which kmalloc()
                 *   memory with GFP_KERNEL to get an area to store the name,and then
                 *   call to vsnprintf() to build the name,fanally,returns the new pointer
                 *   to caller,and the function vsnprintf() is similar to the vsnprintf()
                 *   in C library
                 * # this routine will enter a while-cycle to covert all '/' to '!'
                 */
                extern int kobject_set_name_vargs(struct kobject *kobj, const char *fmt,
                                                  va_list vargs);

                !! IF THE kobject BEEN HAS A NAME,FOR CHANGE NAME,MUST CALL TO kobject_rename().
                
                /* kobject_name - returns @kobj->name */
                static inline const char *kobject_name(const struct kobject *kobj);

                /**
                 * kobject_init - initializes a kobject with a given kobject type
                 * @kobj:         kobject
                 * @ktype:        kobject type for @kobj
                 * # EXPORT_SYMBOL
                 * # this routine dump_stack() when NULL @kobj,or NULL @ktype,or @kobj been
                 *   initialized(@kobj->state_initialized)
                 * # this routine call to kobject_init_internal() to initializes @kobj,and then
                 *   use @ktype to set @kobj->ktype
                 * # kobject_init_internal() is defined in the same file <lib/kobject.c>,and
                 *   which initializes the reference counter @kref,the list @entry,and state indicators,
                 *   at the end,set @state_initialized to 1
                 */
                extern void kobject_init(struct kobject *kobj, struct kobj_type *ktype);

                /**
                 * kobject_add - add a kobject,the kobject name is set and added to the kobject hierarchy
                 *               in this function
                 * @kobj:        kobject
                 * @parent:      its parent
                 * @fmt:         format to name the kobject with
                 * return:       0 OR error code
                 * # EXPORT_SYMBOL
                 * # this function actually call to kobject_add_varg() with a packed va_list variable
                 * # kobject_add_varg() :
                 *     call to kobject_set_name_vargs() to setup @kobj->name with the va_list argument
                 *     set @kobj->parent to @parent
                 *     call to kobject_add_internal() on @kobj
                 *   kobject_add_internal() :
                 *     get parent of @kobj via kobject_get()
                 *     if @kobj->kset is not NULL {
                 *       increase ref counter of @kobj->kset->kobj
                 *       call to kobj_kset_join(),and this routine will increase ref counter of @kobj->kset,next,
                 *       list_add_tail() @kobj->entry to @kobj->kset->list
                 *       set @kobj->parent to local variable parent
                 *     }
                 *     create_dir() for @kobj,dump_stack() if any error encoutnered,set @kobj->state_in_sysfs to 1
                 *     if succeed to create directory in sysfs
                 */
                extern int __must_check kobject_add(struct kobject *kobj, struct kobject *parent,
                                                    const char *fmt, ...);

                /* kobject_init_and_add - initialize a given kobject and add it into hierarchy */
                extern int __must_check kobject_init_and_add(struct kobject *kobj, struct kobj_type *ktype,
                                                             struct kobject *parent, const char *fmt, ...);

                /**
                 * kobject_del - delete a kobject from hierarchy
                 * @kobj:        kobject
                 * # this function will does :
                 *     call to sysfs_remove_dir() on @kobj to remove the corresponding directory in sysfs
                 *     @kobj->state_in_sysfs = 0
                 *     kobj_kset_leave() to let @kobj leave the kset @kobj->kset
                 *     decrease ref counter of @kobj->parent
                 *     @kobj->parent = NULL
                 */
                extern void kobject_del(struct kobject *kobj);

                /**
                 * kobject_create - kzalloc() a new kobject object with gfp flag GFP_KERNEL,and then init it,
                 *                  returns the pointer to the new kobject or NULL
                 */
                extern struct kobject *__must_check kobject_create(void);

                /* kobject_create_and_add - create a new kobject and add it into hierarchy &/
                extern struct kobject *__must_check kobject_create_and_add(const char *name, struct kobject *parent);

                /* kobject_get() and kobject_put() been described above */

                /**
                 * kobject_get_path - generate and return the path associated with a given kobj and kset pair
                 * @kobj:             kobject
                 * @flag:             gfp flag used to allocate memory for the path
                 * return:            pointer to the memory area where stored the path OR NULL
                 *                                                                        (failed allocate)
                 *                                                                        (_zero_ length)
                 * # EXPORT_SYMBOL
                 * # this routine will call to get_kobj_path_length() to know how many bytes of memory should
                 *   to be allocated,returns NULL if the length is _zero_;then kzalloc() memory,returns NULL
                 *   if failed.finally,call to fill_kobj_path() to get the path
                 * # fill_kobj_path() :
                 *     traverse all parents in the hierarchy of @kobj,and copy the name into buffer at each
                 *     iteration.the starting point is @kobj itself
                 */
                extern char *kobject_get_path(struct kobject *kobj, gfp_t flag);

              <linux/sysfs.h>
                /**
                 * sysfs_create_file - create the attribute file of a given kobject in sysfs
                 * @kobj:              kobject
                 * @attr:              attribute
                 * return:             0 OR error code
                 * # this routine actually call to sysfs_add_file() with arguments @kobj->sd, @attr,
                 *   SYSFS_KOBJ_ATTR
                 * # sysfs_add_file() call to sysfs_add_file_mode() with the file mode @attr->mode
                 * # sysfs_add_file_mode() will call to sysfs_new_dirent() for create a new directory
                 *   with @attr->name, (@attr->mode & S_IALLUGO) | S_IFREG, SYSFS_KOBJ_ATTR
                 *   then call to sysfs_add_one() to add the attribute file
                int __must_check sysfs_create_file(struct kobject *kobj, const struct attribute *attr);

                /* sysfs_create_files - create several files,NULL as the end of attributes array */
                int __must_check sysfs_create_files(struct kobject *kobj, const struct attribute **attr);

                /**
                 * sysfs_create_link - create a symlink
                 * @kobj:              kobject used to represents the symlink
                 * @target:            the target link points to
                 * @name:              name of the symlink
                 * return:             0 OR error code
                 * # this routine actually call to sysfs_do_create_link(),which sysfs_new_dirent() for @name,
                 *   set the s_symlink.target_sd of @sd returned from sysfs_new_dirent() to the sysfs dirent
                 *   @target->sd,call to sysfs_add_one() to create the symlink
                 */
                int __must_check sysfs_create_link(struct kobject *kobj, struct kobject *target, const char *name);

      Components of the Device Driver Model :
        !! THE CORE DRIVER MODEL CODE CAN BE FOUND IN "drivers/base".

        Devices :
          each device in the device driver model is represented by a "device" object.

          <linux/device.h>
            /**
             * device - the device in device driver model
             * @parent:        parent device
             * @p:             device private data
             * @kobj:          kobject
             * @init_name:     initial name of the device
             * @type:          device type
             * @sem:           semaphore to synchronize calls to its driver
             * @bus:           type of bus that this device hosts on
             * @driver:        which driver has allocated this device
             *                 that is the controller of this device
             * @platform_data: platform specific data,device core does not touch it
             * @power:         device power management info
             * @numa_node:     NUMA node number
             * @dma_mask:      Direct Memory Access mask(if dma'able device)
             * @coherent_dma_mask:    like dma_mask,but for alloc_coherent mapping
             *                        as not all hardware supports 64 bit addresses
             *                        for consistent allocations such descriptors
             * @dma_parms:     DMA parameters
             * @dma_pools:     list of aggregate DMA buffers
             * @dma_mem:       coherent DMA memory used by the device,internal for
             *                 coherent mem override
             * @archdata:      architecture specified data
             * @devt:          major number and minor number of this device.it will
             *                 appears in "/sys/dev/block" or "/sys/dev/char" used to
             *                 represents this device
             * @devres_lock:   device resouce lock
             * @devres_head:   device resouce list
             * @knode_class:   class node that corresponds this device
             * @class:         class of this device
             * @groups:        attribute groups this device belongs to(optional)
             * @release:       method to release this device
             */
            struct device {
                    struct device *parent;
                    struct device_private *p;
                    struct kobject kobj;
                    const char *init_name;
                    struct device_type *type;
                    struct sempahore sem;
                    struct bus_type *bus;
                    struct device_dirver *driver;
                    void *platform_data;
                    struct dev_pm_info power;

            #ifdef CONFIG_NUMA
                    int numa_node;
            #endif

                    u64 *dma_mask;
                    u64 coherent_dma_mask;
                    struct device_dma_parameters *dma_parms;
                    struct list_head dma_pools;
                    struct dma_coherent_mem *dma_mem;
                    
                    struct dev_archdata archdata;
                    dev_t devt;
                    spinlock_t devres_lock;
                    struct list_head devres_head;

                    struct klist_node knode_class;
                    struct class *class;

                    const struct attribute_group **groups;

                    void (*release)(struct device *dev);
            };

            ! Memory architecture :
                Terminology :
                  Master >
                    the memory request entity,may be CPU PE or GPU,DMA

                  Consistent >
                    read/write order between several Master

                    hardware provides mechanism to prevent such problem,the
                    real protection is hand over to software
                    (load-acquire / store-release, lock, memory barrier)

                  Coherent >
                    data consistent between several Master each has its own
                    cache,if Master A updated data in its cache but have not
                    write-back it into memory,then Master B read data from memory
                    unit has the corresponding position will get an expired data

                    hardware provides mechanism to protect such problem,it do not
                    care about memory,just focus on consistent of caches,and write-back
                    caches if and only if it is necessary
                    ! should not disable cache directly,this will downgrade performance

            related routines -

            /* get_device - atomically increase the reference count for the device */
            extern struct device *get_device(struct device *dev);

            /* put_device - atomically decrease the reference count for the device */
            extern void put_device(struct device *dev);

            /**
             * device_register - register a device with the system
             * @dev:             the device is going to be registered
             * return:           0 OR error code
             * # EXPORT_SYMBOL_GPL
             * # this routine call to two separatible but ordered steps,it just a
             *   wrapper for them.one is device_initialize(),another is device_add().
             *   this routine returns the ret value of device_add()
             * # device_initialize() initializes @dev,this routine also get device
             *   reference count
             *   device_add() add @dev to device hierarchy,the addings include
             *     kobject add
             *     class symlink add
             *     attributes add
             *     bus add
             *     dmp sysfs add
             *     device pm add
             *     platform notify add
             *     create some files
             *   and creates some necessary files in sysfs filesystem
             * # _Never_ directly free @dev after calling this function,even if it 
             *   returned an error!always use put_device() to give up the reference
             *   initialized in this function instead
             */
            extern int __must_check device_register(struct device *dev);

            ! EXPORT_SYMBOL_GPL :
                device_initialize
                device_add
                device_del
                get_device
                put_device

            /**
             * device_unregister - unregister device from system
             * @dev:               device
             * # EXPORT_SYMBOL_GPL
             * # this routine do two steps.one is device_del(),another is put_device()
             * # device_del() removes the device from the lists we control from here,has
             *   it removed from the other driver model subsystems it was added to in
             *   device_add(),and then removes it from kobject hierarchy
             *   the removing :
             *     pm remove
             *     dpm sysfs remove
             *     parent children remove
             *     sys dev entry remove
             *     class symlink remove
             *     related files remove
             *     attributes remove
             *     bus remove
             *     device resource remove(release)
             *     platform notify remove
             *     kobject uevent remove
             *     kobject remove
             *     parent ref put
             *   !! IF CALLED device_add() MANUALLY,THEN SHOULD CALL TO THIS ROUTINE MANUALLY,TOO,
             *      NOT CALL IT FROM device_unregister()
             */
            extern void device_unregister(struct device *dev);

          The devices on the system can be found in "/sys/devices",of course,they are organized hierarchy.
          some devices may cannot work properly without parent device.
          /* e.g. PCI-based computer,a bridge computer between PCI bus and USB bus is the parent device of
           * the devices hosted on USB bus
           */
          the parenthood relationship of the kobject embedded in device also reflect the device hierarchy,
          the structure of the directories below "/sys/devices" matches the physical orgnization of the
          hardware devices.
          /* and of course,kref of kobject could be the reference counter of the device */

          about put_device(),it calls to kobject_put(),and which call to kref_put() with kobject_release().
          kref_put() call to kobject_release(),then it is kobject_cleanup().the destructor in kobject.ktype->release
          will be called by this function.for the kobject is device_ktype,the destructor is device_release().
          if @dev->release has defined,then use it to destructor @dev,otherwise,use @dev->type->release,and the
          last select is @dev->class->dev_release.if no destructor can be called,then kernel printk warnning message.
          at the end of device_release(),kfree() will be called on @dev->p the device private data.

        Drivers :
          drivers in device driver model is represented by a structure named @device_driver.
          
          <linux/device.h>
            /**
             * device_driver - driver of a device
             * @name:          name of the driver
             * @bus:           which bus this driver should attach to
             * @owner:         module owner,also used as usage counter
             * @mod_name:      for built-in modules
             * @suppres_bind_attrs:    disables bind/unbind via sysfs
             * @probe:         routine used to probe device,called by bus device driver
             *                 to probe hardware for checks whether the device could
             *                 possibly be handled by this driver
             * @remove:        routine used to remove device,hot-pluging or unload
             * @shutdown:      routine used to shutdown device,power state change
             * @suspend:       suspend device,power state change
             * @resume:        resume device,power state change
             * @groups:        array of attribute groups this device drive belongs to
             * @pm:            Power Management Operations
             * @p:             driver private data <drivers/base/base.h>
             *                   struct kobject kobj
             *                   struct klist klist_devices
             *                   struct klist_node knode_bus
             *                   struct module_kobject *mkobj
             *                   struct device_driver *driver
             * # reference counter is inclued in @p,it is a member named kobj type of
             *   kobject
             * # routine get_driver() and put_driver() used to increase and decrease the
             *   reference counter,respectively
             *   struct device_driver *get_driver(struct device_driver *drv); => NULL on error
             *   void put_driver(struct device_driver *drv);
             *   (EXPORT_SYMBOL_GPL)
             */ 
            struct device_driver {
                    const char *name;
                    struct bus_type *bus;
                    struct module *owner;
                    const char *mod_name;
                    bool suppres_bind_attrs;
                    
                    int (*probe)(struct device *dev);
                    int (*remove)(struct device *dev);
                    void (*shutdown)(struct device *dev);
                    int (*suspend)(struct device *dev, pm_message_t state);
                    int (*resume)(struct device *dev);
                    const struct attribute_group **groups;
                    const struct dev_pm_ops *pm;
                    struct driver_private *p;
            };

            ! usually,device_driver object is embedded in a large object(container).

          <drivers/base/driver.c>
            /**
             * driver_register - register a device driver into device driver model
             *                   hierarchy
             * @drv:             the device driver
             * return:           0 OR error code
             * # EXPORT_SYMBOL_GPL
             */
            int driver_register(struct device_driver *drv);
            
            brief description for driver_register() :
              at the beginning,do BUG checking,it is a BUG if @drv->bus->p is NULL.
              checks methods @probe @remove @shutdown for @drv and @drv->bus,printk
              KERN_WARNING if either method is implemented both in @drv and @drv->bus,
              tell user that have to update driver to use methods implemented in @drv->bus_type
              as instead.
              next,attempt use @drv->name to find out another such device driver on the bus,
              we can not register a device driver more than once.
              if no found,then invoke bus_add_driver() to adds this device driver,return error
              code if failed.
              finally,call to driver_add_groups() on @drv and @drv->groups,to group this device
              driver,bus_remove_driver() if failed.return the returned value to caller.

            /**
             * driver_unregister - unregister a device driver from device driver model hierarchy
             * @drv:               the device driver
             * # EXPORT_SYMBOL_GPL
             * # driver_remove_groups() and then bus_remove_driver()
             */
            void driver_unregister(struct device_driver *drv);

            ! these two routines realy do just a little works,the most is passof to bus-level call.

        Buses :
          kernel supports several different type of buses,the bus of a specified type is generic
          represented by structure bus_type.

          <linux/device.h>
            /**
             * bus_type - structure used to represent a bus
             * @name:     name of the bus
             * @bus_attrs:    attributes of this bus
             * @dev_attrs:    attributes of devices host on this bus
             * @drv_attrs:    attributes of drivers that attach to this bus
             * @match:        checks whether a given driver supports a given driver
             * @uevent:       userspace event support
             * @probe:        device probe
             * @remove:       remove a device from bus
             * @shutdown:     shutdown a device hosted on this bus
             * @suspend:      suspend device
             * @resume:       resume device
             * @pm:           Power Management Operations
             * @p:            bus private data <drivers/base/base.h>
             *                  struct kset subsys
             *                    - kset that defines this bus
             *                      corresponds to "/sys/bus/*"
             *                  struct kset *drivers_kset
             *                    - drivers attached to this bus
             *                  struct kset *devices_kset
             *                    - devices hosted on this bus
             *                  struct klist klist_devices
             *                    - iterate over @devices_kset
             *                  struct klist klist_drivers
             *                    - iterate @drivers_kset
             *                  struct blocking_notifier_head bus_notifier
             *                    - bus notifier list for anything that cares about
             *                      things on this bus
             *                  unsigned int drivers_autoprobe:1
             *                    - automatically probe
             *                  struct bus_type *bus
             *                    - pinter back to the struct bus_type object
             * # entries under devices and drivers directories of this bus in sysfs are symlinks to
             *   the real device and driver
             */
            struct bus_type {
                    const char *name;
                    struct bus_attribute *bus_attrs;
                    struct device_attribute *dev_attrs;
                    struct driver_attribute *drv_attrs;
            
                    int (*match)(struct device *dev, struct device_driver *drv);
                    int (*uevent)(struct device *dev, struct kobj_uevent_env *env);
                    int (*probe)(struct device *dev);
                    int (*remove)(struct device *dev);
                    void (*shutdown)(struct device *dev);

                    int (*suspend)(struct device *dev, pm_message_t state);
                    int (*resume)(struct device *dev);

                    const struct dev_pm_ops *pm;
            
                    struct bus_type_private *p;
            };

          ! each device's identifier has a format specific to the bus that hosts the device,
            @match can simple to checks whether the device's identifier is inclued in
            driver's supported devices identifier table.

          <drivers/base/bus.c>
            /**
             * bus_for_each_dev - iterate devices of a given bus and invoke callback
             *                    function on them
             * @bus:              the bus
             * @start:            device to start iterating from
             * @data:             data for callback
             * @fn:               callback
             * return:            0 OR error code
             * # EXPORT_SYMBOL_GPL
             * # checks return value of @fn at each time,break out if any error have
             *   detected
             * # device has non-zero return value is not retained in any way,nor is the
             *   refcount incremated,if caller wants,then it can use callback to retain
             *   it,and increase refcount in callback
             */
            int bus_for_each_dev(struct bus_type *bus, struct device *start,
                                 void *data, int (*fn)(struct device *, void *));

            /* bus_for_each_drv - similar to bus_for_each_dev(),but this for drivers */
            int bus_for_each_drv(struct bust_type *bus, struct device_driver *start,
                                 void *data, int (*fn)(struct device_driver *, void *));

        Classes :
          class is described by a structure named class,the corresponding directory for
          each class in sysfs filesystem is "/sys/class/*".
          the classes of the device driver model are essentially aimed to provide a
          standard method for exporting to User Mode applications the interfaces of the
          logical devices.
          /* logical device - the device descriptor,which corresponds to a physical device */

          <linux/device.h>
            /**
             * class - class of a device
             * @name:  name of the class
             * @owner: module owner,includes reference counter
             * @class_attrs:    attributes of this class
             * @dev_attrs:      attributes of the device belongs to this class
             * @dev_kobj:       the kobject object of devices which belongs to this class,
             *                  the default value is the kobject "char",its correspond
             *                  directory is "/sys/dev/char",created and added by
             *                  devices_init() a initialization function
             * @dev_uevent:     device uevent routine
             * @devnode:        makeup a string is the path of device node file
             * @class_release:  release this class
             * @dev_release:    disconnect the device and this class
             * @suspend:        suspend method for the device belongs to this class
             * @resume:         resume device
             * @pm:             Power Management Operations
             * @p:              class private data <drivers/base/base.h>
             *                    struct kset class_subsys
             *                      - the kset that defines this class
             *                    struct klist class_devices
             *                      - devices associated with this class
             *                    struct list_head class_interfaces
             *                      - class interfaces associated with this class
             *                    struct kset class_dirs
             *                      - "glue" directory for virtual devices
             *                        associated with this class
             *                    struct mutex class_mutex
             *                      - mutex to protect children,devices,and interfaces lists
             *                    struct class *class
             *                      - pointer back to the class object
             */
            struct class {
                    const char *name;
                    struct module *owner;
                    
                    struct class_attribute *class_attrs;
                    struct device_attribute *dev_attrs;
                    struct kobject *dev_kobj;

                    int (*dev_uevent)(struct device *dev, struct kobj_uevent_env *env);
                    char *(*devnode)(struct device *dev, mode_t *mode);

                    void (*class_release)(struct class *class);
                    void (*dev_release)(struct device *dev);

                    int (*suspend)(struct device *dev, pm_message_t state);
                    int (*resume)(struct device *dev);

                    const struct dev_pm_ops *pm;
            
                    struct class_private *p;
            };

          <drivers/base/class.c>
            /**
             * class_for_each_device - similar to bus_for_each_dev(),but it is the
             *                         class specified routine
             * # EXPORT_SYMBOL_GPL
             * # do not stores the device descriptor which callback routine failed on,
             *   if you want it,then you have to take it
             */
            int class_for_each_device(struct class *class, struct device *start,
                                    void *data, int (*fn)(struct device *, void *);

          about class.p->class_devices,the device descriptors in the list may represent
          the same device,that is a hardware device might includes several different
          sub-devices,and each of the sub-devices requires a different User Mode interface.
          ! device drivers for the same class are expected to offer the same functionalities
            to the User Mode applications.

      Device Files :
        Unix-like operating systems are based on the notion of a file,which is just an information
        container structured as a sequence of bytes.
        according to this approach,I/O devices are treated as special files called "device files",
        so the same system calls used to interact with regular files on disk can be used to directly
        interact with I/O devices.

        Two types of device files :
          1> block
               the data of a block device can be addressed randomly,and the time needed to transfer
               a data block is small and roughly the same
          2> character
               the data of a character device either cannot be addressed randomly,or they can be
               addressed randomly,but the time required to access a random datum largely depends
               on its position inside the device

          ! network cards are a notable exception,that is they are not directly associated with
            device files.

        inode of device file do not include a pointer to blocks of data on the disk,instead,it must
        includes an identifier of a hardware device which associated with this device file.
        the identifier consists of the type of device file and a pair of numbers(major number and minor number).
        major :
          device files have same major number and same type share the same set of file operations,that is these
          device are managed by the same driver
        minor :
          the specific device in the group,all device in this group has the same major number

        ! block device and character device have independent numbering,that is (3, 0) of a block device is 
          different to (3, 0) of a character device.
        ! block device,such hard disk
          character device,such keyboard
        ! system call mknod() is used to create device file,and in normally,the device files are placed in
          "/dev"
           e.g.
             /dev/null => null device(black hole)
        ! a device file can associated with a hardware device,or with a physical/logical portion of a
          hardware device(hard disk partion)
          in some cases,a device file is not associated with a hardware device,but represents a fictitious
          logical device,such "/dev/tty0","/dev/null",etc.
          the name of a device file is irrelevant.(/dev/disk or /tmp/disk,irrelevant,major number and minor
          number are more important)

      User Mode Handling of Device Files :
        the official registry of allocated device numbers and "/dev" directory nodes is stored in the text
        file "Documentation/devices.txt",and the macros are defined in <linux/major.h>.
        traditionaly,the major number and minor number are represents by a 8-bit unsigned integer,it is not
        suffice.and in Linux 2.6.34.1 dev_t is typedefed to __u32.
        the methods to encode and decode device number are defined in <linux/kdev_t.h>.
        major number is encoded to 12-bit,minor number is encoded to 20-bit.                    
        Kernel also reserved the 8-bit major number and minor number for compatible to old device files.

        <linux/kdev_t.h>
          #define MINORBITS 20
          #define MINORMASK ((1U << MINORBITS) - 1)

          /* extract major number from dev_t value */
          #define MAJOR(dev) ((unsigned int) ((dev) >> MINORBITS))

          /* extract minor number from dev_t value */
          #define MINOR(dev) ((unsigned int) ((dev) & MINORMASK))

          /* combine major number and minor number to a single dev_t value */
          #define MKDEV(ma, mi) ((ma) << MINORBITS | (mi))

          /* old device file compatibility */
          static inline u16 old_encode_dev(dev_t dev);
          static inline dev_t old_decode_dev(u16 val);

          /* encode dev_t value to a single u32 value */
          static inline u32 new_encode_dev(dev_t dev);

          /* decode u32 value to a dev_t value */
          static inline dev_t new_decode_dev(u32 dev);

        the additional available device numbers are not being statically allocated in the official
        registry,they should be used only when dealing with unusual demands for device numbers.
        /* actually,in Linux 2.6.34.1,the allocated maximum major number is 260 for OSD scsi device */
                
          
        Dynamic Device Number Assignment :
          each device driver specifies in the registration phase the range of device numbers that it is
          going to handle.the driver can require the allocation of an interval of device numbers withou
          specifying the exact values.in this case,kernel allocates a suitable range of numbers and
          assigns them to the driver.
          so new devices of new device drivers will no longer need an assignment in the official registry
          of device numbers,simply use whatever numbers are currently available in the system.
          ! in this way,the device file cannot be created once and forever,it must be created right after
            the device driver initialization with the proper major and minor numbers.
          ! Linux provide a way to export device numbers to User Mode,that is sysfs,the "dev" attribute
            contains major number and minor number.

        Dynamic Device File Creation :
          Linux kernel can create the device files dynamically,there is no need to fill the /dev directory
          with the device files of every conceivable hardware device,because the device files can be
          created "on demand".
          udev toolset is a set of User Mode programs.at the system startup,directory /dev is emptied,
          udev program scans /sys/class attempts to find out "dev" files,the file is the "dev" attribute
          of the device.for each such file,udev a create device file in /dev directory,with a proper file name,
          and the major number,minor number.it also create symlink if necessary(depends on configure file).
          the device file can be created after system been initialized,when
            1> a new module is inserted into kernel,which contains a device driver for the still unsupported
               device.
            2> hot-plug event,such a USB peripheral is plugged in the system.
               because device driver model supports device hotplugging,so udev can create a new device file
               automatically.
               /**
                * Understanding the Linux Kernel,mentioned kernel spawns a new process to executes a User Mode
                * shell script /sbin/hotplug,passing it any useful information on the discovered device as
                * environment variable.
                * the shell script take care of any operation required to complete the initialization of the
                * new device.
                * the User Mode program is called when hot-plug event comes can be set in /proc/kernel/hotplug.
                * ! but im neither find such shell script on my computer nor the hotplug file.
                */

      VFS Handling of Device Files :
        VFS can handles regular files and device files,the different between these two is hidden by it.
        for regular file,it is the data blocks on the disk;for device file,it is the hardware device.
        when access a regular file,the related file operations are registered by filesystem it hosting.
        when access a device file,the related file operations are insteaded to device related operations,
        this is handled by open principle.

        e.g.
          a device file named devf is hosting on the ext2 filesystem.
          open the devf
          read inode on disk through a suitable function of the filesystem (ext2_iget())
          ext2_iget() find out that devf is a device file,then call to init_special_inode()
          init_special_inode() set inode's @i_rdev member to the major number and minor number of
          that device,set @i_fop to the device file operations,@def_blk_fops or @def_chr_fops
          /* the checking is to test the inode's @i_mode member */

          function do_sys_open() call to do_filp_open(),then it is do_last(),nameidata_to_filp() is
          called by do_last() later.nameidata_to_filp() will checks if the dentry object corresponding
          to the file path is whether exist,if it is not,then call to __dentry_open(),and __dentry_open()
          will use inode's @i_fop to set this file object's @f_op member,so the new file operations
          for the device file is translated to device related routines.

        <fs/block_dev.c>
          const struct file_operations def_blk_fops = {
                  .open = blkdev_open,
                  .release = blkdev_close,
                  .llseek = block_llseek,
                  .read = do_sync_read,
                  .write = do_sync_write,
                  .aio_read = generic_file_aio_read,
                  .aio_write = blkdev_aio_write,
                  .mmap = generic_file_mmap,
                  .fsync = blkdev_fsync,
                  .unlocked_ioctl = block_ioctl,
          #ifdef CONFIG_COMPAT
                  .compat_ioctl = compat_blkdev_ioctl,
          #endif
                  .splice_read = generic_file_splice_read
                  .splice_write = generic_file_splice_write,
          };

        <fs/char_dev.c>
          const struct file_operations def_chr_fops = {
                  .open = chrdev_open,
          }; /* other methods should be implemented by device driver */

          ! character device also can be considered to a data stream similar to regular file.

      Device Drivers :
        a device driver is the set of kernel routines that makes a hardware device respond to the
        programming interface defined by the canonical set of VFS functions that control a device.
        the detail is delegated to the device driver,that is different device has different I/O
        controller,different commands and different state information.
        /* the mainly differ of device drivers is in the level of support that they offer to the
         * User Mode applications.
         */

        a device driver does not consist only of the functions,before using a device driver,several
        activities must have taken place.

        Device Driver Registration :
          before using a device driver,must register it at first.that is create an object is type
          of device_driver structure,insert it into Device Driver Model,associate it with the
          corresponding device file(s).access to device files whose corresponding drivers have not
          been previously registered return the error -ENODEV.

          for device driver statically compiled in kernel,its registration is performed during
          kernel initialization phase.
          for device driver is implemented in kernel module,its registration is performed when
          the module is loaded,and unregistered when the module is unloaded.

          function driver_register() is used to register a device driver,when a device driver is
          being registered,the kernel checks whether the unsupported hardware device could be
          possibly handled by the driver.this is relies on @match method of bus object and @prove
          method of the device_driver object.if it is,kernel will bind the device with the driver
          through driver_bound(),this is happened after succeed on probe.
          /**
           * bind driver to device.p->knode_driver
           * the function is called by really_probe(),which is called by driver_probe_device(),
           * and it is called by driver_attach().
           * driver_attach() is called by bus_add_driver() if @drivers_autoprobe is true,this
           * is default behavior set by bus_register().
           */

        Device Driver Initialization :
          device driver is registered as soon as possible,but the initialization at the last
          possible moment.
          /* initialization means allocate precious resources of the system */
          /* IRQ line can be shared with others,the resources allocated at the last possible moment
           * are page frames for DMA transfer buffers and the DMA channel itself
           */

          allocate resources when needed,release if they are not requested :
            > a usage counter keeps track of the number of processes that are currently
              accessing the device file.increased by open method,decreased by release method.
            > open method checks the value of the usage counter before the increment.if it is
              _zero_,the device driver must allocate the resources and enable interrupts and DMA
              on the hardware device.
            > release method checks the value of the usage counter after the decrement.if it is
              _zero_,no more processes are using the hardware device,it disables interrupts and
              DMA on the I/O controller,then release the allocated resources.

        Monitoring I/O Operations :
          duration of a hardware device I/O operation is often unpredictable,device driver must 
          rely on a monitoring technique that signals either the termination of the I/O operation
          or a time-out.
            termination :
              driver reads the status register of the I/O interface to determine whether the I/O
              operation was carried out successfully.
              
            time-out :
              driver knows that something went wrong,because the maximum time interval allowed to
              complete the operation elapsed and nothing happened.

          polling mode :
            similar to spinlock,but CPU polls the device's status register repeatedly until its
            value signals that the I/O operation has been completed.but for device driver,it is
            more elaborate,that is driver must also remember to check for possible time-outs.

          interrupt mode :
            interrupt mode can be used only if the I/O controller is capable of signaling via
            an IRQ line the end of an I/O operation.
            /* such press a key on keyboard,I/O controller puts the byte in input register,and
             * raise a keyboard interrupt
             */
            for apply intterupt mode,device driver must provides two routines :
              read - the read method of the file object
              interrupt - handles the interrupt

            e.g.
                    exclusive to the device /* prevent concurrent accessing */
                    issue read/write command
                    wait_event_interruptible until interrupt flag becomes 1
                    put result to User Mode program
                    release lock

                    /* for the interrupt handler,it read input register,setup the interrupt flag and
                     * wake up the process
                     */

                    ! in generally,time-out is implemented through static or dynamic timers,it
                      must be set to the right time before starting the I/O operation,and removed
                      after the operation is completed.

        Accessing the I/O Shared Memory :
          mapping for I/O shared memory is depends on the device,and on the bus type.the physical
          addresses range is different.

          for most devices connected to the ISA bus :
            it is usually mapped into the 16-bit physical addresses ranging from 0xa0000 to 0xfffff.

          for devices connected to the PCI bus :
            it is mapped into 32-bit physical addresses near the 4GB boundary.

          Intel AGP(Accelerated Graphic Port) standard :
            an enhancement of PCI for high-performance graphic cards.besides having its own I/O shared
            memory,this kind of card is capable of directly addressing portions of the RAM by a special
            hardware circuit named Graphic Address Remapping Table(GART).
            from the kernel's point of view,it does not really matter where the phyrical memory is located,
            and GART-mapped memory is handled like the other kinds of I/O shared memory.

          kernel programs can only access the linear addresses starting from PAGE_OFFSET(0xc0000000),the
          fourth gigabyte,thus if the device driver want to access an I/O shared memory location,must
          translate the physical memory to the proper linear address.
          macros __pa() and __va() have described in the section "Final Kernel Page Table",__pa() convert
          an linear address starting from PAGE_OFFSET to the physical address,__va() do the reverse.
          so before device driver access to I/O shared memory,must use __va() to translate it,of course,make
          up a mapping for it(I/O shared mapping) at first.in the case linear address is greater than system
          physical memory,then must make up a mapping for it through noncontiguous memory area,that is the
          vm_struct descriptor.

          <arch/x86/include/asm/io.h>
            /**
             * ioremap_nocache - map bus memory into CPU space without hardware cache,make bus memory CPU
             *                   accessible via mmio helpers
             * @offset:          bus address of the memory
             * @size:            size of the resource to map
             * return:           remapped linear memory address or NULL
             * # EXPORT_SYMBOL
             * # this routine is defined in <arch/x86/mm/ioremap.c>
             * # this routine is actually call to __ioremap_caller() with _PAGE_CACHE_UC_MINUS
             */
            extern void __iomem *ioremap_nocache(resource_size_t offset, unsigned long size);

            /* hardware cached version, _PAGE_CACHE_WB */
            extern void __iomem *ioremap_cache(resource_size_t offset, unsigned long size);

            /* customize version, @prot_val & _PAGE_CACHE_MASK */
            extern void __iomem *ioremap_prot(resource_size_t offset, unsigned long size, unsigned long prot_val);

            /* ioremap_nocache() */
            static inline void __iomem *ioremap(resource_size_t offset, unsigned long size);

            /* unmap io memory */
            extern void iounmap(volatile void __iomem *addr);

          <arch/x86/mm/ioremap.c>
            /**
             * __ioremap_caller - primary routine to remap an arbitrary physical address space into the
             *                    kernel linear address space,needed when the kernel wants to access
             *                    high addresses directly
             * @phys_addr:        I/O shared memory address
             * @size:             size of I/O shared memory
             * @prot_val:         protection flags associated with the page table entry 
             * @caller:           who called this routine,for tracing
             * return:            mapped and aligned kernel space linear address OR NULL
             * # this routine do not remap the low PCI/ISA area,it is always mapped
             */
            static void __iomem *__ioremap_caller(resource_size_t phys_addr, unsigned long size,
                                                  unsigned long prot_val, void *caller);

            brief description for __ioremap_caller() :
              check @phys_addr at first,it can not greater than system physical memory.
              test valid to @phys_addr,return NULL and print KERN_WARNING, if it is not valid addr.
              /* if undefined CONFIG_PHYS_ADDR_T_64BIT,phys_addr_valid() will always returns 1. */
              check if the request spans more than any BAR in the iomem resource tree,print warning
              if it is.
              travers pfns starting from @phys_addr to last addr of iomem,return NULL if current
              address is registered as "System RAM" in iomem_resource list AND current pfn is using
              AND current page is not for reserve.this checking forbid anybody to remap normal RAM that
              we are using.
              do page aligning for @phys_addr and @size,reserve memory type for the aligned @phys_addr
              and @size with type @prot_val,return NULL if failed to reserve memory type.
              a local variable named @new_prot_val is used in reserve_memtype(),the routine returns
              available type without error in the variable.if @prot_val is not equal to @new_prot_val,
              then attempts use the new type,if the mapping allowed,otherwise,print error message and
              return error to caller.
              set local variable @prot to flag PAGE_KERNEL_IO_xxx according to @prot_val,this variable
              will be used later.
              attempts to create a new vm_struct descriptor with VM_IOREMAP,retrun error if failed.
              use @phys_addr to setup the descriptor.
              call to kernel_map_sync_memtype() with arguments @phys_addr, @size, @prot_val,the routine
              call to ioremap_change_attr() change the attritbue of the io memory re-mapping with flag
              @prot_val,return error if failed.
              call to ioremap_page_range() with arguments @vaddr, @vaddr + @size, @phys_addr, @prot.
              this routine is defined in <lib/ioremap.c>,it modify Kernel Page Table entries to make
              the mapping go into effect.
              finally,returns the address of mapped linear address.

          some architecture-dependent functions used when accessing I/O shared memory :
            /* they are defined in <arch/x86/include/asm/io.h> */
            unsigned char readb(const volatile void __iomem *addr)
            unsigned short readw(const volatile void __iomem *addr)
            unsigned int readl(const volatile void __iomem *addr)

            void writeb(unsigned char val, volatile void __iomem *addr)
            void writew(unsigned short val, volatile void __iomem *addr)
            void writel(unsigned int val, volatile void __iomem *addr)

            void memcpy_fromio(void *dst, const volatile void __iomem *src, size_t count)
            void memcpy_toio(volatile void __iomem *dst, const void *src, size_t count)
            void memset_io(volatile void __iomem *addr, unsigned char val, size_t count)

            /* readq and writeq for CONFIG_X86_64 */

            /* virt_to_phys - convert virtual address to physical address */
            static inline phys_addr_t virt_to_phys(volatile void *address);

            #define virt_to_bus virt_to_phys
            #define bus_to_virt phys_to_virt

          e.g.
                  unsigned char rdata = '\0';
                  virt_addr = __va(phys_addr)
                  data = readb(virt_addr);

                  unsigned char wdata = 's';
                  high_virt_addr = ioremap(high_phys_addr);
                  writeb(wdata, high_virt_addr);

        Direct Memory Access(DMA) :
          on older PC architecture,the CPU is the only "bus master" of the system,that is,
          it is the only hardware device that drives the address/data bus in order to fetch
          and store value in the RAM's locations.
          moder bus architectures such as PCI,each peripheral can act as "bus master",if it
          provided with the proper circuitry.

          DMA circuit : it can transfer data between the RAM and an I/O device.once activated
          (auxiliary)   by CPU,it is able to continue the data transfer on its own;issue an
                        interrupt if the transfer completed.
          DMA conflicts with CPU :
            "memory arbiter" detected that CPU and DMA attempt to access the same memory location
            at the same time.

          DMA Drain : drain data from DMA buffers.

          Situation that apply DMA :
            transfer a large number of bytes at once,such disk drivers.
            /* setup time for DMA is relatively high,use CPU to transfer small datum is better */

          ! DMA for older ISA buses is complex and hard to program,physical memory is 16MB.
            DMA for PCI and SCSI buses rely on dedicated hardware circuits in the buses,that is
            less complex than older ISA buses,and easy to program.

          Synchronous and Asynchronous DMA :
            device driver can use DMA in two different ways,one is synchronous DMA,another is
            ansynchronous DMA.

            Synchronous DMA : data transfer triggered by processes
            Asynchronous DMA : data transfer triggered by hardware device

            example for synchronous DMA - process transfer sample data of sound track to sound card's
                                          DSP(Digital Signal Processor).
            example for asynchronous DMA - data received by network card,it stores the data in I/O
                                           shared memory,then issue an interrupt,device driver
                                           acknowledge it and instruct network card copy data
                                           to kernel buffer.

          /**
           * Linux Kernel provides some helper functions for DMA transfers.there are two subset :
           *   1> subset is architecture independent for PCI devices
           *   2> subset is both architecture independent and bus independent
           * the helpers hide the different in the DMA mechanisms of the various hardware architecture.
           */

          bus addresses :
            distinguish from logical addresses,linear addresses and physical address,bus addresses used
            by all hardware device except the CPU to drive the data bus.
            /* logical,linear,are the same */
            every DMA transfer involves at least one memory buffer,before activating the transfer,device driver
            must ensure that the DMA circuit can directly access the RAM locations.

            ! in a DMA operation,the data transfer takes place without CPU intervention,setup must be dealt with
              by kernel previously.
            ! in 80x86,but addresses coincide with physical addresses.but some other architecture includes a
              I/O Memory Management Unit(IO-MMU),analog to paging unit and microprocessor,it maps physical
              addresses into bus addresses.hardware devices use DMA must setup properly the IO-MMU previously.
              different bus addresses have different address size,for ISA,it is 24-bit long.(ZONE_DMA,total 16MB)

            <asm-generic/types.h>
              #ifdef CONFIG_PHYS_ADDR_T_64BIT   /* PAE */
                      typedef u64 dma_addr_t;   /* represents a generic bus address */
              #else
                      typedef u32 dma_addr_t;
              #endif

            <arch/x86/kernel/pci-dma.c>
              /**
               * dma_set_mask - set DMA memory mask for a given device
               * @dev:          device
               * @mask:         mask
               * return:        0 OR -EIO
               * # EXPORT_SYMBOL
               * # if device do not support to DMA,then returns -EIO,otherwise
               *   use @mask to set @dev->dma_mask
               * # this routine may call to dma_supported() to checks whether current
               *   device is support DMA
               */
              int dma_set_mask(struct device *dev, u64 mask);

              ! function int pci_set_dma_mask(struct pci_dev *, u64) is wrapper to dma_set_mask(),
                if PCI enabled,otherwise,the routine returns -EIO.

          Cache coherency :
            cache coherency is hand over to DMA helpers,system architecture does not necessarily offer a coherency
            protocol between the hardware cache circuit and DMA circuit at hardware level.

            apply two different ways to handle DMA buffers by making use of two different classes of helpers :
              /* DMA mapping types */
              1> Coherent DMA mapping(synchronous / consistent)
                   kernel ensures that there will be no cache coherency problems between the memory and
                   hardware device - every write operation performed by CPU on a RAM location is immediately
                   visible to the hardware device,and vice versa(hardware write RAM visible to CPU)
              2> Streaming DMA mapping(asynchronous / non-coherent)
                   device driver must take care of cache coherency problems by using the proper
                   synchronization helpers.

            ! 80x86 architecture no cache coherency problem,hardware devices take care of "snooping"
              the accesses to the hardware caches.so,the two ways described above essentially equivalent.
            ! Coherent DMA mapping is mandatory in the case that the buffer is accessed in unpredictable
              ways by the CPU and the DMA processor.
              prefer to Streaming DMA mapping if the architecture handle the Coherent DMA mapping is
              cumbersome and may lower performance.

          Helper functions for Coherent DMA mappings :
            device driver establish Coherent DMA mapping during initialization phase,and release the mapping
            and buffer when it is unloaded(as kernel module).
            
            <asm-generic/pci-dma-compat.h>
              /**
               * pci_alloc_consistent - allocator for consistent memory buffer called by device driver for
               *                        pci device
               * @hwdev:                PCI device
               * @size:                 buffer size
               * dma_handle:            the corresponding bus address will be stroed in *dma_handle
               * return:                mapped linear address OR NULL
               * # this function actually a wrapper to dma_alloc_coherent with GFP_ATOMIC flag
               * # architecture-independent
               */
              static inline void *
              pci_alloc_consistent(struct pci_dev *hwdev, size_t size, dma_addr_t *dma_handle);

              /* erase mapping and free memory */
              static inline void
              pci_free_consistent(struct pci_dev *hwdev, size_t size, void *vaddr, dma_addr_t dma_handle);

            <arch/x86/include/asm/dma-mapping.h>
              /* architecture-dependent and bus-dependent */

              /**
               * dma_alloc_coherent - architecture dependent helper used to alloc coherent memory buffer
               * @dev:                the device who want memory buffer
               * @size:               size of buffer in bytes
               * @dma_handle:         bus address for DMA
               * @gfp:                GFP flags
               * return:              mapped linear address OR NULL
               * # This routine may call to struct dma_map_ops.alloc_coherent() do the real allocating,
               *   the method will be different for iommu and no-iommu artechitecture
               */
              static inline void *
              dma_alloc_coherent(struct device *dev, size_t size, dma_addr_t *dma_handle, gfp_t gfp);

              brief description for dma_alloc_coherent() :
                get dma_map_ops object of @dev.
                disable __GFP_DMA and __GFP_HIGHMEM and __GFP_DMA32 in @gfp.
                call to dma_alloc_from_coherent(),this function is defined in <drivers/base/dma-coherent.c>,
                it attempt to allocate memory buffer from this device's coherent area;if @dev->dma_mem is NULL,
                then we can not allocate memory from coherent area,so try alloc_coherent() later.
                if @dev is NULL,set @dev to @x86_dma_fallback_dev,it is a dummy device for alloc_coherent().
                call to alloc_coherent(),if it is exit;return NULL if such routine is not exist or the device
                is not support to DMA.
                finally,register DMA debug entry and returns the address returned by alloc_coherent().
                /* if returned address is not NULL,then @dma_handle must stored bus address for the buffer */

              /**
               * alloc_coherent() of AMD architecture with IOMMU => <arch/x86/kernel/amd_iommu.c>
               *   get protection_domain for @dev,if the pointer is invalid,then allocates free pages and
               *   convert the linear address of these pages to physical address,return it in @dma_handle.
               *   domain is valid,disable __GFP_DMA | __GFP_HIGHMEM | __GFP_DMA32 in @flag,enable __GFP_ZERO in it.
               *   __get_free_pages(),use virt_to_phys() to get physical address,call to __map_single() to
               *   enstablish Coherent DMA mapping,return the bus address in @dma_addr if succeed,and return
               *   linear address to caller.
               * ! __map_single() is defined in the same file :
               *     invoke iommu helper iommu_num_pages() to get the number of pages that determined by IOMMU for
               *     the physical address.
               *     call to dma_ops_alloc_addresses() to allocate DMA memory area,it will invoke iommu_area_alloc().
               *     if succeed to allocate memory,then call to dma_ops_domain_map() on each page to maps the memory
               *     area got by dma_ops_alloc_addresses() to the physical address range.
               *     finally returns the "@address(from dma_ops_alloc_addresses()) + @paddr & ~PAGE_MASK" to caller,
               *     this is the bus address.
               */

              /* dma_free_coherent - erase the Coherent DMA mapping and release the corresponding memory */
              static inline void dma_free_coherent(struct device *dev, size_t size, void *vaddr, dma_addr_t bus);
              
              ! unmap and free pages.

          Helper functions for Streaming DMA mappings :
            memory buffers for Streaming DMA mappings are usually mapped just before the transfer and unmapped
            thereafter.but it is also possible to keep the same mapping among several DMA transfers,in this case,
            developer must take care of hardware cache between the memory and the peripheral.

            handle Streaming DMA mapping :
              1> allocate pages(memory) from zoned page frame allocator or from the general purpose slab cache.
              2> call to pci_map_single() or dma_map_single() to establish Streaming DMA mapping.
              3> after transfer completed,call to pci_unmap_single() or dma_unmap_single() to erase the mapping.
              ! to prevent cohereny problems,driver must call to pci_dma_sync_single_for_device() or
                dma_sync_single_for_device() right before string a DMA transfer from the RAM to the device.
                these two routine will flush hardware cache if necessary.
                to prevent coherency problems,driver should not access to the memory buffer right after a DMA
                transfer from the device to RAM completed.before reading,must call to pci_dma_sync_single_for_cpu()
                or dma_sync_single_for_cpu(),these two routine will invalidate hardware cache if neccesary,so
                CPU will retrieve data from RAM not the hardware cache.
              ! for DMA memory buffer is high-mem,should call to pci_map_page() or dma_map_page() to make up
                an linear address mapping at first,and erase the mapping by call to pci_unmap_page() or
                dma_unmap_page() after task finished.

            <asm-generic/pci-dma-compat.h>
              /* wrappers to "dma_" routines */

              /* establish Streaming DMA mapping */
              static inline dma_addr_t
              pci_map_single(struct pci_dev *hwdev, void *ptr, size_t size, int direction);
              
              /* erase Streaming DMA mapping */
              static inline void
              pci_unmap_single(struct pci_dev *hwdev, dma_addr_t dma_addr);

              /* prevent coherency from device to RAM */
              static inline void
              pci_dma_sync_single_for_cpu(struct pci_dev *hwdev, dma_addr_t dma_handle, size_t size, int direction);

              /* prevent coherency from RAM to device */
              static inline void
              pci_dma_sync_single_for_device(struct pci_dev *hwdev, dma_addr_t dma_handle, size_t size, int direction);

              /* map high-mem buffer */
              static inline dma_addr_t
              pci_map_page(struct pci_dev *hwdev, struct page *page, unsigned long offset, size_t size, int direction);

              /* unmap high-mem bufffer */
              static inline void
              pci_unmap_page(struct pci_dev *hwdev, dma_addr_t dma_address, size_t size, int direction);

            <asm-generic/dma-mapping-common.h>
              /* architectur-independent and bus-independent */

              #define dma_map_single(d, a, s, r) dma_map_single_attrs(d, a, s, r, NULL)
              #define dma_unmap_single(d, a, s, r) dmap_unmap_single_attrs(d, a, s, r, NULL)

              /* dma_map_single_attrs - call to dma_map_ops.map_page() for establish a Streaming DMA mapping */
              static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr, size_t size,
                                                            enum dma_data_direction dir, struct dma_attrs *attrs);

              /* dma_unmap_single_attrs - call to dma_map_ops.unmap_page() for unmap a Streaming DMA mapping */
              static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr, size_t size,
                                                        enum dma_data_direction dir, struct dma_attrs *attrs);

              /**
               * dma_sync_single_for_cpu - call to dma_map_ops.sync_single_for_cpu() to invalid hardware cache
               *                           for CPU
               */
              static inline void dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr, size_t size,
                                                         enum dma_data_direction dir);

              /* dma_sync_single_for_device - call to dma_map_ops.sync_single_for_device() to flush hardware cache */
              static inline void dma_sync_single_for_device(struct device *dev, dma_addr_t addr, size_t size,
                                                            enum dma_data_direction dir);

              /**
               * dma_map_page - call to dma_map_ops.map_page() to makeup high-mem linear address mapping and
               *                establish a Streaming DMA mapping
               */
              static inline dma_addr_t dma_map_page(struct device *dev, struct page *page, size_t offset,
                                                    size_t size, enum dma_data_direction dir);
            
              /**
               * dma_unmap_page - call to dma_map_ops.unmap_page() to erase a Streaming DMA mapping and
               *                  unmap high-mem linear address mapping
               */
              static inline void dma_unmap_page(struct device *dev, dma_addr_t addr, size_t size,
                                                enum dma_data_direction dir);

      Levels of Kernel Support :
        Linux does not fully support all possible existing I/O devices,there are three kinds of hardware
        device about Kernel supporting >
          1> No support at all
               the application program interacts directly with the device's I/O ports by issuing
               suitable "in" and "out" assembly language instructions.
               /* this approach does not rely on any kernel device driver,such X Window System.
                * X server is constrained from using the hardware interrupts issued by the I/O device,
                * and access the required I/O ports by itself.
                * # no device driver is loaded,application must do-it-yourself.
                *
                * recent Linux version has a device file "/dev/fb" which is an abstraction of frame buffer,
                * application software can access it without needing to know anyting about the I/O ports
                * of the graphics interface.
                */
               /*
                * DRI(Direct Rendering Infrastructure) :
                *   if kernel support DRI,then application software can make use of the hardware of
                *   accelerated 3D graphics card.
                */
        
          2> Minimal support
               the kernel does not recognize the hardware device,but does recognize its I/O interface.
               /* this approach is used to handle external hardware devices connected to a general-purpose
                * I/O interface.
                * kernel take care of the I/O interface by offering a device file(and thus a device driver).
                * only the serial port and the parallel port of general-purpose I/O interface can be handled
                * with this approach.
                * application should interact with the hardware device through the device file.
                * this is improper when the external device must interact heavily with internal kernel data
                * structures.and in some cases,have to come with extended support,such removable hard disk
                * on general-purpose I/O interface.
                */

          3> Extended support
               the kernel recognizes the hardware device and handles the I/O interface itself.
               /* there might not even be a device file for the device */
               /* for the hardware device with extended support,kernel have to provides a device driver for
                * each such device.
                * external devices attached to the USB,the PCMCIA,or the SCSI - every general-purpose I/O
                * interface except the serial and the parallel ports also require extended support.
                */

        ! it is worth nothing that the standard file-related system calls do not always give the application
          full control of the underlying hardware device.
          the lowest-common-denominator approach of the VFS does not include some special commands that some
          devices need or let an application check whether the devices is an a specific internal state.
          if it is really need to do that,system call ioctl() satisfy the request.

      Character Device Drivers :
        for character device,it is usually do not need buffering strategies and no disk cache involved,so
        handle it is relatively easy than block device.
        different character devices have different requirement :
          > need communication protocl to drive the hardware device
          > just read I/O ports

        structure named "cdev" is used to represent a character device.
        <linux/cdev.h>
          /**
           * cdev - character device
           * @kobj: kobject
           * @owner:    remove proector
           * @ops:  operations related to character device
           * @list: list of inodes relative to device files for this chr device
           * @dev:  initial major and minor numbers assigned to the device driver
           * @count:    number of consecutive device numbers that this character device
           *            required,it represents a range [@dev, @dev + @count - 1]
           * # a character device might has several device files,@list collects the
           *   inodes of these device files
           * # device files whose numbers fall in the range(@count) are handled by the same
           *   character device driver
           */
          struct cdev {
                  struct kobject kobj;
                  struct module *owner;
                  const struct file_operations *ops;
                  struct list_head list;
                  dev_t dev;
                  unsigned int count;
          };

        <fs/char_dev.c>
          /* cdev operation lock,used by chrdev_open(),cd_forget(),cdev_purge(),and etc. */
          static DEFINE_SPINLOCK(cdev_lock);

          /**
           * cdev_alloc - allocate and initialize a new cdev descriptor
           * return:      pointer to the newly allocated object OR NULL
           * # EXPORT_SYMBOL
           * # this routine call to kzalloc() with GFP_KERNEL for allocating
           */
          struct cdev *cdev_alloc(void);

          /**
           * cdev_init - initializes character device descriptor with specified
           *             file operations
           * # EXPORT_SYMBOL
           */
          void cdev_init(struct cdev *cdev, const struct file_operations *fops);

          /**
           * cdev_index - get index of the cdev of a given inode in the
           *              Kobject Mapping Domain
           * return:      -1 if not registered
           * # EXPORT_SYMBOL
           */
          int cdev_index(struct inode *inode);

          /**
           * cdev_add - add a char device to the system
           * @p:        char device
           * @dev:      device number
           * @count:    range of minor numbers
           * return:    0 OR -ENOMEM
           * # EXPORT_SYMBOL
           * # this routine juse set @p->dev and @p->count then call to
           *   function kobj_map() with arguments @cdev_map, @dev, @count, NULL,
           *   @exact_match, @exact_lock, @p
           * # @cdev_map the global character device kobject mapping domain
           */
          int cdev_add(struct cdev *p, dev_t dev, unsigned count);

          /**
           * cdev_del - remove a cdev from the system
           * @p:        char device
           * # EXPORT_SYMBOL
           * # this routine just call cdev_unmap() and then kobject_put() @p->kobj.
           */
          void cdev_del(struct cdev *p);

          /**
           * cdev_unmap - kobj_unmap() remove a device number range from a predefined
           *              kobject mapping domain @cdev_map
           * @dev:        the device number
           * @count:      device number range
           */
          static void cdev_unmap(dev_t dev, unsigned count);

          !! Kobject Mapping Domain
             - kobject mapping domain used to store the information about device numer range.
               for the devices of different types,every device type has a corresponding kobject
               mapping domain.(cdev => @cdev_map,character device kobject mapping domain)
               ! the Kobject Mapping Domain just introduced for character devices.

                +--> idx : major device number
                |
               +-------------------------+
               | e0 | e1 | e2| ... | e254| /* total 255 elements */
               +-------------------------+
                 |
                 |    @range1 <= @range2 <= @range3 ...    @range := ~0UL
                 +--> probe1 -> probe2 -> probe3 -> ... -> probeN -> NULL
                             |                             (dummy probe mark end of list)
                             +--> @next

             <drivers/base/map.c>
               /**
                * kobj_map - kobject mapping domain,defined by Device Driver Model
                * @probes:    probe array,acts like a hash table,the index is
                *            the major number of a specified device number range,
                *            each element is a list of probe objects
                * @lock:     domain concurrently accessing protection
                */
               struct kobj_map {
                       /**
                        * probe - kobject domain probe,represent a device number range
                        * @next:  to next probe,follow @range ascending order
                        *         ! depends on @range,not @dev
                        * @dev:   device number
                        * @range: device number range
                        * @get:   function used to get the corresponding kobject object
                        * @lock:  method used to increase the reference counter of 
                        *         the owner of this probe
                        *         # for cdev,@exact_lock actually call to cdev_get()
                        * @data:  probe private data,it is usually the pointer to the
                        *         probe owner(for cdev,it is the cdev descriptor)
                        */
                       struct probe {
                               struct probe *next;
                               dev_t dev;
                               unsigned long range;
                               struct module *owner;
                               kobj_probe_t *get;
                               int (*lock)(dev_t, void *);
                               void *data;
                       } *probes[255];
                       struct mutex *lock;
               };

               /**
                * kobj_lookup - routine used to lookup a kobject corresponding to the specified
                *               device number
                * @domain:      the domain
                * @dev:         device number
                * @index:       the distance between @dev and the starting device number
                *               of the range that @dev falls in
                *               # [p, ..., @dev, ...]
                *                 @p->dev <= @dev AND @p->dev + @p->range - 1 >= @dev
                *                 @p->dev has the same major number as @dev's,so they at
                *                 the same position in @domain->probes
                *                 if @dev falls in the device number range [@p->dev, @p->dev + @p->range - 1],
                *                 that means,they are in the same array which is allocated by kobj_map()
                * return:       pointer to the kobject of the device that @dev is belong to it
                *               (device files whose numbers fall in a specified device number range are handled
                *                by the same device driver)
                *               OR
                *               NULL
                */
               struct kobject *kobj_lookup(struct kobj_map *domain, dev_t dev, int *index);

               /**
                * kobj_map - map a device number range to a given domain
                * @domain:   the kobject map domain
                * @dev:      device number
                * @range:    device number range
                * @module:   module related to the range
                * @probe:    routine used to get the actual kobject of the range
                * @lock:     routine used to increase the refcount of the range owner
                * @data:     probe private data
                * return:    0 OR -ENOMEM
                * # this function will kmalloc() an array of objects are type of struct probe with
                *   GFP_KERNEL,the number of elements is calculated through @dev and @range(max 255),
                *   initializes each element use these function parameters.next,use a for-cycle
                *   to add the newly allocated probe array to the hash table,follow @range
                *   ascending order
                */
               int kobj_map(struct kobj_map *domain, dev_t dev, unsigned long range,
                            struct module *module, kobj_probe_t *probe,
                            int (*lock)(dev_t, void *), void *data);

               /**
                * kobj_unmap - remove the range from the probe hash table of a given kobject domain
                * @domain:     the domain
                * @dev:        device number
                * @range:      device number range
                * # this routine call to kfree() to deallocate the memory of such array 
                *   allocated by kobj_map() previously
                * # this routine traverse whole hash table to unlink each element in the range
                *   from the hash table,finally,free the array,it is the first found element in
                *   the hash table
                */
               void kobj_unmap(struct kobj_map *domain, dev_t dev, unsigned long range);

               !! NOTE,EACH ELEMENT IN THE RANGE IS LINKED TO DIFFERENT POSITION IN kobj_map.probes,
                  THE POSITION IS CALCULATED THROUGH "@device number % 255",MEMBER NAMED @next
                  IS USED TO LINK THEM TO THE PROPER POSITION IN THE HASH TABLE.

               ! another routine is kobj_map_init(),it require two parameters @base_probe for probe.@get
                 and @lock for kobj_map.lock,creates a new kobj_map object,and use base probe to
                 initialize it,return the pointer to caller.(return NULL if error)
                 /* base probe : @range is ~0UL,@next is NULL */

        Assigning Device Numbers :
          an array @chrdevs is type of struct char_device_struct * is defined in <fs/char_dev.c>.
          kernel use it to keep track of device numbers currently assigned.the index of that array
          is hashed by major number.
          each element is a collision list _ordered_ by increasing major and minor numbers,the devices
          have the same major numbers can share the same entry in the hash table,but minor numbers of
          the devices cannot overlap.

          <fs/char_dev.c>
            /**
             * chrdevs - array acts like a hashtable used by kernel to keep track of device numbers
             *           been assigned to character devices
             *           character devices have the same major number can share the same entry,but
             *           minor numbers can not overlap
             * char_device_struct - entry for character device in the hashtable @chrdevs
             * @next:               to next entry
             * @major:              major number
             * @baseminor:          starting of minor number
             * @minorct:            maximum range of minor number
             * @name:               entry name
             * @cdev:               the character device corresponding to this entry
             */
            static struct char_device_struct {
                    struct char_device_struct *next;
                    unsigned int major;
                    unsigned int baseminor;
                    int minorct;
                    char name[64];
                    struct cdev *cdev;
            } *chrdevs[CHRDEV_MAJOR_HASH_SIZE]; /* <linux/fs.h> : #define CHRDEV_MAJOR_HASH_SIZE 255 */

            static DEFINE_MUTEX(chrdevs_lock); /* lock for @chrdevs */

            ! attention,cdev inserted into this hash table is not means the character device been added
              into system,device driver must call to cdev_add() manually for add the character device.

            static inline int major_to_index(int major)
            {
                    return major % CHRDEV_MAJOR_HASH_SIZE;
            }

            /**
             * register_chrdev_region - register "a range of device numbers",that is add the range to
             *                          the hash table @chrdevs
             * @from:                   where to start
             * @count:                  maximum range
             * @name:                   entry name 
             * return:                  0 OR error code
             * # EXPORT_SYMBOL
             * # range of device number,not only of major number or of minor number
             * # this routine iterate device number in the range [@from, @from + @count - 1],for each
             *   device number,call to __register_chrdev_region() attempt to add the range to
             *   hash table;undo the operation if failed
             * # the calling to __register_chrdev_region() is like this
             *           __register_chrdev_region(MAJOR(n), MINOR(n), next - n, name);
             *   in this,@n is iterator,@next is the next device number,for @n to @from + @count,
             *   @next can not greater than @from + @count (@next := MKDEV(MAJOR(n) + 1, 0))
             *   the region : [(@from, 0) (@from + 1, 0) (@from + 2, 0) ... ]
             *                 |          |            |              |
             *                 |          +-------> interval <--------+
             *                 |                       |
             *                 +-----> interval <------+
             *   thus,for each interval,__register_chrdev_region() is be called once to register it
             */
            int register_chrdev_region(dev_t from, unsigned count, const char *name);
            
            /**
             * __register_chrdev_region - helper for register character device region of device numbers
             * @major:                    major number
             * @baseminor:                base minor number
             * @minorct:                  minor number range
             * @name:                     entry name
             * return:                    pointer of newly allocated char_device_struct object OR error code
             * # this routine handles two different cases :
             *     1> @major == 0 => dynamically allocate a major and return its number
             *        - traverse hash table from end to start,try to find out a NULL entry,
             *          use the its index as the major number,
             *          so the device number information is {0, @baseminor, @minorct, @name}
             *          attempt to add the region to hash table 
             *     2> @major > 0  => attempt to reserve the passed range of minors and return zero on success
             *        - add a specified region to hash table
             *   whatever the case is,this routine must travers the collision list of entries at the index
             *   @major number of the hash table to find out a proper position for inserting
             *   if conflict detected,return -EBUSY means the region have been using by someone
             * # overlap 
             *           old_min := old->baseminor
             *           old_max := old->baseminor + old->minorct - 1
             *           new_min := @baseminor
             *           new_max := @bseminor + minorct - 1
             *   it is OVERLAP if
             *     (nex_max >= old_min && new_max <= old_max) || (new_min <= old_max && new_min >= old_min)
             * # function use kzalloc() to allocate new entry,deallocate it if failed on inserting
             */
            static struct char_device_struct *
            __register_chrdev_region(unsigned int major, unsigned int baseminor, int minorct, const char *name);

            /**
             * unregister_chrdev_region - remove entries of specified range of device number from hash table
             * @from:                     where to start
             * @count:                    maximum range
             * # EXPORT_SYMBOL
             * # this routine iterate device numbers in [@from, @from + @count - 1],for each device number,
             *   invoke __unregister_chrdev_region() to remove it,and kfree the allocated memory previously
             *   in __register_chrdev_region()
             * # helper __unregister_chrdev_region() do the reverse to __register_chrdev_region()
             *   (remove and link remains,then returns the pointer to the object)
             */
            void unregister_chrdev_region(dev_t from, unsigned count);

            /**
             * alloc_chrdev_region - register a range of character device numbers,the major number will be
             *                       chosen dynamically
             * @dev:                 pointer used to return the start of dynamically allocated device number
             *                       range
             * @baseminor:           base minor number
             * @count:               maximum range
             * @name:                entry name
             * # EXPORT_SYMBOL
             * # this routine actually call to __register_chrdev_region() with arguments
             *     _zero_, @baseminor, @count, @name
             *   no any kind of cycles involve,@dev will set to MKDEV(@cd->major, @cd->baseminor)
             *                                                ! @cd is the return value of helper
             */
            int alloc_chrdev_region(dev_t *dev, unsigned baseminor, unsigned count, const char *name);

            /**
             * __register_chrdev - create and register a cdev occupying a range of minors
             * @major:             major number or _zero_
             * @baseminor:         base minor number
             * @count:             maximum range
             * @name:              name of this range of devices
             * @fops:              file operations associated with this devices
             * return:             0 OR error code
             * # EXPORT_SYMBOL
             * # steps this routine will process :
             *     1> __register_chrdev_region
             *     2> cdev_alloc
             *     3> setup cdev
             *          @cdev->onwer := @fops->owner
             *          @cdev->ops := @fops
             *          kobject_set_name(&@cdev->kobj , "%s", @name)
             *     4> cdev_add
             *     5> set @cd->cdev to @cdev
             *        # @cd is returned in step 1>
             *     6> return to caller
             *        # if @major is _zero_,then returns the dynamically allocated
             *          major number or error code
             *          if @major is a postive numbers,then return 0 OR error code
             */
            int __register_chrdev(unsigned int major, unsigned int baseminor, unsigned int cout,
                                  const char *name, const struct file_operations *fops);

            /**
             * __unregister_chrdev - unregister a character device which is registered by
             *                       __register_chrdev(),of couse cdev_del() if necessary
             * # EXPORT_SYMBOL
             * # kfree() the char_device_struct object allocated previously in __register_chrdev()
             */
            void __unregister_chrdev(unsigned int major, unsigned int baseminor, unsigned int count,
                                     const char *name);

            ! header <linux/fs.h> defined two static function,register_chrdev() and unregister_chrdev(),
              they are wrappers for __register_chrdev() and __unregister_chrdev(),respectively.

        Accessing a Character Device Driver :
          the file operation @def_chr_fops have defined open method only,the chrdev_open().the remain
          methods relate to a specified character device must be setup by the device driver.and
          routine chrdev_open() will replace the implementation to the file operation of the file
          object.

          <fs/char_dev.c>
            /**
             * chrdev_open - the default open method for character device
             * @inode:       inode
             * @filp:        file object of the device file associated with @inode
             * return:       0 OR error code
             */
            static int chrdev_open(struct inode *inode, struct file *filp);

            brief description for chrdev_open() :
              get @inode->cdev at first,and check if it is NULL.
              if it is,that means nobody have opened the device.
              then call to kobj_lookup() to see whether this character device been registered by
              device driver(mapped it in Kobject Mapping Domain),return -ENXIO if it is not
              registered.
              get the cdev which contains the kobject,and check again @inode->i_cdev,maybe some
              one opened the device when we in kobj_lookup() /* of course,require @cdev_lock for
                                                              * access to @inode->cdev
                                                              */
              if someone been opened it,cdev_get() it,return -ENXIO if failed.
              if nobody been opened it,assign the cdev got by kobj_lookup() to the @inode's cdev member,
              and add @i_devices to this cdev's inodes list,the @cdev->list.
              next,cdev_put() the new cdev,and replace @filp->f_ops with the cdev's @ops,return -ENXIO,if
              device driver did not implement device methods(@cdev->ops == NULL).
              if @open method is implemented,then open this device,return -ENXIO if failed on open.
              finally,return _zero_ to caller.

        Buffering Strategies for Character Devices :
          for character device,there will are two behaviors about data transfer.
          one is few bytes at once,do not use DMA.
          another is a block of data at once,use DMA.
          
          in the first case,it do not need to buffer data,the time of CPU read data from I/O port
          and send it to User Mode process address space is lower cost.interrupt handler can simply
          read data and acknowledge to device,and interrupt service routine send it to the process
          later.

          in the second case,such sequential devices,device driver must be able to cope with this
          avalanche of data in all possible situations,even when the CPU is temporarily busy
          running some other process.
            solution :
              combine two different techniques >
                1> DMA for block of data
                2> circular buffer of two or more elements
                   /* for example,the double buffering of a display card */
                   /* after DMA completed,shift index for next I/O operation,
                    * release buffer after the data transfered to User Mode process,and 
                    * shift index again if necessary
                    */

            ! buffer handling for sequential devices is easy because the same buffer is
              never reused.(no retransmit request)


/* END OF CHAPTER13 */


Chapter 14 : Block Device Drivers
    Block Devices Handling :
      Kernel Components Affected By a Block Device Operation >
                                  I/O Request
                                       |
                                       V
                                      VFS
                                      | |                     
                      +---------------+ |                     
                      |                 |                     
                      |             Direct I/O
                      V                 |                     
                      Disk Caches       |                     
                          |             V                     
                          +------>Disk Filesystem /* Mapping Layer */
                                        |
                                        V
                                Generic Block Layer
                                        |
                                        V
                                I/O Scheduler Layer /* sort AND group I/O operations to improve performance */
                                        |
                                        V
                                Block Device Driver /* operate block device controller */
                                        |
                                        V
                                    Hard Disk

      Mapping Layer does :
        1> determines the block size of the filesystem including the file cand
           computes the extent of the requested data in terms of "file block numbers".
        2> invokes a filesystem-specified function that accesses the file's disk inode
           and determines the position of the requested data on disk in terms of 
           "logical block numbers".

        ! if I/O request was done on a raw block device,mapping layer does not invoke
          a filesystem-specific method;rather,it translates the offset in the block
          device file to a position inside the disk.

      Generic Block Layer :
        start the I/O operations.because the blocks of the file might not adjacent on disk,
        thus it have to issues several block I/O operations,each of them is represented
        by a structure "bio, Block I/O".
        /* almost all block devices are disks */

      Chunks of Disk Data :
        sector - block device controller transfer data in chunks of fixed length
        block  - VFS,mapping layer,and filesystem group the disk data in logical
                 units called "blocks",it corresponds to the minimal disk storage
                 unit inside a filesystem
        segment - each segment is a memory page - or a portion of a memory page,which
                  including chunks of data that are physically adjacent on disk
        page - disk data in disk cache fits in a page frame
        
        ! Generic Block Layer should to knows all kind of chunks.

      Layout of Chunks :
        even there are several kind of chunks,they usually share the same physical RAM cells.

          +------------+
          |block buffer|-> minimal disk storage unit
          +------------+
          |            |
          V            V  {block buffer}  {block buffer}  {block buffer}
        [ Sector, Sector, Sector, Sector, Sector, Sector, Sector, Sector ] => Page 4kB
          |512B|          |                                            |
                          V                                            V
                          +--------------------------------------------+
                          |                 Segment                    |
                          +--------------------------------------------+
                          |
                          +--> adjacent disk data

    Sectors :
      hard disks and similar devices transfer several adjacent bytes at once to improve performance,
      that is because disk seek will cost a lot of times to the position where the request data is
      stored.
      sector : each data transfer operation for a block device acts on a group of adjacent bytes.
               such group is a sector,and in general,the size of a sector is 512 bytes.(depend on the device)
               sector as the basic unit of data transfer,can not be splited.
      /* the hard disk controller accepts commands that refer to the disk as a large array of sectors. */

      ! Linux use 512B as the sector size,if the device uses larger sector size,then device driver should
        handle conversion.
      ! position of a group data stored on a disk is identified by the index of its first sector and
        its length as number of 512B sectors.

      /* <linux/types.h> : typedef u64 sector_t; // sector number */

    Blocks :
      block is the basic unit of data transfer for the VFS and,consequently,for the filesystems.in Linux,
      size of a block must be a power of 2 and cannot greater than the size of a page frame,moreover,it
      must be a multiple of the sector size.
      block size is not specific to the hard disk device,because hard disk I/O controller use sector as the
      basic unit.
      several disk partitions maybe make use of different block sizes,it depends on the filesystem of
      that disk parition.
      /* "raw" access on a disk device will bypass disk based filesystem,kernel executes it by using blocks
       * of largest size(4096bytes)
       */

      each block has its own block buffer in RAM,where stored the content of this block.read operation read
      disk data and fill the buffer;write operation updates the content of block buffer,and hard disk I/O
      controller read the buffer,update disk records.
      /**
       * buffer_head - structure contains all the information needed by the kernel to know how to
       *               handle the buffer
       * @b_page:      pointer to struct page,the page this buffer head mapped to
       * @b_data:      pointer to char,data with in the page
       *               # if @b_page in high-memory,this field stores the offset of the block buffer inside
       *                 the page;otherwise,it is the starting linear address of the block buffer
       * @b_blocknr:   sector_t,start block number,the logical block number
       * @b_bdev:      pointer to struct block_device,the block device using this buffer head
       */

    Segments :
      in almost all cases,the data transfer is directly performed by the disk controller with a DMA operation.
      the data transferred by a single DMA operation must belong to sectors that are adjacent on disk.
      /* physical constraint :
       *   a disk controller that allows DMA transfers to non-adjacent sectors would have a poor transfer rate,
       *   disk seek is quite a slow operation.
       */

      simple DMA operation :
        data is transferred from or to memory cells that are physically contiguous in RAM.
        /* older block device */
      scatter-gather DMA transfers :
        data can be transferred from or to several noncontiguous memory areas.
        /* newer block device */
        ! device driver must tell block device controller :
            1> the initial disk sector number and the total number of sectors to be transferred
            2> a list of descriptors of memory areas,each of which consists of an address and
               a length

      segment - data unit used in scatter-gather DMA transfer.
                a scatter-gather DMA transfer may involve several segments at once.
      ! block device driver does not need to know about blocks.

      ! physical segment :
          for several segments,if the corresponding page frames happen to be contiguous in RAM and
          the corresponding chunks of disk data are adjacent on disk,Generic Block Layer can merged
          them into a larger memory area,that is physical segment.                   
        hardware segment :                                                  
          on the architecture support IO-MMU,the memory area resulting from this kind of merge operation
          is called hardware segment.                  (merge operation processed by Generic Block Layer)

    The Generic Block Layer :
      it handles the requests for all block devices in the system.
      by make use of its interfaces,the kernel can :
        1> put data buffers in high memory,address mapped when CPU need to to access buffer,unmapped right
           after.
        2> implement "zero-copy"
           zero-copy : disk data is directly put in the User Mode address space without being copied to
                       kernel memory first
        3> manage logical volumes,such as those used by LVM and RAID
           several disk partitions even on different block devices,can be seen as a single partition.
        4> can handle the most recent disk controllers,such as large onboard disk caches,enchanced
           DMA capabilities,onboard scheduling of the I/O transfer requests,...

      The bio Structure :
        bio,the ongoing I/O block device operation descriptor,it is the core data structure of this layer.

        <linux/bio.h>
          /**
           * bio_vec - data vector of bio,that is the segment
           * @bv_page: data page frame
           * @bv_len:  data length
           * @bv_offset:
           *           starting offset from the head of page frame
           */
          struct bio_vec {
                  struct page *bv_page;
                  unsigned int bv_len;
                  unsigned int bv_offset;
          };

          /**
           * bio - ongoing block device I/O descriptor
           * @bi_sector:    initial sector number of this bio
           * @bi_next:      request queue link
           * @bi_bdev:      block device descriptor
           * @bi_flags:     status,command,etc
           * @bi_rw:        bottom bits - READ/WRITE
           *                top bits - priority
           * @bi_vcnt:      number of bio vector entities in the array
           * @bi_idx:       current index into bvl_vec
           *                this field will be updated to keep track of
           *                the first segment in the bio that is yet to be
           *                transferred,if the block device driver cannot
           *                perform the  whole data transfer with one
           *                scatter-gather DMA operation
           * @bi_phys_segments:
           *                number of segments in this bio after
           *                physical address coalescing is performed
           * @bi_size:      residual I/O count
           * @bi_seg_front_size:
           *                first mergeable segments in this bio
           * @bi_seg_back_size:
           *                last mergeable segments in this bio
           * @bi_max_vecs:  max bvl_vecs this bio can hold
           * @bi_comp_cpu:  completion CPU
           * @bi_cnt:       pin count
           *                @bi_cnt == 0,this bio will be destroyed,include
           *                @bio_vec
           * @bi_io_vec:    array of struct bio_vec instances,the exact vec list
           * @bi_end_io:    method invoked at the end of bio's I/O operation
           *                this method usually called by bio_endio(),whose parameter
           *                @err represents status code,0 means succeed
           *                  typedef void (bio_end_io_t)(struct bio *, int)
           * @bi_private:   pointer useed by the generic block layer and the I/O
           *                completion method of the block device driver
           * @bi_integrity: data integrity
           * @bi_destructor:
           *                destructor of this bio
           *                typedef void (bio_destructor_t)(struct bio *)
           * @bi_inline_vecs:
           *                one element array of struct bio_vec,used to avoid
           *                double allocations for a small number of bio_vecs
           */
          struct bio {
                  sector_t bi_sector;
                  
                  struct bio *bi_next;
                  struct block_device *bi_bdev;
                  unsigned long bi_flags;
                  unsigned long bi_rw;

                  unsigned short bi_vcnt;
                  unsigned short bi_idx;
                  unsigned int bi_phys_segments;
                  unsigned int bi_size;

                  unsigned int bi_seg_front_size;
                  unsigned int bi_seg_back_size;
                  unsigned int bi_max_vecs;
                  unsigned int bi_comp_cpu;
                  atomic_t bi_cnt;

                  struct bio_vec *bi_io_vec;
                  bio_end_io_t *bi_end_io;
                  void *bi_private;

          #if defined(CONFIG_BLK_DEV_INTEGRITY)
                  struct bio_integrity_payload *bi_integrity;
          #endif
                  
                  bio_destructor_t *bi_destructor;
                  struct bio_vec bi_inline_vecs[0];
          };

          /* bio flags */
          BIO_UPTODATE     0   ok after I/O completion
          BIO_RW_BLOCK     1   RW_HEAD set,and read/write would block
          BIO_EOF          2   out-out-bounds error
          BIO_SEG_VALID    3   @bi_phys_segments valid
          BIO_CLONED       4   does not own data
          BIO_BOUNCED      5   bio is a bounce bio
          BIO_USER_MAPPED  6   contains user pages
          BIO_EOPNOTSUPP   7   not supported OP
          BIO_CPU_AFFINE   8   complete bio on same CPU as submitted
          BIO_NULL_MAPPED  9   contains invalid user pages
          BIO_FS_INTEGRITY 10  fs owns integrity data,not block layer
          BIO_QUIET        11  make bio quiet

          bio_flagged(bio, flag)  ((bio)->bi_flags & (1 << (flag))

          /* top 4 bits of flags indicate the pool this bio came from */          
          BIO_POOL_BITS    (4)
          BIO_POOL_NONE    ((1UL << BIO_POOL_BITS) - 1)
          BIO_POOL_OFFSET  (BITS_PER_LONG - BIO_POOL_BITS)
          BIO_POOL_MASK    (1UL << BIO_POOL_OFFSET)
          BIO_POOL_IDX(bio)  ((bio)->bi_flags >> BIO_POOL_OFFSET)

          /* bio rw flags */
          /**
           * bit 0 data direction,TRUE - read from device,FALSE - write to device
           * bit 1 fail fast device errors
           * bit 2 fail fast transport errors
           * bit 3 fail fast driver errors
           * bit 4 rw-ahead when set
           * bit 5 barrier
           *       insert a serialization point in the IO queue,forcing previously
           *       submitted IO to be completed before this one is issued
           * bit 6 synchronous I/O hint
           * bit 7 unplug the device imeediately after submitting this bio
           * bit 8 metadata request
           *       used for tracing to differentiate metadata and data IO,may also
           *       used by I/O scheduler
           * bit 9 discard sectors
           *       informs the lower level device that this range of sectors is no longer
           *       used by the file system,and may thus be freed by the device
           *       used for flash based storage
           * bit 10 tell I/O scheduler not to wait for more requests after this one has been
           *        submitted even if it is a SYNC request
           */
          enum bio_rw_flags {
                  /* must match REQ_* */
                  BIO_RW,
                  BIO_RW_FAILFAST_DEV,
                  BIO_RW_FAILFAST_TRANSPORT,
                  BIO_RW_FAILFAST_DRIVER,
                  /* END,must match REQ_* */

                  BIO_RW_AHEAD,
                  BIO_RW_BARRIER,
                  BIO_RW_SYNCIO,
                  BIO_RW_UNPLUG,
                  BIO_RW_META,
                  BIO_RW_DISCARD,
                  BIO_RW_NOIDLE,
          };

          /* macro used to traverse segments in the bio from @bi_idx */
          #define bio_for_each_segment(bvl, bio, i) \
                   __bio_for_each_segment(bvl, bio, i, (bio)->bi_idx)
                   |                      |    |    |  |
                   |                      |    |    |  +--> @start_idx
                   |                      |    |    +--> @i
                   |                      |    +--> @bio
                   |                      +--> @bvl => type is struct bio_vec *
                   |               +--> &(bio->bi_iovec[@index])
                   |               |
                   +--> for (bvl = bio_iovec_idx((bio), (start_idx)), i = (start_idx);
                             i < (bio)->bi_vcnt;
                             bvl++, i++)
          
          /* slab caches for bio vector
           * each slab cache will be initialized in init function biovec_init_slabs()
           */
          #define BIOVEC_NR_POOLS 6
          #define BV(x) { .nr_vecs = x, .name = "biovec-"__stringify(x) }
          struct biovec_slab bvec_slabs[BIOVEC_NR_POOLS] __read_mostly = {
                  BV(1), BV(4), BV(16), BV(64), BV(128), BV(BIO_MAX_PAGES),
          };

          /**
           * bio_alloc - allocate a bio structure object with specified bio vectors
           * @gfp_mask:  GFP flag mask used to by mempool_alloc()
           * @nr_iovecs: number of bio vectors should to be allocated for the bio
           * return:     pointer to the bio object OR NULL
           * # EXPORT_SYMBOL
           * # this routine actually call to bio_alloc_bioset(),and which allocate memory
           *   through the method mempool_alloc() in @fs_bio_set->bio_pool,@fs_bio_set is
           *   type of struct bio_set *,member @bio_pool is type of mempool_t
           * # @fs_bio_set is defined in <fs/bio.c>,it is allocated in init function init_bio()
           *   through bioset_create(),memory pool size is BIO_POOL_SIZE 2 defined in <linux/bio.h>.
           *   if necessary,new slab for bioset will be created
           * # member @bvec_pool of structure bio_set is a mempool,and used for allocate bio vector,
           *   the initial size of memory pool is also BIO_POOL_SIZE
           * # this routine will set @bio_cnt to 1 if succeed to allocate new bio,and set @bi_destructor
           *   to bio_fs_destructor()
           */
          extern struct bio *bio_alloc(gfp_t gfp_mask, int nr_iovecs);

          /**
           * bio_put - put bio,destroy it if necessary
           * @bio:     bio instance
           * # EXPORT_SYMBOL
           * # decrease @bio->bi_cnt,call @bio->bi_destructor if refcount becomes _zero_
           */
          extern void bio_put(struct bio *bio);

      Representing Disks and Disk Partitions :
        a disk is a logical block device that is handled by the generic block layer,usually,it
        corresponds to a hardware block device.
        a disk can be a virtual device built upon several physical disk partitions,or a storage
        area living in some dedicated pages of RAM.

        +--> sda
        | +--> sda1
        | |       +--> sda2
        | |       |
        | |       |
        +---------------------------------------------+
        | part0 | part1 | part2 | part3 | part4 | ... | => logical block device,disk
        +---------------------------------------------+
          |       |       |
          |       |       |
          |       |       +--> partition 2
          |       |            (@major, @first_minor + 2)
          |       |            hd_struct structure
          |       |            gendisk.part_tbl->part[2]
          |       |
          |       +--> partition 1
          |            (@major, @first_minor + 1)
          |            hd_struct structure
          |            gendisk.part_tbl->part[1]
          |
          |
          +--> inavailable
               used for setup gendisk
               gendisk structure
               gendisk.part_tbl->part[0]
                       |         |
                       |         +--> *hd_struct[]
                       |
                       +--> disk_part_tbl structure

        The gendisk Structure :
          <linux/genhd.h>
            /**
             * gendisk - structure used to represent a disk
             * @major:   major number of driver
             * @first_minor:
             *           the first minor number,it is the MINOR(this disk's device number),
             *           thus (@major, @first_minor) is the whole block device 
             * @minors:  maximum number of minors, = 1 for disk that cant be paritioned
             *           # @major,@first_minor and @minors are input parameters only,
             *             dont use directly,use disk_devt() and disk_max_parts()
             * @disk_name:
             *           name of major driver (DISK_NAME_LEN - 32)
             * @devnode: method used to returns a string describe device node
             * @part_tbl:
             *           disk partitions table
             * @part0:   the first disk partition
             * @fops:    device file operations
             * @queue:   disk request queue
             * @private_data:
             *           driver private data
             * @flags:   disk flags
             * @driverfs_dev:
             *           device object of this disk
             * @slave_dir:
             *           slave directory
             * @random:  timing of the disk's interrupts,used by kenrel builti-in random
             *           number generator
             * @sync_io: counter of sectors written to disk,used only for RAID
             * @async_notify:
             *           async operation notify
             * @integrity:
             *           block device integrity
             * @node_id: NUMA node id
             */
            struct gendisk {
                    int major;
                    int first_minor;
                    int minors;
                    char disk_name[DISK_NAME_LEN];
                    char *(*devnode)(struct gendisk *gd, mode_t *mode);
                    struct disk_part_tbl *part_tbl;
                    struct hd_struct part0;
                    const struct block_device_operations *fops;
                    struct request_queue *queue;
                    void *private_data;

                    int flags;
                    struct device *driverfs_dev;
                    struct kobject *slave_dir;
                    struct timer_rand_state *random;

                    atomic_t sync_io;
                    struct work_struct async_notify;
            #ifdef CONFIG_BLK_DEV_INTEGRITY
                    struct blk_integrity *integrity;
            #endif
                    int node_id;
            };

            /* gendisk flags */

            #define GENHD_FL_REMOVABLE 1

            /* 2 unused */

            #define GENHD_FL_MEDIA_CHANGE_NOTIFY 4

            #define GENHD_FL_CD 8

            #define GENHD_FL_UP 16

            #define GENHD_FL_SUPPRESS_PARTITION_INFO 32

            /* allow extended devt */
            #define GENHD_FL_EXT_DEVT 64

            #define GENHD_FL_NATIVE_CAPACITY 128

        The disk_part_tbl Structure :
          a disk_part_tbl object represents the disk partitions table.
          
          <linux/genhd.h>
            /**
             * disk_part_tbl - disk partitions table
             * @rcu_head:      table updating RCU
             * @len:           number of records
             * @last_lookup:   pointer to the partition which is the result of
             *                 last lookup operation
             * @part:          partition record array
             */
            struct disk_part_tbl {
                    struct rcu_head rcu_head;
                    int len;
                    struct hd_struct *last_lookup;
                    struct hd_struct *part[];
            };

        The block device related device file opeartions :
          <linux/blkdev.h>
            /**
             * block_device_operations - block device operations
             * @open:                    open the block device file
             * @release:                 closing the last reference to a block device file
             * ...
             * @media_changed:           checking whether the removable media has been changed
             * @set_capacity:            set disk capacity
             * @revalidate_disk:         checking whether the block device holds valid data
             */
            struct block_device_operations {
                    int (*open)(struct block_device *, fmode_t);
                    int (*release)(struct gendisk *, fmode_t);
                    int (*locked_ioctl)(struct block_device *, fmode_t, unsigned, unsigned long);
                    int (*ioctl)(struct block_device *, fmode_t, unsigned, unsigned long);
                    int (*compat_ioctl)(struct block_device *, fmode_t, unsigned, unsigned long);
                    int (*direct_access)(struct block_device *, sector_t, void **, unsigned long *);
                    int (*media_changed)(struct gendisk *);
                    unsigned long long (*set_capacity)(struct gendisk *, unsigned long long);
                    int (*revalidate_disk)(struct gendisk *);
                    int (*getgeo)(struct block_device *, struct hd_geometry *);
                    struct module *owner;
            };

        The hd_struct Structure :
          in general,a disk can be splited to several partitions,and the partitions inside a disk
          are characterized by consecutive minor numbers.this is why structure gendisk has members
          @first_minor and @minors.
          generally,the master disk's minor number should be zero,and partition has minor in the range
          [1, 1 + @minors - 1].
          the first partition has minor 1,and the second partition has minor 2,...

          for master disk,there should be a device file without number suffixed is exist,such "sda".
          and for the partitions,there should are several device files such "sda1", "sda2", and so on.

          <linux/genhd.h>
            /**
             * hd_struct - structure used to represent a disk partition
             * @start_sect:
             *             starting sector number
             * @nr_sects:  number of secots of this partition
             * @alignment_offset:
             *             sector alignment offset
             * @discard_alignment:
             *             discard alignment
             * @__dev:     device object of this partition
             * @holder_dir:
             *             directory of holder of this partition in
             *             device hierarchy
             * @policy:    policy,1 means read-only,0 otherwise
             * @partno:    partition number
             * @make_it_fail:
             *             make it fail request indicator
             * @stamp:     time stamp
             * @in_flight: partition in flight info
             * @dkstats:   disk stats(__percpu for SMP)
             * @rcu_head:  Read-Copy-Update mechanism used to update the informations
             */
            struct hd_struct {
                    sector_t start_sect;
                    sector_t nr_sects;
                    sector_t alignment_offset;
                    unsigned int discard_alignment;
                    struct device __dev;
                    struct kobject *holder_dir;
                    int policy;
                    int partno;

            #ifdef CONFIG_FAIL_MAKE_REQUEST
                    int make_it_fail;
            #endif

                    unsigned long stamp;
                    int in_flight[2];

            #ifdef CONFIG_SMP
                    struct disk_stats __percpu *dkstats;
            #else
                    struct disk_stats dkstats;
            #endif
                    
                    struct rcu_head rcu_head;
            };

        The alloc_disk() and add_disk() functions :
          <block/gendisk.c>
            /**
             * alloc_disk - allocate a logical disk object
             * @minors:     maximum number of minors
             * return:      newly created gendisk object's address
             *              OR
             *              NULL
             * # EXPORT_SYMBOL
             * # this routine actually call to alloc_disk_node(@minors, -1)
             */
            struct gendisk *alloc_disk(int minors);

            /**
             * alloc_disk_node - allocate a logical disk object from a specified node
             * @minors:          maximum number of minors
             * @node_id:         NUMA node id
             * return:           newly created gendisk object's address OR NULL
             * # EXPORT_SYMBOL
             * # this function does the following steps :
             *     1> kmalloc_node() a gendisk object @disk
             *     2> if succeed on allocating,then
             *          init partition stats of @disk->part0,kfree and return NULL if failed
             *          actually,it call to alloc_percpu() for allocate @disk->part0->dkstats
             *          use @node_id to set @disk->node_id
             *          disk_expand_part_tbl() to expand @disk->part_tbl for part0
             *          # actually,the second parameter of disk_expand_part_tbl() is the number
             *            of disk parts.the routine expand part table by allocate a new
             *            disk_part_tbl object,the number of elements in array part[]
             *            is @2nd-parameter + 1(the part[0],represent whole disk).
             *            assign pointers in part[] of old table to the pointers in part[] of
             *            new table.
             *            finally,use the new one to replace the old one
             *
             *          # the size in bytes of allocated memory area of @disk_part_tbl is
             *              sizeof(struct disk_part_tbl) + (@partno + 1) * sizeof(struct hd_struct)
             *            ! variable array part[] is the tail of structure disk_part_tbl,this is
             *              important for expand disk partition table
             *
             *          # alloc_disk_node() just expand part table with @partno 0,it only
             *            need to setup part[0] - the whole disk
             *
             *          set
             *            @disk->part_tbl->part[0]
             *            @disk->minors
             *            @class of device object of @disk to &block_class
             *            @type of device object of @disk to &disk_type
             *          init
             *            @disk's @random
             *            device object of @disk
             *            @disk->async_notify with notification handler entity @media_change_notify_thread
             *     3> return @disk to caller(it is NULL if failed)
             */
            struct gendisk *alloc_disk_node(int minors, int node_id);

            /**
             * add_disk - add partitioning information to kernel list
             * @disk:     logical disk object
             * # EXPORT_SYMBOL
             * # this function does the following steps :
             *     1> WARN_ON if @minors is not _zero_ but @major and @first_minor are _zero_
             *        WARN_ON if !@minors AND no GENHD_FL_EXT_DEVT is set
             *        (@minors == 0 indicates to use ext devt from part0,
             *         should be accompanied with EXT_DEVT flag)
             *     2> enable GENHD_FL_UP
             *     3> call to blk_alloc_devt() to get a device number,return to caller if failed
             *     4> use the device number to set @devt of device object of @disk
             *     5> set @major,@first_minor
             *     6> call to blk_register_region() to register device number range in the
             *        Kernel Mapping Domain @bdev_map
             *     7> call to register_disk() to register this logical disk
             *        # this routine is defined in <fs/partitions/check.c>.
             *          at the end,it traverse the partition table of the disk.
             *          for each partition,trigger a kobject user event KOBJ_ADD on
             *          the @__dev->kobj of partition object hd_struct in the partition table
             *     8> call to blk_register_queue(),this routine actually register @disk->queue
             *        to elevator algorithm request queue(elv_register_queue())
             *     9> bdi_register_dev() to register the backing device info of @queue of @disk
             *    10> create sysfs link for @disk
             */
            void add_disk(struct gendisk *disk);

      Submitting a Request :
        the common steps that kernel have to executes for submit a I/O operation to
        Generic Block Layer :
          1> allocate new bio object
          2> set
               @bi_sector - the initial sector number of the data
               @bi_size   - number of sectors covering the data
               @bi_bdev   - address of the block device descriptor
               @bi_io_vec - initial address of an array of bio_vec data structures(segments)
               @bi_vcnt   - total number of segments in the bio
               @bi_rw     - requested operation flags(READ / WRITE)
               @bi_end_io - address of a completion procedure
          3> invoke generic_make_request(),the main entry point of Generic Block Layer
          4> generic_make_request() does :
               1> checks if @bi_sector is valid.
                  set BIO_EOF in @bi_flags if invalid,and report error,call bio_endio(),
                  which updates @bi_size and @bi_sector,then call to @bi_end_io
               2> get @queue in @bd_disk of the block device descriptor(@bi_bdev)
               3> call block_wait_queue_running() to check whether the I/O scheduler
                  currently in use is being dynamically replaced,if it is,then put
                  current process to sleep until new I/O scheduler started

                  /* !! block_wait_queue_running() been removed in Linux 2.6.34.1,
                   *    instead,routine __make_request() get request queue through
                   *    get_request_wait(),which call to get_request(),this routine
                   *    returned with @queue_lock held,returns NULL if failed to get
                   *    a free request.
                   *    if get_request() returned NULL,then get_request_wait() will
                   *    put @current into TASK_UNINTERRUPTIBLE() and call to io_schedule(),
                   *    then try again get_request() until we succeed to get a free
                   *    request.
                   * # __make_request() is the default value for @make_request_fn.
                   */

               4> invoke blk_partition_remap() to check whether the block device
                  refers to a disk partition,if it is,get hd_struct object of the partition
                  from @bi_bdev
                  and executes :
                    1> let @bi_sector += hd_struct.start_sect
                       /* translate relative sector number of the partition to the
                        * sector number of whole disk
                        */
                    2> set @bi_bdev to @bi_bdev->bd_contains
                       /* make use of whole disk,not the partition */
                    ! these two steps will only executed if number of sectors that covering
                      I/O data is valid AND @bi_bdev is not equal to @bi_bdev->bd_contains
               5> call to @queue->make_request_fn() to issue an I/O request
               6> return to caller /* generic_make_request() ended normally */

        And the @queue of gendisk structure is default set to elevator request queue as
        preceding described,so this I/O request will be served at next elevator started.

        elevator algorithm brief description :

                                    (L -> R AND R -> L)
                               +--> current position
                               |
          L                    |                                           R
          <---------------------------------------------------------------->
              |      |             |               |
              |      |             |               +--> new req2
              |      |             +--> new req1
              |      +--> new req4
              +--> new req3

          if new req is fall into the current sector direction,then queue it in this time.
          if new req is not fall into the current sector direction,then queue it in next time.
          
          the range is [@start sector, maximum sector of current partition]
          for next time,disk have to seek to the start sector of the first request in the queue.
          but of course,we can sort the queue to ensure that start sector of first request is the
          minimum sector number than others in the queue of current disk partition,and the last
          has the maximum start sector number

          <block/blk-core.c>
            /**
             * generic_make_request - main entry point of Generic Block Layer,use to issue
             *                        block I/O operation request
             * @bio:                  the onging block I/O operation descriptor
             * # EXPORT_SYMBOL
             * # this routine does the following steps :
             *     1> check @current->bio_list,if it is not NULL,then adds @bio to @bio_list and
             *        return,that means make_request() is active,we do not need restart it,just
             *        insert the new request as well
             *        # adding set @bi_next member
             *     2> initialize local bio_list object @bio_list_on_stack
             *        and set @current->bio_list to the address of this local variable
             *     3> do-while cycle until @bio becomes NULL(@bio as iterator)
             *          call to __generic_make_request(@bio),it does the works described preceding
             *          update @bio to next bio in @current->bio_list,if no more,it must be NULL
             *          # might there is a kernel control path added new bio when we waiting for
             *            I/O scheduler
             *     4> reset @current->bio_list to NULL and return
             */
            void generic_make_request(struct bio *bio);

    The I/O Scheduler :
      kernel try to cluster several sectors and handle them as a whole whenever possible,for
      reducing the average number of head movements.
      block device request,it essentially describes the requested sectors and the kind of operation
      to be performed on them.request is not satisfied immediately,but schedule it and will be
      performed at a later time.

      User Mode process -> open file -> VFS open file -> block device driver -> read inode from disk
      -> queue request -> request satisfied -> store inode information -> return inode to VFS ->
      -> VFS create file object -> VFS put the file object to process file table -> sys_open return
      -> process get a file descriptor
      /* after process issued the system call,it will be blocked until VFS use the inode to create
       * a file object,and put it into process file table,and then,sys_open return to User Mode
       * process
       * "queue request" => notify I/O scheduler a new I/O operation
       * "block device driver" => cannot be suspended during waiting for request to be satisfied,if it is,
       *                          then other process try to access the same disk would be blocked as
       *                          well,even if it wants to operate a different file
       */

      for prevent Block Device Driver be suspended,each I/O operation is processed asynchronously,and
      in particular,the block device driver is interrupt-driven -
        generic block layer create a new block device request or to enlarge an already existing
        one and then terminates.block device driver calls the "strategy routine" to select a
        pending request and satisfy it by issuing suitable commands to disk controller,after
        disk raised an interrupt,block device driver does some cleaning works,and select next
        request to satisfy it if necessay.

      request queue : contain the list of pending requests for the device,each block device driver
                      maintains its own request queue.
                      if the I/O controller is handling several disks,there is usually one
                      request queue for each physical block device,and I/O scheduler separately
                      performed on each request queue.

      Request Queue Descriptors :
        <linux/blkdev.h>
          /**
           * request_queue - queue of I/O operation requests
           * @queue_head:    head of queue of pending requests,head is a dummy element
           * @last_merge:    request in the queue to be considered
           *                 first for possible merging
           * @elevator:      elevator object
           * @rq:            the queue request freelist,one for reads(SYNC) and one
           *                 for writes(ASYNC)
           *                 # requeust_list - free request list
           *                 # @count:         read rq(s) counter and write rq(s) counter
           *                 # @starved:       rq starved indicator 0 / 1
           *                 # @elvpriv:       number of requests that using elevator,not
           *                 #                 the FIFO
           *                 # @rq_pool:       memory pool used to allocate rq
           *                 # wait:           two wait queues for processes sleeping
           *                 #                 for available READ and WRITE
           *                 # * @count,@starved,@wait are indexed by BLK_RW_SYNC - 1
           *                 #   and BLK_RW_ASYNC - 0
           *                 struct request_list {
           *                         int count[2];
           *                         int starved[2];
           *                         int elvpriv;
           *                         mempool_t *rq_pool;
           *                         wait_queue_head_t wait[2];
           *                 };
           * @request_fn:    method that implement the entry point for "strategy routine"
           *                 of device driver
           * @make_request_fn:
           *                 method to create a new request
           * @prep_rq_fn:    method to build the commands to be sent to the hardware
           *                 device to process this request
           * @unplug_fn:     method to unplug the block device
           * @merge_bvec_fn: method to return the number of bytes that can be inserted
           *                 into an existing bio when adding a new segment
           * @prepare_flush_fn:
           *                 method to prepare flush rq
           * @softirq_done_fn:
           *                 method to be called after softirq for I/O scheduler is done
           * @rq_timed_out_fn:
           *                 method to be called when request timeout
           * @dma_drain_needed:
           *                 method to be called when DMA drain needed
           * @lld_busy_fn:   method to export busy state,lower-level-driver busy function
           * @end_sector:    sector number of the tail request
           * @boundary_rq:   the request at queue tail
           *                 # @end_sector and @boundary_rq are used for dispatch queue sorting
           * @unplug_timer:  dynamic timer used to perform device plugging
           * @unplug_thresh: unplug tresh,if the number of pending requests in the queue
           *                 exceeds this value,the device is immediately unplugged
           * @unplug_delay:  time delay before device unplugging
           * @unplug_work:   struct work object used to unplug the device
           * @backing_dev_info:
           *                 information about the I/O data flow traffic for the
           *                 underlying hardware block device
           *                 e.g.
           *                   information about read-ahead and about request queue
           *                   congestion state
           * @queuedata:     queue private data,owner of the queue can get to use this
           *                 for whatever they like
           * @bounce_gfp:    queue needs bounce pages for pages above this limit,
           *                 gfp flag for allocate bounce pages
           * @queue_flags:   queue flags
           * @__queue_lock:  protector for reentrancy,queue private
           * @queue_lcok:    protect reentrancy
           *                 # @__queue_lock should _never_ be used directly,it is queue private,
           *                   use @queue_lock instead
           * @kobj:          request queue kobject
           * @nr_requests:   maximum number of requests in the queue
           * @nr_congestion_on:
           *                 queue is considered congested if the number of pending requests
           *                 rises above this threshold
           * @nr_congestion_off:
           *                 queue is considered not congested if the number of pending requests
           *                 falls below this threshold
           * @nr_batching:   maximum number of pending requests that can be submitted even when
           *                 the queue is full by a special "batcher" process
           *                 # usually is 32
           * @dma_drain_buffer:
           *                 DMA drain buffer
           * @dma_drain_size:
           *                 DMA drain size
           * @dma_pad_mask:  DMA pad mask
           * @dma_alignment: DMA alignment
           * @queue_tags:    bitmap of free/busy tags,used for tagged requests
           * @tag_busy_list: list of tage in busy
           * @nr_sorted:     number of sorted requests
           * @in_fligh:      queue in flight info
           * @rq_timeout:    request time out threshold
           * @timeout:       timed-out timers
           * @timeout_list:  queue timeout list
           * @limits:        queue limits
           * @sg_timeout:    user-defined command time-out,used only by SCSI generic devices
           * @sg_reserved_size:
           *                 essentially unsed
           * @node:          NUMA node
           * @blk_trace:     block device operation tracing
           *
           * @ordered:       
           * @next_ordered:
           * @ordseq:
           * @orderr:
           * @ordcolor:
           * @pre_flush_rq:
           * @bar_rq:
           * @post_flush_rq:
           * @orig_bar_rq:   these are reserved for flush operations
           *
           * @sysfs_lock:    sysfs mutex lock
           * @bsg_dev:       SCSI Generic block class 
           */
          struct request_queue {
                  struct list_head queue_head;
                  struct request *last_merge;
                  struct elevator_queue *elevator;

                  struct request_list rq;

                  request_fn_proc *request_fn;
                  make_request_fn *make_request_fn;
                  prep_rq_fn *prep_rq_fn;
                  unplug_nf *unplug_fn;
                  merge_bvec_fn *merge_bvec_fn;
                  prepare_flush_fn *prepare_flush_fn;
                  softirq_done_fn *softirq_done_fn;
                  rq_timed_out_fn *rq_timed_out_fn;
                  dma_drain_needed_nf *dma_drain_needed;
                  lld_busy_fn *lld_busy_fn;

                  sector_t end_sector;
                  struct request *boundary_rq;

                  struct timer_list unplug_timer;
                  int unplug_thresh;
                  unsigned long unplug_delay;
                  struct work_struct unplug_work;

                  struct backing_dev_info backing_dev_info;

                  void *queuedata;

                  gfp_t bounce_gfp;
                  unsigned long queue_flags;

                  spinlock_t __queue_lock;
                  spinlock_t *queue_lock;

                  struct kobject kobj;

                  unsigned long nr_requests;
                  unsigned int nr_congestion_on;
                  unsigned int nr_congestion_off;
                  unsigned int nr_batching;

                  void *dma_drain_buffer;
                  unsigned int dma_drain_size;
                  unsigned int dma_pad_mask;
                  unsigned int dma_alignment;

                  struct blk_queue_tag *queue_tags;
                  struct list_head tag_busy_list;

                  unsigned int nr_sorted;
                  unsigned int in_flight[2];

                  unsigned int rq_timeout;
                  struct timer_list timeout;
                  struct list_head timeout_list;
                
                  struct queue_limits limits;

                  unsigned int sg_timeout;
                  unsigned sg_reserved_size;
                  int node;
          #ifdef CONFIG_BLK_DEV_IO_TRACE
                  struct blk_trace *blk_trace;
          #endif

                  unsigned int ordered, next_ordered, ordseq;
                  int orderr, ordcolor;
                  struct request pre_flush_rq, bar_rq, post_flush_rq;
                  struct request *orig_bar_rq;
                  struct mutext sysfs_lock;
          #if defined(CONFIG_BLK_DEV_BSG)
                  struct bsg_class_device bsg_dev;
          #endif
          };

          /* request queue flags */

          /* cluster several segments into 1 */
          #define QUEUE_FLAG_CLUSTER 0

          /* uses generic tag queueing */
          #define QUEUE_FLAG_QUEUED 1

          /* queue is stopped */
          #define QUEUE_FLAG_STOPPED 2

          /* read queue has been filled */
          #define QUEUE_FLAG_SYNCFULL 3

          /* write queue has been filled */
          #define QUEUE_FLAG_ASYNCFULL 4

          /* queue beging torn down */
          #define QUEUE_FLAG_DEAD 5

          /* re-entrancy avoidance */
          #define QUEUE_FLAG_REENTER 6

          /* queue is plugged */
          #define QUEUE_FLAG_PLUGGED 7

          /* do not use elevator,just do FIFO */
          #define QUEUE_FLAG_ELVSWITCH 8

          /* queue supports bidi requests */
          #define QUEUE_FLAG_BIDI 9

          /* disable merge attempts */
          #define QUEUE_FLAG_NOMERGES 10
    
          /* force complete on same CPU */
          #define QUEUE_FLAG_SAME_COMP 11

          /* fake timeout */
          #define QUEUE_FLAG_FAIL_IO 12

          /* supports request stacking */
          #define QUEUE_FLAG_STACKABLE 13

          /* non-rotational device (SSD) */
          #define QUEUE_FLAG_NONROT 14

          /* paravirt device */
          #define QUEUE_FLAG_VIRT QUEUE_FLAG_NONROT

          /* do IO stats */
          #define QUEUE_FLAG_IO_STAT 15

          /* supports DISCARD */
          #define QUEUE_FLAG_DISCARD 16

          /* no extended merges */
          #define QUEUE_FLAG_NOXMERGES 17

        +-------------------------------------------+
        | ... | rqN | @queue_head | rq0 | rq1 | ... | => request queue
        +-------------------------------------------+
                                    |
                                    +--> struct request.queuelist

      Request Descriptors :
        each pending request for a block devices is represented by a request descriptor.

        <linux/blkdev.h>
          /**
           * request - structure represent an I/O request
           * @queuelist:
           *           linked into request_queue.@queue_head
           * @csd:     IPI data for SMP
           * @q:       the request queue contains this request
           * @cmd_flags:
           *           flags of cmd of this request
           * @cmd_type:
           *           request command types
           * @atomic_flags:
           *           internal atomic flags for request handling
           *           # in header <block/blk.h>,only one enumerate
           *             value is defined REQ_ATOM_COMPLETE = 0
           * @cpu:     cpu number who handles this request
           * @__data_len:
           *           internal field,_never_ access directly
           *           total data len
           * @__sector:
           *           internal field,_never_ access directly
           *           sector cursor
           * @bio:     first bio in the request that has not been
           *           completely transferred
           * @biotail: last bio in the request list
           * @hash:    merge hash
           * @rb_node: this member is only used inside the I/O scheduler,
           *           requests are pruned when moved to the dispatch queue,
           *           so let the @completion_data share space with the @rb_node
           * @completion_data:
           *           completion data
           * @elevator_private:
           *           specific to I/O schedulers
           * @elevator_private2:
           *           specific to I/O schedulers
           * @rq_disk: the target disk this request operate on
           * @start_time:
           *           request's starting time in jiffies
           * @nr_phys_segments:
           *           number of scatter-gather DMA "addr + len" pairs
           *           after physical address coalescing is performed
           *           that is the number of physical segments of this
           *           request
           * @ioprio:  I/O priority
           * @ref_count:
           *           request reference counter
           * @special: opaque pointer available for LLD use,used when the
           *           request includes a "special" command to the hardware device
           * @buffer:  kaddr of the current segment if available
           * @tag:     request tag
           * @errors:  request error status
           * @__cmd:   packet command carrier,maximum length is BLK_MAX_CDB = 16
           *           bytes
           * @cmd:     pointer to @__cmd
           * @cmd_len: length of @__cmd
           * @extra_len:
           *           length of alignment and padding
           * @sense_len:
           *           length of buffer pointed by @sense
           * @resid_len:
           *           residual count
           * @sense:   pointer to buffer used for ouput of sense commands
           * @deadline:
           *           deadline of this request,it is usually current
           *           jiffies + @timeout
           * @timeout_list:
           *           request timeout timer
           *           routine blk_add_timer() is used to start a timeout timer of
           *           single request,thus list_add_tail() is called by which to
           *           add this request's @timeout_list into the tail of the
           *           request queue's @timeout_list of this request
           * @timeout: time out value in jiffies
           * @retries: used by SCSI Generic disk ioctl
           * @end_io:  request completion callback
           * @end_io_data:
           *           callback routine data
           * @next_rq: the next request
           * # each request consists of one or more bio structures,initial is
           *   one created by Generic Block Layer,and I/O scheduler may 
           *   extend request later by adding a new segment to the original bio,
           *   or by linking another bio structure into the request(new data is
           *   physically adjacent to the data already in the request)
           * # during the request is handling,several members may dynamically
           *   change,so this structure can be used to record current status
           */
          struct request {
                  struct list_head queuelist;
                  struct call_single_data csd;

                  struct request_queue *q;

                  unsigned int cmd_flags;
                  enum rq_cmd_type_bits cmd_type;
                  unsigned long atomic_flags;

                  int cpu;
                  unsigned int __data_len;
                  sector_t __sector;
                  struct bio *bio;
                  struct bio *biotail;
                  struct hlist_node hash;
                  
                  union {
                          struct rb_node rb_node;
                          void *completion_data;
                  };

                  void *elevator_private;
                  void *elevator_private2;

                  struct gendisk *rq_disk;
                  unsigned long start_time;
                  unsigned short nr_phys_segments;
                  unsigned short ioprio;
                  int ref_count;

                  void *special;
                  char *buffer;
                  int tag;
                  int errors;

                  unsigned char __cmd[BLK_MAX_CDB];
                  unsigned char *cmd;
                  unsigned short cmd_len;
                  unsigned int extra_len;
                  unsigned int sense_len;
                  unsigned int resid_len;
                  void *sense;

                  unsigned long deadline;
                  struct list_head timeout_list;
                  unsigned int timeout;
                  int retries;
                
                  rq_end_io_fn *end_io;
                  void *end_io_data;

                  struct request *next_rq;
          };

          /* request cmd flags */
          REQ_RW                        not set,read;set,write
          REQ_FAILFAST_DEV              no driver retries of device errors
          REQ_FAILFAST_TRANSPORT        no driver retries of transport errors
          REQ_FAILFAST_DRIVER           no driver retries of driver errors
          /* first fours match BIO_RW*,important */

          REQ_DISCARD                   request to discard sectors
          REQ_SORTED                    elevator knows about this request
          REQ_SOFTBARRIER               may not be passed by io scheduler
                                        request acts a barrier for the I/O scheduler
          REQ_HARDBARRIER               may not be passed by driver either
                                        request acts a barrier for the I/O scheduler
                                        and the device dirver
          REQ_FUA                       forced unit access
          REQ_NOMERGE                   do not touch this for merging
          REQ_STARTED                   drive already may have started this one
          REQ_DONTPREP                  dont call prep_rq_fn() for this one
          REQ_QUEUED                    uses queueing
          REQ_ELVPRIV                   elevator private data attached
          REQ_FAILED                    set if the request failed
          REQ_QUIET                     do not worry about errors
          REQ_PREEMPT                   set for "ide_preempt" requests
          REQ_ORDERED_COLOR             is before or after barrier
          REQ_RW_SYNC                   request is sync w/r
          REQ_ALLOCED                   request came from alloc pool
          REQ_RW_META                   metadata io request
          REQ_COPY_USER                 contains copies of user pages
          REQ_INTEGRITY                 integrity metadata has been remapped
          REQ_NOIDLE                    dont anticipate more IO after this one
          REQ_IO_STAT                   account I/O stat
          REQ_MIXED_MERGE               merge of differen types,fail separately

          /**
           * the name prefixed "__" enumerates to these cmd flags are defined in the
           * same header,they represents the bits of the flags.
           * for a given cmd flag,it is come from "1UL << __REQ_XXX"
           */

          /* cmd type bits */
          enum rq_cmd_type_bits {
                  /* fs request */
                  REQ_TYPE_FS = 1,

                  /* scsi command */
                  REQ_TYPE_BLOCK_PC,

                  /* sense request */
                  REQ_TYPE_SENSE,

                  /* suspend request */
                  REQ_TYPE_PM_SUSPEND,

                  /* resume request */
                  REQ_TYPE_PM_RESUME

                  /* shutdown request */
                  REQ_TYPE_PM_SHUTDOWN,

                  /* driver defined type */
                  REQ_TYPE_SPECIAL

                  /* generic block layer message */
                  REQ_TYPE_LINUX_BLOCK,

                  /* 
                   * for ATA/ATAPI devices
                   * this really doesn't belong here,ide should use REQ_TYPE_SPECIAL
                   * and use rq->cmd[0] with the range of driver private REQ_LB
                   * opcodes to differentiate what type of request this is
                   */
                  REQ_TYPE_ATA_TASKFILE,
                  REQ_TYPE_ATA_PC,
          };

          /* should not use this directly,use rq_for_each_segment() instead */
          #define __rq_for_each_bio(_bio, rq)  \
                  if ((rq->bio))               \
                          for (_bio = (rq)->bio; _bio; _bio = _bio->bi_next)

          #define rq_for_each_segment(bvl, _rq, _iter)  \
                  __rq_for_each_bio(_iter.bio, _rq)     \
                          bio_for_each_segment(bvl, _iter.bio, _iter.i)

        Managing the allocation of request descriptors :
          a bottleneck for processes that want to add a new request into a request queue may
          come when very heave loads and high disk activity,only limited amount of free dynamic
          memory can available.
          structure request_queue is introduced for cope this kind of situation.

          <block/blk-core.c>
            /**
             * blk_get_request - routine to allocate a new rq on a specified request queue
             * @q:               request queue
             * @rw:              READ or WRITE
             * @gfp_mask:        GFP mask
             * return:           NULL or new rq descriptor pointer
             * # EXPORT_SYMBOL
             * # if __GFP_WAIT is enabled in @gfp_mask,then this routine call to
             *   get_request_wait();otherwise call to get_request()
             */
            struct request *blk_get_request(struct request_queue *q, int rw, gfp_t gfp_mask);

            /**
             * blk_put_request - put a request got through blk_get_request()
             * @req:             the request
             * # EXPORT_SYMBOL
             * # this routine actually call to __blk_put_request() with request queue lock
             *   holding,and release the lock before return
             */
            void blk_put_request(struct request *req);

            /**
             * blk_alloc_request - alloc a new request from memory pool of a request queue
             * @q:                 request queue
             * @flags:             request flags
             * @priv:              indicator for elevator private data attaching
             * @gfp_mask:          gfp mask for mempool_alloc()
             * return:             NULL or new request pointer
             * # this routine allocate rq from @q->rq.rq_pool through mempool_alloc()
             *   function blk_rq_init() is called for initialize @rq
             *   the @rq->cmd_flags will be @flags | REQ_ALLOCED
             *   if @priv is TRUE,then @rq->cmd_flags |= REQ_ELVPRIV
             *   retur NULL and mempool_free() @rq if elv_set_request() returned TRUE
             *   AND @priv is TRUE
             * # elv_set_request :
             *     get elevator of request queue
             *     if elevator_set_req_fn() is defined,then call to it and return the
             *     result to caller
             *     else set request's @elevator_private to NULL and return 0
             */
            static struct request *
            blk_alloc_request(struct request_queue *q, int flags, int priv, gfp_t gfp_mask);

            # blk_alloc_request() is called by get_request() to get a new request.
              the function get_request() do some checks for elevator and request queue
              request limit.
              if elevator can not queue this rq,then set starved[r/w] to 1 and return NULL.
              if it will exceed limits of the request queue by adding a new request,then 
              return NULL,can not get a new request,set request queue full status if necessary.
              /**
               * request queue has a maximum number of allowed pending requests for each data
               * transfer direction.
               * if @count[r/w] >= @nr_congestion_on,allows creating,but set queue congestion statue.
               * if @count[r/w] >= @nr_requests,forbid creating
               * if elevator can not queue the coming requests,set @starved[r/w] to 1
               */

              if passed checking,call to blk_alloc_request(),and return the result to caller.

              @count[r/w] increase 1 if succeed to allocate a new one.
              @elvpriv increase 1 if succeed to allocate a new one AND QUEUE_FLAG_ELVSWITCH is not set.
              @starved will be set to _zero_ if succeed to allocate a new one.

            # get_request_wait() will call to io_schedule() is get_request() returned NULL,
              and try again get_request() until succeed to get a new one.
              the process may sleep with TASK_UNINTERRUPTIBLE,and it will be puting into
              @wait[r/w] of the request_list object of the request queue.

            !! get_request() RETURNED WITH QUEUE UNLOCK IF IT SUCCEED TO GET A FREE REQUEST,
               OTHERWISE,RETURNS NULL WITH QUEUE LOCKED.
               get_request_wait() RETURNED WITH QUEUE UNLOCK BECAUSE IT WAIT UNTIL THE ROUTINE
               get_request() SUCCEED TO GET A FREE REQUEST.

            /* blk_free_request - elv_put_request() if REQ_ELVPRIV,mempool_free() @rq */
            static inline void blk_free_request(struct request_queue *q, struct request *rq);

    Activating the Block Device Driver :
      delay activation of the block device driver for to wait clustering requests can improve
      performance.the technique to achieve this is known as "device plugging" and "unplugging".
      even if the request queue of the block device driver has some pending requests,the
      block device driver still not activated as long as a block device driver is plugged.

      function blk_plug_device() plugs a block device - a request queue serviced by some block
      device driver.(set QUEUE_FLAG_PLUGGED,and start @unplug_timer)
      function blk_remove_plug() unplugs a request queue.(clear QUEUE_FLAG_PLUGGED,and cancel timer)
      this routine can be explicitly called by kernel when all mergeable requests "in sight" have
      been added to the queue.

      I/O scheduler will unplug the queue if number of requests of the queue exceeded @unplug_thres.

      if the timer @unplug_timer timeout(the time interval length is @unplug_delay),then routine
      blk_unplug_timeout() is executed,as a consequence,"kblockd" kernel thread is awakened,the
      thread servicing the @kblockd_workqueue,and blk_unplug_timeout() call to kblockd_schedule_work()
      to queue the work @unplug_work to @kblockd_workqueue.the function of the work is initialized
      to blk_unplug_work(),this function call to @unplug_fn() of the request queue.

      pointer @unplug_fn usually is set to generic_unplug_device(),which checks if request queue is plugged
      at first.if it is,then acquires @queue_lock,and call to __generic_unplug_device(),release the lock
      after __generic_unplug_device() returned.

      function __generic_unplug_device() just call to @request_fn() of the request queue if
        the request is not stopped
        AND
        (succeed to remove plug
        OR
        QUEUE_FLAG_NONROT is set)
      otherwise,return to caller.

      @request_fn start processing the next request in the queue.
      implementation of @request_fn is hand over to block device driver,function blk_init_queue() is used
      to initialize a request queue with a specified @rfn(request_fn_proc) and a spinlock.for example,
      the driver of block device hd,@rfn is do_hd_request(),and the lock is @hd_lock.

      <block/blk-core.c>
        /* kblockd_workqueue - workqueue of kernel thread "kblockd" */
        static struct workqueue_struct *kblockd_workqueue;

        /**
         * blk_plug_device - plug a request queue,and start @unplug_timer with timer interval
         *                   @jiffies + @q->unplug_delay
         * @q:               request queue
         * # EXPORT_SYMBOL
         * # this is called with interrupts off and no requests on the queue and with the queue lock held
         */
        void blk_plug_device(struct request_queue *q);

        /**
         * blk_remove_plug - unplug a request queue and cancel the timer
         * @q:               request queue
         * return:           1 => succeed to unplug and deleted timer
         *                   0 => failed to unplug and timer still activing
         */
        int blk_remove_plug(struct request_queue *q);

        /**
         * blk_unplug_timeout - timeout routine for block request queue
         * @data:               address of the request queue
         * # EXPORT_SYMBOL
         * # this routine just call to kblockd_schedule_work()
         */
        void blk_unplug_timeout(unsigned long data);

        /**
         * kblockd_schedule_work - queue_work() @work into @kblockd_workqueue
         * @q:                     request queue
         * @work:                  @q->unplug_work
         * # EXPORT_SYMBOL
         */
        int kblockd_schedule_work(struct request_queue *q, struct work_struct *work);

        /**
         * blk_unplug_work - call to container_of(@work, struct request_queue, unplug_work)->unplug_fn()
         * @work:            @unplug_work
         */
        void blk_unplug_work(struct work_struct *work);

        /**
         * generic_unplug_device - unplug the request queue @q if it is plugged now
         * @q:                     request queue
         * # EXPORT_SYMBOL
         */
        void generic_unplug_device(struct request_queue *q);

        /**
         * __generic_unplug_device - unplug the request queue and call to @q->request_fn()
         * @q:                       request queue
         * # this routine will return withou @request_fn invoking :
         *     1> @q is stopped
         *     2> failed to remove plug AND no QUEUE_FLAG_NONROT is set
         */
        void __generic_unplug_device(struct request_queue *q);

    I/O Scheduling Algorithms :
      the purpose of I/O scheduling algorithms is to reduce disk seeking time and improve system
      performance,besides,prevent I/O strave.
      indeed,the completion time of a data transfer strongly depends on the physical position of
      the data on the disk.

      Linux 2.6 offers four different types of I/O schedulers - or elevators - called
        Anticipatory
        Deadline
        CFQ(Complete Fairness Queueing)
        Noop(No Operation)

      kernel parameter "elevator=<name>" can be used to specify the default I/O scheduling algorithm
      during kernel boot time.
        <name> => as / deadline / cfq / noop
      if no specified elevator,then kernel will use "Anticipatory" I/O scheduler.

      Device Driver can replace the using I/O scheduler,and of course,it can provides a custom
      I/O scheduling algorithm,but this is very seldom done.

      kernel exported the selected scheduler in sysfs file "/sys/block/sdX"/queue/scheduler",by write
      the name of I/O scheduler into the file can let kernel start to make use the new I/O scheduler.

      each request queue has elevator queue associated with it,the elevator object includes several
      methods covering all possible operations of the elevator,such linking and unlinking the elevator
      to a request queue,adding and merging requests to the queue,removing requests from the queue,
      getting the next request to be processed from the queue,and so on.
      member named @elevator_private of struct request points to an additional data used by the
      I/O scheduler to handle the request.

      <linux/elevator.h>
        /**
         * elevator_queue - the elevator object,used to represent an I/O scheduler
         * @ops:            elevator operations
         *                  this pointer will points to @elevator_type->ops by default
         * @elevator_data:  information required to handle the associated request queue
         * @kobj:           kobject
         * @elevator_type:  type of this elevator
         * @sysfs_lock:     mutex used to protect the changing of terms in sysfs
         * @hash:           elevator hash list array,the maximum number of elements
         *                  is ELV_HASH_ENTRIES - 1 << 6(@elv_hash_shift)
         *                  the hash list will stores the requests from the associated
         *                  request queue,and the hash expression produce array index is
         *                          hash_long((blk_rq_pos(rq) + blk_rq_sectors(rq)) >> 3, @elv_hash_shift)
         *                          |          |                                 |
         *                          |          +------>  rq_hash_key(rq)  <------+
         *                          +--> ELV_HASH_FN(sector number)
         */
        struct elevator_queue {
                struct elevator_ops *ops;
                void *elevator_data;
                struct kobject kobj;
                struct elevator_type *elevator_type;
                struct mutex sysfs_lock;
                struct hlist_head *hash;
        };

        /**
         * elevator_type - identifies an elevator type
         * @list:          linker used for add an elevator type to the list
         *                 @elv_list
         * @ops:           operations supported by this kind of elevator
         * @elevator_attrs:
         *                 elevator attributes
         *                 this structure contains one data member named @attr
         *                 is type of struct attribute;and two member methods
         *                 named @show and @store,they are used to export
         *                 elevator attributes to sysfs
         * @elevator_name: name of this elevator
         * @elevator_owner:
         *                 unload protection
         */
        struct elevator_type {
                struct list_head list;
                struct elevator_ops ops;
                struct elv_fs_entry *elevator_attrs;
                char elevator_name[ELV_NAME_MAX];
                struct module *elevator_owner;
        };

      I/O scheduler descriptions :
        designing an I/O scheduler is much like designing a CPU scheduler :
          the heuristics and the values of the adopted constants are the result of
          an extensive amount of testing and benchmarking.

        all algorithms make use of a "dispatch queue",which includes all requests
        sorted according to the order in which the requests should be processed by
        the device driver - the next request to be serviced by the device driver
        is always the first element in the dispatch queue.
        the dispatch queue,exactly,the list_head object named @queue_head of the
        request queue.

        ! almost all algorithms also make use of additional queues to classify and sort
          requests,and allow device driver to add bios to existing requests,merge two
          "adjacent" requests if necessary.

        Noop elevator :
          no ordered queue -
            new requests are always added either at the front or at the tail of the dispatch
            queue,and the next request to be processed is always the first request in the
            queue
            ! usually,adopted by SSD
        
        CFQ elevator :
          ensuring a fair allocation of the disk I/O bandwidth among all the processes that
          trigger the I/O requests.
          algorithm make use of a large number of sorted queues(default is 64),store the
          requests coming from the different processes.
          whenever a process triggered an I/O request,kernel hash its tgid into array index
          of the array of sorted queues,and insert new request to the tail of queue.

          elevator refill dispatch queue by scans the I/O input queues in a round-robin
          fashion,selects the first nonempty queue,and moves a batch of requests from that
          queue into the tail of the dispatch queue.

        Deadline elevator :
          besides the dispatch queue,elevator also make use of four additional queues.
          the first two queues :
            1> sorted read requests,order according initial sector number
            2> sorted write requests,order according initial sector number

          the second two queues :
            1> same read requests,but sorted according their deadline
            2> same write requests,but sorted according their deadline

          deadline elevator is essentially an expire timer that starts ticking when the
          request is passed to the elevator.default read request expire time is 500ms,and
          the default write request expire time is 5s.if the I/O request been waiting a long
          time,then dispatch it even if it is low in the sort.

          dispatch queue refill :
            1> determines the data direction of the next request,if both read and write requests
               are existing,then select read unless the write direction has been discarded too
               many times.
            2> checks deadline queue of selected data direction.
               if the deadline of the first request is elapsed,elevator moves that request to
               the tail of the dispatch queue;also moves a batch of requests taken from the
               sorted queue,starting from the request following the expired one.length is
               depends on adjacent sectors.
            3> no request is expired,elevator dispatches a batch of requests starting with
               the request following the last one taken from the sorted queue.
               when cursor reached the tail,the search starts again from the top.

            [ req1, req2, req3, req4, ... ] => read direction
            [ req1, req2, req3, req4, ... ] => write direction
            [ req3, req1, req4, req2, ... ] => read deadline
            [ req1, req4, req2, req3, ... ] => write deadline

            [ dispatch queue ]

            select read
            req3 in read deadline expired
            move req3 to the tail of dispatch queue
            read deadline => [ req1, req4, req2, ... ]
            req1 in read deadline expired
            move req1 to the tail of dispatch queue
            read deadline => [ req4, req2, ... ]
            no expired read request
            move a batch request from sorted read queue folloing req1(last expired) to dispatch queue
            dispatch queue => [ ... req3, req1, req2 ]

        Anticipatory elevator :
          the most sophisticated I/O scheduler algorithm offered by Linux.basically it is an evolution
          of Deadline elevator.
          four sorted request queues(r/w sec, r/w dl) and one dispatch queue.
          I/O scheduler keeps scanning the sorted queues,alternating between read and write requests,
          but prefer to read.
          scanning is sequential unless a request expires.default read expire time is 125ms,default
          write expire time is 250ms.
          
          additional heuristics :
            1> in some cases,the elevator might choose a request behind the current position in the
               sorted queue,thus forcing a backward seek of the disk head.
               this happens when the distance of seek back is less than half the seek front for the
               next request after the current position in the sorted queue.
            2> elevator collects statistics about the patterns of I/O operations triggered by every
               process in the system.
               _right_ _after_ dispatching a read request that comes from process P,it checks whether
               the next request is also come from P;if it is,then dispatch the request immediately,
               otherwise,look at collected statistics about P - if it decides that P will likely issue
               another read request soon,then elevator stalls for a short period of time(default,roughly 7ms).
               thus,the elevator might anticipate a read request coming from P that is "close" on disk
               to the request just dispatched.

    Issuing a Request to the I/O Scheduler :
      the function generic_make_request() is entry point of generic block layer,which call to @make_request_fn
      of the request queue with the onging bio object.
      @make_request_fn has the default value __make_request(),as the preceding description,which call to
      get_request_wait() to issue a new request in the request queue.

      <block/blk-core.c>
        /**
         * __make_request - makeup a request in the specified request queue with the
         *                  ongoing bio
         * @q:              request queue
         * @bio:            ongoing bio
         * return:          0
         * # status reported by bio_endio()
         */
        static int __make_request(struct request_queue *q, struct bio *bio);

        description for __make_request() :
          checks whether @bio flagged BIO_RW_BARRIER AND @q->next_ordered is equal to QUEUE_ORDERED_NONE,
          if it is,then call to bio_endio() with error code -EOPNOTSUPP,the wishing operation is not supported.
          /* QUEUE_ORDERED_NONE => hard barrier not supported */

          call to blk_queue_bounce(),it setup a bounce buffer if required.if a bounce buffer is created,
          blk_queue_bounce() will initialize the bounce buffer,and use it to instead *@bio,let the original
          bio @bio become @bi_private member of the bounce buffer.
          /* the bounce buffer is a new allocated struct bio object */

          lock request queue,checks whether BIO_RW_BARRIER is flagged OR elv_queue_empty(@q) returned
          TRUE,resulted TRUE,then goto "get_rq" label.

          attempts to merge elv of @q through elv_merge(),the routine try to merge @bio with the @q->last_merge.
          there are three possible situations will be reported :
            nomerges      =>        no merges at all attempted
            noxmerges     =>        only simple one-hit cache try
            merges        =>        all merge tries attempted
          and three possible values will be returned :
            ELEVATOR_NO_MERGE => nomerges || noxmerges
            ELEVATOR_BACK_MERGE => merge to back
            ELEVATOR_FRONT_MERGE => merge to front
          if @q->last_merge is OK,then ELEVATOR_BACK_MERGE or ELEVATOR_FRONT_MERGE will be returned.
          if @q->last_merge is not OK AND noxmerges is FALSE,then look at elevator hash table of requests
          try to find out an appropriate request which can be merged with @bio through function elv_rqhash_find(),
          which receives two parameters,one is request queue descriptor,another is the ongoing bio's @bi_sector.
          /**
           * elv_rqhash_find() :
           *   get hash list @q->elevator->hash[ELV_HASH_FN(@bio->bi_sector)
           *   traverse hash list -
           *     for each request in the hash list
           *       BUG_ON() current request is hlist_unhashed(&this_request->hash)
           *       it is not rq_mergeable(),then __elv_rqhash_del() it and continue to next
           *                                     # hlist_del_int(&this_rq->hash)
           *                 # request @rq is mergeable if
           *                     !(@rq->cmd_flags & RQ_NOMERGE_FLAGS) AND (blk_discard_rq(@rq) OR blk_fs_request(@rq))
           *                                                              # @cmd_flags & REQ_DISCARD
           *                                                                                     # cmd_type == REQ_TYPE_FS
           *       rq_hash_key() of current request is equal to @bio->bi_secotr,returns this request
           *   if no proper request is find,return NULL to caller.
           */
          if elv_rqhash_find() returned a request descriptor,then try to merge it with @bio,return ELEVATOR_BACK_MERGE
          when succeed on merging.
          last try is routine @elevator_merge_fn,if it is not defined,then elv_merge() returns ELEVATOR_NO_MERGE to caller,
          otherwise,returns the value returned by @elevator_merge_fn().

          __make_request() next checks the returned value from elv_merge() :
            back merge - (req: elv_rqhash_find() return)
              @req->biotail->bi_next = @bio
              @req->biotail = @bio
              @req->__data_len += @bio->bi_size
              @req->ioprio = ioprio_best(@req->ioprio, bio_prio(@bio))

              call to attempt_back_merge() to merge @req with latter request of the elevator
              call to elv_merged_request() to update @q->last_merge to @req if attempt_back_merge() succeed

              goto out label.

            front merge - (req: elv_rqhash_find() return) 
              @bio->bi_next = @req->bio
              @req->bio = bio
              @req->buffer = bio_data(@bio)
              @req->__sector = @bio->bi_sector
              @req->__data_len += @bio->bi_size
              @req->ioprio = ioprio_best(@req->ioprio, bio_prio(@bio))

              set @req->cpu to @bio->bi_comp_cpu if @req's cpu is valid

              call to attempt_back_merge() to merge @req with latter request of the elevator
              call to elv_merged_request() to update @q->last_merge to @req if attempt_back_merge() succeed

              goto out label

          proceed executing,try to get a free request,because we failed on merging request.

          get_rq:
          get data direction of @bio
          if BIO_RW_SYNCIO is specified,then set REQ_RW_SYNC
          invoke get_request_wait() to get a free request
          initialize the request from @bio
          lock request queue /* because get_request_wait() unlocked queue */
          if necessary,set @cpu member of the request
          if queue_should_plug(@q) AND elv_queue_empty(@q) is TRUE
          then call to blk_plug_device(@q) to plug the request queue
          invoke add_request() to add the new request to @q

          out: (normal return path)
          if BIO_RW_UNPLUG is enabled OR !queue_should_plug(@q)
          then call to __generic_unplug_device(@q) to unplug the request queue
          unlock request queue
          return 0 to caller


      Buffer Bouncing :
        buffer bouncing is necessary when some of the buffers in the request are located in
        high memory and the hardware device is not able to address them.
        older DMA for ISA buses only handled 24-bit physical addresses,in this case,the buffer
        bouncing threshold is set to 16MB,to page frame number 4096.
        block device driver might directly allocate DMA memory via ZONE_DMA rather than
        setup buffer bouncing.

      <mm/bounce.c>
        /**
         * blk_queue_bounce - buffer bouncing for request whose buffer is in high-memory
         * @q:                request queue
         * @bio_orig:         the onging bio
         * # EXPORT_SYMBOL
         */
        void blk_queue_bounce(struct request_queue *q, struct bio **bio_orig);

        brief description for blk_queue_bounce() :
          if @bio_orig have not data that @bi_io_vec is NULL,then return without buffer bouncing.

          if no GFP_DMA is set in @q->bounce_gfp,then checks whether @q->limits.bounce_pfn is
          greater than or equal to global variable @blk_max_pfn,if it is,then return to caller
          without bouncing.
          /* @blk_max_pfn is equal to @max_pfn - 1,if @bounce_pfn >= @blk_max_pfn,that means
           * the device can address whole system memory,so do not need to bouncing buffer
           */
          if @bounce_pfn < @blk_max_pfn,then use the memory pool @page_bool for buffer bouncing.

          GFP_DMA is set,that means we in the case that ISA buffer boucning,then use the memory
          pool @isa_page_pool.

          finally,call to __blk_queue_bounce() for buffer bouncing on the selected memory pool.

          ! @page_pool and @isa_page_pool is setup by routines init_emergency_pool() and
            init_emergency_isa_pool(),respectively.
            @min_nr of @page_pool is 64 and of @isa_page_pool is 16.
            for ISA buses,the pages is allocated with gfp mask GFP_KERNEL | GFP_DMA.
            for non ISA buses,the pages is allocated with gfp mask GFP_KERNEL.
            each page structure object in the mempool(element in @elements) is holding one page.

          ! rounte
            static __blk_queue_bounce(struct request_queue *q, struct bio **bio_orig, mempool_t *pool) :
              it just bouncing the buffer whose page number is greater than @q->limits.bounce_pfn,
              thus,it must iterates each segments in the bio.
              if we detected some pages should to be bouncing,then allocate a new bio object with
              gfp flag GFP_NOIO through bio_alloc(),make use the vec counter of @bio_orig,and zeros
              the bio vectors.
              bouncing :
                select bio vector from new bio,index is @i,that the index of the vector in @orig_bio need
                to be bounced.
                allocate page through mempool_alloc() on previously selected memory pool with gfp mask
                @q->bounce_gfp.
                copy @bv_len and @bv_offset,set NR_BOUNCE for the bounce page.
                if current bio data direction is WRITE,then copy contents from the original page to the
                bounce page.
                /* for do memcpy(),it must kmap() the orignal page at first,and kunmap() it after
                 * copying is accomplished.
                 * the reason that do not need to kmap() the new pages,that is these pages are
                 * allocated with gfp mask GFP_KERNEL
                 */

              if at least one page was bounced,then traverse vectors in @bio_orig,start index is _zero_.
              for all un-bounced pages in orignal vector,let new bio's vectors map them.that is,for
              each NULL @bv_page in new bio,set @bv_page to the corresponding original vector's @bv_page,
              and @bi_len,@bi_offset.

              next,use orignal bio's properties to setup new bio,besides,enable BIO_BOUNCED flag.
              if selected pool is @page_pool,then new bio's @bi_end_io is set to
                bounce_end_io_write() / bounce_end_io_read()
              else if selected pool is @isa_page_pool,then new bio's @bi_end_io is set to
                bounce_end_io_write_isa() / bounce_end_io_read_isa()
              ! these read functions will copy data from bounce page to orignal page when I/O
                operation terminated.
                all end io methods,will release the bounce bio when I/O operation terminated.

              finally,set new bio's @bi_private to *@bio_orig,and use new bio to instead
              *@bio_orig.

    Block Device Drivers :
      block device driver,the lower component of the Linux Block Subsystem.the role it plays is
      to service I/O request get from I/O scheduler.

      Block Devices :
        a block device driver may handles several block device.

        block_device structure - represent a block device
          <linux/fs.h>
            /**
             * block_device - a block device in the system
             * @bd_dev:       device number
             * @bd_inode:     inode of the file associated with the block device in bdev fs
             * @bd_super:     super block of this device
             * @bd_openers:   counter of how many times the block device has been opened
             * @bd_mutex:     mutex used when open/close device
             * @bd_inodes:    list of inodes of opened block device files for this device
             * @bd_holder:    current holder of block device descriptor
             *                holder is not  block device driver,it is a kernel component
             *                that make use of the device and has exclusive,special privileges,
             *                generally,it is the fs mounted over it,or a file object which
             *                opened for exclusive access
             * @bd_holders:   counter of multiple settings of the @bd_holder
             *                # kernel routine bd_claim() set @bd_holder,and increase this
             *                  member.the same kernel component can invoke bd_claim() many times,
             *                  each invocation will increase this member.kernel routine
             *                  bd_release() clear @bd_holder to NULL,and decrease this member.
             *                  ! times of release must corresponds to times of claim
             * @bd_holder_list:
             *                list of holders of this device
             * @bd_contains:  if this device is a partition,then it points to the block device
             *                descriptor of the whole disk;otherwise,it points to this block
             *                device descriptor(this device is whole disk)
             * @bd_block_size:
             *                size in bytes of a block
             * @bd_part:      if this device is a partition,then this member points to the
             *                disk partition descriptor,otherwise,it is NULL
             * @bd_part_count:
             *                number of times partitions within this device have been opened
             * @bd_invalidated:
             *                flag set when the partition table on this block device needs to
             *                be read
             * @bd_disk:      gendisk of the disk underlying this block device
             * @bd_list:      list(linked into @all_bdevs) of block device descriptors in this system
             * @bd_private:   block device holder private data
             * @bd_fsfreeze_count:
             *                counter of freeze processes
             *                used to sync fs state
             * @bd_fsfreeze_mutex:
             *                mutex for freeze
             */
            struct block_device {
                    dev_t bd_dev;
                    struct inode *bd_inode;
                    struct super_block *bd_super;
                    int bd_openers;
                    struct mutex bd_mutex;
                    struct list_head bd_inodes;
                    void *bd_holder;
                    int bd_holders;
            #ifdef CONFIG_SYSFS
                    struct list_head bd_holder_list;
            #endif
                    struct block_device *bd_contains;
                    unsigned bd_block_size;
                    struct hd_struct *bd_part;
                    unsigned bd_part_count;
                    int bd_invalidated;
                    struct gendisk *bd_disk;
                    struct list_head bd_list;
                    unsigned long bd_private;
                    int bd_fsfreeze_count;
                    struct mutex bd_fsfreeze_mutex;
            };

        bd_holder structure : 
          <fs/block_dev.c>
            /**
             * bd_holder - identifier for kernel component which is a holder of 
             *             a block device
             * @list:      list of holders of the bdev,linked into bdev->@bd_holder_list
             *             require bdev->@bd_mutex is hold
             * @count:     ref counter
             * @sdir:      slave device directory in sysfs
             * @hdev:      holder device in sysfs
             * @hdir:      holder device directory in sysfs
             * @sdev:      slave device in sysfs
             * # these @*dir and @*dev members are kobject descriptors,and they are used to
             *   create symlink in sysfs by add_bd_holder()
             *   sysfs_create_link() @sdir -> @sdev, @hdir -> @sdev, name(@sdir / @hdir)
             */
            struct bd_holder {
                    struct list_head list;
                    int count;
                    struct kobject *sdir;
                    struct kobject *hdev;
                    struct kobject *hdir;
                    struct kobject *sdev;
            };

            /**
             * bd_claim_by_kobject - claim a block device by kobject
             * @bdev:                block device
             * @holder:              holder added to the block device
             * @kobj:                kobject
             * return:               0 OR error code
             * # process steps :
             *     1> checks @kobj,NULL => -EINVAL
             *     2> alloc_bd_holder() through @kobj
             *     3> bd_claim() the @bdev with @holder
             *        if the block device alread a holder or its container alread a holder,
             *        then return -EBUSY
             *        otherwise,increase @bdev->bd_contains->bd_holders,@bdev->bd_holders
             *        use @holder to set @bdev->bd_contains->bd_holder and @bdev->bd_holder
             *     4> find_bd_holder() to find new created holder through @kobj
             *        if find out,that means there been a same holder,then destroye the newly
             *        created and return to caller
             *     5> add_bd_holder()
             *          call to bd_holder_grab_dirs()
             *            - @sdir = kobject_get() @sdir
             *            - @hdev = kobject_get() @sdir->parent
             *            - @sdev = kobject_get() kobject of @bdev->bd_part
             *            - @hdir = kobject_get() kobject of @bdev->bd_part->holder_dir
             *          # setup the newly created bd_holder object
             *          create sysfs symlink
             *            - @sdir -> @sdev
             *            - @hdir -> @hdev
             *          list add bd_holder.list to @bdev->bd_holder_list
             */
            static int bd_claim_by_kobject(struct block_device *bdev, void *holder,
                                           struct kobject *kobj);

            ! we have known that bdev->bd_holder is type of void *,but object of type struct bd_holder
              is linked into bd_holder_list of bdev.
              so it turns out current block device holder @bd_holder is type of void * corresponds to
              an object is type of struct bd_holder in bd_holder_list.
              the bd_holder object in bd_holdr_list of bdev can be deleted through routine
              del_bd_holder(),which unlink the bd_holder object,clean sysfs symlink,and returns the
              address of this object,routine free_bd_holder() is used to destroy it,which just call
              kfree() on it.
          
            /**
             * bd_claim - claim a block device
             * @bdev:     block device
             * @holder:   block device holder
             * return:    0 OR -EBUSY
             * # EXPORT_SYMBOL
             * # require spin lock @bdev_lock
             */
            int bd_claim(struct block_device *bdev, void *holder);

            /**
             * bd_release - release block device holder
             * @bdev:       block device
             * # EXPORT_SYMBOL
             * # require spin lock @bdev_lock
             * # bd_contains->bd_holder = bd_holder = NULL
             *   --bd_contains->bd_holders, --bd_holders
             */
            void bd_release(struct block_device *bdev);            

        relationship image :

                                     (queue_head)                 (elevator)                        
                                     [ requests ] <--+       +--> [ dispatch queue ]
            ...                                      |       |
           +-----------------------------------------+-------+--------+
           |                                         |       |        |
          +--------------+ (bd_disk)                 |       |        |
          | block_device |-------------+    (queue)  |       |        |
          | (part)       |             |    +----> [ request queue ]  |
          +--------------+             |    |                         |
           |                          +--> +---------+                |
           |                               | gendisk |                |(bd_part)
           |(bd_contains)             +--> +---------+                |
           |                          |     |                         |
           |    +--------------+      |     |(part_tbl)               |
           +--> | block_device |------+     |    (part_tbl->part[])   V
                | (disk)       | (bd_disk)  +--> [ part0 | part1 | part2 | ... ]
           +--> +--------------+                 (hd_struct)
           |      |
           +------+
           (bd_contains)

      Accessing a block device :
        when kernel need to open a block file,it must first determine whether the device file is
        already open.if it been opened,kernel should not create a new descriptor,rather to update
        the already existing block device descriptor.
        the block device files that have the same major number and the same minor number with
        different pathnames are not the same device file viewed by VFS.
        so we cannot determine whether the block device is in use by simply checking for the existence
        in the inode cache of an object for a block device file.
        bdev special filesystem :
          maintain the relationship between major number and minor number and block device descriptor.
          each block device descriptor is coupled with a bdev special file -
            bdev->bd_inode => the inode of the corresponding bdev special file
          the inode encodes major number and minor number of the block device,and the address of the
          block device descriptor.

        bdget() - routine used to lookup bdev the inode has the given major number and minor number.
                  if such inode is not exist,then create a new inode and a new block device descriptor,
                  and return the address of the newly created block device descriptor.
                  when a new inode is created,the corresponding bdev special file also be created.

        onece a block device descriptor is created,kernel can determines whether it is in use by
        checking member @bd_openers.
        all opened block device file,whose inode's @i_devices member is linked into the member
        named @bd_inodes of the corresponding block device descriptor.

        <fs/block_dev.c>
          /**
           * bdget - get a block device's descriptor associated with the given device number,
           *         create it if does not exist
           * @dev:   the device number
           * return: pointer to the block device descriptor OR NULL
           * # EXPORT_SYMBOL
           */
          struct block_device *bdget(dev_t dev);

          what bdget() does :
            1> call to iget5_locked() with the arguments
                 @blockdev_superblock, @hash(@dev), @bdev_test, @bdev_set, &@dev
                 /**
                  * routine iget5_blocked() is defined in <fs/inode.c>,
                  * this is  its prototype :
                  *   struct inode *iget5_locked(struct super_block *sb, unsigned long hashval,
                  *                              int (*test)(struct inode *, void *),
                  *                              int (*set)(struct inode *, void *),
                  *                              void *data);
                  * works this routine does is to
                  *   ifind() an inode corresponding to the device number @*data
                  *   if find out,then return this inode;otherwise,call to get_new_inode()
                  *   to create a new inode which is associated with the device number *@data,
                  *   return the new inode to caller.
                  * ifind() - searches for the inode specified by @data in the inode cache,
                  *           it is a generalized version of ifind_fast() for file systems
                  *           where the inode number is not sufficient for unique identification
                  *           of an inode.
                  *           if the inode is in the cache,the inode is returned with an
                  *           incremented reference counter;otherwise,return NULL.
                  * the head of hash list used by ifind() is get through -
                  *   @inode_hashtable + hash(@sb, @hashval)
                  * the case that corresponding inode is find -
                  *   inode->i_sb == @sb    # must in the specified super block
                  *   AND
                  *   test(inode, @data)    # must pass test
                  *   AND
                  *   !(inode->i_state & (I_FREEING | I_CLEAR | I_WILL_FREE))
                  *                         # cant be going to be freed
                  * get_new_inode() - called without the inode lock held,otherwise,dead lock.
                  *                   this routine first alloc_inode() to allocate a new inode
                  *                   in super block @sb,then checks whether there been an inode
                  *                   satisfied the request(through find_inode()).
                  *                   if an old one is existing,then make use of the old;
                  *                   otherwise,use @data to set the new one,and insert it
                  *                   into inode cache of @sb,return it to caller.
                  * if method alloc_inode() of super block is not defined,routine alloc_inode() will
                  * allocate inode from slab cache @inode_cachep.
                  */

            2> if succeed to get an inode,then get the pointer of block device descriptor of this
               inode through "&BDEV_I(inode)->bdev".
               BDEV_I() is a wrapper of container_of(),which get the bdev_inode structure which
               contains this inode.
               such structure is allocated during alloc_inode(),it is the bdev filesystem super
               block method for allocate a new iode.
               /**
                * # BDEV_I() can get this structure
                * struct bdev_inode {
                *         struct block_device bdev;     # block device associated with @vfs_inode
                *         struct inode vfs_inode;       # alloc_inode() get this object
                * };
                * allocating to this type of structure is make use of slab cache @bdev_cachep.
                */

            3> if the inode is enabled I_NEW in @i_state,then setup this inode and the corresponding
               block device descriptor.
               /**
                * if we exactly allocated a new inode and no old one is existing,
                * flag I_NEW must been set by get_new_inode()
                */
               setting :
                 bd_contains = NULL
                 bd_inode = inode
                 bd_block_size = (1 << inode->i_blkbits)
                 bd_part_count = 0
                 bd_invalidated = 0
                 i_mode = S_IFBLK
                 i_rdev = @dev
                 i_bdev = the block device descriptor
                 i_data.a_ops = &def_blk_aops
                 mapping_set_gfp_mask(&inode->i_data, GFP_USER)
                 inode->i_data.backing_dev_info = &default_backing_dev_info
               list add bdev->bd_list into &@all_bdevs /* this list is defined in <fs/block_dev.c>
                                                        * spin lock @bdev_lock used to protect it
                                                        */
               unlock_new_inode(inode)

    Device Driver Registration and Initialization :
      block device usually belongs to a standard bus architecture such as PCI or SCSI,when block
      device driver have to registers itself to the bus,it can make use of bus helper functions.
      different block device driver has customized driver descriptor,used to stores the data
      required to drive the hardware device,in addition,some data needed by block I/O subsystem.

      e.g.
        typedef struct bdev_foo {
                ...
                spinlock_t lock;        /* SMP protecting */
                struct gendisk *disk;   /* whole disk managed by this driver */
                ...
        } bdev_foo_t;

      Reserving the major number -
        routine "int register_blkdev(unsigned int major, const char *name)" is used to
        register a new block device,and of course,it also can reserving the major number for
        the block device that will managed by this driver.
        routine "void unregister_blkdev(unsigned int major, const char *name)" is used to
        unregister a block device.

        block device registering make use of a static array of pointer of structure blk_major_name
        named @major_names,the size is BLKDEV_MAJOR_HASH_SIZE - 255.
        if @major is _zero_,then routine will attempts to find out a useable index in the array,
        and use the index as the major number.
        next,kmalloc() a struct blk_major_name object,use the major number and @name to initialize
        it.
        use a for-cycle to traverse elements in the hash list in the @major_names at index
        major_to_index(major),if there been an object owns the major number in the hash list,then
        we can not make use of this major number,so return -EBUSY;otherwise add the new blk_major_name
        number into the tail of hash list.
        /**
         * struct blk_major_name {
         *         struct blk_major_name *next;     # next in hash list
         *         int major;                       # major number
         *         char name[16];                   # name of the block device
         * };
         */

        routine unregister_blkdev() does the reverse,it find the blk_major_name object from hash list,
        and kfree() it,link the remains.

        registered block device is visible in /proc/devices.

      Initializing the custom descriptor - 
        the most important thing to initialize the driver custom descriptor is initialize the data
        structure that will used by block I/O subsystem.
        after reserving the major number,we can initialize the lock,and call to alloc_disk() to
        allocate a logical disk object,and register device number range in the corresponding
        Kernel Mapping Domain.store the pointer of this new logical disk object in driver custom
        descriptor.

      Initializing the gendisk descriptor -
        routine alloc_disk() have been setup a part of members of gendisk structure.
        the remains we have to setup manually to corresponds to the block device driver.
        e.g.
          bdev_foo.disk->private_data = &bdev_foo; /* reserved for low-level driver method */

          bdev_foo.disk->major = BDEV_FOO_MAJOR;
          bdev_foo.disk->minors = BDEV_FOO_MINORS;
          bdev_foo.disk->first_minor = 0;
          bdev_foo.disk->fops = &bdev_foo_disk_fops;
          set_capacity(bdev_foo.disk, bdev_foo_disk_capacity_in_sectors);
          /**
           * <linux/genhd.h>
           *   static inline void set_capacity(struct gendisk *disk, sector_t size);
           *   # @disk->part0.nr_sects = @size;
           *   # 1 sector is 512 bytes
           */
          strcpy(bdev_foo.disk->disk_name, "bdev_foo");

      Initializing the table of block device methods -
        member @fops of structure gendisk is type of struct block_device_operations *.
        the block device driver must provides block device operations for the disk it drives.
        the operatios are specific to the device driver,different drivers have different implementations.

        @ioctl of block_device_operations is used when generic block layer does not know how to
        handle some ioctl command on the specified disk,thus,this method also is specific to device
        driver.

      Allocating and initializing a request queue -
        after setup block device operations table,we must allocate a request queue associated with 
        the disk.
    
        <block/blk-core.c>
          /**
           * blk_init_queue - allocate and initialize a request queue
           * @rfn:            request strategy routine
           * @lock:           request queue spin lock,usually is the driver's spin lock
           * return:          new request queue OR NULL
           * # EXPORT_SYMBOL
           * # actually,call to blk_init_queue_node(rfn, lock, -1)
           */
          struct request_queue *blk_init_queue(request_fn_proc *rfn, spinlock_t *lock);

          /**
           * blk_init_queue_node - allocate and initialize a request queue from a specified
           *                       NUMA node
           * @rfn:                 request strategy routine
           * @lock:                request queue spin lock
           * @node_id:             NUMA node
           * return:               new request queue pointer OR NULL
           * # EXPORT_SYMBOL
           */
          struct request_queue *
          blk_init_queue_node(request_fn_proc *rfn, spinlock_t *lock, int node_id);

          brief description for blk_init_queue_node() :
            first,allocate a new request queue through blk_alloc_queue_node() with gfp mask
            GFP_KERNEL on NUMA node @node_id,-1 means current NUMA node.

            function blk_alloc_queue_node() allocate the new request queue by invoke
            kmem_cache_alloc_node() to process allocating on slab cache @blk_requestq_cachep,
            with passed gfp mask | __GFP_ZERO on NUMA node @node_id.
              if succeed to allocating,it will :
                setup backing_dev_info of the request queue
                  unplug_io_nf = blk_backing_dev_unplug
                  unplug_io_data = the request queue
                  ra_pages = (VM_MAX_READAHEAD * 1024) / PAGE_CACHE_SIZE
                  capabilities = BDI_CAP_MAP_COPY
                  name = "block"
                  call to bdi_init() to initialize the remain members
  
                init_timer(this request queue's @unplug_timer)
                invoke setup_timer() to setup @timeout of this request queue,timer function
                is @blk_rq_timed_out_timer,timer data is the address of the request queue
  
                initilize @timeout_list, @unplug_work with @blk_unplug_work(), @kobj with
                kobject type @blk_queue_ktype, @sysfs_lock, @__queue_lock
  
              finally,return the request queue to caller,return NULL if detected any error.

            after blk_alloc_queue_node() returned a valid request queue descriptor,this routine
            proceeding does :
              set @node to @node_id
              call blk_init_free_list() to initialize @rq which is type of struct request_list
              set @request_fn to @rfn
              set @prep_rq_fn to NULL
              set @unplug_fn to generic_unplug_device
              set @queue_flags to QUEUE_FLAG_DEFAULT
              set @queue_lock to @lock(the lock passed by device driver)
              set @sg_reserved_size to INT_MAX /* for SCSI */

              call to blk_queue_make_request() for set @make_request_fn to @__make_request,and
              set the remain members to their default values
                include {
                        @nr_request => BLKDEV_MAX_RQ - 128
                        dma alignment 511
                        set congestion threshold by call to blk_queue_congestion_threshold
                        @nr_batching => BLK_BATCH_REQ - 32
                        @unplug_thresh => 4
                        @unplug_delay => 1s OR 3ms
                        @unplug_timer function => blk_unplug_timeout
                        @unplug_timer data => this request queue
                        default limits
                        maximum hardware sectors => BLK_SAFE_MAX_SECTORS - 255
                        {
                                @max_hw_sectors of @limits
                                @max_sectors of @limits
                        }
                        bounce limit => BLK_BOUNCE_HIGH - (u64)blk_max_low_pfn << PAGE_SHIFT - 32 bit
                }                                         -1ULL - 64 bit

              call to elevator_init() with @name == NULL,this will let the request queue make
              use of the system default elevator
              /* driver is able to change elevator later */
                if succeed to initialize elevator,then call to blk_queue_congestion_threshold()
                to setup @nr_congestion_on, @nr_congestion_off
                /* this routine been called one in blk_queue_make_request(),
                 * but in it only default value is setup,
                 * this time reset congestion threshold according to elevator status
                 */

            finally,return the request queue descriptor to driver,return NULL if any error detected.
        
      Setting up the interrupt handler -
        driver next should invoke request_irq() to register IRQ line and associated IRQ handler.
        because driver must wait the result of command that send to disk I/O controller,so it
        must listen on it specified IRQ line to wait for disk interrupt raise.

      Registering the disk -
        finally,driver should register the disk.for do that,call to add_disk() routine on its
        gendisk object. /**
                         * the parameter @minors of add_disk() is the maximum number of partitions
                         * of this disk
                         * size of new disk partition table is @minors + 1
                         */
        as we have described above,the routine will call to blk_register_region() to register
        the device number range in Kernel Mapping Domain @bdev_map.

    The Strategy Routine :
      strategy routine - a group of functions of the block device driver that interacts with
                         the hardware block device to satisfy the requests collected in the
                         dispatch queue.
      request queue member @request_fn is the entrance of strategy routine.
      general implementation of strategy routine -
        for each request in dispatch queue,remove it,and interact with the block device controller
        to service the request,wait until data transfer accomplish,then proceed with the next.

        ! but not very efficient,because the strategy routine must suspend itself,thus it
          must executes on a dedicated kernel thread.
          the modern disk controllers that can process multiple I/O data transfers at a time,
          strategy routine support multiple I/O data transfer can improve performance and
          without to wait I/O complete.

      most block device drivers adopt these strategy :
        > strategy routine starts a data transfer for the first request in the queue and
          sets up the block device controller so that it raises an interrupt when the data
          transfer completes.
          then the strategy routine terminates.
        > when disk controller raises the interrupt,the interrupt handler invokes the strategy
          routine again(immediately or deferred work).
          the strategy routine either starts another data transfer for the current request,
          or if all the chunks of data of the request have been transferred,removes the
          request from the dispatch data and starts processing the next request.

      two ways that block device driver make use of DMA :
        1> for each segment of each bio of the request,set up a different DMA transfer
        2> use a single scatter-gather DMA transfer to service all segments in all bios
           of the request

        ! a request can composed by several bio,and a bio can be composed by several segments.

      ! the design of strategy routine depends on block device controller.

      some helper functions :
        <block/blk.h>
          /**
           * __elv_next_request - get next request in elevator's dispatch queue
           * @q:                  the request queue
           * return:              next request descriptor OR NULL
           * # the works this routine does :
           *     1> enter a infinite while-cycle
           *     2> first,use a while-cycle,stop condition is @q->queue_head become empty
           *     3> get request entry @q->queue_head.next
           *     4> call to blk_do_ordered(),if it results TRUE,then return this request
           *     5> if @q->queue_head become empty,then invoke elevator routine
           *        @elevator_dispatch_fn() in elevator @ops.the implementation is
           *        different for different I/O Scheduler,the main work of this routine
           *        is select the best request,move it from request queue into
           *        dispatch queue
           *     6> if @elevator_dispatch_fn() returned _zero_,then return NULL to
           *        caller,that means an error is happened when dispatch request
           */
          static inline struct request *__elv_next_request(struct request_queue *q);

        <block/blk-core.c>
          /**
           * blk_peek_request - peek the request at the top of request queue
           * @q:                request queue
           * return:            request descriptor OR NULL
           * # EXPORT_SYMBOL
           * # this routine call __elv_next_request() in a while-cycle,the stop condition
           *   is __elv_next_request() returned NULL
           *   # so in general case,call to this routine instead call to __elv_next_request()
           *     directly in device driver code
           *
           *   if this req no REQ_STARTED enabled in @cmd_flags,then call to 
           *   @q->elevator->ops->elevator_activate_req_fn() to notify IO Scheduler if
           *   @cmd_flags of this req enabled REQ_SORTED;next enable REQ_STARTED flag
           *
           * # routine proceeding exuecte some checkings and settings,then break the
           *   while-cycle,return the request to caller
           *   if the request does not passed the examining,then start next iterating
           * # ! the returned request of this routine should be started using
           *     "void blk_start_request(struct request *)"(EXPORT_SYMBOL) before
           *     low-level-driver starts processing it
           *
           *     function blk_start_request() first dequeue this request from
           *     request queue,next set @resid_len of it,moreover,set @resid_len of
           *     @next_rq if necessary,finally,call to blk_add_timer() on this request
           */
          struct request *blk_peek_request(struct request_queue *q);

        <block/blk-barrier.c>
          /**
           * blk_do_ordered - do request ordering
           * @q:              request queue
           * @rqp:            selected request
           * return:          TRUE OR FALSE
           */
          bool blk_do_ordered(struct request_queue *q, struct request **rqp);

          what blk_do_ordered() does :
            1> initialize a local variable @is_barrier from 
                 blk_fs_request(*@rqp) && blk_barrier_rq(*@rqp)
                 /* cmd_type == REQ_TYPE_FS - fs request */
                                          /* cmd_flags & REQ_HARDBARRIER - acts like a barrier */
            2> if @q->ordseq is FALSE - request flushing is not started  - QUEUE_ORDERED_NONE
               then we must start ordering.
            3> checks @is_barrier is TRUE,if it is,then we can not sort this request,because
               fs request can not be ordered,and the position of barrier can not be changed.
            4> checks whether @q->next_ordered != QUEUE_ORDERED_NONE,if it is,then
               call to start_ordered() for start flushing.
            5> if @q->next_ordered == QUEUE_ORDERED_NONE - queue ordering not supported
               then terminate with prejudice.
                 blk_dequeue_request() on *@rqp
                 __blk_end_request_all() on *@rqp with error code -EOPNOTSUPP - end bio will be called
                 set *@rqp to NULL and return FALSE
            6> if @q->ordseq is TRUE - ordered sequence in progress
               checks if *@rqp is not fs request AND the request is not the pre-flush request AND
                         the request also is not post-flush request
                      then return TRUE to caller
                      otherwise,checks whether the request is ordered by draining
                                if it is,then set *@rqp to NULL
            7> finnaly return TRUE to caller

        <block/blk-merge.c>
          /**
           * blk_rq_map_sg - map a request to scatter-gather DMA list
           * @q:             request queue
           * @rq:            request
           * @sglist:        scatter-gather DMA list
           * return:         number of sg entries setup
           * # caller must make sure sg can hold @rq->nr_phys_segments entries
           * # EXPORT_SYMBOL
           * # routine use macro @rq_for_each_segment to traverses all segments in this
           *   request,for each of them,map them into @sglist,besides,if previous segment
           *   is able to be merged with current segment,then merge them as one segment
           * # if REQ_COPY_USER enabled AND @rq->__data_len  & @q->dma_pad_mask
           *   then this routine must pad sg list
           * # finally,setup DMA drain if necessary,mark sg list end,return number of entries setup
           * # to caller
           */
          int blk_rq_map_sg(struct request_queue *q, struct request *rq, struct scatterlist *sglist);

      general steps that strategy routine execute :
      /* example from do_cciss_request() <drivers/block/cciss.c> */

        1> invoke blk_peek_request() to peek the request at the top of request queue 
        2> call to blk_start_request() on the returned request before LLD start process it
        3> set driver specific settings
        4> call to sg_init_table() to init scatter-gather DMA list
        5> call to blk_rq_map_sg() to map segments
        6> after some prepare works,start DMA transfer

        ! if block device does not support scatter-gather DMA transfer,then block device
          driver must adopts method 1 described above to process data transfer - for each
          segment,start a DMA transaction for data transfer

    The Interrupt Handler :
      when block device DMA data transfer terminated,it will raises an interrupt to CPU,block device
      driver must register interrupt handler to response to the interrupt signal.
      if all chunks of data been transfered,then interrupt handler should invoke strategy routine
      for next request;otherwise,it should update fields of current request,invoke strategy routine
      to process the data transfer yet to be performed.

      example from <drivers/block/swim3.c> - Super Woz Integrated Machine 3 :
        routine start_request() - strategy routine
          this function first checks whether the floppy is in idle.
          if it is in idle now,call to blk_fetch_request() to peek a request from dispatch queue,
          and invoke blk_start_request() on it.
          after some checkings and settings,call to routine act(),which will commands to block
          device I/O controller to finish some works;in the case that have to transfer data,
          function setup_transfer() will be called for to setup DMA.

        routine swim3_interrupt - the interrupt handler
          the ISR acknowledge device interrupt signal,and do different work corresponding to
          the command that previous request have required.
          for data transfer command -
            if there remains some chunks of data of current request,then update fields of
            the request,and invoke act() again.the checking is executed by routine
            swim3_end_request() - FALSE: no more data have to be transferred
                                  TRUE:  there is some remained data have to be transfered
            if any error is detected,it will give up to current request,call to swim3_end_request_cur()
            to process end bio with error code -EIO,set floppy state to idle,and call to
            strategy routine start_request() for next request.
            if current request have been finished without error,then it set floppy to idle
            state,and call to strategy routine start_request() for next request.
            finally,return IRQ_HANDLED.

          swim3_end_request() - routine used to end request
            this routine just call to __blk_end_request(),if which is resulted in TRUE,return TRUE to
            caller.

          /* <block/blk-core.c> */

          bool __blk_end_request(rq, error, nr_bytes) - block I/O subsystem helper for drivers to complete the request
            this routine just one statement :
              return __blk_end_bidi_request(@rq, @error, @nr_bytes, 0)
                                            |    |       |          |
                                            |    |       |          +--> @bidi_bytes - number of bytes
                                            |    |       |               to complete @rq->next_rq
                                            |    |       +--> number of bytes to complete
                                            |    +--> 0 success, < 0 error 
                                            +--> the request being processed
            # EXPORT_SYMBOL

          static bool __blk_end_bidi_request(rq, error, nr_bytes, bidi_bytes)
            - complete a bidi request with queue lock held

            invoke blk_update_bidi_request() with received parameters to update request,return TRUE
            if the function returned TRUE
            otherwise,call to blk_finish_request(),and return FALSE - that error detected

          static bool blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes) - update bidi request
            call to blk_update_request() with function's parameters to update the request,
            if the routine returned TRUE,then return TRUE to calller
            if the bidi request must be completed as a whole,then call blk_update_request() again
            on @rq->next_rq if @next_rq is not NULL
            otherwise,return FALSE to caller

          bool blk_update_request(req, error, nr_bytes)
            - special helper function for request stacking drivers

            ends I/O on a number of bytes attached to the request,but does not complete
            the request structure even if the request does not have leftover
            if the request has leftover,sets it up for the next range of segments
            the routine returns TRUE if the request has more data
            the routine returns FALSE if the request does not have any more data

            this routine might call to req_bio_endio() on @req,if @nr_bytes is greater than
            current bio's @bi_size or the last bio have not completed.
            /**
             * if @nr_bytes spanned several BIOs,but the last BIO remained some data have not
             * completed,then req_bio_endio() will be called on that BIO which the number of
             * completed bytes of it to update its fields for the remained segments
             * of course,@nr_bytes will be updated during while-cycle iterating,if it is
             * greater than current BIO,then req_bio_endio() should be called on current BIO
             * to mark it that been completed
             */
            if more data have to be transfered,blk_recalc_rq_segments() is called for update
            this request @req,update some its fields for remained segments

            # EXPORT_SYMBOL            

    Opening a Block Device File :
      kernel opens a block device file every time that a filesystem is mounted over a disk or
      partition,every time that a swap partition is activated,and every time that a User Mode
      process issues an open() system call on a block device file.
      the common step is that -
        kernel first lookup such block device descriptor,create a new one if it is not exist.
        next,setup the file operation methods for the forthcoming data transfers.
        invoke file operation open() on the device file.

      we have known that @open method of @def_blk_fops is set to blkdev_open(),and in which,
      kernel will call to bd_acquire() routine to get the block device descriptor,use it
      to set filp object.
      the sys_open() system call on a block device file,finally,call to blkdev_open().

      <fs/block_dev.c>
        /**
         * blkdev_open - generic routine to open a block device file
         * @inode:       block device file inode
         * @filp:        block device file object
         * return:       0 OR error code
         * # the steps this routine does :
         *     1> enable O_LARGEFILE
         *     2> enable FMODE_NDELAY, FMODE_EXCL, FMODE_WRITE_IOCTL,depends on
         *        @f_flags of @filp
         *     3> get block device descriptor through bd_acquire()
         *     4> set @f_mapping of @filp to bd_inode->i_mapping of the block device
         *     5> get the block device through blkdev_get()
         *     6> if FMODE_EXCL is enabled,then call to bd_claim() to set bd_holder
         *     7> return to caller 0 OR any error code
         */
        static int blkdev_open(struct inode *inode, struct file *filp);

        /**
         * bd_acquire - acqurie a block device through block device file inode
         * @inode:      device file inode
         * return:      block device descriptor OR NULL
         */
        static struct block_device *bd_acquire(struct inode *inode);

        brief description for bd_acquire() :
          checks whether the block device of this inode been opened by some one.if 
          it is,@i_bdev must not be NULL,then we can simply increase the ref counter
          and return it to caller.
          if this is the first time to open the block device,then call bdget()
          with the device number @inode->i_rdev.
          if bdget() returned a valid descriptor,then increase its ref counter,
          set @i_bdev,@i_mapping to the block device descriptor and its @bd_inode->i_mapping
          member,respectively.
          list add @i_devices to @bd_inodes of the block device descriptor.
          return the descriptor to caller,return NULL if bdget() failed.

        /**
         * blkdev_get - attempt to get the block device with wanted file mode
         * @bdev:       block device descriptor
         * @mode:       file mode
         * return:      0 OR error code
         * # EXPORT_SYMBOL
         * # actually call to __blkdev_get() with @for_part == 0
         */
        int blkdev_get(struct block_device *bdev, fmode_t mode);

        /**
         * __blkdev_get - main routine to get a block device
         * @bdev:         block device descriptor
         * @mode:         file mode
         * @for_part:     whether getting for partition
         * return:        0 OR error code
         */
        static int __blkdev_get(struct block_device *bdev, fmode_t mode, int for_part);

        what __blkdev_get() does :
          1> initialize local variable @perm with given file mode @mode
             enable MAY_READ if FMODE_READ
             enable MAY_WRITE if FMODE_WRITE
          2> call to cgroup helper devcgroup_inode_permission() to process permission checking,
             bdput() the block device and return error to caller if failed on checking
          3> lock kernel through lock_kernel() /* unlock_kernel() does the reseve */
          4> invoke get_gendisk() on @bdev->bd_dev.
             routine get_gendisk() get partitioning information for a given device.
             if the device number is not equal to BLOCK_EXT_MAJOR
             then get the disk through kobj_lookup() on Kernel Mapping Domain @bdev_map.
             otherwise,get the part through idr_find() on the IDR @ext_dev_idr,then use the
             part to get the disk which contains it.
             goto label "out_unlock_kernel" if we failed to get the disk.
             /* get_gendisk() returns struct gendisk *(local variable @disk) */
          5> mutex_lock_nested() @bd_mutex with @for_part.
             checks @bd_openers of @bdev.
             if it is _zero_,that means we are the first kernel control path that opened the block device,
             thus we must setup it.
               set -
                 @bd_disk to @disk
                 @bd_contains to @bdev

             if @partno returned by get_gendisk() is _zero_,that means the @disk is not a
             partition,it is whole disk.
               then
                 set - 
                   @bd_part to disk_get_part(@disk, @partno) /* the routine returns struct hd_struct * */
                 goto label "out_clear" if disk_get_part() returned NULL
  
                 if @disk->fops->open() is defined,then call it.
                 if the method open() is interrupted,then do some cleaning works,and jump to step 4>
                 for restart again.
                 if the method open() returned another postive value,that means an error detected,
                 goto label "out_clear".
  
               if @bd_openers stay _zero_ after @open() method returned,then we call to
               bd_set_size() to set @i_size, @bd_block_size and @i_blkbits.
               next,set @bd_inode->i_data.backing_dev_info,the value either is get from request queue
               on the disk through blk_get_backing_dev_info(),or the @default_backing_dev_info.
               finally,checks whether @bd_invalidated is TRUE,if it is,call to rescan_partitions()
               to rescan disk parition table and use the partition info to set @part_tbl.

             if @partno returned by get_gendisk() is not _zero_,that means @disk is a partition.
               in this case,we have to get the whole disk at first.for do that,call to bdget_disk(@disk, 0).
               next,recursive calling to __blkdev_get() on the whole disk with @for_part == 1.
               if the recursion returned FALSE,goto label "out_clear",
               otherwise,set @bd_contains to the whole disk,and set @bd_inode->i_data.backing_dev_info to
               the whole disk's backing device info,set @bd_part to this partition.
               if @disk is not in fly(no GENHD_FL_UP) OR @bd_part is NULL OR @bd_part->nr_sects is _zero_,
               then set error code to -ENXIO,and goto label "out_clear"
               finally,call to bd_set_size() to set @i_size,@bd_block_size and @i_blkbits.
          6> if @bd_openers is not _zero_,that means someone have opened this disk,then we should undo
             get_gendisk().
             module_put() @disk->fops->owner,put_disk() on @disk.
             proceeding checks whether @bd_contains is equal to @bdev,if it is,that means this block
             device descriptor represents the whole disk,then call to @bd_disk->fops->open() if the method
             is exist,goto "out_unlock_bdev" if @open() failed. 
             call to rescan_partitions() to sescan partition table if @bd_invalidated is TRUE.
          7> normal end path -
               increase @bd_openers
               increase @bd_part_count if @for_part is TRUE
               unlock mutex and kernel
               return _zero_ to caller
              
          label "out_clear" :
            disk_put_part() the @bd_part
            @bd_disk = @bd_part = NULL
            set @backing_dev_info to @default_backing_dev_info
            __blkdev_put() @bd_contains if we had recursive calling to __blkdev_put
            set @bd_contains to NULL
            unlock mutex("out_unlock_bdev") and kernel("out_unlock_kernel")
            module_put() @disk->fops->owner if we did get_gendisk()
            put_disk() on @disk
            bd_put() the block device descriptor @bdev
            return error code to caller


/* END OF CHAPTER14 */


Chapter 15 : The Page Cache
    The Page Cache :
      page cache is the main disk cache used by the Linux Kernel.
      an entry in the page cache contains the data read from disk or write to disk.
      for read access,if the disk data contents does not contained in the page cache,
      a new entry would be created and filled with the disk data;other process can access
      to this page in the page cache instead to issues an I/O request to the disk.
      for write access,if the page cache entry corresponding to the request disk block
      is not contained,a new entry would be created,then the data is written into the entry;
      flush operation is not happen immediately,it will be delayed a few seconds,if there
      is another process need to read access the disk data or write access the disk data,
      it can simply operates on the page cache entry;the dirty page cache will be flushed by
      a special kernel thread,which flush the dirty pages in a well-defined period.

      !! KERNEL CODE AND KERNEL STRUCTURES DO NOT NEED TO BE READ FROM OR WRITTEN TO DISK.

      types of pages in the page cache :
        1> pages containing data of regular file
        2> pages containing directories
        3> pages containing data directly read from block device files
        4> pages containing data of User Mode processes that have been swapped out on disk
        5> pages belonging to files of special filesystems(Shared memory, Interprocess Communication, etc)

      ! the file(the file's inode) is called the page's owner.
      ! open flag O_DIRECT will by-pass page cache for the I/O operations on the opened file.
        through O_DIRECT flag,some programs can make use of their owns disk cache algorithm.

      page cache implementation requirements -
        > quickly locate a specific page containing data relative to a given owner.
          to take the maximum advantage from the page cache,searching it should be a very
          fast operation.
        > keep track of how every page in the page cache should be handled when reading or
          writing its content.

    The address_space Object :
      member @i_mapping usually points to the member @i_data of the same inode,@i_data is
      the page cache of this inode.
      member @mapping and @index of a page descriptor,establish a link between the page
      and the page cache.

      because pages in page cache have not to to physically adjacent in the disk,thus can
      not identify it by a device number and a block number,instead,identified by an owner
      and by an index within the owner's data - usually,an inode and an offset.

      ! the page cache may contain multiple copies of the same disk data.
        - in the regular file's inode's page cache
        - in the device file's master inode's page cache
        same disk data appears in two different pages referenced by two different address_space
        objects.

      <linux/fs.h>
        /**
         * address_space_operations - methods that define how the owner's pages are
         *                            handled
         * @writepage:                called by the VM to write a dirty page to backing store
         * @readpage:                 called by the VM to read a page from backing store
         * @sync_page:                called by the VM to notify the backing store to perform
         *                            all queued I/O operations for a page
         *                            I/O operations for other pages associated with this
         *                            address_space object may also performed
         * @writepages:               called by the VM to write out pages associated with the
         *                            address_space object
         * @set_page_dirty:           called by the VM to set a page dirty
         * @readpages:                called by the VM to read pages associated with the
         *                            address_space object
         * @write_begin:              called by the generic buffered write code to ask the
         *                            filesystem to prepare to write @len bytes at the
         *                            given offset in the file
         * @write_end:                after a successful write_begin(),and data copy,write_end()
         *                            must be called,@len is the original @len passed to write_begin()
         *                            and copied is the amount that was able to be copied
         * @bmap:                     called by the VFS to map a logical block offset,within object
         *                            to physical block number
         * @invalidatepage:           if a page has PagePrivate set,then invalidatepage will be called
         *                            when part or all of the page is to be removed from the
         *                            address space
         * @releasepage:              called on PagePrivate pages to indicate that the page should be
         *                            freed if possible
         * @direct_IO:                called by the generic read/write routines to perform direct IO
         * @get_xip_page:             called by the VM to translate a block number to a page
         * @migrate_page:             used to compact the physical memory usage
         * @launder_page:             called before freeing a page - it writes back the dirty page
         * @error_remove_page:        normally set to generic_error_remove_page() if truncation is ok
         *                            for this address space
         */
        struct address_space_operations {
                int (*writepage)(struct page *page, struct writeback_control *wbc);
                int (*readpage)(struct file *, struct page *);
                void (*sync_page)(struct page *);

                int (*writepages)(struct address_space *, struct writeback_control *);
                int (*set_page_dirty)(struct page *page);
                int (*readpages)(struct file *filp, struct address_space *mapping,
                                 struct list_head *pages, unsigned nr_pages);
                int (*write_begin)(struct file *, struct address_space *mapping,
                                   loff_t pos, unsigned len, unsigned flags,
                                   struct page **pagep, void **fsdata);
                int (*write_end)(struct file *, struct address_space *mapping,
                                 loff_t pos, unsigned len, unsigned copied,
                                 struct page *page, void *fsdata);
                secotr_t (*bmap)(struct address_space *, sector_t);
                void (*invalidatepage)(struct page *, unsigned long);
                int (*releasepage)(struct page *, gfp_t);
                ssize_t (*direct_IO)(int, struct kiocb *, const struct iovec *iov,
                                     loff_t offsetm unsigned long nr_segs);
                int (*get_xip_mem)(struct address_space *, pgoff_t, int, void **,
                                   unsigned long *);
                int (*migratepage)(struct address_space *, struct page *, struct page *);
                int (*launder_page)(struct page *);
                int (*is_partially_uptodate)(struct page *, read_descriptor_t *,
                                             unsigned long);
                int (*error_remove_page)(struct address_space *, struct page *);
        };          

        /**
         * address_space - object used to group and manage pages in the page cache
         * @host:          owner of the page cache,inode or block device inode in bdev
         * @page_tree:     radix tree of all pages
         * @tree_lock:     lock protecting @page_tree
         * @i_mmap_writable:
         *                 count VM_SHARED mappings
         * @i_mmap:        tree of private and shared mappings
         *                 # priority search tree
         * @i_mmap_nolinear:
         *                 list of VM_NONLINEAR mappings
         * @i_mmap_lock:   lock used to protect i_mmap count,list,tree
         * @truncate_count:
         *                 cover race condition with truncate
         * @nrpages:       number of total pages
         * @wirteback_index:
         *                 index that writeback starts
         * @a_ops:         address space methods
         * @flags:         error bits / gfp mask
         * @backing_dev_info:
         *                 device readahead,etc
         * @private_lock:  lock,usually protect @private_list
         * @private_list:  usually,a list of dirty buffers of indirect blocks
         *                 associated with the inode
         *                 # generic list can be freely used by the filesystem
         *                   for its specific purposes
         *                 # this list usually collects @b_assoc_buffers of buffer heads
         * @assoc_mapping: usually,pointer to the address_space object of the block
         *                 device including the indirect blocks
         */
        struct address_space {
                struct inode *host;
                struct radix_tree_root page_tree;
                spinlock_t tree_lock;
                unsigned int i_mmap_writable;
                struct prio_tree_root i_mmap;
                struct list_head i_mmap_nonlinear;
                spinlock_t i_mmap_lock;
                unsigned int truncate_count;
                unsigned long nrpages;
                pgoff_t writeback_index;
                const struct address_space_operations *a_ops;
                unsigned long flags;
                struct backing_dev_info *backing_dev_info;
                spinlock_t private_lcok;
                struct list_head private_list;
                struct address_space *assoc_mapping;
        } __attribute__((aligned(sizeof(long))));

      if the owner of a page in the page cache is a file,the address_space object is embedded
      in the @i_data field of a VFS inode object,@i_mapping field of the inode always points
      to the address_space object(@i_data) of the owner of the pages containing the inode's
      data.the @host of address_space object points to the inode object in which the descriptor
      is embedded.

      if a page contains data read from a block device file,that is it stores "raw" data of a block
      device,the address_space object is embedded in the "master" inode of the file in the bdev
      special filesystem associated with the block device.
      /* the inode is referenced by @bd_inode */
      @i_mapping points to the address_space object of the "master" inode,and @host points to
      the master inode.thus,all pages containing data read from a block device have the same
      address_space object even if they have been accessed by referring to different block device
      files. /* page descriptor's @mapping member */

    The Radix Tree : 
      radix tree is introduced since Linux 2.6 for improve page cache lookup performance,each
      address_space object has a member named @page_tree,it is the root of radix tree.

      <linux/radix-tree.h>
        #define RADIX_TREE_MAX_TAGS 2

        /**
         * radix_tree_root - root structure of radix tree
         * @height:          current tree height
         * @gfp_mask:        gfp mask for new node
         * @rnode:           root node(top level 1)
         * # the pages in page cache is stored in leaves
         */
        struct radix_tree_root {
                unsigned int height;
                gfp_t gfp_mask;
                struct radix_tree_node *rnode;
        };

      <lib/radix-tree.c>
        /* general is 2^6 => 64 */
        #define RADIX_TREE_MAP_SIZE (1UL << RADIX_TREE_MAP_SHIFT)
                                    /* RADIX_TREE_MAP_SHIFT
                                     * #ifdef __KERNEL__ => CONFIG_BASE_SMALL ? 4 : 6
                                     * #else  3
                                     */

        #define RADIX_TREE_TAG_LONGS \
                ((RADIX_TREE_MAP_SIZE + BITS_PER_LONG - 1) / BITS_PER_LONG)
                /* 64 + 64 - 1 = 127, 127 / 64 = 1 */

        /**
         * radix_tree_node - node structure of radix tree
         * @height:          node height
         * @count:           not NULL pointers counter
         * @rcu_head:        RCU
         * @slots:           pointer array
         * @tags:            array of flags,used for quick search
         *                   RADIX_TREE_MAX_TAGS == 2
         *                     index 0 => PG_dirty (PAGECACHE_TAG_DIRTY)
         *                     index 1 => PG_writeback (PAGECACHE_TAG_WRITEBACK)
         *                                             <linux/fs.h>
         * # each node of radix tree can have up to 64(generally) pointers
         *   to other nodes or to page descriptors
         */
        struct radix_tree_node {
                unsigned int height;
                unsigned int count;
                struct rcu_head rcu_head;
                void *slots[RADIX_TREE_MAP_SIZE];
                unsigned long tags[RADIX_TREE_MAX_TAGS][RADIX_TREE_TAG_LONGS];
        };

      radix_tree_node.slots[] is used to stores node pointers or page descriptors,if current
      node is not leaf,@slots stores pointers to other nodes;otherwise,it stores page descriptors.
      and the size of @slots is 64,thus,for a leaf node,it can stores 64 pages from index 0 - 63.
      if a new page has the index greater than current highest index need to be inserted,
      the tree's height must be increased.
      e.g.
        radix_tree_root.height == 1
        => only one radix_tree_node entity

        radix_tree_node.height == 1
        radix_tree_node.slots[0] -- radix_tree_node.slots[63] => represents page0 - page63

        insert page64

        radix_tree_node.height == 1
        radix_tree_node.slots[0] => radix_tree_node(leaf,index 0 - 63)
        radix_tree_node.slots[1] => radix_tree_node(leaf,index 64 - 127)
        radix_tree_node.slots[2] => radix_tree_node(leaf,index 128 - 191)
        ...
        radix_tree_node.slots[63] => radix_tree_node(leaf,index 4031 - 4095)

        leaf1.height == 2
        leaf2.height == 2
        leaf2.count == 1 => page64 <= leaf2.slots[0]
    
        radix_tree_root.height == 2 /* current height of the tree is 2 */

                +------+
                | root | @height = 2
                +------+
                 |
                 +--> +------+ @height = 1
                      | node | @count = 2
                      +------+ 
                       |
                       V
                     | 0 | 1 | 2 | ... | 63 | => slots
                       |   |
                       |   +------+
                       |          |
                       V          V
           @height = 2 +------+   +------+ @height = 2
           @count = 1  | node |   | node | @count = 1
                       +------+   +------+
                        |          |
                        |          V
                        |        | 0 | 1 | 2 | ... | 63 | => slots
                        |          |
                        |          +--> page64
                        V
                      | 0 | 1 | 2 | ... | 63 | => slots

      32 bit index - page size 4kB
      | radix tree height | highest index | maximum file size |
        0                   none            0bytes
        1                   2^6 - 1         256kB
        2                   2^12 - 1        16MB
        3                   2^18 - 1        1GB
        4                   2^24 - 1        64GB
        5                   2^30 - 1        4TB
        6                   2^32 - 1        16TB

      page index and array index translate : (32 bit page index)
        similar to paging system and Page Table.

        for index @i

        if tree height is 1
        then
          index of slots[] with height 1 =>
            @i & 111111
        else if tree height is 2
        then
          index of slots[] with height 1 =>
            @i & 111111 000000
          index of slots[] with height 2 =>
            @i & 000000 111111
        else if tree height is 3
        then
          index of slots[] with height 1 =>
            @i & 111111 000000 000000
          index of slots[] with height 2 =>
            @i & 000000 111111 000000
          index of slots[] with height 3 =>
            @i & 000000 000000 111111
        AND SO ON

        if tree height is 6
        then
          index of slots[] with height 1 =>
            @i & 11 000000 000000 000000 000000 000000
          index of slots[] with height 2 =>
            @i & 00 111111 000000 000000 000000 000000
          index of slots[] with height 3 =>
            @i & 00 000000 111111 000000 000000 000000
          index of slots[] with height 4 =>
            @i & 00 000000 000000 111111 000000 000000
          index of slots[] with height 5 =>
            @i & 00 000000 000000 000000 111111 000000
          index of slots[] with height 6 =>
            @i & 00 000000 000000 000000 000000 111111

    Page Cache Handling Functions :
      Finding a page :
        routine find_get_page() is used to find and get a page reference from page cache.

        <mm/filemap.c>
          /**
           * find_get_page - find and get a page reference
           * @mapping:       address space
           * @offset:        the page index
           * return:         page descriptor OR NULL
           * # EXPORT_SYMBOL
           * # this routine returns a page descriptor with refcount increased
           * # steps :
           *     1> acquire rcu_read_lock()
           *     2> invokes radix_tree_lookup_slot() with arguments @mapping->page_tree, @offset
           *     3> if returned void ** is not NULL
           *        # lookup slot returns the address of a slot in radix tree member @slots[]
           *          so,the type is void **
           *     4> then get page descriptor through radix_tree_deref_slot() on the void ** pointer
           *        # radix_tree_deref_slot() make use of rcu_dereference() to get the element
           *          in that slot
           *     5> repeat from step 2> if
           *          NULL page descriptor OR page descriptor == RADIX_TREE_RETRY
           *          page_cache_get_speculative() on the page descriptor was failed
           *          # increase struct page._count member
           *            it is a virtual memory BUG if
           *              in_interrupt()
           *              !in_atomic()
           *              @_count == 0
           *              PageTail(the page) is TRUE
           *          address of the page descriptor != *(the void ** pointer)
           *          # page_cache_release() current page descriptor
           *     6> rcu_read_unlock()
           *     7> return the page descriptor
           */
          struct page *find_get_page(struct address_space *mapping, pgoff_t offset);

          /**
           * find_get_pages - find and get a group of pages having contiguous indices
           * @mapping:        address space
           * @start:          start page index
           * @nr_pages:       maximum number of pages
           * @pages:          page descriptor pointer array provided by caller
           * return:          number of pages which were found
           * # search returns a group of mapping-contiguous pages with ascending indexes
           * # there may be holes in the indices due to not-present pages
           *                # page index hole(not-present),not array hole
           */
          unsigned find_get_pages(struct address_space *mapping, pgoff_t start,
                                  unsigned int nr_pages, struct page **pages);

          brief description for find_get_pages() :
            the primary routine is radix_tree_gang_lookup_slot().
            find_get_pages() call to this routine with rcu_read_lock(),the arguments
            are @mapping->page_tree, (void ***)@pages, @start, @nr_pages.
                                     |
                                     +--> /* used as slot pointers array */
            radix_tree_gang_lookup_slot() returns the number of slots it has found.
            then find_get_pages() processes a for-cycle,from 0 to number of found slots - 1,
            it call to radix_tree_deref_slot() on (void **)@pages[current index] to
            get the page descriptor,if the page descriptor passed some checkings,then place
            it in current position of void * array @pages.
            finally,rcu_read_unlock() and returns the number of found pages to caller.
            /**
             * the checkings in for-cycle is similar to find_get_page()
             *   continue to next iteration if NULL page descriptor
             *   restart searching from call to radix_tree_gang_lookup_slot() if page is RADIX_TREE_RETRY
             *   repeat radix_tree_deref_slot() if page_cache_get_speculative() resulted FALSE
             *   page_cache_release() current page and repeat radix_tree_deref_slot() if
             *     this page is not equal to *((void **)@pages[current index])
             */

          /* find_lock_page - find and get a page in page cache,set PG_locked bit for it */
          # EXPORT_SYMBOL
          struct page *find_lock_page(struct address_space *mapping, pgoff_t offset);

          /**
           * this routine invokes lock_page() function to process setup of PG_locked bit.
           * lock_page() is defined in <linux/pagemap.h> as a static function,it will invokes
           * try_lock_page() at first.if failed,then invokes __lock_page() routine,in which,
           * current process will be put in TASK_UNINTERRUPTIBLE state,and also be inserted into
           * wait queue -
           *   page_zone(the page)->wait_table[hash_ptr(the page, page_zone(the page)->wait_table_bits)]
           * routine unlock_page() is used to release a locked page,it clear PG_locked bit,and process
           * a SMP memory barrier after cleaned the bit,wake up the process waiting for this page.
           *
           * # __lock_page() and unlock_page() are defined in <mm/filemap.c>,each of them is
           * EXPORT_SYMBOL
           * # __lock_page() invokes routine __wait_on_bit_lock(),which is defined in <kernel/wait.c>.
           *   function sync_page() as the parameter @action passed to __wait_on_bit_lock().
           *   the function sync_page(),it retrieves @mapping of the page descriptor,then call to
           *   @sync_page of @a_ops of @mapping if the method is defined,call to io_scheduler() to
           *   relinquish CPU time;after resume,return _zero_ to caller that the routine __wait_on_bit_lock().
           * # for block device,the default value of method @sync_page is block_sync_page() in <fs/buffer.c>,
           *   which will call to blk_run_backing_dev() defined in <linux/backing-dev.h> if this page is in
           *   page cache(@mapping of the page is not NULL),finally,method unplug_io_fn() is invoked if
           *   it is exist.                                                # unplug method of this bdi
           */

          /**
           * find_or_create_page - locate or add a page in page cache
           * @mapping:             page's address space
           * @index:               page index into the mapping
           * @gfp_mask:            gfp mask used for create new page
           * return:               page descriptor OR NULL
           * # EXPORT_SYMBOL
           * # this routine first invokes find_lock_page() attempts to get and lock a page at given index
           *   into @mapping
           *   if NULL pointer is returned,it will invokes __page_cache_alloc() with @gfp_mask to creates
           *   a new page,then invokes add_to_page_cache_lru() attempts insert the new page to page cache
           *   on the given index;finally,return the page descriptor with refcount been increased and bit
           *   PG_locked is set
           */
          struct page *find_or_create_page(struct address_space *mapping, pgoff_t index, gfp_t gfp_mask);

      Adding a page :
        <linux/pagemap.h>
          /**
           * add_to_page_cache - lock the page and insert it into page cache at a given position
           * @page:              page descriptor
           * @mapping:           page mapping space
           * @offset:            index into page cache
           * @gfp_mask:          gfp mask used to allocate new radix tree node
           * return:             0 OR error code
           * # routine clear PG_locked if any error was happened
           * # routine actually call to add_to_page_cache_locked() to deal with inserting
           * # ! this routine returned with PG_locked is set
           */
          static inline int add_to_page_cache(struct page *page, struct address_space *mapping,
                                              pgoff_t offset, gfp_t gfp_mask);

        <mm/filemap.c>
          /**
           * add_to_page_cache_locked - add a locked page to the page cache
           * @page:                     page descriptor
           * @mapping:                  address space
           * @offset:                   page index
           * @gfp_mask:                 node allocation mode
           * return:                    0 OR error code
           * # EXPORT_SYMBOL
           * # ! this routine returned with PG_locked is set
           */
          int add_to_page_cache_locked(struct page *page, struct address_space *mapping,
                                       pgoff_t offset, gfp_t gfp_mask);

          brief description for add_to_page_cache_locked() :
            first,call to mem_cgroup_cache_charge(),this function is a part of memory
            control group./* cgroup for virtual memory subsystem */
            /**
             * Control Groups -
             *   provide a mechanism for aggregating/patitioning sets of tasks,and all
             *   their future children,into hierarchical groups with specialized behavior.
             * A cgroup associates a set of tasks with a set of parameters for one or 
             * more subsystems.
             */
            for page is file cache and shared memory page,the function make use of different
            charge type - file cache => MEM_CGROUP_CHARGE_TYPE_CACHE
                          shm  page  => MEM_CGROUP_CHARGE_TYPE_SHM
            if current process is kernel thread,then the function make use the memory descriptor
            of "init" process. /* because kernel thread task_struct.mm is NULL */
            after succeed to charge memory control group(return error code to caller if failed),
            radix_tree_preload() is invoked with @gfp_mask & ~__GFP_HIGHMEM.
            /**
             * radix_tree_preload -
             *   disable preempt
             *   get per-CPU variable @radix_tree_preloads which is type of struct radix_tree_preload
             *   enter a while-cycle until @radix_tree_preloads->nr >= RADIX_TREE_MAX_PATH
             *     enable preempt
             *     allocate new radix tree node through kmem_cache_alloc() from slab cache 
             *     radix_tree_node_cachep with the given gfp mask
             *     disable preempt
             *     get the @radix_tree_preloads again
             *     if no kernel control path allocated new radix tree node cause @nodes[]
             *     become full,then place the newly allocated radix tree node at next position
             *     else if @nodes[] become full,then free the radix tree node allocated previously
             *   finally,return _zero_ or error code -ENOMEM to caller
             *   # this routine returned with kernel preempt disabled
             *
             * struct radix_tree_preload {
             *         int nr;  /* current number of elements */
             *         struct radix_tree_node *nodes[RADIX_TREE_MAX_PATH]; /* node pointer array */
             * }; /* per-CPU pool of preloaded nodes */
             *
             * by make use of radix_tree_preload(),seperate node memory allocating from insertion
             */
           
           if we succeed to preload radix tree node,then _setup_ the page descriptor @page.
           next,acquire @mapping->tree_lock with local interrupt disabled.
           invoke radix_tree_insert() to process new node insertion.
           succeed to insert new node,then
             increase @mapping->nrpages
             increase zone state NR_FILE_PAGES of current page's zone
             increase zone state NR_SHMEM of current page's zone if the page is swap entry
             release spin lock and enable interrupt
           failed to insert new node,then
             reset page descriptor
             release spin lock and enable interrupt
             uncharge this page from memory control group
             invoke page_cache_release() on this page
           call to radix_tree_preload_end(),this routine enable kernel preempt
           finally return error code to caller,if no error detected,_zero_ will be returned
           /* of course,if preload was failed,mem_cgroup_uncharge_cache_page() is called for
            * uncharge @page from memory control group
            */
          
          /**
           * add_to_page_cache_lru - add a given page into page cache,and add the page
           *                         to LRU list if successfully add it into page cache
           * # EXPORT_SYMBOL
           * # if @mapping has capability of swap-backed,then set SwapBacked for @page
           *   in this case,the page becomes swap entry,routine add it to anonymous LRU
           *   list instead to add it into file LRU list
           * # routine add the page to page cache by call to add_to_page_cache(),thus
           *   the page will resulted in PG_locked is set
           */
          int add_to_page_cache_lru(struct page *page, struct address_space *mapping,
                                    pgoff_t offset, gfp_t gfp_mask);

        <lib/radix-tree.c>
          /**
           * radix_tree_insert - do radix tree node insertion
           * @root:              radix tree root
           * @index:             new page's index
           * @item:              item,usually is page descriptor
           * return:             0 OR error code
           * # EXPORT_SYMBOL
           * # about this routine -
           *     make sure the tree is high enough by checks radix_tree_maxindex(@root->height)
           *     if @index is greater than maxindex,then invoke radix_tree_extend() to extend
           *     the tree
           *     # extending would add a single node on top,that assign it to the root's @rnode
           *       extending invoke radix_tree_node_alloc() to allocate new node
           *     go down the tree by decode @index,at the bottom,leaf's height is _zero_
           *     # go down make use of while-cycle until height becomes _zero_
           *       if current node is NULL,then have to invoke radix_tree_node_alloc() to allocate
           *       new tree node,because of radix tree preload,this routine first try per-CPU
           *       preloaded nodes,allocate new node(might failed) if no free node is avaiable
           *       after get a tree node,setup it with current height,and assign its pointer to
           *       upper node's @slots[](for root,it is @rnode member)
           *     if the final slot is NULL,return -EEXIST to caller
           *     if the final radix tree node is not NULL,then place the @item(page) into the @slots[]
           *     by invoke rcu_assign_pointer()
           *     if the final radix tree node is NULL,that means we are deal with a new radix-tree,
           *     so @item must be a radix-tree node,assign it to @rnode of root
           *     finally,return _zero_ to caller
           */
          int radix_tree_insert(struct radix_tree_root *root, unsigned long index, void *item);

      Removing a page :
        <mm/filemap.c>
          /**
           * remove_from_page_cache - remove a page from page cache
           * @page:                   page descriptor
           * # for remove @page,it must be locked(PG_locked)
           * # this routine actually does
           *     > acquire @tree_lock and disable local interrupt
           *     > call to routine __remove_from_page_cache()
           *     > release @tree_lock and enable local interrupt
           *     > invoke mem_cgroup_uncharge_cache_page() to remove the page from
           *       memory control group
           */
          void remove_from_page_cache(struct page *page);

          /**
           * __remove_from_page_cache - do remove page from page cache
           * @page:                     page descriptor
           */
          void __remove_from_page_cache(struct page *page);

          brief description for __remove_from_page_cache() :
            call to radix_tree_delete() from remove the radix tree leaf node that corresponding
            to @page.
            /**
             * <lib/radix-tree.c>
             * void *radix_tree_delete(radix tree root pointer, index) - 
             *   makeup an array of radix_tree_path object,size is RADIX_TREE_MAX_PATH + 1
             *   checks whether @index is valid(must less than or equal to maximum index)
             *   if current height of tree is _zero_,then invoke root_tag_clear_all() to
             *   clear tags of the tree root,and set @rnode to NULL,return NULL to caller
             *   enter a do-while-cycle,stop condition is height becomes _zero_
             *     go down the tree,if encountered a NULL node,return NULL to caller
             *     during the traverse,fill radix_tree_path objects in the array
             *     # the go down is rely on decoding of @index
             *     # local variable @pathp points to the radix_tree_path array
             *       and it will skip the element whose index is 0
             *   when cycle stopped,if the last node is NULL,then return NULL to caller
             *   otherwise,use a for-cycle with index 0 and 1,call radix_tree_tag_clear() on the
             *   just-deleted item if it has been tagged;this will clear the corresponding tag flag
             *   bits in @tags[0][0] and @tags[1][0](depends on the offset decoded from @index)
             *   # just-deleted item,the node its the slot in @slots[] points to the leaf contains
             *     the page that we want to delete
             *   next,iterate @pathp until @pathp->node is NULL(that the path has index 0 in array)
             *   # path go back
             *   for each node on the path,sets @slots[@pathp->offset] to NULL,and decrease its
             *   @count member.if its counter become _zero_,then we will free it at next iteration.
             *   if its counter is not _zero_,but @pathp->node is equal to @rnode,then we have to
             *   shrink the tree,and returns the delete item's address to caller.
             *   finally,invoke root_tag_clear_all() on root,set tree height to _zero_,set @rnode
             *   to NULL,free the last node on the path if necessary,then return the address of
             *   delete item to caller.
             *   # at the last iteration,the element has index 1 in the array is @rnode,so,
             *     if its counter become _zero_,then we must free it on normal return path;
             *     if the counter is not _zero_,tree shrink must happened
             */
            
            after radix_tree_delete() returned,reset @page->mapping to NULL.
            decrease the address space object's @nrpages member.
            decrease zone state NR_FILE_PAGES of the zone that this @page come from
            # increased in add_to_page_cache()
            decrease zone state NR_SHMEM of the zone that this @page come from,if it is
            a swap entry.
            # after deleting and counter decreasing,if this page still mapped,then there must
              be a BUG,because we are operating on it with PG_locked.
            finally,checks if @page is dirty AND the address space object enabled dirty account,
            then decrease zone state NR_FILE_DIRTY of the zone that this @page come from;decrease
            backing device info stat BDI_RECLAIMABLE of the backing device of the address space
            object.
      
      Updating a page :
        <mm/filemap.c>
          /**
           * read_cache_page - read into page cache,fill it if needed
           * @mapping:         address space
           * @index:           page index
           * @filler:          routine used to fill page
           * @data:            data for @filler
           * return:           page descriptor OR -EIO
           * # EXPORT_SYMBOL
           * # routine actuall call to wait_on_page_read(),argument for the function is
           *   get from read_cache_page_async()
           * # routine wait_on_page_read() first checks if the page pointer is a valid
           *   pointer(IS_ERR() returned FALSE),then wait for the page to be unlocked,
           *   checks PageUptodate(),if failed to up to date,then do page_cache_release()
           *   for the page,and return -EIO to caller
           *   - wait_on_page_read() will invokes wait_on_page_locked(),which will call
           *     to wait_on_page_bit(),if PG_locked flag is enabled,the wait on the 
           *     wait queue page_waitqueue(@page),action is @sync_page
           * # routine read_cache_page_async() actually call to do_read_cache_page()
           *   - the routine do not wait for page to become unlocked after submitting it
           *     to the filler
           */
          struct page *read_cache_page(struct address_space *mapping, pgoff_t index,
                                       int (*filler)(void *, struct page *), void *data);

          /**
           * do_read_cache_page - do read cache page,and fill it if needed
           * @mapping:            address space
           * @index:              page index
           * @filler:             filler routine
           * @data:               data for @filler
           * @gfp:                gfp flag
           */
          static struct page *do_read_cache_page(struct address_space *mapping, pgoff_t index,
                                                 int (*filler)(void *, struct page *), void *data,
                                                 gfp_t gfp);

          brief description for do_read_cache_page() :
            invokes __read_cache_page() to process the main works.
            /**
             * <mm/filemap.c>
             *   static struct page *__read_cache_page(struct address_space *mapping, pgoff_t index,
             *                                         int (*filler)(void *, struct page *), void *data,
             *                                         gfp_t gfp);
             *   call to find_get_page() to get the cache page
             *   if no such page is exist,then invoke __page_cache_alloc() with @gfp | __GFP_COLD to
             *   allocate a new cache page,return -ENOMEM if allocating failed
             *   next,call to add_to_page_cache_lru() to add the newly allocated page to cache and LRU list
             *   if any error happens,page_cache_release() the page,and checks if the error is -EEXIST,that
             *   someone have allocated such cache page during we in __page_cache_alloc(),then repeat again
             *   from find_get_page();otherwise,return the error code to caller
             *   proceeding invokes @filler(@data, page descriptor)
             *   if @filler() returned a negative value,page_cache_release() the page and return the error code
             *   to caller
             *
             *   if the page is exist in page cache or no error happened when have to create new page,
             *   just return its descriptor to caller(of course,@filler() would not be called if the page
             *   been exist in page cache,moreover,the page will have not PG_locked)
             */
            after __read_cache_page() returned a valid page descriptor(error code just return to caller).
            checks PageUptodata(),if the page been up to date,then return the page descriptor to caller as well.
            /* if the page is newly created in __read_cache_page(),then @filler() must be called if no error
             * is returned from __read_cache_page()
             */
            if the page have not been up to data,then we lock_page(). /* not a newly created page */
            checks @page->mapping,if it is NULL,that means the page is unmapped(because,add_to_page_cache_lru(),
            its @mapping must be set to the parameter @mapping in __read_cache_page()),so we have to unlock it,
            page_cache_release() it,and retry __read_cache_page().
            |                                 /* if the page is in page cache,@mapping must not be NULL */
            +--> /* page_cache_release -> put_page - free the page if recount becomes _zero_ */

            if its @mapping is not NULL,then do PageUptodate() again,unlock it and return to the page descriptor to
            caller if it been up to date;otherwise,invokes @filler() in do_read_cache_page().
                                                   /* this is because @filler() only called by __read_cache_page()
                                                    * when the page has to be newly allocated
                                                    */
            if @filler() returned an error code,page_cache_release() the page and return error code to caller.
            at the end,if any thing is OK,then mark the page is accessed,return its descriptor to caller.
                                          /* mark_page_accessed() <mm/swap.c>,mark a page as having
                                           * seen activity.
                                           */
            ! note that,before @filler() is called,the page is PG_locked by directly call to lock_page()(in
              do_read_cache_page()) or by indirectly call to add_to_page_cache_locked()(in __read_cache_page())

    The Tags of the Radix Tree :
      page cache allows the kernel to quickly retrieve pages in the cache that are in a given state.
      for quickly search of pages in a given state,member @tags of radix tree node is used to stores flags
      to indicate that whether such pages in the given state is exist in its subtree's leaves.the flag
      will be set if and only if at least one leaf in the state,the flag of leaf is copied from the page descriptor.
      in this way,kernel can skip other subtrees that are not have leaves in the given state.
      /* intermediate node stores flags for each child node(or leaf) */

      e.g.
                                      ROOT
                                        |
                                        V
                                      rnode
                                      |                          T
                                      +--> [0] => | 0 | => | 0 | 1 | 2 | ... | 63 |
                                      |
                                      +--> [1] => | 0 | => | 0 | 1 | 2 | ... | 63 |
                                      |
                                      +--> | 0 | 1 | 2 | ... | 63 |
                                                 |
                                                 V
                                               node
                                               |                      T       
                                               +--> [0] => | 0 | => | 0 | 1 | 2 | ... | 63 |
                                               |
                                               +--> [1] => | 0 | => | 0 | 1 | 2 | ... | 63 |
                                               |
                                               +--> | 0 | 1 | 2 | ... | 63 |
                                                      |
                                                      V
                                                     ...
                                                      |
                                                      V
                                                    node(leaf) /* leaf node do not store flags */
                                                    |                            
                                                    +--> [0] => | 0 | => | 0 | 1 | 2 | ... | 63 |
                                                    |
                                                    +--> [1] => | 0 | => | 0 | 1 | 2 | ... | 63 |
                                                    |
                                                    +--> | 0 | 1 | 2 | ... | 63 |
                                                                   |
                                                                   +--> page descriptor    

      <lib/radix-tree.c>
        /**
         * radix_tree_tag_set - set a tag on a radix tree node
         * @root:               radix tree root
         * @index:              index key
         * @tag:                tag index
         * return:              return the address of the tagged item(page descriptor)
         * # EXPORT_SYMBOL
         * # it is BUG if slot is NULL
         * # routine go down the tree by decode index,for each intermediate tree nodes,
         *   if it is not tagged,then tag it(do not tag leaf node)
         *   finally,set radix tree root's tag bit if necessary(NOT NULL slot AND root un-tagged)
         */
        void *radix_tree_tag_set(struct radix_tree_root *root, unsigned long index,
                                 unsigned int tag);
        
        ! radix_tree_tag_set() get tag of node by make use of tag_get(),set tag of node by make use
          of tag_set();get tag of root by make use of root_tag_get(),set tag of root by make use of
          root_tag_set().

        /* __set_bit(@offset, @node->tags[@tag]) */
        static inline void tag_set(struct radix_tree_node *node, unsigned int tag, int offset);

        /* test_bit(@offset, @node->tags[@tag]) */
        static inline int tag_get(struct radix_tree_node *node, unsigned int tag, int offset);

        /* __clear_bit(@offset, @node->tags[@tag]) */
        static inline void tag_clear(struct radix_tree_node *node, unsigned int tag, int offset);

        ! tag_get(),tag_set(),tag_clear(),operate on a specified bit at a given index.

        /* @root->gfp_mask |= (__force gfp_t)(1 << (@tag + __GFP_BITS_SHIFT)) */
        static inline void root_tag_set(struct radix_tree_root *root, unsigned int tag);

        /* (__force unsigned)@root->gfp_mask & (1 << (@tag + __GFP_BITS_SHIFT)) */
        static inline int root_tag_get(struct radix_tree_root *root, unsigned int tag);

        /* @root->gfp_mask &= (__force_gfp_t)~(1 << (@tag + __GFP_BITS_SHIFT)) */
        static inline void root_tag_clear(struct radix_tree_root *root, unsigned int tag);

        /* @root->gfp_mask &= __GFP_BITS_MASK */
        static inline void root_tag_clear_all(struct radix_tree_root *root);

        ! root_tag_set(),root_tag_get(),root_tag_clear(),root_tag_clear_all(),make use one
          bit in @gfp_mask member.

        /**
         * radix_tree_tag_clear - clear a tag on a radix tree node
         * @root:                 tree root
         * @index:                item index
         * @tag:                  tag index
         * return:                return address of the tagged item on success
         *                        return NULL if failed
         * # EXPORT_SYMBOL
         * # this routine does reverse to radix_tree_tag_set(),thus,it must make use
         *   of an array of radix_tree_path objects to record the go down path,and
         *   then go back to root,for each node on the path,invokes tag_clear() on it
         * # if any one node is on the path but without the given tag is set,then it
         *   must be an error,return NULL to caller
         * # the go back might earlier stopped,if the node has other slot is tagged
         *   with @tag,then we can not clear the tag of nodes in front of current node
         * # if @index guide us to a NULL slot,that must be an error,then return NULL
         *   to caller
         * # if all intermediate nodes have been un-tagged,then we have to clear
         *   root's tag if necessary(root_tag_get() returns TRUE)
         */
        void *radix_tree_tag_clear(struct radix_tree_root *root, unsigned long index,
                                   unsigned int tag);
                                                
        /* radix_tree_tagged - test whether any items in the tree are tagged,just checks root node */
        int radix_tree_tagged(struct radix_tree_root *root, unsigned int tag);

      <mm/filemap.c>
        /**
         * find_get_pages_tag - find and return pages that match given tag
         * @mapping:            address space
         * @index:              the starting page index
         * @tag:                the tag index
         * @nr_pages:           the maximum number of pages
         * @pages:              array of page descriptors that match @tag
         * return:              0 OR error code
         * # EXPORT_SYMBOL
         * # the primary searching is handled by routine radix_tree_gang_lookup_tag_slot(),
         *   the function is similar to radix_tree_gang_lookup_slot() but is only fill
         *   the array that slots match the given @tag;function radix_tree_gang_lookup_slot()
         *   rely on __lookup(),function radix_tree_gang_lookup_tag_slot() rely on __lookup_tag()
         * # this routine might update @index to the index of page which next to the page we
         *   last found,this is happens we exactly found out some pages
         */
        unsigned find_get_pages_tag(struct address_space *mapping, pgoff_t *index, int tag,
                                    unsigned int nr_pages, struct page **pages);

    Storing Blocks in the Page Cache :
      in old versions of Kernel,there were two different main disk caches -
        page cache     => whole pages of disk data resulting from accesses to the contents of disk files
        buffer cache   => used tokeep in memory the contents of the blocks accessed by VFS to manage the
                          disk-based filesystems

      now,buffer cache is no longer allocated individually,they are stored in dedicated pages called
      "buffer pages" which are kept in the page cache.

      Block Buffers and Buffer Heads :
        a buffer page is a page of data associated with additional descriptors called "buffer head",
        whose main purpose is to quickly locate the disk address of each individual block in the page.
        /* the chunks of data stored in a page belonging to the page cache are not necessarily adjacent
         * on disk.
         */

        <linux/buffer_head.h>
          /**
           * buffer_head - buffer head,descriptor of buffer page
           * @b_state:     buffer state bitmap
           * @b_this_page: circular list of page's buffers
           *               # each buffer page has a buffer_head descriptor
           * @b_page:      the page this bh is mapped to
           *               # that the buffer page holding this block
           * @b_blocknr:   start block number of data of this buffer
           *               # logical block number
           * @b_size:      size of mapping
           * @b_data:      pointer to data within the page
           *               # @page is high-memory => @b_data contains offset
           *                 @page is not high-memory => @b_data is linear address
           * @b_bdev:      block device which owns to this bh
           *               # which contains the block
           * @b_end_io:    routine called when I/O completed
           *               # typedef void (bh_end_io_t)(struct buffer_head *bh, int uptodate)
           * @b_private:   reserved for @b_end_io
           * @b_assoc_buffers:
           *               associated with another mapping
           * @b_assoc_map: mapping this bufer is associated with
           * @b_count:     users using this buffer_head
           *               # @b_count is increased right before each operation on the block
           *                 buffer,and decreased right after the operation done
           * # buffer page,used to mapping a block
           * # historically,a buffer_head was used to map a single block within
           *   a page,and of course as the unit of I/O through the filesystem
           *   and block layers
           *   nowadays the basic I/O unit is the bio,and bufer_head(s) are used for
           *   extracting block mappings,for tracking state within a page,and for
           *   wrapping bio submission for backward compatibility reasons
           * # buffer page is available when CONFIG_BLOCK is defined
           */
          struct buffer_head {
                  unsigned long b_state;
                  struct buffer_head *b_this_page;
                  struct page *b_page;

                  sector_t b_blocknr;
                  size_t b_size;
                  char *b_data;
                 
                  struct block_device *b_dev;
                  bh_end_io_t *b_end_io;
                  void *b_private;
                  struct list_head b_assoc_buffers;
                  struct address_space *b_assoc_map;

                  atomic_t b_count;
          };

          ! block buffers in the page cache are examined both periodically
            and when free memory becomes scarce,and only the block buffers
            having _zero_ @b_count may be reclaimed.

          enum bh_state_bits {
                  BH_Uptodate,      /* contains valid data */
                  BH_Dirty,         /* is dirty */
                  BH_Lock,          /* is locked */
                  BH_Req,           /* has been submitted for I/O */
                  BH_Uptodate_Lock, /* used by the 1st bh in a page,to serialise
                                     * I/O completion of other buffers in the page
                                     */
                  BH_Mapped,        /* has a disk mapping */
                  BH_New,           /* disk mapping was newly created by get_block */
                  BH_Async_Read,    /* under end_buffer_async_read I/O */
                  BH_Async_Write,   /* under end_buffer_async_write I/O */
                  BH_Delay,         /* buffer is not yet allocated on disk */
                  BH_Boundary,      /* block is followed by a discontiguity
                                     * this means,the addressing for next block(the followed block)
                                     * is indirectly,thus at the next operation,disk
                                     * must move its head
                                     */
                  BH_Write_EIO,     /* I/O error on write */
                  BH_Ordered,       /* ordered write */
                  BH_Eopnotsupp,    /* unsupport operation(barrier) */
                  BH_Unwritten,     /* buffer is allocated on disk but not written */
                  BH_Quiet,         /* buffer error prinks to be quiet */
                  BH_PrivateStart,  /* not a state bit,but the first bit avaiable
                                     * for private allocation by other entities
                                     */
          };

        +--> address space <-- @i_mapping
        |
        |             +--> general page(page descriptor)
        |             |
        +---------------------------------------------+
        | ... | page | page | ... | page | page | ... | => page cache
        +---------------------------------------------+
               ^|                          |
               ||                          +--> buffer page
               |+--> buffer page                ^
               |     ^                          |  +-------------+
               |     |                          +--| buffer head |
               |     |  +-------------+            +-------------+
               |     +--| buffer head |
               |        +-------------+
               |          |
               +----------+
                (@b_page)

      Managing the Buffer Heads :
        use alloc_buffer_head() to allocate a new buffer head object,and use free_buffer_head() to
        free it.
        use __getblk() to locate a buffer page in the page cache,if it is not exist,will attempts
        to create it automatically. /* increase @b_count automatically */
        if kernel control path stops accessing a block buffer,then should invoke __brelse() or
        __bforget() on it to decrease the correspondiung usage counter.

        ! routine get_bh() and put_bh() increase and decrease @b_count,respectively.


        <fs/buffer.c>
          /* bh_cachep - slab cache used to allocate buffer_head object */
          static struct kmem_cache *bh_cachep;

          /* max_buffer_heads - maximum number of buffer heads are allowed in the machine,
           *                    start stripping buffer heads in writeback if exceeded maximum
           *                    number
           */
          static int max_buffer_heads;

          /* buffer_heads_over_limit - buffer heads over limit indicator,TRUE or FALSE */
          int buffer_heads_over_limit;

          struct bh_accounting {
                  int nr;         /* number of live bh(s) */
                  int ratelimit;  /* limit cacheline bouncing */
          };

          static DEFINE_PER_CPU(struct bh_accounting, bh_accounting) = {0, 0};

          #define BH_LRU_SIZE 8

          struct bh_lru {
                  struct buffer_head *bhs[BH_LRU_SIZE];
          };

          /* per-CPU buffer head LRU(Least Recently Used)
           * # the same element in the array of a CPU can only appears once,but it
           *   might appears several times between several CPUs
           *   for each occurrence of a buffer head in the LRU block cache,
           *   @b_count member is increased by one
           */
          static DEFINE_PER_CPU(struct bh_lru, bh_lrus) = {{ NULL }};

          /**
           * alloc_buffer_head - allocate a buffer head
           * @gfp_flags:         GFP flags used for allocating
           * return:             buffer head pointer OR NULL
           * # EXPORT_SYMBOL
           * # this routine invoke kmem_cache_zalloc() on @bh_cachep with @gfp_flags,
           *   if succeed,init @b_assoc_buffers,increase per-CPU @bh_accounting.nr,next
           *   invoke recalc_bh_state(),the function checks whether buffer heads over limit
           *   finally,return pointer to caller
           * # note,this routine do not increase @b_count
           */
          struct buffer_head *alloc_buffer_head(gfp_t gfp_flags);

          /**
           * free_buffer_head - free a buffer head
           * @bh:               the buffer head
           * # EXPORT_SYMBOL
           * # kmem_cache_free()
           *   decrease per-CPU @bh_accounting.nr
           *   recalc_bh_state()
           * # it is a BUG,if @bh->b_assoc_buffers is not empty,we can only free
           *   a buffer head which is not mapped any block
           */
          void free_buffer_head(struct buffer_head *bh);

          /**
           * __getblk - locate(and,if necessary,create) the corresponding buffer_head 
           *            in page cache
           * @bdev:     block device descriptor
           * @block:    logical block number
           * @size:     size of block
           * # EXPORT_SYMBOL
           * # this routine might sleep,and it can not fail,automatically retry 
           *   if it have failed
           * # this routine may return a buffer_head represents non-existent block
           *   if @block is illegal
           * # the block buffer returned by this routine does not necessarily contain
           *   valid data,that the BH_Uptodate flag of the buffer head might be cleared
           * # steps -
           *     invoke __find_get_block() to get buffer head
           *     try slow path through __getblk_slow() if __find_get_block() returned NULL
           *     return buffer head pointer to caller
           */
          struct buffer_head *__getblk(struct block_device *bdev, sector_t block,
                                       unsigned size);

          /**
           * __find_get_block - find and get a buffer head corresponding to the specified
           *                    block
           * @bdev:             block device descriptor
           * @block:            logical block number
           * @size:             size of block
           * return:            buffer head pointer OR NULL
           * # EXPORT_SYMBOL
           * # this routine perform a page cache lookup for the matching buffer
           *   if it is there,refresh it in the LRU and mark it as accessed;otherwise,
           *   return NULL
           * # this routine first try lookup_bh_lru()
           *     lookup_bh_lru() - traverse current cpu's variable @bh_lrus,if such buffer head
           *                       is found,call get_bh() on it,then return it to caller
           *                                     # increase @b_count
           *                       # found: @b_bdev == @bdev AND @b_blocknr == @block AND @b_size == @size
           *   if lookup_bh_lru() failed,then try __find_get_block_slow();resulted in succeed,then install
           *   the buffer head into @bh_lrus
           *   before return to caller,if routine succeed to get such buffer head,then invoke
           *   touch_buffer() on it(actually,invoke mark_page_accessed())
           */
          struct buffer_head *__find_get_block(struct block_device *bdev, secotr_t block,
                                               unsigned size);

          /* __find_get_block_slow - slow path for __find_get_block() */
          static struct buffer_head *__find_get_block_slow(struct block_device *bdev, sector_t block);

          brief description for __find_get_block_slow() :
            get @bd_inode from @bdev,get the address space through @bd_inode(@i_mapping).
            invoke find_get_page() on the address space of this block device.
            /* index get from "@block >> (PAGE_CACHE_SHIFT - @bd_inode->i_blkbits)" */
            lock @priave_lock of the address space.
            checks whether the page is private(PagePrivate()),if the page has not buffers,then
            page_cache_release() it and return NULL to caller.
            if the page is private,then retrieve buffer head object from its @private member.
            next,traverse @b_this_page to find a buffer head object whose @b_blocknr is equal to @block;
            if find,then increase its @b_count,and return it to caller;otherwise,enter the "normal return
            path" - printk messages if some of the buffers on this page are not mapped,page_cache_release()
            the page got by find_get_page(),return the result to caller.              /* put page */
            /* @private_lock will be released before function return to caller */
            /* routine make use of a local variable @all_mapped to records whether there is some of
             * buffers on the page are not mapped.if all mapped but we failed to find the buffer head,
             * then have to printk error messages
             * the variable maybe reset to _zero_ during traversing,if buffer_mapped(current buffer head)
             * returned FALSE.
             */

          /**
           * __getblk_slow - slow path for __getblk()
           * @bdev:          block device descriptor
           * @block:         logical block number
           * @size:          size of block
           * return:         buffer head pointer OR NULL
           * # this routine may return NULL and dump stack if size is not multiple of hard sectorsize
           * # this routine make use for infinity for-cycle to get the wished buffer head object
           *     first,try __find_get_block(),this routine return a valid pointer if such buffer head
           *     is exist in the page cache of this block device and matched @block and @size
           *     __find_get_block() returned valid buffer head,then return it to caller
           *     otherwise,invoke grow_buffers()
           *       grow_buffers() - create buffers for the specified block device block's page
           *                        if that page was dirty,the buffers are set dirty also
           *                        # grow_buffers() calculate index of page in the page cache through
           *                          @size
           *                            sizebits = -1
           *                            do-while { sizebits++ until @size << sizebits >= PAGE_SIZE }
           *                            index = @block >> sizebits
           *                        # the creating is handled by grow_dev_page() - create the page-cache
           *                          that contains the requested block.
           *                          which invoke find_or_create_page() to find the page from page cache
           *                          or create a new cached page.
           *                          if the page has buffers,then get buffer head from @private.
           *                          checks its @b_size,if equal to the expected size,then call to init_page_buffers()
           *                          on the page,and return it to caller;otherwise,try_to_free_buffers() of the
           *                          page,return NULL to caller if try_to_free_buffers() failed.
           *                          if the page is newly created,it must has not buffers # find_or_create_page()
           *                             OR the page has not buffers # not a new page,and it is not buffer page
           *                             OR the page has buffers but size is different # not a new page,but splited
           *                          invoke alloc_page_buffers() with disabled retry on the page to create new buffers.
           *                          alloc_page_buffers() succeed : lock @private_lock of @i_mapping
           *                                                         set @b_this_page(link device buffers) of buffers
           *                                                         attach the head to page's @private
           *                                                         init page buffers(@b_bdev, @b_blocknr, ...)
           *                                                         return the page
           *                          alloc_page_buffers() failed  : unlock the page
           *                                                         page_cache_release() it
           *                                                         return NULL
           *                          ! alloc_page_buffers() invoke alloc_buffer_head() several times for
           *                            the new buffers
           *     if grow_buffers() returned a negative value,then return NULL to caller
           *     if grow_buffers() returned _zero_ then invoke free_more_memory() and enter next
           *     iteration                  # grow_dev_page() returned NULL
           *                                # grow_buffers() do not returns the page to __getblk_slow(),
           *                                  the newly established mapping will be found in next
           *                                  __find_get_block()
           */
          static struct buffer_head *__getblk_slow(struct block_device *bdev, sector_t block, int size);

          /**
           * __brelse - put buffer head if @b_count is greater than _zero_
           * # EXPORT_SYMBOL
           * # wrapper : brelse() in <linux/buffer_head.h>
           */
          void __brelse(struct buffer_head *buf);

          /**
           * __bforget - forget a buffer
           *             clear buffer dirty
           *             delete and init @b_assoc_buffers if @b_assoc_map is not NULL,and set
           *             @b_assoc_map to NULL
           *             __brelse() @bh
           * # EXPORT_SYMBOL
           * # by marks the buffer as clean to forcing the kernel to forget any change
           *   in the buffer that has yet to be written to disk
           * # wrapper : bforget() in <linux/buffer_head.h>
           * # this routine invoke __brelse() on @bh at the end
           */
          void __bforget(struct buffer_head *bh);
            
      Buffer Pages :
        two common cases in which the kernel creates buffer pages -
          1> when reading or writing pages of a file that are not stored in contiguous disk blocks.
             this happens either because the filesystem has allocated noncontiguous blocks to the
             file,or because the file contains holes.
          2> when accessing a single disk block(super block or inode block). /* block device buffer pages */
             ! a strong constraint :
                 all the block buffers must refer to adjacent blocks of the underlying block device.

        all block buffers in a single block buffer page,must have the same size.for 80x86 architecture,the
        page size usually is 4096 bytes,and the size of a single sector of block device usually is 512 bytes,
        hence,a single block buffer page can include [1, 8] block buffers. /* depends on block size */

        member @b_this_page is the list of buffer heads,for traverse the block buffers of a single block
        buffer page.we can get the buffer head from page's @private member,then traverse @b_this_page until
        @b_this_page is equal to the head.
        /* if a page is a buffer page,PagePrivate() must returns TRUE */

        suppose block size is 1024 bytes,and page size is 4096 bytes -
        
                +-------------------------------------------------------------+
                |              |               |              |               |
                | first buffer | second buffer | third buffer | fourth buffer | => a single buffer page
                |              |               |              |               |
           +--> +-------------------------------------------------------------+
           |      ^ @b_data           ^                   ^               ^
           |      |                   |                   |               |                   
           |      |                   |                   |               +---+               +--> @b_this_page
           |      |                   |                   |                   |               |
           +----- +-------------+---->+-------------+---->+-------------+---->+-------------+----> the first 
         @b_page  | buffer_head |     | buffer_head |     | buffer_head |     | buffer_head |      buffer_head
                  +-------------+ <-+ +-------------+     +-------------+     +-------------+
                                    |
                                    |
                  @page.private ----+

      Allocating Block Device Buffer Pages :
        kernel allocate a new block device buffer page when it discovers that the page cache does not
        include a page containing the buffer for a given block.
        the lookup operation on radix-tree of block device might fail because -
          1> no such buffer page
             kernel create a new buffer page and insert it into radix tree
          2> it is not a buffer page
             kernel create a new buffer head and link it to the page
          3> page have been splited in blocks of size different from the size of requested block
             kernel have to release the old heads,and allocate a new set of heads,then link them
             to the page

        the grow_buffers() routine :
          as description as above,grow_buffers() is called in slow path of __getblk() that such
          buffer page is not exist in radix tree.

          <fs/buffer.c>
            /**
             * grow_buffers - create buffers for the specified block device block's page
             * @bdev:         block device descriptor
             * @block:        logical block number
             * @size:         block size
             * return:        1 => succeed
             *                0 / error code => failed
             * ! this routine will return -EIO if the index of the block is outside the maximum
             *   possible page cache index
             */
            static int grow_buffers(struct block_device *bdev, sector_t block, int size);

            /**
             * alloc_page_buffers - create the appropriate buffers when given a page for data area
             *                      and the size of each buffer
             * @page:               data area
             * @size:               buffer size,it could be the block size
             * @retry:              shall we will retry if failed
             * return:              buffer head object of the head buffer OR NULL
             * # EXPORT_SYMBOL                     
             */
            struct buffer_head *alloc_page_buffers(struct page *page, unsigned long size, int retry);

            brief description for alloc_page_buffers() :
              we have known that,it will be called by grow_dev_page() in the slow path of __getblk().
              this routine is called when kernel must create buffer page.
              if @size is equal to PAGE_SIZE,there only a single buffer;for 80x86,the number of buffers
              in @page has range [1, 8](4096 bytes -- 512 bytes).
              creation is processed in a while-cycle,stop condtion is no remain space in @page.
              new buffer_head is created through alloc_buffer_head() with GFP flag GFP_NOFS.
              if failed on create new buffer head,jump to "no_grow" label.
              for each new buffer head 
                @b_bdev = NULL
                @b_this_page - the first buffer head's @b_this_page is NULL,the second's is the first,
                               and so on
                @b_blocknr = -1
                @b_state = 0
                @b_count = 0
                @b_private = NULL
                @b_size = @size
                @b_data = current offset

                /* these two members are setup in init_buffer() */
                @b_end_io = NULL
                @b_private = NULL
              if no error happened,routine return the first buffer head's address to caller;otherwise,
              enter "no_grow" path.

              "no_grow" :
                because we have encountered error during allocating new buffer heads,thus we must
                release all the buffer heads we have allocated previously.this UNDO is processed rely
                on @b_this_page and routine free_buffer_head().
                if @retry is FALSE,then routine return NULL to caller.
                if @retry is TRUE,then invoke free_more_memory() and restart from the while-cycle.
                /**
                 * <fs/buffer.c>
                 *   static void free_more_memory(void);
                 *   - kick the writeback threads then try to free up some ZONE_NORMAL memory.
                 *   # in alloc_page_buffers(),free more memory and try again.
                 */

            ! as the description,we can understand that no more page is allocated,just split the page
              with several buffer head objects.

      Releasing Block Device Buffer Pages :
        block device buffer pages are released when kernel tries to get additional free memory.
        if a buffer page contains dirty or locked buffers,then the page can not be freed.
        routine try_to_release_page() is used to handle free buffer page.

        <mm/filemap.c>
          /**
           * try_to_release_page - release old fs-specific metadta on a page
           * @page:                the page
           * @gfp_mask:            memory allocation flags(and I/O mode)
           * return:               1 => released
           *                       0 => failed
           * # EXPORT_SYMBOL
           * # @gfp_mask specifies whether I/O may be performed to release this page(__GFP_IO),
           *   and whether the call may block(__GFP_WAIT & __GFP_FS)
           * # if PG_fscache is set on @page,the page is known to the local caching routines
           */
          int try_to_release_page(struct page *page, gfp_t gfp_mask);

          brief description for try_to_release_page() :
            get address space object of @page.
            do BUG checks,if @page is unlocked,then it is a BUG.
            checks PG_writeback,return _zero_ to caller if this @page is under writeback,we
            can not free it until writeback have accomplished.
            if this @page is cached,then its @mapping must not be NULL,if @a_ops->releasepage
            have been implemented,then call to it and return the returned value to caller.
            otherwise,invoke generic buffer release routine try_to_free_buffers() on @page,
            return the returned value to caller.

        <fs/buffer.c>
          /**
           * try_to_free_buffers - try to free buffers on a given page
           * @page:                the buffer page
           * return:               1 => succeed
           *                       0 => failed
           * # EXPORT_SYMBOL
           */
          int try_to_free_buffers(struct page *page);

          brief description for try_to_free_buffers() :
            first,do the same checking as try_to_free_page(),if unlocked page or writeback page
            report BUG and return 0 to caller,respectively.
            if @mapping of page is NULL,that means this page is not cached,just invoke
            drop_buffers() and jump to label "out".
            if this @page have cached,then acquire @private_lock and invoke drop_buffers().
            next,checks the return value from drop_buffers(),if it is TRUE,then invoke
            cancel_dirty_page(),this routine test dirty flag of @page,if it is setup,then clear
            it,and up to date some account data if @mapping is not NULL.
            before enter "out" - the normal exit path,release @private_lock.

            "out" :
              local variable named @buffers_to_free is type of struct buffer_head *,its address
              as the second argument passed into drop_buffers().routine drop_buffers() traverse
              and check the buffers on @page,ollect the buffers by set @buffers_to_free points
              to the head buffer_head object.
              at the end of try_to_free_buffers(),if @buffers_to_free is not NULL,then make use
              of a do-while cycle to iterate the buffers,for each buffer head object,invoke
              free_buffer_head() on it.
              finally,returns the return value of drop_buffers() to caller.

          /**
           * drop_buffers - check and collect the buffers on a buffer page that is going to be freed
           * @page:         the page
           * @buffers_to_free:
           *                make use of the link-list @b_this_page to collect the buffers
           * return:        1 => succeed
           *                0 => failed
           * # this routine process two do-while cycles(1st -> state check, 2nd -> another mapping check)
           *   the first do-while :
           *     traverse buffers on @page
           *     if current buffer head state indicates write I/O error AND the @page is cached,
           *     then set_bit() AS_EIO on @page->mapping->flags
           *     next check if current buffer head is busy,that the buffer's usage ref count is
           *     greater than 1 OR it is dirty or locked,then return 0 to caller
           *   after the first do-while completed,enter the second do-while :
           *     traverse buffers on @page
           *     if current buffer have associated with another mapping,invoke __remove_assoc_queue()
           *     on it,this routine will unlink this buffer and the another mapping;if the buffer
           *     is write I/O error,set_bit() AS_EIO in the another mapping's @flags,finally,set
           *     @b_assoc_map of this buffer to NULL
           *     # list_del_init() is called on current buffer head's @b_assoc_buffers
           *   after the second do-while completed,set *@buffers_to_free to the head buffer_head,
           *   invoke __clear_page_buffers() on @page,return 1 to caller
           *          # defined in the same file,the routine clear PG_private,clear @private of @page,
           *            invoke page_cache_release() on @page
           */
          static int drop_buffers(struct page *page, struct buffer_head **buffers_to_free);

      Searching Blocks in the Page Cache :
        routines that are used for searching blocks in the page cache have been described above(Managing
        the Buffer Heads).

        the entry point is __getblk(),fast path for __getblk() is __find_get_block(),and the slow paths
        for __getblk() are __find_get_block_slow() OR __getblk_slow().

        the page index of a cached page of a block deivce which mapping a block is encoded through the
        logical block number.
        /* @bdev->bd_block_size,a bit shift operation on the logical block number.for example,suppose
         * the @bd_block_size is 1024 bytes,and page size is 4096 bytes,then the index is logical block
         * number / 4
         */

        <fs/buffer.c>
          /**
           * __bread - read a specified block and return the buffer head
           * @bdev:    the block device to read from
           * @block:   logical block number
           * @size:    size(in bytes) to read
           * return:   buffer head object pointer OR NULL if the block was unreadable
           * # EXPORT_SYMBOL
           * # this routine first try __getblk()
           *   if the buffer is not NULL AND it has not up to date(BH_Uptodate),then
           *   invoke __bread_slow(),return the buffer head get from __bread_slow() to caller
           *   otherwise,return the buffer head get from __getblk() to caller
           * # buffer head is not NULL,but BH_Uptodate is cleared,then must read the block
           *   from disk(not sync)
           */
          struct buffer_head *__bread(struct block_device *bdev, sector_t block, unsigned size);

          /**
           * __bread_slow - slow path for __bread()
           * @bh:           buffer head
           * return:        updated buffer head OR NULL
           * # this routine process the following steps -
           *     1> lock buffer @bh
           *     2> if BH_Uptodate is set,then unlock buffer and return @bh
           *     3> BH_Uptodate cleared,then
           *          invoke get_bh() to increase ref count
           *          set @b_end_io of @bh to end_buffer_read_sync()
           *                                  # this routine will invoke
           *                                    __end_buffer_read_notouch() and put_bh()
           *                                  # __end_buffer_read_notouch() -
           *                                      set OR clear BH_Uptodate and unlock
           *                                      the buffer
           *          invoke submit_bh() on @bh with action READ,this routine will
           *          makeup a bio from @bh and call to sumbit_bio() submit the
           *          requestion to Generic Block Layer
           *          invoke wait_on_buffer() # wait until BH_Lock is cleared in @bh->b_state
           *          # this routine invoke __wait_buffer() if @bh have locked OR
           *            its ref count is _zero_
           *            __wait_buffer() finally endup at __wait_on_bit_lock(),thus
           *            BH_Lock will be set after waiting have completed(that means
           *            the buffer have been locked by this kernel control path)
           *          after the kernel control path awaken,checks again BH_Uptodate,
           *          if it is set,then return @bh to caller
           *     4> even submitted a new bio,it stay BH_Uptodate is cleared(not sync),
           *        brelse() @bh and return NULL to caller
           */
          static struct buffer_head *__bread_slow(struct buffer_head *bh);

      Submitting Buffer Heads to the Generic Block Layer :
        kernel routines submit_bh() and ll_rw_block() are used to submit buffer heads
        to Gernric Block Layer,that is allow the kernel to start an I/O data transfer on
        one or more buffers described by their buffer heads.

        The submit_bh() function :
          /* this routine is also called in the slow path for __bread() */

          <fs/buffer.c>
            /**
             * submit_bh - submit a buffer to Generic Block Layer to start I/O
             *             data transfer
             * @rw:        data direction READ / WRITE
             * @bh:        buffer head
             * return:     0 OR error code
             * # EXPORT_SYMBOL
             */
            int submit_bh(int rw, struct buffer_head *bh);

            brief description for submit_bh() :
              this routine process BUG checking immediately when start processing.
              it is a BUG if 
                buffer unlocked
                buffer is not a valid mapping
                @b_end_io is NULL
                buffer is delayed
                BH_Delay is set
                BH_Unwritten is set
              if @bh has set BH_Ordered AND @rw enabled WRITE,then enable WRITE_BARRIER in @rw.
              next,check if test_set_buffer_req() AND @rw enabled WRITE,then clear BH_Write_EIO.
              /* test_set_buffer_req() => test_and_set_bit() BH_Req in @bh->b_state */
              /* if this buffer have been submitted for I/O then clear out write error when
               * rewriting
               * BH_Req is used to record that the buffer head have been submitted.
               */
              proceed executing,function invoke bio_alloc() with gfp flag GFP_NOIO and 1 vector
              to create a bio object.
              init the @bio from @bh {
                      bi_sector = @b_blocknr * (@b_size >> 9)
                      bi_bdev = @b_bdev
                      bi_io_vec[0].bv_page = @b_page
                      bi_io_vec[0].bv_len = @b_size
                      bi_io_vec[0].bv_offset = @b_data & ~PAGE_MASK /* bh_offset() */
              }
              init the remains of @bio {
                      bi_vcnt = 1
                      bi_idx = 0
                      bi_size = @bh->b_size
                      bi_end_io = end_bio_bh_io_sync
                      /**
                       * called when I/O completed
                       * end_bio_bh_io_sync() - get @bh from @bi_private,set some error bits
                       *                        according to the parameter @err
                       *                        set BH_Quiet if BIO_QUIET is set
                       *                        invoke @b_end_io() of @bh with parameter 
                       *                          @uptodate = test_bit(BIO_UPTODATE, @bi_flags)
                       *                        invoke bio_put()
                       */
                      bi_private = @bh
              }
              invoke bio_get() on the @bio to increase ref count,and call to submit_bio(@rw, @bio)
              to submit this new bio object.
              finally,check the result,if @bio is flagged BIO_EOPNOTSUPP,then the return value will be
              -EOPNOTSUPP,otherwise,it is _zero_.
              invoke bio_put() on @bio to decrease ref count and return the return value to caller.
              /**
               * <block/blk-core.c>
               *   void submit_bio(int rw, struct bio *bio);
               *   - this routine process some checkings,update some account data,and invoke
               *     generic_make_request() to access Generic Block Layer for submit this @bio
               *   - per-CPU variable named @vm_event_states is type of struct vm_event_state,
               *     the structure has a member is type of array of unsigned long named @event
               *     in the header <linux/vmstat.h> defined a lot of values of enumerate type
               *     as the index of @event.
               *     if data direction is WRITE,then add bio_sectors(@bio) to @event[PGPGOUT];
               *     otherwise,add bio_sectors(@bio) to @event[PGPGIN],besides,update task io
               *     read account with @bi_size
               */

        The ll_rw_block() function :
          this routine have been flagged with "DEPRECATED",it is a low-level routine for
          access to block devices.

          <fs/buffer.c>
            /**
             * ll_rw_block - low-level entry point for access to block devices [DEPRECATED]
             * @rw:          data direction
             * @nr:          number of buffers
             * @bhs:         buffer head pointer array
             * # EXPORT_SYMBOL
             * # this routine traverse @bhs and invoke submit_bh() if the buffer satisfied
             *   some requestions -
             *     steps :
             *       get next buffer head @bh
             *       if @rw is SWRITE OR SWRITE_SYNC OR SWRITE_SYNC_PLUG then lock @bh
             *       else if trylock_buffer() on @bh failed,then continue to next iteration
             *       next,check if @rw is write direction(WRITE, SWRITE, SWRITE_SYNC, SWRITE_SYNC_PLUG)
             *       then
             *         @bh is dirty => clear dirty state
             *                         set @b_end_io to end_buffer_write_sync()
             *                         get_bh() on @bh to increase ref count
             *                         invoke submit_bh() on @bh,rw indicator depends on @rw,
             *                         WRITE_SYNC or WRITE
             *                         continue to next iteration without unlock this @bh
             *         # if @rw request writing but @bh is not dirty,we do not submit it
             *       if @rw is read
             *       then
             *         BH_Uptodate is cleared in @bh => set @b_end_io to end_buffer_read_sync()
             *                                          get_bh() on @bh to increase ref count
             *                                          invoke submit_bh(),the rw indicator is @rw
             *                                          continue to next iteration without unlock @bh
             *       invoke unlock_buffer() at the end of iteration
             *       # if @rw is write direction,we lock buffer at the starting of iteration,and
             *         unlock it when @bh is not dirty,in this case,we do not need to writeback it
             *         to disk
             *         if @rw is read direction,we try to lock buffer also at the starting,if failed,
             *         we must give up to READ for this @bh and get into next iteration(someone did this
             *         or someone is using it)
             *         @bh will be unlocked if t is in BH_Uptodate state,that means we do not need
             *         to submit it for READ from disk
             */
            void ll_rw_block(int rw, int nr, struct buffer_head *bhs[]);

    Writing Dirty Pages to Disk :
      Unix systems allow the deferred writes of dirty pages into block devices,because this
      noticeably improves system performance.several write operations on a page in cache
      could be satisfied by just one slow physical update of the corresponding disk sectors.
      /* read operation is more critical than write operation
       * by use of delay writing,physical block devices will service,on the average,
       * many more read requests than write ones.
       */

      ! a dirty page might stay in main memory until the last possible moment - system shutdown.
        but there may happens some problem -
          > power supply failure occurs,updated contents will lost
          > size of page cache would have to be huge - at least as big as the size of the
            accessed block devices

      conditions for pages are flushed to disk :
        1> page cache gets too full and more pages are needed,or the number of
           dirty pages becomes too large
        2> too much time has elapsed since a page has stayed dirty
        3> a process requests all pending changes of a block device or of a particular file
           to be flushed(sync(), fsync(), fdatasync() system calls)

      PG_dirty on buffer page -
        the PG_dirty flag of the buffer page should be set if at least one of associated
        buffer heads has the BH_Dirty flag set.
        when kernel selected the PG_dirty buffer page to be flushed,it can traverse the buffers
        on the page,and only flush the buffers have BH_Dirty is set.
        after kernel flushed dirty contents,it will clear the PG_dirty flag of the page.

      The pdflush Kernel Threads :
        earlier versions of Linux used a kernel thread called "bdflush" to systematically scan
        the page cache looking for dirty pages to flush,and they used a second kernel thread
        called "kupdate" to ensure that no page remains dirty for too long.
        since Linux 2.6,they are replaced with a group of general purpose kernel threads called
        "pdflush".

        the number of pdflush threads in the system is dynamically adjusted -
          new threads are created when they are too few and existing threads are killed when
          they are too many.
          /* these kernel threads can block */

        create and kill rules :
          1> there must be at least two "pdflush" kernel threads and at most eight.
          2> if there were no idle "pdflush" during the last second,a new "pdflush" should
             be created.
          3> if more than one second elapsed since the last "pdflush" became idle,a "pdflush"
             should be removed.

        !! LINUX 2.6.34.1 HAVE HAND OVER THE "WRITEBACK" TO BACKING DEVICE.IN <mm/backing-dev.c>,
           FOR EACH REGISTERED BDI,VM WILL START A KERNEL THREAD,THE NAME FORMAT IS
           "bdi-$device_name",THREAD FUNCTION IS bdi_forker_task().
           AND IN FUNCTION bdi_forker_task(),IT MIGHT START A NEW KERNEL THREAD HAS THE NAME FORMAT
           "flush-$device_name",THREAD FUNCTION IS bdi_start_fn().
           THE WRITEBACK IS HANDLED BY FUNCTION bdi_writeback_task(),WHICH WILL CALLED BY
           bdi_start_fn().
           THUS,NO STRUCTURE struct pdflush_work IS DEFINED IN LINUX 2.6.34.1 .


        <linux/backing-dev.h>
          /**
           * backing_dev_info - low-level device information
           * @bdi_list:         linked to global registered bdi list
           * @rcu_head:         updating
           * @ra_pages:         max readahead in PAGE_CACHE_SIZE units
           * @state:            bitmap for bdi state,operate it with atomic bitops
           *                    # bit 0 - BDI_pending          => being activated
           *                      bit 1 - BDI_wb_alloc         => default embedded wb allocated
           *                      bit 2 - BDI_async_congested  => async write queue is getting full
           *                      bit 3 - BDI_sync_congested   => sync queue is getting full
           *                      bit 4 - BDI_registered       => registered
           *                      bit 5 - BDI_unused           => unused bit
           * @capabilities:     device capabilities
           * @congested_fn:     function pointer if device is md/dm
           *                    # typedef int (congested_fn)(void *, int);
           * @congested_data:   pointer to aux data for congested func
           * @unplug_io_fn:     unplug I/O function
           * @unplug_io_data:   data for @unplug_io_fn
           * @name:             name
           * @bdi_stat:         per-CPU variable for bdi stat
           *                    # BDI_RECLAIMABLE - 0
           *                      BDI_WRITEBACK   - 1
           * @completions:      floting proportions for completions
           * @dirty_exceeded:   is threshold of dirty data has exceeded
           * @min_ratio:        minimum ratio for dirty data
           * @max_ratio:        maximum ratio for dirty data
           * @max_prop_frac:    maximum fraction for proportion
           * @wb:               default writeback info for this bdi
           * @wb_lock:          protects update side of @wb_list
           * @wb_list:          the flusher threads hanging off this bdi
           * @wb_mask:          bitmask of registered tasks
           * @wb_cnt:           number of registered tasks
           * @work_list:        collection of bdi works
           * @dev:              device descriptor
           * @debug_dir:        filesystem debug dir
           * @debug_stats:      filesystem debug stats
           */
          struct backing_dev_info {
                  struct list_head bdi_list;
                  struct rcu_head rcu_head;
                  unsigned long ra_pages;
                  unsigned long state;
                  unsigned int capabilities;
                  congested_fn *congested_fn;
                  void *congested_data;
                  void (*unplug_io_fn)(struct backing_dev_info *, struct page *);
                  void *unplug_io_data;

                  char *name;

                  struct percpu_counter bdi_stat[NR_BDI_STAT_ITEMS];
            
                  struct prop_local_percpu completions;
                  int dirty_exceeded;

                  unsigned int min_ratio;
                  unsigned int max_ratio, max_prop_frac;

                  struct bdi_writeback wb;
                  spinlock_t wb_lock;
                  struct list_head wb_list;
                  unsigned long wb_mask;
                  unsigned int wb_cnt;
                  
                  struct list_head work_list;

                  struct device *dev;

          #ifdef CONFIG_DEBUG_FS
                  struct dentry *debug_dir;
                  struct dentry *debug_stats;
          #endif
          };


        <linux/writeback.h>
          /* number of pdflush threads,this global variable is exported to sysctl read-only */
          extern int nr_pdflush_threads;

          /* sync modes for writeback */
          enum writeback_sync_modes {
                  WB_SYNC_NONE,     /**
                                     * do not wait on anything
                                     * locked buffer(page) are simply skipped
                                     */
                  WB_SYNC_ALL,      /**
                                     * wait on every mapping - buffer
                                     * if a locked buffer(page) is encountered,
                                     * must be waited upon and not just skipped over
                                     */
                                    
          };

          /**
           * writeback_control - structure used for writeback work control,the control
           *                     structure which tells the writeback code what to do
           * @bdi:               bdi of the block device,if !NULL,only write back
           *                     this queue
           * @sb:                super block of file system,if !NULL,only write
           *                     inodes from this super_block
           * @sync_mode:         writeback sync mode
           * @older_than_this:   if !NULL,only write back inodes older than this
           *                     # that is the pages dirtied before @older_than_this,
           *                       because time is go forward,the recent time point
           *                       is newer,and time point less than the newer is older
           *                       time point
           * @wb_start:          time writeback_inodes_wb() was called,this is needed
           *                     to avoid extra jobs and livelock
           * @nr_to_write:       write this many pages,and decrement this for each page
           *                     written
           * @pages_skipped:     pages which were not written
           * @range_start:       hint for a_ops->writepages() to start at this position(byte)
           * @range_end:         hint for a_ops->writepages() to end at this position(byte)
           * @nonblocking:       do not get stuck on request queues
           * @encountered_congestion:
           *                     an output: a queue is full
           * @for_kupdate:       a kupdate writeback
           * @for_background:    a background writeback
           * @for_reclaim:       invoked from the page allocator,reclaim pages
           * @range_cyclic:      @range_start is cyclic
           * @more_io:           more I/O to be dispatched
           * @no_nrwrite_index_update:
           *                     write_cache_pages() would not update wbc->nr_to_write
           *                     and mapping->writeback_index if @no_nrwrite_index_update
           *                     is set
           *                     # write_cache_pages() may write more than we requested
           *                       and we want to make sure @nr_to_write and @writeback_index
           *                       are updated in a consistent manner so we use a single
           *                       control to update them
           * # un-specified fields are initialized to _zero_
           */
          struct writeback_control {
                  struct backing_dev_info *bdi;
                  struct super_block *sb;
                  enum writeback_sync_modes sync_mode;
                  unsigned long *older_than_this;
                  unsigned long wb_start;
                  long nr_to_write;
                  long pages_skipped;
                  loff_t range_start;
                  loff_t range_end;
                  unsigned nonblocking:1;
                  unsigned encountered_congestion:1;
                  unsigned for_kupdate:1;
                  unsigned for_background:1;
                  unsigned for_reclaim:1;
                  unsigned range_cyclic:1;
                  unsigned more_io:1;
                  unsigned no_nrwrite_index_update:1;
          };

          ! kupdate style :
              periodic checking,the structure bdi_writeback has a member named @last_old_flush.
              which might be reset to current @jiffies in wb_check_old_data_flush().
              if "@last_old_flush + ms_to_jiffies(@dirty_writeback_interval * 10)" is time after
              current @jiffies,then reset @last_old_flush to @jiffies,get number of pages have to
              be writeback through -
                /* dirty file pages */             /* unstable nfs pages */
                global_page_state(NR_FILE_DIRTY) + global_page_state(NR_UNSTABLE_NFS) +
                (inodes_stat.nr_inodes - inodes_stat.nr_unused)
                 /* inodes in icache */  /* inodes in icache is in unused */

                ! zone based page accounting with per CPU differentials.
                  # global_page_state() - retrieve zone accounting
                ! @inodes_stat.nr_unused-- when __iget() is called successfully.
              then trigger wb_writeback() again for the kupdated writeback.
              /*
               * writeback arg :
               *   nr_pages = result of the expression
               *   sb = unspecified,this will let wb_writeback() match all inodes
               *   sync_mode = WB_SYNC_NONE # async writeback
               *   for_kupdate = 1
               *   range_cyclic = 1
               *   for_background = 0 # we want to trigger proactive writeback
               *
               * !! @nr_pages IS THE REQUEST NUMBER OF PAGES SHOULD TO BE WRITEBACK,
               *    BUT THE writeback_control OBJECT CREATED BY wb_writeback() DO NOT
               *    RELY ON @nr_pages,AND WHICH ONLY WRITEBACK INODES IN THE DISPATCH
               *    QUEUE OF THE GIVEN bdi_writeback OBJECT.
               *    THUS EVEN @nr_pages IS GREATER THAN THE NUMBER OF INODES IN @b_io,
               *    wb_writeback() DO NOT WRITEBACK THE ADDITIONAL INODES.
               *    IN wb_writeback(),IF @nr_pages <= 0,WRITEBACK WOULD STOP,BECAUSE
               *    THERE IS NO MORE DIRTY PAGES.
               *    # NUMBER OF PAGES TO BE WRITEBACK IN ONCE ITERATION OF wb_writeback()
               *      IS MAX_WRITEBACK_PAGES = 1024
               */
              /* wb_writeback() is called by wb_do_writeback() for each bdi work in the work
               * list.
               * wb_check_old_data_flush() is called before wb_do_writeback() return.
               * that is kernel will does periodic checking after current bdi work all
               * been accomplished.
               */

          ! structure block_device has a member named "bd_super",which is type of
            struct super_block
            structure super_block has a member named "s_bdi",which is type of
            struct backing_dev_info
            structure backing_dev_info has a member named "wb",it is type of
            struct bdi_writeback
          ! backing_dev_info object initialized through bdi_init(),the members
            wb_mask and wb_cnt all set to 1 - just one thread support for now
            # wb_mask => bitmask of registered tasks
              wb_cnt  => number of registered tasks(wb threads)
            bdi_writeback object initialized through bdi_wb_init(),but @task
            member is setup later in bdi_register()
            /* bdi_register_dev() is called in add_disk() */

        <linux/backing-dev.h>
          /**
           * bdi_writeback - writeback structure contains some info about
           *                 the writeback thread of a bdi
           * @list:          hangs off the bdi(linked into @bdi->wb_list)
           *                 # @bdi->wb_list : the flusher threads hang of this @bdi
           * @bdi:           parent bdi
           * @nr:            bit number used by get_next_work_item() to checks
           *                 the next bdi_work item registered in the bdi
           *                 whether been processed by this wb thread
           *                 # for gendisk,its initial value is zero,because
           *                   alloc_disk() get a new gendisk object with
           *                   GFP flag __GFP_ZERO
           * @task;          writeback task
           *                 # usually is "bdi_forker_task" or "bdi_start_fn"
           * @b_dirty:       dirty inodes
           * @b_io:          parked for writeback - dispatch list
           *                 # in the page writeback routines,they usually check
           *                   whether @b_io is become empty
           *                   if an inode is not dirty after flushing,it will
           *                   be moved into @inode_in_use
           *                   @b_dirty is the collection of dirty inode of "this"
           *                   bdi,the connector is @i_list the member of inode
           *                   when an inode become dirty,it will be marked,and
           *                   moved into the list @b_dirty,the connector also
           *                   is the @i_list member
           * @b_more_io:     parked for more writeback
           * # lock @wb_lock of backing_dev_info object is used to protect
           *   its @wb member
           * # @task is woke up when a new bdi_work have queued
           */
          struct bdi_writeback {
                  struct list_head list;
                  struct backing_dev_info *bdi;
                  unsigned int nr;
                  unsigned long last_old_flush;
                  struct task_struct *task;
                  struct list_head b_dirty;
                  struct list_head b_io;
                  struct list_head b_more_io;  
          };

        ! kernel have introduced two different source files for writeback -
            <fs/fs-writeback.c> : contains all the functions related to writing
                                  back and waiting upon dirty inodes against
                                  superblocks,and writing back dirty pages against
                                  inodes
                                  /* normal case */
            <mm/page-writeback.c> : contains functionsrelated to writing back
                                    dirty pages at the address_space level
                                    /* focus on "laptop mode" */

      file system writeback :
        the source file <fs/fs-writeback.c> contains implementations of file system
        writeback mechanism.actually,a lot of block devices are hard disks,and many
        filesystems based on physical disk,so fs-writeback is the normal case.

        <fs/fs-writeback.c>
          /**
           * wb_writeback_args - arguments passed into wb_writeback(),essentially a
           *                     subset of writeback_control
           * @nr_pages:          number of pages need to be writeback
           * @sb:                the relative super block
           * @for_kupdate:       kupdate style writeback - periodic writeback
           * @range_cyclic:      only writeback specified range
           * @for_background:    writeback only the background dirty thresh have overed
           */
          struct wb_writeback_args {
                  long nr_pages;
                  struct super_block *sb;
                  enum writeback_sync_modes sync_mode;
                  int for_kupdate:1;
                  int range_cyclic:1;
                  int for_background:1;
          };

          /**
           * bdi_work - work item for the bdi_writeback threads
           * @list:     pending work list
           * @rcu_head: work RCU
           * @seen:     threads that have seen this work
           * @pending:  number of threads still to do work
           *            # actually,it is initalized to @wb_cnt of the
           *              bdi
           * @args:     writeback arguments
           * @state:    flag bits(WS_*)
           *            # WS_USED_B = 0
           *              WS_ONSTACK_B = 1
           *              WS_USED = 1 << WS_USED_B
           *              WS_ONSTACK = 1 << WS_ONSTACK_B
           */
          struct bdi_work {
                  struct list_head list;
                  struct rcu_head rcu_head;
                  unsigned long seen;
                  atomic_t pending;
                  struct wb_writeback_args args;
                  unsigned long state;
          };

          /**
           * bdi_work_init - initialize a bdi work
           * # INIT_RCU_HEAD(&@work->rcu_head)
           *   @work->args := args
           *   @work->state := WS_USED
           */
          static inline void bdi_work_init(struct bdi_work *work, struct wb_writeback_args *args);

          /**
           * bdi_work_free - free a bdi work
           * # this routine get bdi_work object through @head
           *   if the work is on stack,then invoke kfree() to free it
           *   otherwise,invoke bdi_work_clear() to free it
           */
          static void bdi_work_free(struct rcu_head *head);

          /**
           * bdi_work_clear - clear a bdi work which is not on stack
           * # this routine first clear WS_USED_B bit of @work->state,change
           *   state to unused
           *   then process a SMP memory barrier through smp_mb_after_clear_bit()
           *   finally,invoke wake_up_bit() to wake up processes that waiting
           *   for WS_USED_B of @work->state to be cleared
           */
          static void bdi_work_clear(struct bdi_work *work);

          /**
           * writeback_in_progress - determine whether there is writeback in progress
           * @bdi:                   the device's backing_dev_info structure
           * return:                 1 => in progress
           *                         0 => not in progress
           * # this routine actually check list_empty() for @bdi->work_list
          int writeback_in_progress(struct backing_dev_info *bdi);

        Queue a bdi work :

          /**
           * bdi_queue_work - queue a bdi work into backing device
           * @bdi:            the device's backing_dev_info
           * @work:           bdi work
           */
          static void bdi_queue_work(struct backing_dev_info *bdi, struct bdi_work *work);

          brief description for bdi_queue_work() :
            this routine set @work->seen to @bdi->wb_mask when at the starting.
            if @work->seen is _zero_,that must be a BUG.
            then set @work->pending to @bdi->wb_cnt,of course,it also is a BUG if
            @bdi->wb_cnt is _zero_.
            if no BUG detected,then acquire @bdi->wb_lock.
            invoke list_add_tail_rcu() to link @work->list at the tail of @bdi->work_list.
            /* list_add_tail_rcu() contains the necessary barriers to make sure the
             * above stores are seen before the item is noticed on the list
             */
            next,release the lock and check @bdi->wb_list whether is empty.
            if it is empty,that means the default thread is not there,thus must add it.
            this is completed by wake up process @default_backing_dev_info.wb.task .
            if default thread been there,then we can simply wake up the task of the
            bdi_writeback object of @bdi if task have specified,that is @bdi->wb->task is
            not NULL.

          /**
           * bdi_alloc_queue_work - allocate and initialize a new bdi work,then queue it
           * @bdi:                  the device's backing_dev_info
           * @args;                 writeback arguments
           * # this routine make use of kmalloc() with GFP flag GFP_ATOMIC to allocate
           *   new bdi work
           *   if new work get allocated,then init it through bdi_work_init(),queue it
           *   through bdi_queue_work()
           *   else,wake up @bdi->wb->task if @task have specified
           *        # wake up the thread for old dirty data writeback(WB_SYNC_NONE writeback)
           */
          static void bdi_alloc_queue_work(struct backing_dev_info *bdi,
                                           struct wb_writeback_args *args);

        The bdi_lock,bdi_list,and bdi_pending_list :
          <mm/backing-dev.c>
            /* bdi_lock - lock protect @bdi_list and @bdi_pending_list */
            DEFINE_SPINLOCK(bdi_lock);

            /* bdi_list - global list collect registered and active bdi's */
            LIST_HEAD(bdi_list);

            /* bdi_pending_list - global list collect registered bdi's each of them has
             *                    pending works
             */
            LIST_HEAD(bdi_pending_list);

            ! if the bdi in @bdi_list has pending work,it will be moved into @bdi_pending_list,
              and get back into @bdi_list later by bdi_start_fn().


        Start writeback :
          /**
           * bdi_sync_writeback - start and wait for writeback
           * @bdi:                the bdi
           * @sb:                 super block
           * # this routine create new bdi work on stack(that is the function stack frame)
           *   writeback argument initialized with {
           *           .sb = @sb,
           *           .sync_mode = WB_SYNC_ALL,
           *           .nr_pages = LONG_MAX,
           *           .range_cyclic = 1,
           *   }
           *   set WB_ONSTACK in the state member of the bdi work object
           *   queue it and invoke bdi_wait_on_work_clear()
           *                       # wait until WB_USED_B cleared,process enter TASK_UNINTERRUPTIBLE
           */
          static void bdi_sync_writeback(struct backing_dev_info *bdi, struct super_block *sb);

          /**
           * bdi_start_writeback - async version,and new bdi work is not on stack
           * # for writeback arguments
           *     range_cyclic set to 1
           *     sync_mode set to WB_SYNC_NONE
           *     nr_pages set to @nr_pages OR LONG_MAX(if @nr_pages is 0,in this case,for_background set
           *                                           to 1)
           * # this routine call to bdi_alloc_queue_work() to create bdi work,so the new bdi work
           *   object is in general slab cache
           */
          void bdi_start_writeback(struct backing_dev_info *bdi, struct super_block *sb, long nr_pages);

        The bdi_forker_task() and bdi_start_fn() functions :
          function bdi_forker_task() is executed by kernel bdi flush forker thread,it will start flush
          thread for dirty pages on-demand
          
          ! a bdi flush forker thread is created and started when the new bdi is registered,and which
            must has capability - flush forker.
            /* bdi_cap_flush_forker() check this capability,only @default_backing_dev_info is able
             * to create bdi flush forker thread,thus there is only one flush forker task on the
             * system which is attached to @default_backing_dev_info during kernel boot phase.
             */

          <mm/backing-dev.c>
            /**
             * bdi_register - register a bdi
             * @bdi:          the bdi to be registered
             * @parent:       parent device of the going to be created device of @bdi
             * @fmt:          variable parameters describing string
             * @...:          variable parameters
             * return:        0 OR error code
             * # EXPORT_SYMBOL
             * # this routine is called by bdi_register_dev(),in this case,variable parameters
             *   are major number and minor number
             * # steps - 
             *     1> check @bdi->dev,for a new one,it can not has @dev specified,in this case,
             *        just return 0 to caller and do nothing
             *        # driver needs to use separate queues per device
             *     2> analyze variable parameters
             *     3> invoke device_create_vargs() to create new device for @bdi with analyzed
             *        variable parameters,return error code if failed
             *        # parent device of the new device is @parent
             *     4> acquire @bdi_lock and disable bottom half
             *        add @bdi->bdi_list to global bdi list @bdi_list through list_add_tail_rcu()
             *        release @bdi_lock and enable bottom half
             *     5> check bdi_cap_flush_forker(),start forker thread for default backing_dev_info
             *        if condition satisfied,then invoke kthread_run() to start kernel thread,
             *        function is bdi_forker_task(),parameter is &@bdi->wb
             *        # init function default_bdi_init() is called during kernel boot phase,the first
             *          item in global bdi list @bdi_list is @default_backing_dev_info,and of course,
             *          bdi_register() is called in default_bdi_init() for register the default bdi
             *        if kernel thread creating failed,@bdi->bdi_list will be unlinked,and return
             *        -ENOMEM to caller
             *     6> if everything is OK,then invoke bdi_debug_register() to register bdi in debugfs,
             *        and set BDI_registered in @bdi->state,return 0 to caller
             */
            int bdi_register(struct backing_dev_info *bdi, struct device *parent, const char *fmt, ...);

            /**
             * bdi_unregister - unregister a bdi
             * @bdi:            bdi is going to be unregistered
             * # EXPORT_SYMBOL
             * # if @bdi->dev is not NULL,that means this bdi have been registered,we can unregister it
             * # unregister -
             *     invoke bdi_prune_sb()
             *     # make sure no super blocks point to it
             *     if @bdi is not default bdi,invoke bdi_wb_shutdown()
             *     # remove bdi from @bdi_list,and shutdown any threads we have running
             *     invoke bdi_debug_unregister()
             *     # unregister from debugfs
             *     invoke device_unregister() for @bdi->dev
             *     set @bdi->dev to NULL
             */
            void bdi_unregister(struct backing_dev_info *bdi);

            /**
             * bdi_task_init - init bdi task
             * @bdi:           the bdi that contains @wb
             * @wb:            writeback structure
             * # this routine make use the task @current,because it is invoked in bdi_forker_task(),
             *   thus @current is the kernel thread
             *   # except bdi_forker_task(),this routine also called by bdi_start_fn()
             * # routine invoke list_add_tail_rcu() to add @wb to @bdi->wb_list,for do this,must
             *   acquire @wb_lock of @bdi
             *   then enable PF_FLUSHER | PF_SWAPWRITE flags in @task->flags 
             *   next,disable PF_NOFREEZE flag in @task->flags,this let freezer can freeze this task
             *   set user nice value of @task to 0 - normal priority
             */
            static void bdi_task_init(struct backing_dev_info *bdi, struct bdi_writeback *wb);

            /**
             * bdi_forker_task - function executed by kernel flush forker thread
             * @ptr:             the parameter bound by kthread_run()
             * return:           always return 0
             */
            static int bdi_forker_task(void *ptr);

            description for bdi_forker_task() :
              this routine is executed by bdi thread which started by @default_backing_dev_info.
              at the beginning,routine convert @ptr to bdi_writeback pointer @me.
              /* @me -> writeback info of @default_backing_dev_info */
              next,invoke bdi_task_init() to initialize the kernel thread itself.
              the main work is handled in a infinite for cycle.
              for {
                      invoke wb_has_dirty_io(@me) and list_empty(@me->bdi->work_list),
                      check their results - if @default_backing_dev_info has dirty data or
                      new writeback has queued into work list,then invoke wb_do_writeback()
                      with parameters @me and 0 to start writeback.
                      if no dirty data or new works,then acquire @bdi_lock and disable
                      bottom half.
                      call list_for_each_entry_safe() to traverse @bdi_list the global bdi list
                      of registered bdi.

                      if current bdi's wb has specified a task - there been a writeback thread is running,
                      then continue to next iteration - next bdi.
                      if current bdi's work list is empty AND which has not dirty data,also continue
                      to next iteartion - next bdi.
                      otherwise,invoke bdi_add_default_flusher_task() on current bdi.
                      /* bdi_add_default_flusher_task() -
                       *   add the default flusher task that gets created for any bdi that has dirty data
                       *   pending writeout.
                       *   # check bdi_cap_writeback_dirty() and BDI_registered,if conditions are not satisfied,
                       *     return to caller.
                       *   # check and set bit BDI_pending,if it is not pending,then invoke RCU callback
                       *     bdi_add_to_pending().
                       *   # before call to RCU callback,this routine will remove the bdi from @bdi_list
                       *     the global bdi list.
                       * bdi_add_to_pending() -
                       *   reset @bdi_list of the bdi,add the @bdi_list to the global list @bdi_pending_list,
                       *   it collects the bdi's that have work is pending.
                       *   wake up @default_backing_dev_info.wb.task - the kernel thread that executes
                       *   bdi_forker_task()
                       */

                      after list traversing,set this kernel thread to TASK_INTERRUPTIBEL.
                      check list @bdi_pending_list,if bdi_add_to_pending() have called in previous step,then
                      it must not be empty - some bdi's have work is pending.
                      @bdi_pending_list is empty -
                        convert result of "@dirty_writeback_interval * 10" to jiffies(ms -> jiffies)

                        invoke schedule_timeout(wait jiffies)
                        try to freeze this kernel thread -
                          no result checking at there,if this kernel thread is sucessfully freezed,
                          TIF_FREEZE PF_FREEZING will be set,task enter TASK_UNINTERRUPTIBLE until unfreezed.
                          after unfreezed,task state reset to TASK_INTERRUPTIBLE.
                          /* PF_FROZEN also be set in the saved process flags,but only TIF_FREEZE flag is set
                           * in the thread flags,try_to_freeze() will freeze the process and place it into
                           * refrigerator.
                           * in most cases,process need to be freezed when system suspend,so that,bdi forker
                           * thread and flush threads will not be freezed when system is running
                           */

                        /* might wake up by bdi_add_to_pending() if schedule_timeout() have not timeout */

                        continue the next iteration of infinite for-cycle - no pending work,restart.

                      @bdi_pending_list is not empty,that means the registered bdi's have some work is pending,
                      thus kernel thread enter TASK_RUNNING.
                      retrieve the next item in @bdi_pending_list,remove the bdi from @bdi_pending_list,because
                      we are going to handle it.
                      release @bdi_lock and enable bottom half.
                      for this bdi has pending work,invoke kthread_run() to start a new kernel flusher thread,
                      the function will be executed is bdi_start_fn(),parameter is @wb of this bdi.
                      if we failed on kthread_run() with pending bdi's,@wb->task IS_ERR,then
                        reset @wb->task to NULL /* default */
                        insert this pending bdi to @bdi_pending_list again
                        invoke bdi_flush_io() on it /* force writeout of the bdi from this forker thread */
                        /**
                         * bdi_flush_io() -
                         *   create writeback_control message {
                         *     sync_mode = WB_SYNC_NONE
                         *     older_than_this = NULL
                         *     range_cyclic = 1
                         *     nr_to_write = 1024
                         *   call writeback_inodes_wbc() on the created wbc
                         * # do filesystem writeback
                         */
              }

            /**
             * bdi_start_fn - function executed by kernel flush thread,that is attached
             *                to the bdi's in @bdi_list except @default_backing_dev_info
             * @ptr:          member @wb of the bdi which has pending work
             * return:        0 OR error code
             */
            static int bdi_start_fn(void *ptr);

            brief description for bdi_start_fn() :
              add the bdi into the global @bdi_list. /* removed in bdi_add_default_flusher_task() */
              /* need @bdi_lock and bottom half disabled,release and enabled after adding finished */
              invoke bdi_task_init() to initialize kernel flush thread itself.
              clear BDI_pending in @state of the bdi.
              call smp_mb__after_clear_bit() to sync with SMP CPUs.
              wake up the tasks waiting for BDI_pending cleared.
              invoke bdi_writeback_task() with paramter @wb = (struct bdi_writeback *)@ptr.

              unlink @wb from the bdi's @wb_list. /* need @wb_lock,release after operation finished */
              /* @wb->list added into bdi's @wb_list by bdi_task_init() */
              
              check the bdi's work_list,if any bdi_work is raced with the @wb exiting,flush them -
                invoke wb_do_writeback() with parameter @wb, 1
              set @wb->task to NULL,return result of bdi_writeback_task() to caller.

        Do writeback :
          the entry point of filesystem writeback task is the routine wb_do_writeback().
          it is called in bdi_forker_task() for writeout dirty data of @default_backing_dev_info,and
          also called in bdi_writeback_task() in kernel flush thread for writeout dirty data of other
          bdi's.

          <fs/fs-writeback.c>
            /**
             * bdi_writeback_task - handle writeback of dirty data for the device backed by the bdi,
             *                      also wake up periodically and does kupdated style flushing
             * @wb:                 bdi writeback
             * return:              always return 0
             * # this routine is executed by kernel flush thread - bdi_start_fn()
             * # steps this routine process -
             *     use a while-cycle with stop condition is !kthread_should_stop() - kthread should stop
             *
             *     declare a local variable named @pages_written to record how many dirty pages have been
             *     written by wb_do_writeback() with parameters @wb, 0
             *     if have written at least one dirty page,then set local variable @last_active to current
             *     jiffies
             *     otherwise,check local variable @wait_jiffies(default value is -1UL) is equal to -1UL
             *     # the default value is -1UL,if we did some works and have waitted a while,the value of
             *       @wait_jiffies must changed
             *     if it is not equal,that means we have waited enough for dirty data at last iteration and no
             *     new dirty data has came,thus function will exit if current @jiffies is after to
             *     "max(5UL * 60 * HZ, @wait_jiffies) + @last_active" => timeout
             *     # if the dirty data have seen again later,the task will get recreated automatically
             *     # the expression "5UL * 60 * HZ" - x86 platform,it is 300 * 1000
             *     # sysctl binary interface have provided a way to modify @dirty_writeback_interval,the file
             *       is "/proc/sys/vm/dirty_writeback_centisecs"
             *
             *     next,check @dirty_writeback_interval,if it has defined,then schedule_timeout_interruptible()
             *     will be called,@wait_jiffies is equal to "@dirty_writeback_interval * 10" - ms -> jiffies
             *     otherwise,just invoke schedule()
             *     at the end of iteration,invoke try_to_freeze() which freeze the process if needed
             */
            int bdi_writeback_task(struct bdi_writeback *wb);

            /**
             * wb_do_writeback - retrieve work items and do the writeback they describe
             * @wb:              writeback info
             * @force_wait:      sync all
             * return:           number pages that have been writeout
             */
            long wb_do_writeback(struct bdi_writeback *wb, int force_wait);

            description for wb_do_writeback() :
              this function make use of a while-cycle to handle writeback works,the condition is
              get_next_work_item() return NULL.
              /**
               * <fs/fs-writeback.c>
               *   get_next_work_item - get next work item of the work list
               *   static struct bdi_work *get_next_work_item(struct backing_dev_info *bdi,
               *                                              struct bdi_writeback *wb);
               *   # the current work item has seen by @wb,then clear seen bit of it,and
               *     return it to caller
               *     the checking is test_bit(@wb->nr, @work->seen),if have not seen by @wb,
               *     then test the next work item,and so on,until the end of list
               *     if no work item find,return NULL
               */
               
              get writeback arguments from @work->args.
              if @force_wait is TRUE,then sync_mode is WB_SYNC_ALL,otherwise,it remains unchanged.
              if sync_mode is WB_SYNC_NONE,that means this is not a data integrity operation,so
              we will starting to handle this work.invoke wb_clear_pending() on it to decrease
              @work->pending,if pending becomes _zero_,that means this work should be freed,
              thus it will be removed from bdi's @work_list and wb_work_complete() will be called.
                                                                /* bdi_work_free() for !on-stack
                                                                 * bdi_work_clear() for on-stack
                                                                 */

              now trigger writeback by invoke wb_writeback() with arguments @wb, @work->args.
              after finished,check sync_mode whether is WB_SYNC_ALL,if it is,invoke wb_clear_pending().
              /* WB_SYNC_NONE - we decrease pending before we starting work handle
               * WB_SYNC_ALL  - we decrease pending after all have finished
               */

              after while-cycle stopped,invoke wb_check_old_data_flush(),this is for check periodic
              writeback,kupdated() style writeback.
              /* wb_check_old_data_flush() may invoke wb_writeback to trigger a writeback operation,
               * if there is some old data have to be flushed         # wb_writeback()
               * return the number of wrote pages,if no old data,return 0
               */

              finally,function returns the number of written pages that is the total number of pages
              have written in while-cycle add to return value of wb_check_old_data_flush().

            /**
             * wb_writeback - explicit flushing or periodic writeback of "old" data
             * @wb:           writeback info needed by flush thread
             * @args:         writeback arguments
             * return:        number of pages have written out
             * # "old" - the first time one of an inode's pages is dirtied,we mark the dirtying-time
             *           in the inode's address_space.so this periodic writeback code just walks
             *           the superblock inode list,writing back any inodes which are older than
             *           a specific time point
             * # @older_than_this takes precedence over @nr_to_write,so we will only write back
             *   all dirty pages if they are all attached to "old" mappings
             */
            static long wb_writeback(struct bdi_writeback *wb, struct wb_writeback_args *args);

            description for wb_writeback() :
              create writeback control structure @wbc and initialize it from @wb and @args.
              @older_than_this of @wbc default is NULL.
              if @for_kupdate is TRUE,then set @older_than_this points to a local variable @oldest_jif,
              and set its value to current @jiffies - @dirty_expire_interval * 10(covert to jiffies).
              if @range_cyclic is FALSE,then set @wbc.range_start to 0,@wbc.range_end = LLONG_MAX.
              make use of an infinite for-cycle to handle main works.
              for {
                      if @nr_pages <= 0 - no remain pages,then break cycle.
                      if specified background mode but have not over background thresh,then break cycle.
                         /* @for_background */
                         /**
                          * the threshold dirty ratio for background writeback can be adjusted by writing
                          * in the /proc/sys/vm/dirty_background_ratio file,or by set sysctl config
                          * in /etc/sysctl.conf - vm.dirty_background_ratio =<value>
                          */
                      set some @wbc's members :
                              @more_io = 0 /* only once */
                              @nr_to_write = MAX_WRITEBACK_PAGES /* defined to 1024 */
                              @pages_skipped = 0
                      invoke the inode writeback routine writeback_inodes_wb() with parameters @wb and @wbc.
                      update @args->nr_pages,the result is equal to "@nr_pages -= MAX_WRITEBACK_PAGES - @nr_to_write".
                      /* @nr_to_write will be decreased automatically if succeed to writeback at least one page */
                      update local variable @wrote to record how many pages have written out in this iteration,
                      the variable will be returned to called at the end of this routine.

                      check continue conditions -
                        if @nr_to_write <= 0 - we have write out all(1024 pages),then break cycle,otherwise,continue
                        to next iteration.
                        if @more_io is FALSE - we have not write out all,but we can only process once,break cycle.
                        
                        /* check if have written out something */
                        if @nr_to_write < MAX_WRITEBACK_PAGES - we have written out something,then continue to next
                        iteration.(if no @more_io have specified,we can not retry again)

                      ! if more_io is FALSE,for-cycle will have broken.

                      if no a branch is taken from above branches,that means we wrote nothing,then acquire
                      @inode_lock,check @wb->b_more_io.if it is not empty,then retrieve the previous inode of
                      @b_more_io,and invoke inode_wait_for_writeback() on the previous inode.
                      /**
                       * routine queue_io() splice inode list @b_more_io and @b_io.prev,and move
                       * inodes in @b_dirty into @b_io.
                       * queue_io() need second parameter is @older_than_this,only inode's dirtying-time
                       * is older than this time point can be moved into dispatch queue.
                       *    # time value less than recent time value
                       */
                      release @inode_lock and go to next iteration.
              }

              finally,return @wrote to caller.              

            /**
             * writeback_inodes_wb - do writeback dirty inodes
             * @wb:                  writeback info
             * @wbc:                 writeback control info
             * # may be preempted
             */
            static void writeback_inodes_wb(struct bdi_writeback @wb, struct writeback_control *wbc);

            description for writeback_inodes_wb() :
              this routine traverse @b_io the dispatch queue,for each inode,pin its super block for 
              writeback,and write a portion of @b_io inodes which belong to the super block.the real
              writeback work is handled by writeback_single_inode(),which finally invoke do_writepages().

              first,set @wbc->wb_start to current @jiffies,and acquire @inode_lock.
              check @for_kupdate is FALSE OR @b_io is empty,if the result is TRUE,then invoke queue_io()
              to move some dirty inodes into @b_io.
              make use of a while-cycle until @b_io is empty.
              in the while-cycle {
                      get current inode - from the tail of @b_io
                      get the super block of this inode.
                      check whether the super block is matched @wbc->sb,if it is not matched,then
                      redirty_tail() this inode,and skip it,get into next iteration.
                      /* we are handling @wbc->sb,not the super block of this inode */
                      /* if @wbc->sb is NULL,that means match all super block objects */
                      /* bdi_writeback object is get from inode_to_bdi(@inode)->wb */
                      /* if @b_dirty is not empty,have to update @inode->dirtied_when to current jiffies
                       * if the dirty time of the object at @b_dirty->next is time_after() than this inode's.
                       * at the end del @i_list,and add it at @b_dirty->next.
                       */
                      function pin_sb_for_writeback() is called,pin the super block for writeback.
                      /* pinned => @s_count++,@s_root must not be NULL - have mounted */
                      if SB_PIN_FAILED returned - pin failed,invoke requeue_io() on this inode,and continue to
                      next iteration.                               /* list_move() the inode into @b_more_io */
                                                                    /* it will be dealt with at next period */
                      invoke writeback_sb_inodes() to writeback dirty inodes of super block which this inode
                      is belong to.
                      unpin_sb_for_writeback().
                      if writeback_sb_inodes() returned 1 - the caller writeback routine should be interrupted,
                      then break while-cycle;otherwise,continue to next iteration.
              }
              release @inode_lock and return to caller.

            /**
             * writeback_sb_inodes - writeback inodes of the given super block
             * @sb:                  super block
             * @wb:                  writeback info
             * @wbc:                 writeback control
             * return:               0 => the caller writeback routine should continue
             *                       1 => the caller writeback routine should be interrupted
             * # this routine is called by writeback_inodes_wb()
             */
            static int writeback_sb_inodes(struct super_block *sb, struct bdi_writeback *wb,
                                           struct writeback_control *wbc);              

            description for writeback_sb_inodes() :
              check the dispatch queue @wb->b_io,make use of while-cycle to taverse each element
              in the queue until queue become empty.
              get the inode at the tail of @b_io.
              /* list_entry(@b_io.prev, ...) */
              process the same match checking between @wbc->sb and the super block of current inode,
              if super block is not matchecd,redirty_tail() the inode and get into next iteration.
              /* if @wbc->sb is NULL,that means match all super block objects */
              process another match checking between @sb and the super block of current inode,if
              it is not matched,routine return 0.
              /* @sb been finished
               * no dispatching inodes of @sb - the remaining inodes are not belong to @sb,or
               * everthing have done - all inodes in @b_io are belong to @sb and have flushed
               */
              proceed check inode state,if it is new inode(I_NEW) or it will be freed(I_WILL_FREE),
              we requeue_io() on it and continue to next iteration.
              check dirty time of this inode,if it is dirtied after @wbc->wb_start,we return 1 to
              caller.
              /* when writeback started,the inode dirtied before @wbc->wb_start will be wrote out,
               * the newly dirtied will be wrote out later.
               * # this inode is dirtied after sync_sb_inodes() was called.
               */
              do BUG test - BUG_ON,inode's state is I_FREEING or I_CLEAR.
              all necessary checkings have done,prepare writeback it.
              invoke __iget() on it. /* increase ref count */
              set local variable @pages_skipped to @wbc->pages_skipped,used as a record.
              call to writeback_single_inode() to writeback this inode.
              check @pages_skipped whether not equal to @wbc->pages_skipped,if it is TRUE,then
              redirty_tail() this inode,that means we failed on writeback it.
              /* writeback is not making progress due to locked buffers,skip this inode for now */
              release @inode_lock,this lock is acquired in writeback_inodes_wb(),the reason of
              release it at there because we have to check kernel preempt.
              iput() this inode and call cond_resched() /* we release @inode_lock before every 
                                                         * possible kernel preempt
                                                         */
              next,acquire @inode_lock again.
              check @nr_to_write of @wbc,if it is less than or equal to _zero_,that means have written out
              all required pages,set @more_io to 1,and return 1 to caller.
                                      /* wb_writeback() continue,but writeback_inodes_wb() should stop */
              otherwise,we have work to do,check @b_more_io,if it is not empty,also set @more_io to 1.
              this tell kernel control path continue wb_writeback() after writeback_inodes_wb() finished.
              /* this routine get into next iteration */
              at the end of this routine,@b_io is empty,we return 1 to caller to stop writeback_inodes_wb().
              /**
               * if we did not consumed everything,@wbc->nr_to_write must be a postive.
               * if @b_more_io is not empty,@more_io is set to 1,thus wb_writeback() will check whether 
               * writeback routine have written out something.
               * when writeback_inodes_wb() is restarted by wb_writeback(),it will invoke to queue_io(),
               * splice @b_more_io into @b_io,move some dirty inodes from @b_dirty to @b_more_io if
               * the inodes are "older".
               */

            /**
             * writeback_single_inode - write out an inode's dirty pages
             * @inode:                  the inode is going to be wrote out
             * @wbc:                    writeback control
             * return:                  0 OR error code
             * # this routine must be called under @inode_lock held
             * # the inode either the caller has ref on it or it has I_WILL_FREE set
             */
            static int writeback_single_inode(struct inode *inode, struct writeback_control *wbc);

            what writeback_single_inode() does :
              1> get mapping of this inode.
                 check @i_count,print warning message if @i_count is _zero_ and I_WILL_FREE or I_FREEING
                 have not  setup.
                 if @i_count is not _zero_,print warning message if I_WILL_FREE is set.
              2> check I_SYNC,if it is set,must check @sync_mode.
                 if @sync_mode is not WB_SYNC_ALL,have to requeue_io() this inode and return 0 to caller,
                 /* this inode is locked for writeback,and we are not doing writeback-for-data-integrity */
                 if sync_mode is WB_SYNC_ALL,invoke inode_wait_for_writeback() on @inode.
                 /* inode_wait_for_writeback() - # data-integrity sync
                  *   create wait bit queue,task enter TASK_UNINTERRUPTIBLE state,wait until __I_SYNC
                  *   is cleared.
                  */
              3> bug test,BUG_ON if I_SYNC is set even we have waited on bit.
              4> record I_DIRTY status,set I_SYNC,this tells other kernel control path that this inode
                 is dealt with writeback.
                 clear I_DIRTY status.
                 unlock @inode_lock.
              5> invoke do_writepages() on the mapping of @inode with writeback control @wbc.
                 /**
                  * routine do_writepages() is defined in <mm/page-writeback.c>.
                  * if @wbc->nr_to_write is _zero_,this routine return 0 to caller with do nothing.
                  * otherwise,invoke either @a_ops->writepages() or generic_writepages(),return the value
                  *                  # routine has implemented
                  * returned by the called routine.
                  *
                  * # generic_writepages() invoke write_cache_pages() with write page back routine
                  *   __writepage(),and write_cache_pages() invoke this routine for each dirty page.
                  *   __writepage() exactly invoke @a_ops->writepage().
                  *   for ext2 filesystem,the implementation is ext2_writepage(),which call to
                  *   block_write_full_page(),then __block_write_full_page() is called,finally,
                  *   submit_bh() is called for convert the buffer page into bio and submit it.
                  *
                  * # write_cache_pages() will construct a vector for dirty pages of the mapping,
                  *   thus it call to pagevec_lookup_tag(),this function is defined in <mm/swap.c>,
                  *   then find_get_pages_tag() is invoked to retrieve all dirty pages in the
                  *   mapping,and pagevec_lookup_tag() return the number of pages in the vector to
                  *   write_cache_pages().(tag is PAGECACHE_TAG_DIRTY)
                  *   if @wbc->range_cyclic is TRUE,then writeback index is starting from
                  *   @mapping->writeback_index,so the find_get_pages_tag() will starts at there,end
                  *   index is -1;otherwise,the index is "@wbc->range_start >> PAGE_CACHE_SHIFT",end
                  *   index is "@wbc->range_end >> PAGE_CACHE_SHIFT".
                  *
                  *   |...|===========...|              |===========...|
                  *       ^index         ^end index -1  ^index 0       ^end index -1
                  *        writeback_index               writeback_index
                  *   
                  *   |==============#===|              |===========...|
                  *   ^index         ^end index         ^index         ^end index
                  *    range_start    range_end          range_start 0  range_end LLONG_MAX
                  *
                  *   ! end index is used vertify the page's validity,if exceeded end index,the page
                  *     and the rest would not be considered.
                  *   ! after writeback accomplished,write_cache_pages() might update the @writeback_index
                  *     member of that mapping.
                  *
                  *     it use a local variable named @done as indicator to indicates whether the writeback
                  *     is done,and use a local variable named @done_index(default is the index) to record
                  *     index of page that is unwritten.
                  *
                  *     when @wbc->range_cyclic is TRUE,and index is starting from 0,that means index
                  *     cycled,thus,set local variable @cycled to 1,else set to 0.
                  *     # if @range_cyclic is FLASE,@cycled is set to 0.
                  *
                  *     after writeback done,if not @cycled AND not @done,it will set @cycled to 1,set 
                  *     @index to 0,end index to @writeback_index - 1,then retry writeback again.
                  *     retry                      the first time
                  *     |<-- wb -->|               |<---- wb ---->|
                  *     |==========#=====|         |===========...|
                  *     ^index     ^end index      ^index         ^end index
                  *      0          wb-index - 1    0              -1
                  *
                  *     if we did not retry again,and @no_nrwrite_index_update of @wbc is FALSE,
                  *     then update @mapping->writeback_index with @done_index if @range_cyclic is TRUE
                  *     OR (@range_whole is TRUE AND number to write is greater than 0),then update
                  *     @nr_to_write of @wbc to the remained number of pages have to be written,it
                  *     maybe 0 if we have written all @wbc->nr_to_write pages.
                  *     # local variable @range_whole,if @wbc->range_start is _zero_ AND @wbc->range_end
                  *       is LLONG_MAX,this variable is set to 1;otherwise,set to 0.
                  *     # @nr_to_write greater than 0 --> remained something
                  *     
                  *   ! function make use two cycles,the second is nested inside the first.
                  *     the first cycle,stop condition is @done is TRUE OR page index > end-index.
                  *     it invoke pagevec_lookup_tag() in the first cycle,and handle the tagged pages
                  *     in second cycle.
                  *     if no pages are found,cycle just stop.
                  */
              6> if we are in WB_SYNC_ALL mode,process filemap_fdatawait() on the mapping.
                 /* make sure to wait on the data before writing out the meta data.
                  * this is important for filesystems that modify metadata on data I/O completion.
                  */
                 /**
                  * <mm/filemap.c>
                  *   int filemap_fdatawait(struct address_space *mapping);
                  *   - wait for all under-writeback pages to complete,walk the list of under-writeback
                  *     pages of the given address space and wait for all of them.
                  *     actually,invoke filemap_fdatawait_range() on @mapping in byte range [0, @i_size - 1].
                  *     # EXPORT_SYMBOL
                  *   int filemap_fdatawait_range(struct address_space *mapping, loff_t start_byte,
                  *                               loff_t end_byte);
                  *   - for pages in page cache have spanned page index @start_byte >> PAGE_CACHE_SHIFT
                  *     to @end_byte >> PAGE_CACHE_SHIFT,for each of them has tagged PAGECACHE_TAG_WRITEBACK,
                  *     wait on the page until PG_writeback is cleared.
                  *     at the end of iteration of first while-cycle,call to cond_resched() for check
                  *     kernel preempt.
                  *     # EXPORT_SYMBOL
                  *     # the first while-cycle's stop condition is start index > end index AND
                  *       pagevec_lookup_tag() returned _zero_.but the iterator @index do not
                  *       increase,thus the stop condition is there is no more pages during writeback 
                  *       in the mapping.cond_resched() is used to wait block I/O finished.
                  */
                 if any error has happened,this routine will return the error to caller at the end.
              7> if dirty state of @inode is I_DIRTY_SYNC or I_DIRTY_DATASYNC,invoke write_inode()
                 on @inode with @wbc.
                 /* do not write the inode if only I_DIRTY_PAGES was set */
                 if any error has happened,this routine will return the error to caller at the end.
              8> re-acquire @inode_lock again.
                 clear I_SYNC,that means writeback have completed,but we do not wake up the tasks
                 waiting on the bit immediately.
              9> check inode state,if it is not in I_FREEING or not in I_CLEAR,
                 then select one of the following branch -
                         > I_DIRTY_PAGES AND @for_kupdate,goto label "select_queue"
                           /* more pages get dirtied by a fast dirtier */
                         > I_DIRTY,redirty_tail() @inode
                           /* at least,XFS will redirty the inode during the writeback
                            * (delalloc) and on IO completion(isize)
                            */
                         > mapping of @inode has tagged PAGECACHE_TAG_DIRTY,check @for_kupdate
                           /* did not write back all the pages,redirty it and move it from
                            * dispatch queue to @b_more_io OR @b_dirty
                            */
                             @for_kupdate is TRUE
                             /* caller is kupdate function,put @inode at the head of @b_dirty */
                                     set I_DIRTY_PAGES
                               "select_queue":
                                     check @nr_to_write <= 0
                                     TRUE:
                                             requeue_io() @inode
                                     FALSE:
                                             redirty_tail() @inode /* retry later */
                             @for_kupdate is FALSE
                             /* caller is not kupdate function,put @inode at the tail */
                                     set I_DIRTY_PAGES
                                     redirty_tail() @inode
                         > @i_count is not _zero_,list_move() @inode->i_list into @inode_in_use
                         > otherwise,list_move() @inode->i_list into @inode_unused
             10> invoke inode_sync_complete() on @inode,return the final ret value to caller.
                 /* process a SMP memory barrier,and wake up the task waiting for __I_SYNC clear */


          The flow of functions invocations starting from bdi_start_fn() :
            bdi_start_fn() -> bdi_writeback_task() ->
            wb_do_writeback() -> {
                    wb_writeback() ->
                    writeback_inodes_wb() -> {
                            writeback_sb_inodes() -> writeback_single_inode() ->
                            do_writepages() ->
                            write_cache_pages() -> {
                                    @a_ops->writepage() -> (FOR EXT2) ext2_writepage() ->
                                    block_write_full_page() -> __block_write_full_page() ->
                                    submit_bh() -> ...

                                    pick up next dirty page in the mapping
                            }

                            pick up next item in the dispatch queue @b_io of the writeback object
                    }

                    pick up next bdi work in @work_list of the BDI
            }
            ! bdi_writeback_task() will stopped if kthread_should_stop() returned TRUE or longest
              period have passed but no dirty pages occurred.

        Wake up flusher threads :
          except the kupdate style writeback - periodic checking,there are some cases the flusher
          threads are woke up by kernel control path,such in the executing of free_more_memory().
          in the function,try_to_free_pages() would be called,then is do_try_to_free_pages().
          in function do_try_to_free_pages(),except shrink zones and shrink slabs,if the writeback
          threshold has reached,then function wakeup_flusher_threads() will be invoked.
          /**
           * try_to_free_pages(),do_try_to_free_pages() are defined in <mm/vmscan.c>
           * @writeback_threshold := scanning control->number to reclaim +
           *                         scanning control->number to reclaim / 2
           * if the total number of scanned reclaimable pages is greater than @writeback_threshold,
           * wakeup_flusher_threads() will be called.
           * # for laptop mode,the argument is 0,otherwise,it is the total scanned pages.
           */

          <fs/fs-writeback.c>
            /**
             * wakeup_flusher_threads - start writeback
             * @nr_pages:               the number of pages should be written back
             * # if @nr_pages is zero,write back the whole 'world' - everything
             *   - @nr_pages is set to global_page_state() NR_FILE_DIRTY + NR_UNSTABLE_NFS
             *     # per-CPU has different accounting info
             * # this routine actually invoke bdi_writeback_all() with NULL and @nr_pages
             */
            void wakeup_flusher_threads(long nr_pages);

            /**
             * bdi_writeback_all - schedule writeback for all backing devices
             * @sb:                specified super block,if it is NUL,that means
             *                     match all
             * @nr_pages:          number of pages should be written back
             * # this routine makeup writeback argument @args 
             *     .sb = @sb
             *     .nr_pages = @nr_pages
             *     .sync_mode = WB_SYNC_NONE
             *     # bdi_sync_writeback() process WB_SYNC_ALL writeback
             *   then traverse each registered bdi on the system,for each of them,if
             *   anyone has dirty io,then invoke bdi_alloc_queue_work() for it with
             *   the argument @args
             */
            static void bdi_writeback_all(struct super_block *sb, long nr_pages);


          ! writeback flusher threads usually are woke up when :
              1> syscalls sync(),fsync(),fdatasync() have issued by user space
              2> grow_buffers() was failed in __getblk_slow()
              3> page frame reclaiming algorithm invokes free_more_memory() or
                 try_to_free_pages()
              !! 4> mempool_alloc() fails to allocate a new memory pool element
                 IN LINUX 2.6.34.1,THIS ROUTINE JUST WAIT UNTIL @alloc FUNCTION
                 THE MEMBER OF mempool structure SUCCEED TO ALLOCATE A NEW ELEMENT,
                 OR A FREE ELEMENT IS AVAILABLE FROM THE mempool.
                 # mempool_alloc() INVOKE io_schedule_timeout(5 * HZ) IF @curr_nr
                   IS 0.

        Retrieving Old Dirty Pages :
          the book have mentioned "dirty_writeback_centisecs",it is the period that
          timer @wb_timer decays.if timeout,the timer handler @wb_timer_fn() would
          be started automatically,which invoke wb_kupdate() function.
          
          but in Linux 2.6.34.1,only laptop mode has a timer named @laptop_mode_wb_timer,
          the handler is laptop_timer_fn().
          after the period past,the timer function invoke wakeup_flusher_threads() with
          parameter 0 to wake up all writeback threads.

          actually,the work of wb_kupdate() have handed over to wb_check_old_data_flush()
          and it is called in wb_do_writeback().

        The sync(),fsync(),fdatasync() system calls :
          <fs/sync.c>
            /**
             * sys_sync - system call sync
             * return:    0 or negative value
             * # this routine just do :
             *     1> invoke wakeup_flusher_threads() with parameter 0
             *     2> invoke sync_filesystems() with parameter 0
             *        # first flushing,skip all locked buffers
             *     3> invoke sync_filesystems() with parameter 1
             *        # second flushing,wait on all locked buffers for do writeback
             *     4> if we are in laptop mode,invoke laptop_sync_completion()
             *     5> return 0 to caller if everything is OK
             */
            SYSCALL_DEFINE0(sync)
            => asmlinkage long sys_sync(void);

            /**
             * sync_filesystems - sync all mounted filesystems
             * @wait:             1 => do sync_inodes_sb()
             *                    0 => do writeback_inodes_sb()
             * # this routine traverse all super block objects in the list @super_blocks,
             *   for each of them,set @s_need_sync to 1
             *   next,traverse super block objects again,for each super block whose
             *   @s_need_sync is 1,set the member to 0,and invoke __sync_filesystem()
             *   on current super block if it is not MS_RDONLY AND its @s_root is not NULL
             *   AND its @s_bdi is not NULL
             *   the traversing maybe restart fron the head of list @super_blocks,if
             *   current super block is no longer on the list
             *   ! because we need to sync the super block,thus we have to increase @s_count,
             *     after the sync is completed,@s_count would be decreased,if it is equal to 0,
             *     that means it will be destroyed.but the checking for @s_list is empty is
             *     before @s_count decreasing
             *     if @s_list is empty,that mean the super block is unlinked from @super_blocks,
             *     then we decrease @s_count and restart the traversing;otherwise,just decrease
             *     @s_count as well(if the super block is not unlinked,but @s_count become _zero_,
             *     it is a BUG)
             *   ! before we call to __sync_filesystem(),we must acquire @s_umount rw_semaphore to
             *     avoid other kernel control path umount it(down_read()),and release it right after
             *     __sync_filesystem() return,thus other kernel control path may umount it before
             *     we determining if it is need to restart the traversing
             */
            static void sync_filesystems(int wait);

            /**
             * routine used to sync a file system
             * the filesystem must have a backing device,can not be @noop_backing_dev_info
             * if @s_qcop->quota_sync has implemented,must do quota sync at first
             * if @wait is 1,invoke sync_inodes_sb,otherwise,it is writeback_inodes_sb()
             * if @s_op->sync_fs has implemented,must invoke the routine to sync filesystem state
             * finally,sync block device,if @wait is 0,filemap_flush() would be called,otherwise,
             * invoke filemap_write_and_wait(),each of these two functions are flush the mapping
             * of the master inode in bdev filesystem of the block device
             * # sync_inodes_sb() => just invoke bdi_sync_writeback() and wait_sb_inodes()
             */
            static int __sync_filesystem(struct super_block *sb, int wait);

            /**
             * <fs/fs-writeback.c>
             *   static void wait_sb_inodes(struct super_block *sb);
             *   # acquire @inode_lock
             *     for each inode of @sb
             *       check its state,if it is in I_FREEING OR I_CLEAR OR
             *       I_WILL_FREE OR I_NEW,skip current inode and get next iteration
             *       # I_NEW no dirty pages
             *       
             *       if current inode's mapping's @nrpages is 0 - no page cached,skip it
             *       
             *       otherwise,__iget() it,and release @inode_lock,iput() the last inode @old_inode,
             *       invoke filemap_fdatawait() on the mapping of current inode 
             *       # of course,@old_inode is up date to current inode
             *      
             *       check kernel preempt,and acquire @inode_lock again,get into next iteration
             *     before return,release @inode_lock and iput() the last inode @old_node
             */


            /**
             * sys_fsync - system call fsync
             * @fd:        file descriptor
             * return:     0 OR negative value
             * # sync on @fd,this routine just invoke do_fsync() with parameters @fd,0
             * # writeback all dirty pages and the inode information associated to the file(
             *   buffer containing the inode)
             */
            SYSCALL_DEFINE1(fsync, unsigned int, fd)
            => asmlinkage long sys_fsync(unsigned int fd);

            /**
             * sys_fdatasync - system call fdatasync
             * @fd:            file descriptor
             * # data sync on @fd,this routine just invoke do_fsync() with parameters @fd,1
             * # writeback all dirty pages but without inode information
             */ 
            SYSCALL_DEFINE1(fdatasync, unsigned int, fd)
            => asmlinkage long sys_fdatasync(unsigned int fd);

            /**
             * do_fsync - do fsync
             * @fd:       file descriptor
             * @datasync: whether do data sync only
             * return:    0 OR error code
             * # this routine is sync on file object that associated to @fd,it invoke
             *   fget() to get the file object and invoke vfs_fsync() on it if it is not
             *   NULL,fput() it after work accomplished
             * # vfs_fsync() : (@file, @dentry, @datasync)
             *                 invoke vfs_fsync_range() on range 0 -- LLONG_MAX
             *                 ! dentry is @file->f_path.dentry
             *                 
             */
            static int do_fsync(unsigned int fd, int datasync);


            /**
             * <fs/buffer.c>
             *   ##
             *   # sync_mapping_buffers - write out AND wait upon a mapping's "associated" buffers
             *   # @mapping:              the mapping
             *   # return:                0 OR error code
             *   # ! EXPORT_SYMBOL
             *   # ! this routine retrieve the @assoc_mapping of @mapping,if associated mapping is
             *   #   NULL or @mapping's @private_list is empty,function return 0 with do nothing;
             *   #   otherwise,invoke fsync_buffers_list(),pass it @assoc_mapping->private_lock and
             *   #   @mapping->private_list as parameters,return the result of fsync_buffers_list()
             *   ##
             *   int sync_mapping_buffers(struct address_space *mapping);
             *
             *   ##
             *   # fsync_buffers_list - write out and wait upon a list of buffers
             *   # @lock:               spinlock have to be acquired when operating associate buffers
             *   # @list:               list of buffers
             *   # return:              0 OR error code
             *   # ! this routine make sure that all initially dirty buffers get waited on,but that
             *   #   any subsequently dirtied buffers do not.
             *   #   thus there are two main stages :
             *   #     1> first,copy dirty buffers to a temporary inode list,queueing the writes
             *   #     2> clean up,waiting for those writes to complete
             *   ##
             *   static int fsync_buffers_list(spinlock_t *lock, struct list_head *list);
             *   ! description for fsync_buffers_list() :
             *       acquire @lock,enter a for-cycle stop when @list is empty.
             *       for { !! first stage
             *
             *               retrieve next buffer in @list
             *               retrieve the buffer's @b_assoc_map,the another mapping it associated with,
             *               stored it in local variable @mapping
             *               call to __remove_assoc_queue() for unlink current buffer from @b_assoc_buffers,
             *               disconnect with @b_assoc_map                                  !! ^@private_list
             *               if current buffer is dirty OR it is locked
             *                       use @b_assoc_buffers to add current buffer into a temporary list
             *                       restore @b_assoc_map of current buffer
             *                       BH_Dirty is set
             *                               get_bh()
             *                               release @lock
             *                               invoke ll_rw_block() with parameters SWRITE_SYNC_PLUG,1,current buffer
             *                               next,if local variable @prev_mapping is not NULL and @prev_mapping is
             *                               not equal to @mapping
             *                                       invoke blk_run_address_space() on @prev_mapping
             *                                       !! call to @prev_mapping->backing_dev_info->unplug_io_fn() with
             *                                          page is NULL,if both BDI and method unplug_io_fn() are valid.
             *                               set @prev_mapping to @mapping !! we queue a new one,and kick off the previous
             *                                                                mapping,if some buffers come from the same
             *                                                                mapping,we defer kick off until another mapping's
             *                                                                buffer is queued
             *                               brelse() current buffer
             *                               acquire @lock again
             *       }
             *       while-cycle stop when temporary list is empty { !! second stage
             *               ! at there,we traverse list from tail to head.
             *               ! temporary list collecting buffers either BH_Locked or BH_Dirty,
             *                 but function ll_rw_dirty() would clear BH_Dirty for the buffer which
             *                 is going to be submitted for I/O.
             *
             *               retrieve the last buffer from temporary list
             *               get_bh() on it
             *               get its @b_assoc_map,store it in @mapping
             *               !! if it is neither dirty nor locked,the mapping should be NULL
             *               invoke __remove_assoc_queue() for the buffer !! unlink it from temporary list
             *               if BH_Dirty is set !! newly dirtied buffer
             *                       list add the buffer to @mapping->private_list through @b_assoc_buffers
             *                       set @b_assoc_map to @mapping
             *               release @lock -- we acquired it in stage 1
             *               call to wait_on_buffer() on current buffer !! wait until BH_Locked is clear
             *               check BH_Uptodate for it,it have not set,record the error code -EIO.
             *               brelse() current buffer
             *               acquire @lock again
             *       }
             *       release @lock
             *       call to osync_buffers_list(),pass it @lock,@list as parameters
             *       return result to caller -- 0 OR -EIO OR result of osync_buffers_list()
             *   
             *   ##
             *   # osync_buffers_list - routine designed to support O_SYNC I/O
             *   # @lock:               lock for operating on @list
             *   # @list:               buffers
             *   # return:              0 OR error code
             *   # ! this routine waits synchronously for all already-submitted IO to complete,
             *   #   but does not queue any new writes to the disk
             *   # ! acquire @lock
             *   #   traverse @list from tail to head
             *   #     for each buffer,if it is BH_Locked !! because ll_rw_block() set BH_Locked for submitted buffers
             *   #             get_bh() on it
             *   #             release @lock
             *   #             wait_on_buffer() until BH_Locked is clear
             *   #             if current buffer still have no BH_Uptodate is set,record error code -EIO
             *   #             brelse() current buffer
             *   #             acquire @lock
             *   #             restart list traversing
             *   #   release @lock,return error code or 0 to caller
             *   ##
             *   static int osync_buffers_list(spinlock_t *lock, struct list_head *list);
             */


            /**
             * vfs_fsync_range - helper to sync a range of data & metadata to disk
             * @file:            file to sync
             * @dentry:          file dentry
             * @start:           offset in bytes of the beginning of data range to sync
             * @end:             offset in bytes of the end of data range(inclusive)
             * @datasync:        whether perform only datasync
             *                   # if it is 1,only metadata needed to access modified file data
             *                     is written
             * return:           0 OR error code
             * # for called from nfsd,@file may be NULL and @dentry is set,this can only happen
             *   when the filesystem implements the export_operations API
             * # for ext2 filesystem,the fsync() is implemented by ext2_fsync(),which just invoke
             *   simple_fsync() defined in <fs/libfs.c>
             *   function simple_fsync() create writeback control with @sync_mode = WB_SYNC_ALL,
             *   @nr_to_write = 0,then invoke sync_mapping_buffers() on the mapping of inode;
             *   next,check @datasync and I_DIRTY_DATASYNC,if @datasync is TRUE AND no I_DIRTY_DATASYNC,
             *   return the return value of sync_mapping_buffers() to caller(no inode sync);otherwise,
             *   invoke sync_inode(),and writeback_single_inode() is called,then is write_inode() is called
             *   for ext2 filesystem,it is ext2_write_inode() - writeback inode information
             *   ! if sync_mapping_buffers() failed but sync_inode() succeed,the error code of
             *     sync_mapping_buffers() will be returned to caller
             *   ! but set @nr_to_write to 0 let writeback_single_inode() only write inode info,do not
             *     touch pages in the page cache.
             */
            int vfs_fsync_range(struct file *file, struct dentry *dentry, loff_t start,
                                loff_t end, int datasync);

            brief description for vfs_fsync_range() :
              if @file is not NULL,get mapping @f_mapping and file operation set @f_op,
              otherwise,get maping @dentry->d_inode->i_mapping and file operation set @dentry->d_inode->i_fop.
              if @fop->fsync have not implemented,return -EINVAL to caller.
              next,invoke filemap_write_and_wait_range() for the @mapping on range @start -- @end.
              acquire @i_mudex of the host of the mapping,invoke @fop->fsync() on it,release mutex,
              return the value returned by @fop->fsync() to caller.
              /* the return value might be the error code returned from filemap_write_and_wait_range(),
               * if that function returned an error code exactly;otherwise,it is the return value from
               * @fop->fsync().
               * parameter @datasync will as an argument pass to @fops->fsync().
               */

            !! filemap_write_and_wait() => filemap_fdatawrite() => __filemap_fdatawrite_range() => do_writepages()
                                                                            /* 0 -- LLONG_MAX */
    
                                        if filemap_fdatawrite() succeeed => filemap_fdatawait()

                                              +--> "call to"
                                              |
               filemap_write_and_wait_range() => __filemap_fdatawrite_range()

                                        if __filemap_fdatawrite_range() succeed => filemap_fdatawait_range()

               both functions are return -EIO if writing was failed

            !! simple_fsync() writeback dirty associated buffers,inode information.
               the dirty pages in page cache @file->f_mapping is handled by vfs_fsync_range() itself,that is
               invoke routine filemap_write_and_wait_range() on all pages in the page cache.


/* END OF CHAPTER15 */
              

Chapter 16 : Accessing Files
    The different ways to access a file :
      1> Canonical mode -
           file is opened with the O_SYNC and O_DIRECT flags cleared,and its content is accessed by means
           of the read() and write() system calls.
           read() blocks until the file data is copied into User Mode address space.
           write() do not block,it terminates right after the data has copied into the page cache.

      2> Synchronous mode -
           file is opened with the O_SYNC is set(or set by fcntl()).this flag affects only the write operation,
           process will be blocked until the data is effectively written to disk.

      3> Memory mapping mode -
           after file opened,mmap() it into a memory area,the file appears as an array of bytes in RAM.

      4> Direct I/O mode -
           file is opened with the O_DIRECT flag is set,any read or write operation transfers data directly from
           the User Mode address space to disk,or vice versa,bypassing the page cache.

      5> Asynchronous mode -
           file is accessed -- either through a group of POSIX APIs,or by means of Linux-specific system calls.
           the requests for data transfers never block the calling process,rather,they are carried on 
           "in the background" while the application continues its normal execution.

    Reading and Writing a File :
      reading a file is page-based,the kernel always transfers whole pages of data at once.
      some disk-based filesystem's file operation is set to the "generic_ro_fops",it is type of
      struct file_operations,and in which,only reading related methods have implemented.the @read
      is do_sync_read(),the @aio_read is generic_file_aio_read(),etc.
      /* @generic_ro_fops is defined in <fs/read_write.c> */

      writing a file is more complicated to handle,because the file size could increase,and therefore
      the kernel might allocate some physical blocks on the disk.
      the common write method for some filesystems is do_sync_write(),and the aio write method is
      generic_file_aio_write(),which is defined in <mm/filemap.c>,in the file function generic_file_aio_read()
      is defined,too.
      /* for ext2 filesystem,the read method usually is do_sync_read(),and
       * the write method usually is do_sync_write()
       */

      Reading from a File :
        because do_sync_read() actually invoke aio_read() method of the file operations,which usually is
        generic_file_aio_read(),thus the function performs an asynchronous I/O operation.
        the kernel structure kiocb is used to control kernel synchronous/asynchronous I/O request.

        <linux/aio.h>
          /**
           * kiocb - kernel io control block,the kiocb struct to advance by peforming an operation
           *         this structure represents the aio control block in kernel space,and the
           *         structure "iocb" in <linux/aio_abi.h> represents the aio control block in
           *         user space
           * @ki_run_list:
           *         linked into the pending io list
           * @ki_flags:
           *         flags
           * @ki_users:
           *         users of this kio
           * @ki_key:
           *         id of this request
           *         for synchronous I/O,it is KIOCB_SYNC_KEY => ~0U
           * @ki_filp:
           *         file object
           * @ki_ctx:
           *         kio context,may be NULL for sync ops
           * @ki_cancel:
           *         method to cancel this request
           * @ki_retry:
           *         method to retry this request,invoked in aio_run_iocb(),
           *         initialized in aio_setup_iocb(),usually is
           *         aio_rw_vect_retry()
           * @ki_dtor:
           *         method to destroy this request
           * @ki_obj:
           *         User Mode user identifier
           *         @user => point to user iocb
           *                  # asynchronous
           *         @tsk  => task struct for the User Mode process
           *                  # synchronous
           * @ki_user_data:
           *         user's data for completion,value to be returned to
           *         the User Mode process
           * @ki_pos:
           *         current file offset
           * @private:
           *         private data,freely usable by the filesystem layer
           * @ki_opcode:
           *         IOCB_CMD
           * @ki_nbytes:
           *         copy of iocb->aio_nbytes - length of transfer
           * @ki_buf:
           *         location of buffer
           * @ki_left:
           *         remaining bytes
           * @ki_inline_vec:
           *         inline vector
           * @ki_iovec:
           *         io vector array,if there is only one vector,then
           *         make use of @ki_inline_vec,in this case,
           *         @iov_base is @ki_buf,and @iov_len is @ki_left
           *         # struct iovec is defined in <linux/uio.h>
           *           it contains two members: void __user *iov_base
           *                                    __kernel_size_t iov_len
           * @ki_nr_segs:
           *         number of segments,each io vector represents one segment
           * @ki_cur_seg:
           *         current segment
           * @ki_list:
           *         the aio core uses this for cancellation
           *         linked into the list of active onging I/O operation
           *         on an asynchronous I/O context
           * @ki_eventfd:
           *         kio event fd,if the @aio_resfd field of the userspace iocb
           *         is not zero,this is the underlying eventfd context to deliver
           *         events to
           * # slab cache : @kiocb_cachep
           */
          struct kiocb {
                  struct list_head ki_run_list;
                  unsigned long ki_flags;
                  int ki_users;
                  unsigned ki_key;

                  struct file *ki_filp;
                  struct kioctx *ki_ctx;
                  int (*ki_cancel)(struct kiocb *, struct io_event *);
                  ssize_t (*ki_retry)(struct kiocb *);
                  void (*ki_dtor)(struct kiocb *);

                  union {
                          void __user *user;
                          struct task_struct *tsk;
                  } ki_obj;

                  __u64 ki_user_data;
                  loff_t ki_pos;
                    
                  void *private;

                  unsigned short ki_opcode;
                  size_t ki_nbytes;
                  char __user *ki_buf;
                  size_t ki_left;
                  struct iovec ki_inline_vec;
                  struct iovec *ki_iovec;
                  unsigned long ki_nr_segs;
                  unsigned long ki_cur_seg;

                  struct list_head ki_list;

                  struct eventfd_ctx *ki_eventfd;
          };

          /**
           * init_sync_kiocb - initialize a synchronous I/O request
           * @x:               pointer to kiocb object
           * @filp:            the file I/O operation on
           * # @ki_obj.tsk = @current
           *   @ki_filp = @filp
           *   @ki_users = 1
           *   @ki_key = KIOCB_SYNC_KEY
           *   other fields are initialize to NULL or ZERO
           */
          #define init_sync_kiocb(x, filp)


          /**
           * kioctx - kernel io context,usually used for asynchronous I/O operation
           * @users:  users of this context
           * @dead:   whether this context is dead
           * @mm:     usually points to the memory descriptor of @current
           * @user_id:
           *          user id
           * @list:   linked into @ioctx_list of the memory descriptor
           * @wait:   wait queue head
           * @ctx_lock:
           *          concurrent protection
           * @reqs_active:
           *          number of activated requests
           * @active_reqs:
           *          list of active requests,used for cancellation
           * @run_list:
           *          list collected pending kiocbs,used for kicked requests
           * @max_reqs:
           *          maximum number of requests
           * @ring_info:
           *          aio ring info
           * @wq:     bottom half workqueue
           *          the handler is aio_kick_handler()
           * @rcu_head:
           *          RCU updating
           */
          struct kioctx {
                  atomic_t users;
                  int dead;
                  struct mm_struct *mm;

                  unsigned long user_id;
                  struct hlist_node list;

                  wait_queue_head_t wait;

                  spinlock_t ctx_lock;
                  
                  int reqs_active;
                  struct list_head active_reqs;
                  struct list_head run_list;
        
                  unsigned max_reqs;

                  struct aio_ring_info ring_info;

                  struct delayed_work wq;

                  struct rcu_head rcu_head;
          };

          /**
           * aio_ring_info - aio ring information
           * @mmap_base:     the starting linear address of memory mapping
           * @mmap_size:     size of the memory mapping
           * @ring_pages:    if the request pages is less than AIO_RING_PAGES,
           *                 then make use of @internal_pages;otherwise,make
           *                 use of the memory chunk get through kcalloc()
           * @ring_lock:     concurrent protection
           * @nr_pages:      number of pages
           * @nr:            usually is the number of io events,the result rely
           *                 on @max_reqs of the kioctx structure
           * @tail:          indicates the tail of @ring_pages
           * @internal_pages:
           *                 internal page descriptor array has predefined fixed size
           * # the starting of page @ring_pages[0] containing the object "struct aio_ring",
           *   and the remaining space contains the objects are type of "struct io_event"
           *   [ struct aio_ring | io_event | io_event | ... ][ io_event | io_event | ... ]...
           *   |                                              |                            |
           *   |                                              |                            +--> [2 ... N]
           *   |                                              +--> [1] 
           *   +--> [0]            <-------------------- @nr io events -------------------->
           */
          struct aio_ring_info {
                  unsigned long mmap_base;
                  unsigned long mmap_size;
                 
                  struct page **ring_pages;
                  spinlock_t ring_lock;
                  long nr_pages;

                  unsigned nr, tail;
                  
                  struct page *internal_pages[AIO_RING_PAGES];
          };

          /**
           * aio_ring - aio ring
           * @id:       kernel internal index number
           * @nr:       number of io_events
           * @head:     head of the ring
           * @tail:     tail of the ring
           * @magic:    aio magic,default is AIO_RING_MAGIC - 0xa10a10a1
           * @compat_features:
           *            compat features,default is AIO_RING_COMPAT_FEATURES - 1
           * @incompat_features:
           *            incompat features,default is AIO_RING_INCOMPAT_FEATURES - 0
           * @header_length:
           *            size of struct aio_ring
           * @io_events:
           *            variable array for io_event elements
           */
          struct aio_ring {
                  unsigned id;
                  unsigned nr;
                  unsigned head;
                  unsigned tail;

                  unsigned magic;
                  unsigned compat_features;
                  unsigned incompat_features;
                  unsigned header_length;
                  
                  struct io_event io_events[0];
          };

        <linux/aio_abi.h>
          /**
           * io_event - structure used to represent io event
           * @data:     data field from the iocb
           * @obj:      what iocb this event came from
           * @res:      result code for this event
           * @res2:     secondary result
           * # read() from /dev/aio returns these structures
           */
          struct io_event {
                  __u64 data;
                  __u64 obj;
                  __s64 res;
                  __s64 res2;
          };

          /**
           * iocb - io control block for userland
           * @aio_data:
           *        data to be returned in event's data
           * @aio_key:
           *        kernel sets this field to the req
           * @aio_reserved1:
           *        reserve
           * @aio_lio_opcode:
           *        userland aiocb.aio_lio_opcode
           * @aio_reqprio:
           *        request priority
           * @aio_fildes:
           *        file descriptor
           * @aio_buf:
           *        buffer address
           * @aio_nbytes:
           *        buffer size
           * @aio_offset:
           *        file offset
           * @aio_reserved2:
           *        reserve
           * @aio_flags:
           *        flags
           * @aio_resfd:
           *        if IOCB_FLAG_RESFD(1 << 0) is set,this is
           *        an eventfd to signal AIO readiness to 
           */
          struct iocb {
                  __u64 aio_data;
                  __u32 PADDED(aio_key, aio_reserved1);
            
                  __u16 aio_lio_opcode;
                  __s16 aio_reqprio;
                  __u32 aio_fildes;

                  __u64 aio_buf;
                  __u64 aio_nbytes;
                  __s64 aio_offset;

                  __u64 aio_reserved2

                  __u32 aio_flags;

                  __u32 aio_resfd;
          };

        !! kernel make use of aio_context_t the memory address that is the
           @mmap_base of aio_ring_info of a kioctx object to find out the
           exactly object.
           then allocate a new kiocb object,initialize it from iocb the userland
           io control block,and invoke aio_run_iocb() on the kiocb object.

        <mm/filemap.c>
          /**
           * generic_file_aio_read - generic filesystem aio reading
           * @iocb:                  kernel io control block
           * @iov:                   io vector array
           * @nr_segs:               number of segments
           * @pos:                   file offset
           * return:                 size of data have copied to buffer => succeed
           *                         error code => failed
           */
          ssize_t generic_file_aio_read(struct kiocb *iocb, const struct iovec *iov,
                                        unsigned long nr_segs, loff_t pos);

          brief description for generic_file_aio_read() :
            this routine process generic_segment_checks() on all segments,the access
            flag is VERIFY_WRITE,if each segment is access_ok(),then return 0,otherwise,
            return error code;thus generic_file_aio_read() would return it to caller,too.
            /* &@nr_segs is passed to generic_segment_checks(),the routine will adjust
             * number of segments will be readed,and calculate the total number of bytes
             * of data have to be reded from disk,the result is stored in @count.
             * @count is a local variable of generic_file_aio_read(),its address passed to
             * generic_segment_checks().
             */

            if I/O mode is O_DIRECT,that means bypass page cache,then get inode size of the
            file,we can not read the data at the position exceeded file size.
            @pos < file size
              try filemap_wrtie_and_wait_range() at first,
              if the function was succeed(returned 0),try @a_ops->direct_IO()
              if direct_IO() returned a positive value,increase @iocb->ki_pos,touch_atime()
              of the file through file_accessed() /* O_NOATIME is set,do not touch atime */
            in other case that @pos >= file size OR these two tries all are failed,give up
            direct I/O,continue page cache I/O /* returned 0 - nothing have readed */
            of course,if direct I/O succeed,do not need to proceed page cache I/O.

            I/O mode is not O_DIRECT,make use of a for-cycle,stop condition is reach maximum
            segments.
            for {
                    create a read_descriptor_t object @desc
                    initialize it from current segment - current io vector
                    /* skip zero length vector */
                    invoke do_generic_file_read(),the read method is file_read_actor()
                    updating readed bytes
                    check if any error was encountered,set return value to error code,break cycle
                          OR @count of @desc is greater than 0,this means the remaining
                             data of @filp is not enough to fulfill current vector,break cycle
                                           /* no more data */
            }

            if anything is OK,return the number readed bytes,otherwise,return error code.


            /**
             * <linux/fs.h>
             *   typedef struct {
             *           size_t written;  /* number of bytes has written into buffer */
             *           size_t count;    /* number of bytes expect to be readed */
             *           union {
             *                   char __user *buf;  /* buffer from userspace */
             *                   void *data;        /* data buffer in kernel space,
             *                                       * for special purpose,such TCP tss
             *                                       */
             *           } arg;
             *           int error;                 /* record error code */
             *   } read_descriptor_t;
             */

          /**
           * do_generic_file_read - do generic file reading
           * @filp:                 file object
           * @ppos:                 current file position
           * @desc:                 read descriptor
           * @actor:                read method,usually is file_read_actor()

           */
          static void do_generic_file_read(struct file *filp, loff_t *ppos,
                                           read_descriptor_t *desc, read_actor_t actor);

          description for do_generic_file_read() :
            get mapping of @filp,the @f_mapping.
            get host of the mapping,that is @filp->f_mapping->host.
            /* if the file is a block device file,then @host is the inode in bdev filesystem,
             * rather than the inode pointed to by @filp->f_dentry->d_inode
             */
            convert *@ppos to the index of cached page of @filp through "*@ppos >> PAGE_CACHE_SHIFT",
            then get page offset through "*@ppos & ~PAGE_CACHE_MASK".
            get the record about last read() on page cache,that is stored in @filp->f_ra.
            /**
             * get prev index := @filp->f_ra->prev_pos >> PAGE_CACHE_SHIFT
             * get prev offset := @filp->f_ra->prev_pos & (PAGE_CACHE_SIZE - 1)
             *
             * get last index := (*@ppos + @desc->count + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT
             * # the last index of page which should holding the request data
             */
            enter a infinite for-cycle {
                    check kernel preempt
                    find_page:
                      get the target cached page trough find_get_page()

                      if no such page,then invoke page_cache_sync_readahead() and find_get_page() again,
                      if it is still not exist,goto label "no_cached_page".
                      /* page_cacyhe_sync_readahead() is defined in <mm/readahead.c>,it do force readahead
                       * or ondemand readahead
                       */

                      check if the page is readahead page - PG_reclaim is set,then invoke page_cache_async_readahead().
                      /* if no such cached page,we do synchronous readahead,otherwise,do asynchronous readahead */

                      next check whether the page is not up to date - PG_uptodate is cleared,if it is the case,then check
                        > @i_blkbits is equal to PAGE_CACHE_SHIFT OR @a_ops->is_partially_iptodate is implemented,
                          goto label "page_not_up_to_date"
                        > try lock the page failed,then goto "page_not_up_to_date"
                        > if @a_ops->is_partially_uptodate() returned FALSE,goto "page_not_up_to_date"
                        otherwise,unlock the page and get into "page_ok"
                      
                    /* the main procedure to prepare reading */
                    "page_ok" : the page is cached and ready for reading
                      if inode size is zero OR the page index is greater than end index,then we can not read the
                      cache /* end index - (inode size - 1) >> PAGE_CACHE_SHIFT */,release this page and goto
                      "out". /* maybe truncated by other */

                      if the page index is end index,then set the size @nr(default is PAGE_CACHE_SIZE) to
                      "(inode size - 1) & ~PAGE_CACHE_MASK + 1,it is the maximum number of bytes to copy from this
                      cached page.
                      of course,if @nr is less than or equal to page offset,that means no more data can be readed,then
                      release the cached page and goto label "out".

                      after checking accomplished,update @nr to @nr - offset,the length of remaining data in the page
                      starting from the offset.

                      next,check mapping_writably_mapped(),if it resulted in TRUE,invoke flush_dcache_page() on the
                      page before reading the page on the kernel side.
                      /* users can be writing to this page using arbitrary virtual addresses */

                      mark_page_accessed(),if we are not progress on previous readahead index. 
                      /* mark the page as accessed only at the first time */
                      /* prev index != current index OR prev offset != current offset */
                      /* PG_referenced | PG_active,the page is using,should not be swapped out */
                      /**
                       * inactive,unreferenced -> inactive,referenced
                       * inactive,referenced -> active,unreferenced
                       * active,unreferenced -> active,referenced
                       */

                      update previous index to this page's index. /* prev index */

                      invoke @actor() to read data

                      if @count is not decreased to _zero_ 
                      AND no error happned -  return value of @actor is equal to @nr(the size of data we wanted to read)
                      then we must get into next for-cycle iteration to read remaining data,so,update offset and index
                      - we will read this page at next iteration,release current page and continue.
                      /* prev offset is reset to the updated offset == offset + return value from @actor() */

                      otherwise,@count is _zero_ - all wanted data have readed,goto "out"(no more data have to be readed)
                             OR return value of @actor is not equal to @nr - failed to read the data has expected length,
                                goto label "out".(@count is not _zero_ AND return value is not equal to @nr)

                    /**
                     * lock_page_killable() - <linux/pagemap.h>
                     *   try to lock the page,if failed,wait in page_waitqueue(@page) until PG_locked clear
                     *   process enter TASK_KILLABLE,the action is sync_page_killable()
                     * # some routine such readpage() would clear PG_locked before return to caller.
                     */

                    page_not_up_to_date:
                      try lock_page_killable() on current page,goto readpage_error label if lock failed.
                      otherwise,proceed "page_not_up_to_date_locked"
                    page_not_up_to_date_locked:
                      continue to next iteration if current page's mapping is NULL
                      /* the page is not cached,maybe is has removed by another kernel control
                       * path when we waiting for PG_locked cleared.
                       * in the next iteration,we will operate on the page has same index.
                       */
                      check PageUptodate() on current page,may be another kernel control path have updated it,it is
                      the case,unlock the page and goto "page_ok".
                      otherwise,proceed "readpage".
                    readpage:
                      clear page error flags
                      invoke @a_ops->readpage(),goto "readpage_error" if any error except AOP_TRUNCATED_PAGE was encountered.
                      /* for AOP_TRUNCATED_PAGE,release page cache,goto find_page label and try again */
                      if the page is still have not up to date,then lock_page_killable(),goto "readpage_error"
                      if failed on locking.
                      check up to date again /* we have waited a while for I/O transfer */,if it still have not up to date,
                      then
                        check its mapping,if it is NULL,unlock the page,release it,and goto "find_page" - routine
                        invalidate_mapping_pages() got it /* invalidate all the unlocked pages of one inode */
                      otherwise,unlock the page,invoke shrink_readahead_size_eio(),set error code to -EIO,goto
                      label "readpage_error".
                      if we succeed to readahead the page,goto "page_ok" to process reading.
                    readpage_error:
                      set @desc->error to the error code
                      release the page
                      goto out
                    
                    no_cached_page: the page has not cached
                      alloc a new cold page for the mapping,if failed,the return error code is -ENOMEM,goto out.
                      invoke add_to_page_cache_lru() to the mapping,if failed,then
                        release the page
                        failed because -EEXIST,goto "find_page"
                        otherwise,set @desc->error to error code,goto out
                      if we succeed to add the page cache,goto "readpage" - it is a new page,no valid data in it,
                      have to fill it from disk file.    
            }

            out:
              update members of @filp->f_ra to record last readahead info.
              /**
               * updating :
               *   @prev_pos := (prev index << PAGE_CACHE_SHIFT) | prev offset
               *   # do the reverse
               */
              update *@ppos,this parameter is @ki_pos,and finally,the file's position will be updated by use @ki_pos.
              /* *@ppos may remain unchanged if we breaked cycle before we start @actor(),this is because
               * file has truncated,or readpage error,etc.
               */
              invoke file_accessed() on @filp to update atime,if necessary.
                     /* call to touch_atime() with @file->f_path.mnt and @file->f_path.dentry
                      *                                          ^vfsmount             ^dentry
                      * update atime of the inode get from @dentry->d_inode
                      * # __makr_inode_dirty(@inode, I_DIRTY_SYNC),sync inode's access time
                      * # before set inode's atime,have to invoke mnt_want_write() on @mnt to get
                      *   write access to the mount,and invoke mnt_drop_write() later
                      */
            image >

              if @len spanned two or more pages,the for-cycle must get continue into next iteration for read more
              data,except the file is end or other error was happend.

              if @pgindex falled inside "un-cached" zone,must process readahead some contents of the file on disk to
              fill several pages and add them into page cache of the inode.
              /* depends on readahead strategy */

              !O_DIRECT --+                                                                      O_DIRECT --+
                          |                                                                                 |
                          |                                                                                 |
            +-------------+                                                                                 |
            |                                                    +--> page index                            |
            |                   +-> len <-+ => spanned two pages |                                          |
            V                   |         |                      |                                          |
            [ page0 | page1 | page2 | page3 | ... | pageN | <-- un-cached --> ] => inode mapping <--+       |
                              |  |                                                                  |       |
                              |  |                                                                  |       |
                              |  |                                                                  |       |
                              +--|--> page index    /* f_pos >> PAGE_CACHE_SHIFT */                 |       |
                              |  |                                                                  |       |
                              |  +--> page offset   /* f_pos & ~PAGE_CACHE_MASK */                  |       |
                              |  |                                                                  |       |
                         +--> f_pos                                              (direct IO failed) |       |
                         |                                                              +-----------+       |
            <=======================================================>  => file stream   |                   |
            [ sector1 | sector2 | sector3 | sector4 | sector5 | ... ]  => file on disk <+-------------------+            
              |<----------- one page ------------>|
              /* suppose PAGE_SIZE is 4096,sector size is 512 */

          /**
           * file_read_actor - file read actor for do_generic_file_read()
           * @desc:            read descriptor
           * @page:            page descriptor
           * @offset:          current page offset
           * @size:            size of data to be readed
           * return:           size of data have readed
           */
          int file_read_actor(read_descriptor_t *desc, struct page *page,
                              unsigned long offset, unsigned long size);

          brief description for file_read_actor() :
            first,compare @size and @desc->count,if the size to be readed from current @offset
            is greater than the expected length,then set @size to @desc->count - we do not need
            more data. /* may be the size of buffer is not great enough to contain the data */
            invoke fault_in_pages_writeable() on @desc->arg.buf attempt find out page fault on
            the userspace page.
            /**
             * <linux/pagemap.h>
             *   static inline int fault_in_pages_writeable(char __user *uaddr, int size);
             *   - try to put _zero_ to @uaddr,because the type of *@uaddr is char,so
             *     __put_user_1() will be called.if __put_user_1() returned -EFAULT,that
             *     means the @uaddr is not a valid userspace address(__put_user() bypass address checking),
             *     then return -EFAULT to caller.
             *     if we succeed to put to user at the starting memory unit of buffer,then we
             *     next have to check then end memory unit of the buffer.
             *     if the page index of @uaddr is not equal to the page index of end address of buffer,
             *     try to __put_user_1() on the end address.if we failed,that means the buffer spanned
             *     two pages and the second page is not valid.
             */
            if the buffer is access ok,then kmap the cached page would be readed into kernel linear address,
            the kmap type is KM_USER0. /* via kmap_atomic(),establish kernel fix-mapping */
            invoke __copy_to_user_inatomic() for copy data in memory started at "@kaddr(mapped linear address)
            + @offset" to userspace @desc->arg.buf,the size of data is @size.
            then unmap @kaddr,check whether there is data left,if no left,then update @desc and return @size.
            /* all expected data have copied */
            if there remaining some data have to be copied,then invoke kmap() on @page again /* slow way */,
            invoke __copy_to_user(),the arguments are not changed,that means we overwrite the data in buffer
            they are copied in previous __copy_to_user_inatomic().
            if still data left,update @size to @size - left bytes,set @desc->error to -EFAULT,update @desc
            and return @size;if no left,just update @desc and return @size.
            /**
             * @desc updating -
             *   @count -= @size
             *   @written += @size
             *   @buf += @size => step buffer position for next reading
             */

        The "readpage" method for regular files :
          function do_generic_file_read() invoke the method readpage() of address space of the inode to
          read the page of the file from disk to page cache.
          for ext2 filesystem,readpage() method of its @ext2_aops is set to mpage_readpage(),ext2 pass
          it the parameters @page that need to be filled,and a function ext2_get_block().
          function mpage_readpage() is a common interface to read page from disk and add it into page cache.

          <fs/mpage.c>
            - contains functions related to preparing and submitting BIOs which contain multiple pagecache
              pages

            /**
             * mpage_readpage - common interface to read a page from disk to page cache
             * @page:           the page have to be filled
             * @get_block:      filesystem-dependent block mapper function,
             *                  used to translate the block in file to the logical block in disk
             *                  # block in file : get through page index 
             *                  # logical block in disk : the logical block relative to
             *                  #                         position of the block in the disk partion
             *                  # typedef int (get_block_t)(struct *inode inode, sector_t iblock,
             *                  #                           struct buffer_head *bh_result, int create);
             *                  # <linux/fs.h> 
             * return:          always 0
             * # EXPORT_SYMBOL
             * # this function create a buffer_head object named @map_bh,set its @b_state and @b_size
             *   to _zero_,pass it to the _main_ routine do_mpage_readpage().the arguments are -
             *     NULL, @page, 1, address of local variable @last_block_in_bio, &@map_bh,
             *     address of local variable @first_logical_block, @get_block
             *   @last_block_in_bio is type of sector_t,initialized to 0
             *   @first_logical_block is type of unsigned long,initialized to 0
             *   if do_mpage_readpage() returned a valid pointer points to a bio object,then invoke
             *   mpage_bio_submit() for submit block i/o operation READ on the bio
             *   finally,return 0 to caller
             * # if the first parameter @bio of do_mpage_readpage() is NULL,then it must invoke
             *   mpage_alloc() to allocate a new bio object,return NULL if allocating failed
             */
            int mpage_readpage(struct page *page, get_block_t get_block);

            /**
             * do_mpage_readpage - do mpage_readpage
             * @bio:               make use this bio to submit bio,if it is NULL,allocate a new one
             * @page:              the page have to be filled
             * @nr_pages:          number of pages,if this function is called from mpage_readpage(),
             *                     this parameter should be 1
             * @last_block_in_bio: the last logical block number in the bio,if @bio is NULL,this parameter
             *                     should be 0
             * @map_bh:            buffer head,represents a the buffer will be used to map the block
             * @first_logical_block:
             *                     the first logical block number
             * @get_block:         filesystem-dependent function used to get the block
             * return:             NULL OR newly allocated bio OR @bio
             */
            static struct bio *do_mpage_readpage(struct bio *bio, struct page *page, unsigned nr_pages,
                                                 sector_t *last_block_in_bio, struct buffer_head *map_bh,
                                                 unsigned long *first_logical_block, get_block_t get_block);

            what do_mpage_readpage() does :
              1> get inode object through @page->mapping->host.
                 get block bits from the inode's @i_blkbits.
                 get number blocks for per-page through "PAGE_CACHE_SIZE >> block bits".
                 /* PAGE_CACHE_SIZE usually is PAGE_SIZE */
                 the block size is "1 << block bits" == 2^block-bits
                 set local variable @first_hole to @blocks_per_page,the value is PAGE_CACHE_SIZE / block size.
                 set local variable @fully_mapped to 1.
                 decalre an array is type of sector_t named @blocks,the size is MAX_BUF_PER_PAGE,it usually
                 is PAGE_CACHE_SIZE / 512(sector size).
                 /* the array @blocks will storing the logical block numbers that we deal with */
              2> invoke page_has_buffers() on @page,if @page has PG_private,then goto label "confused".
              3> calculate block in file,this is the reverse that did in __find_get_block_slow().
                 the block in file is "@page->index << (PAGE_CACHE_SHIFT - blkbits)".
                 /* PAGE_CACHE_SHIFT usually is PAGE_SHIFT */
                 we can consider the file is composed by many pages.
                 [ page0 | page1 | page2 | ... | pageN ] => file A
                 the number of blocks in the file A is (N + 1) * blocks-per-page.
                 @page->index * 2^(PAGE_CACHE_SHIFT - block bits)
                 => @page->index * 2^PAGE_CACHE_SHIFT / 2^block-bits
                 => current block number
                 /* @page->index * 2^PAGE_CACHE_SHIFT can get the length of mapping before current position,
                  * the result divide the size of a block can get current block number.
                  */
              4> calculate last block,it is "block-in-file + @nr_pages * blocks-per-page".
                 calculate last block in file,it is the last block at the file's end.result is
                 "(i_size_read(inode) + block-size - 1) >> block-bits"
                 /* if at the end of file,data length is not sufficient to fill a block,the disk stay
                  * transfer a sector of data,thus we count it as a block
                  */
                 check,if the last-block is greater than last-block-in-file,reset it to last-block-in-file,
                 we can not read other sectors they are not belong to the file.
                 set counter @page_block to _zero_.
              5> check,if buffer_mapped() @map_bh AND block-in-file is greater than *@first_logical_block AND
                 block-in-file is less than (*@first_logical_block + nblocks(: @map_bh->b_size >> block-bits))
                 then                                               /* number of blocks in the mapping */
                         /* buffer_mapped() => BH_Mapped is set,if the buffer has been mapped,
                          * current block-in-file must greater than *@first_logical_block,and
                          * less than maximum allowed block number in file
                          */
                         set local variable @map_offset to block-in-file - *@first_logical_block
                         /* current offset in mapping */
                         set local variable @last to @nblocks - @map_offset
                         /* the end position in mapping */


                         /* fill @blocks */
                         enter a for-cycle,it would be breaked if some conditions have satisfied :
                           iterator started from _zero_,and increase before next iteration
                             if @i is equal to @last,then clear_buffer_mapped() for @map_bh,and break.
                             /* we have iterated all blocks in the mapping */
                             if @page_block is equal to blocks-per-page,then break.
                             /* the maximum number of traversable blocks has reached */
            
                             fille @blocks[@page_block] to @map_bh->b_blocknr + @map_offset + @i
                             ++@page_block
                             ++@block_in_file

                         set local variable @bdev to @map_bh->b_bdev

                         !! if do_mpage_readpage() is called from mpage_readpage(),then @b_bdev and
                            @b_blocknr of @map_bh must be _zero_.
              6> set @map_bh->b_page to @page,the mapping resided on @page.
                 make use of a while-cycle,the stop condition is @page_block is greater than or equal to
                 blocks-per-page.
                 while {
                         /* if we processed the branch at step 5>,then @page_block might increased.
                          * if @page_block is not reach blocks-per-page,then we have to determine whether
                          * need to do more get_blocks calls until we are done with this page.
                          * if we did not process the branch,then @page_block must be _zero_,thus we must
                          * get blocks.
                          * # @map_bh is not BH_mapped OR block-in-file is not valid
                          * # if this function is called from mpage_readpage(),then its @b_size and @b_state
                          *   is _zero_,thus the branch at step 5> would not be processed.
                          */

                          set @map_bh->b_state to 0,set @map_bh->b_size to 0.
                          if block-in-file is less than last-block
                          then
                                  /* in most case,this branch would be processed,so @get_block() will be called */
                                  set @map_bh->b_size to (last-block - block-in-file) << block-bits

                                  /**
                                   * we attempt to get a chunk of blocks they covered all pages at once.
                                   * if succeed,@get_block() will set BH_Mapped,we will split them to individual
                                   * blocks later.
                                   */
                
                                  invoke @get_block() with arguments -
                                    inode, block-in-file, @map_bh, 0
                                  if @get_block() returne TRUE,goto label "confused".
                                  /* for ext2,@get_block return _zero_ if succeed,otherwise,return a negative error code */
                                  /* for ext2,the function is ext2_get_block(),which invoke ext2_get_blocks(),
                                   * parameters is -
                                   *   @inode:      inode
                                   *   @iblock:     the starting logical block number
                                   *   @max_blocks: defined the range block numbers [@iblock, @iblock + @max_blocks)
                                   *   @map_result: result of buffer mapping
                                   *   @create:     if it is _zero_ OR error code is -EIO,do not allocate new block
                                   */

                                  set *@first_logical_block to block-in-file,it is the first block that we succeed to
                                  get block.

                          check if @map_bh is still unmapped
                          then
                                  /* if filesystem succeed to get block,it will invoke map_bh() to set the buffer.
                                   * map_bh() is defined in <linux/buffer_head.h>
                                   * - set BH_Mapped
                                   *   set @b_bdev # super block object's @s_bdev
                                   *   set @b_blocknr
                                   *   set @b_size # super block object's @s_blocksize
                                   * thus,@map_bh is still unmapped,that means filesystem failed to get block,ot
                                   * it failed on plain lookup.
                                   */
                                  set @fully_mapped to _zero_ /* we failed on get a specific block,so it is not
                                                               * fully mapped.
                                                               */
                                  if @first_hole is equal to blocks-per-page,then set @first_hole to @page_block
                                  ++@page_block
                                  ++block-in-file
                                  continue to next iteration

                          check if @map_bh is up-to-date -- BH_Uptodate,then
                                  invoke map_buffer_to_page() with arguments - @page,@map_bh,@page_block
                                  /* the filesystem supplied @get_block might return an up to date buffer,
                                   * this is used to map that buffer into the page,which allows readpage to
                                   * avoid triggering a duplicate call to @get_block.
                                   * for ext2,if succeed to get block,it will set BH_New,but without BH_Uptodate.
                                   *
                                   * if @page is not have buffers -- PG_private is cleared
                                   * then
                                   *         if @inode->i_blkbits == PAGE_CACHE_SHIFT AND @map_bh has BH_Uptodate
                                   *         then we can just set @page PG_uptodate and return.
                                   *         else,invoke create_empty_buffers() to create buffers on @page,the block
                                   *         size is 1 << @inode->i_blkbits,state is 0. # call to alloc_buffer_pages()
                                   *         # a proper number of buffers will be created to cover the whole page.
                                   *           and,if the page is PG_uptodate or PG_dirty,then it will traverse
                                   *           buffers again;for each buffer,if page is dirty,set BH_Dirty for it,
                                   *           if page is uptodate,set BH_Uptodate for it
                                   * if @page have buffers -- PG_private is set
                                   * then
                                   *         traverse the buffer heads in @page
                                   *         for the buffer which corresponding to @page_block,set it -
                                   *           @b_state   = @map_bh->b_state
                                   *           @b_bdev    = @map_bh->b_bdev
                                   *           @b_blocknr = @map_bh->b_blocknr
                                   *           # let the anonymous buffer to represent @map_bh  
                                   */

                          check,if @first_hole is not equal to blocks-per-page,then goto "confused".
                          /* hole.
                           * if we did not encounter block hole,then @first_hole must be equal to blocks-per-page,
                           * otherwise,it must recorded the first block hole that detected in the branch
                           * checking for BH_Mapped,in this case,the cycle will be breaked
                           * !! hole - some blocks are not adjacent on disk
                           *           some blocks fall inside a "file hole"
                           */

                          if @page_block is not _zero_ AND @blocks[@page_block - 1] != @map_bh->@b_blocknr - 1
                          then,goto "confused" /* contiguous blocks ? */
                                               /* blocks are not adjacent on disk */

                          set @nblocks to @map_bh->b_size >> block-bits
                          
                          /* similar to the  cycle in step 5>,we make use @blocks to record the logical block numbers
                           * of the blocks has size "PAGE_CACHE_SIZE / MAX_BUF_PER_PAGE".
                           */
                          enter a for-cycle,iterator @i started from _zero_,increase before next iteration {
                                  if @i == @nblocks
                                          clear buffer mapped on @map_bh,break
                                          /* we must clear BH_Mapped,because the buffer @b_size spanned all pages. */
                                  else if @page_block == blocks-per-page,break /* reached maximum blocks in a page */

                                  /* record each block number */
                                  set @blocks[@page_block] to @map_bh->b_blocknr + @i
                                  ++@page_block
                                  ++block-in-file
                          }

                          set local variable @bdev to @map_bh->b_bdev
                 }
              7> if @first_hole is not equal to blocks-per-page /* has hole */
                 then
                         zero_user_segment() to fill the hole by _zero_,the range is -
                           [kaddr of @page + "@first_hole << block-bits", PAGE_CACHE_SIZE - 1]
                         /*
                          *                                          |<-- file hole -->|
                          * for case > [ block | block | ... | block | <hole> | <hole> ] => a page
                          *                                          |<-- fill to 0 -->|
                          * BH_Mapped is set by filesystem interface,and the chunk of blocks has been
                          * splited to several individual blocks.
                          * while-cycle is stopped at the beginning of next iteration -- @page_block == blocks-per-page
                          */

                         next,check if @first_hole is 0,then set PG_uptodate,unlock @page,goto "out".
                 else if @fully_mapped is TRUE
                 then
                         set PG_mappedtodisk for @page
              8> if @bio is valid AND *@last_block_in_bio != @blocks[0] - 1
                 then invoke mpage_bio_submit() for READ operation on @bio.
                 /* this page will go to BIO,and we need to send this BIO off at first */
              alloc_new:
              9> if @bio is NULL,then invoke mpage_alloc() to allocate a new bio.
                 its first sector is @blocks[0] << (block-bits - 9),number of sectors is
                 the minimum value in "@nr_pages, bio_get_nr_vecs(@bdev)".
                 if failed on allocating,goto label "confused".
             10> set local variable @length to @first_hole << block-bits
                 invoke bio_add_page() on the newly allocated bio,attempt to add @page to it,length
                 is @length,offset is _zero_.
                 if succeed,then invoke mpage_bio_submit() for READ operation,and goto "alloc_new".
                 /* mpage_bio_submit() always return NULL,it invoke submit_bio() with @rw operation. */
             11> set local variable @relative_block(this variable exactly is @i in the cycles for fill @blocks)
                 to block-in-file - *@first_logical_block.
                 set @nblocks to @map_bh->b_size >> block-bits.
                 check,if (@map_bh has set BH_Boundary AND @relative_block is equal to @nblocks) OR
                 @first_hole is not equal to blocks-per-page
                 then
                         invoke mpage_bio_submit() for READ operation on @bio
                 else
                 then
                         set *@last_block_in_bio to @blocks[blocks-per-page - 1] /* the last logical block number
                                                                                  * we have dealt with
                                                                                  */
             out:
             12> return @bio.
             confused: /* confused cases */
             13> if @bio is not NULL,set @bio to mpage_bio_submit() for READ operation,the result must be NULL.
                 if no PG_uptodate has set,invoke block_read_full_page() on @page,the method is @get_block.
                 else,unlock @page.
                 /* block_read_full_page() -
                  *   we have known that,the while-cycle attempt to get blocks they covered all required
                  *   pages.but if we found hole,then must each get block in a fixed size of the page.
                  *   that is what block_read_full_page() does.
                  *   it invoke @get_block with a result buffer head has @b_size "2^@i_blkbits".
                  *   the procedure would stopped when the traversing to buffer head list has accomplished.
                  *   finally,for each no BH_Uptodate buffer,invoke submit_bh() on it to submit BIO.
                  *   ! @get_block only be called if current block number is less than the last block number,
                  *     and current block number would be updated everytime at the end of iteration.
                  */

                 goto "out".

            !! [ buffer0 | buffer1 | ... | buffer(MAX_BUF_PER_PAGE - 1) ] => a page in page cache
                 |
                 +--> block number of buffer0 := page->index << (PAGE_CACHE_SIZE - i_blkbits)
                      block number of buffer1 := block number of buffer0 + 1
                      ...
               once do_mpage_readpage(),will create MAX_BUF_PER_PAGE buffers on the page,and get block
               for each of them.if any holes are detected at the tail,fill them with _zero_.
               thus,block-in-file must be a multiple of MAX_BUF_PER_PAGE.

               [ block0 | block1 | <hole> ] the last page of file on disk
               last block number in file is -
                 number of block0 + MAX_BUF_PER_PAGE

        The "readpage" method for block device files :
          the default address space operations for a block device is @def_blk_aops,which is defined in
          <fs/block_dev.c>.

          <fs/block_dev.c>
            static const struct address_space_operations def_blk_aops = {
                    .readpage = blkdev_readpage,
                    .writepate = blkdev_writepage,
                    .sync_page = block_sync_page,
                    .write_begin = blkdev_write_begin,
                    .write_end = blkdev_write_end,
                    .writepages = generic_writepages,
                    .releasepage = blkdev_releasepage,
                    .direct_IO = blkdev_direct_IO,
            };

            /**
             * blkdev_readpage - readpage method for block device files on page cache
             * @file:            file descriptor
             * @page:            the page have to be filled
             * return:           always return 0
             * # this routine actually invoke block_read_full_page() with @get_block := blkbdev_get_block()
             */
            static int blkdev_readpage(struct file *file, struct page *page);

            /**
             * blkdev_get_block - common get_block method for block device files
             * @inode:            device file inode
             * @iblock:           block number
             * @bh:               buffer used to mapping the block
             * @create:           used to indicate whether allow to allocate new block,
             *                    for this function,it usually is _zero_
             * return:            0 OR -EIO
             */
            static int blkdev_get_block(struct inode *inode, sector_t iblock,
                                        struct buffer_head *bh, int create);
            
            brief description for blkdev_get_block() :
              block device file play the role that handle communication between user and block device driver.
              thus,it has no disk image,hence no block and sector.
              nevertheless,the function stay works on blocks.
              first,invoke max_block() to get the maximum number of blocks of the device,pass it the argument
              I_BDEV(@inode). /* bdev_inode->bdev */
              /**
               * max_block() is defined in the same file.it requires a block device descriptor,return the value
               * is type of sector_t.
               * the value would be returned has two cases :
               *   1> i_size_read() on @bdev->bd_inode returned _zero_,the return value would be ~((sector_t)0)
               *   2> i_size_read() on @bdev->bd_inode returned a non _zero_ value,retrieve block-size in the
               *      block device through block_size(),and calculate the block-size-bits through blksize_bits(),
               *      finally,return the value "size-of-inode >> block-size-bits"
               */
              if @iblock is greater than or equal to the max-block,just return 0 to caller as well.
                                                                   /* if @create is TRUE,return -EIO.
                                                                    * we might need to create a new block
                                                                    * for WRITE operation.
                                                                    */
              otherwise,set @bh->b_bdev to the block device descriptor that is got through I_BDEV(inode),
              set @bh->b_blocknr to @iblock,set BH_Mapped for @bh,return _zero_ to caller.
              /**
               * no BH_Uptodate is set,block_read_full_page() have to update buffers by itself.
               * if @iblock exceeded max-block,kernel can still try to dispatch a read request for
               * the last data of a block device,and the corresponding buffer page is only partially mapped.
               * # this is why return 0 to caller for READ operation even exceeded max-block.
               */

          <fs/buffer.c>
            /**
             * block_read_full_page - generic readpage function for block devices that have the
             *                        normal get_block functionality
             * @page:                 the page have to be filled
             * @get_block:            get_block method
             * return:                always 0
             * # EXPORT_SYMBOL
             * # read the page asynchronously
             *   the unlock_buffer() and set/clear_buffer_uptodate() functions propagate buffer state
             *   into the page struct once I/O has completed
             */
            int block_read_full_page(struct page *page, get_block_t *get_block);

            description for block_read_full_page() :
              the function do_mpage_readpage() attempts to get a chunk of blocks,if it failed on that,
              then this routine would be called.
              
              first,do BUG checking,the page must be PG_locked,otherwise,it is a BUG.
              get block-size through "1 << @page->mapping->host->i_blkbits".
              if @page has no buffers,then invoke create_empty_buffers() on it,size is the block-size,
              state is 0.
              let local variable @head points to @page->private,the head buffer of this page.
              calculate starting block number,it is same as block-in-file in do_mpage_readpage(),
              the last-block is also same as last-block in do_mpage_readpage().
              make use of a do-while cycle,it would stopped when we have traversed each buffer of @page.
              do-while {
                      if current buffer has set BH_Uptodate,we will skip it and get into next iteration.
                                                /* contains valid data */    

                      check BH_Mapped for current buffer
                      F =>
                              set local variable @fully_mapped to 0,it plays the same role in do_mpage_readpage().
                              print warning if @b_size of current buffer is not equal to block-size.
                              /* the total number of buffers of the page might different from us expected */

                              invoke @get_block(),at there,this method would not get a chunk of blocks,the size
                              of block is block-size,which is set in buffer head.
                              set PG_error if @get_block() returned an error code.
                              
                              because we have known filesystem-dependent @get_block migth set BH_Mapped for the buffer,
                              if the buffer stay unmapped,must use _zero_ to fill the hole,and set BH_Uptodate for it
                              if and only if @get_block() did not return an error code;then continue.

                              if the buffer been mapped,then check if @get_block have updated it synchronously,if it
                              is the case,continue to next iteration.                 /* BH_Uptodate */

                      BH_Mapped is set but has not uptodate,so we record it in @arr,the local array of buffer_head
                      objects,size is MAX_BUF_PER_PAGE. /* ++array_index */
              } (++index, ++current-block-number)
                /* @index is used to calculate page offset for zero filling */

              if the page is fully mapped disk image,then set PG_mappedtodisk for it. /* @fully_mapped is TRUE */
                                                                                      /* no hole */
              if @array_index is _zero_,that means all buffers are uptodate.we can set PG_uptodate for @page,if no
              PG_error /* maybe @get_block() returned error code */;then unlock @page,return 0.
              /* @array_index is not _zero_,that means there are some buffers has not updated */

                                                 /* BH_Lock,for async READ */
              for each buffer it has not updated,lock it,and invoke mark_buffer_async_read() on it.
                                                 /* might sleep */  /* set @b_end_io to end_buffer_async_read()
                                                                     * set BH_Async_Read
                                                                     */

                                                                   /* underlying blockdev brought it uptodate */
              for each buffer it has not updated,check BH_Uptodate,if it has set,then invoke end_buffer_async_read() on it
              with @uptodate := 1;otherwise,invoke submit_bh() to submit a READ operation for it.

              finally,return 0.

              /**
               * statid void end_buffer_async_read(@bh, @uptodate); # defined in <fs/buffer.c>
               *   it is a BUG if @bh has not set BH_Async_Read.
               *   if @uptodate is TRUE,set BH_Uptodate for @bh;otherwise,clear BH_Uptodate,if no BH_Quiet has set,
               *   invoke buffer_io_error() to printk kernel message.set PG_error because @uptodate is FALSE.
               *   store local interrupt state,disable local interrupt.
               *   wait on BH_Uptodate_Lock(bit spinlock) of the first buffer of the page. # get from @bh->b_page
               *   after acquired spinlock,clear "BH_Async_Read" for @bh,unlock @bh.
               *                                 # because we are end buffer async read on @bh
               *   next,use a do-while cycle to traverse each buffer starting from @bh.(doubly linked list)
               *           # there stay some not updated buffers,so we can not set PG_uptodate for the page.
               *           if current buffer has not set BH_Uptodate,set local variable @page_uptodate to _zero_.
               *                                                                        # default value is 1
               *           # except @bh,if there at least one buffer is in async READ -- still busy
               *           if BH_Async_Read is set for current buffer,then do BUG checking,it is a BUG if no BH_Lock,
               *           and goto label "still_busy" -- the I/O is in progressing.
               *   release spinlock,restore local interrupt.
               *   if @page_uptodate is TRUE AND no PG_error,set PG_uptodate for the page.
               *   unlock the page,return to caller.
               *
               *   "still_busy" :
               *     the asynchronous READ operations in progess.just release the spinlock,restore local interrupt,
               *     return to caller.
               *     # do not check page errors AND page uptodate,of course,would not set PG_uptodate.
               */

      Read-Ahead of Files :
        usually,regular files are stored on disk in large groups of adjacent sectors,so that they can be retrieved
        quickly with few moves of the disk heads.
        Read-Ahead consists of reading several adjacent pages of data of a regular file or block device file before
        they are actually requested.
        Read-Ahead can enchances disk performance,improve system responsiveness;but Read-Ahead is useless when a process
        random accesses to file.
        kernel reduces or stops Read-Ahead when it determines that the most recently issued I/O access is not sequential
        to the previous one.

        reasons for sophisticated algorithm needed by Read-Ahead :
          1> because data is read page by page,the read-ahead algorithm does not have to consider the offsets
             inside the page,but only the positions of the accessed pages inside the file.
          2> read-ahead may be gradually increased as long as the process keeps accessing the file sequentially.
          3> read-ahead must be scaled down or even disabled when the current access is not sequential with respect
             to the previous one(random access).
          4> read-ahead should be stopped when a process keeps accessing the same pages over and over again,or when
             almost all pages of the file are already in the page cache.
          5> the low-level I/O device driver should be activated at the proper time,so that the future pages will
             have been transferred when the process needs them.

        ! kernel considers a file access as sequential with respect to the previous file access if the first page requested
          is the page following the last page requested in the previous access.

        current window :
          consits of pages requested by the process or read in advance by the kernel and included in the page cache.
          /* a page in current window is not necessarily up-to-date,because its I/O data transfer could be still in progress */
          contains both the last pages sequentially accessed by the process and possibly some of the pages that have been read
          in advance by the kernel but that have not yet been requested by the process.

        ahead window :
          consists of pages following the ones in the current window that are being currently being read in advance by the kernel.
          /* kernel assumes these pages would be requested by the process.
           * when the kernel recognizes a sequential access and the initial page belongs to the current window,it attempts to
           * setup(creates a new ahead window,triggers READ operations) ahead window if the window has not been set up
           */
          when the process needs the pages included in ahead window,the ahead window becomes the new current window.
          /* similar to GPU Double Buffering */

        ! Read-Ahead mechanism make use of these two windows to implement the purpose.
        ! kernel use the structure named "file_ra_state" to keep track a single file's readahead state,
          and the Read-Ahead information about the file is stored in its file object,member name is @f_ra.

        The file_ra_state_init() routine :
          <mm/readahead.c>
            /**
             * file_ra_state_init - initialize file's readahead state
             * @ra:                 pointer to file_ra_state object of the file
             * @mapping:            page cache
             * # EXPORT_SYMBOL
             * # this routine set @ra->ra_pages to @mapping->backing_dev_info->ra_pages,and
             *   set @ra->prev_pos to -1;the other fields should been initialized to zero by
             *   kmem_zalloc() in get_empty_filp()
             * # __dentry_open() call to this routine by pass it the file's @f_ra and the file's
             *   @f_mapping->host->i_mapping
             *   (for device files,the mapping should be the mapping of their master inode)
             *   ! if the inode associated with a regular file,the backing_dev_info usually is
             *     @default_backing_dev_info;if the super block provide bdev,then the client inode
             *     make use the backing device info of the bdev(these are setup in init_inode_always())
             */
            void file_ra_state_init(struct file_ra_state *ra, struct address_space *mapping);

            ! POSIX.1 interface posix_fadvise() is able to give a file data advise to kernel.
              the command POSIX_FADV_NORMAL is used to set maximum read-ahead to default size(usually 32),
              command POSIX_FADV_SEQUENTAL is used to set maximum read-ahead size to two times of the default size,
              command POSIX_FADV_RANDOM is used to disable read-ahead -- set maximum read-ahead size to 0.

          !! about file_ra_state :
               STRUCTURE file_ra_state IN LINUX 2.6.34.1 HAS NO MEMBER @flags,THUS NO MACRO DEFINITIONS
               RA_FLAG_MISS AND RA_FLAG_INCACHE.
               RA_FLAG_MISS : SET WHEN A PAGE THAT HAS BEEN READ IN ADVANCE IS NOT FOUND IN THE PAGE CACHE
                              (BECAUSE KERNEL MEMORY FREE)
                              THE SIZE OF THE NEXT AHEAD WINDOW TO BE CREATED IS SOMEWHAT REDUCED
               RA_FLAG_INCACHE : SET WHEN THE KERNEL DETERMINES THAT THE LAST 256 PAGES REQUESTED BY THE PROCESS
                                 HAVE ALL BEEN FOUND IN THE PAGE CACHE
                                 READ-AHEAD IS TURNED OFF BECAUSE THE KERNEL ASSUMES THAT ALL THE PAGES REQUIRED
                                 BY THE PROCESS ARE ALREADY IN THE CACHE

        The time Read-Ahead algorithm is execute :
          1> when the kernel handles a User Mode request to read pages of file data.
             /* page_cache_sync_readahead() and page_cache_async_readahead() in do_generic_file_read() */
          2> when the kernel allocates a page for a file memory mapping.
          3> when a User Mode process executes posix_fadvise() system call with the POSIX_FADV_NOREUSE or
             POSIX_FADV_WILLNEED commands.
          4> when a User Mode process executes the madvise() system call with the MADV_WILLNEED command.

        The Read-Ahead routines :
          !! LINUX 2.6.34.1 NO FUNCTION NAMED page_cache_readahead() IS EXIST.

          <mm/readahead.c>
            /**
             * page_cache_sync_readahead - generic file readahead for cache pages not in async zone
             * @mapping:                   page cache
             * @ra:                        file readahead state
             * @filp:                      file object
             * @offset:                    start offset into @mapping,in pagecache page-sized units
             * @req_size:                  total size of the read which the caller is performing on
             *                             pagecache pages
             * # EXPORT_SYMBOL_GPL
             * # this routine just perform some checkings and then invoke helper functions
             *   if NULL @ra->ra_pages - ReadAhead disabled,just return to caller
             *   if @filp is TRUE AND FMODE_RANDOM is set,invoke force_page_cache_readahead()
             *                        # not sequental
             *   else,invoke ondemand_readahead()
             */
            void page_cache_sync_readahead(struct address_space *mapping, struct file_ra_state *ra,
                                           struct file *filp, pgoff_t offset, unsigned long req_size);

            /**
             * force_page_cache_readahead - readahead routine for non-sequental(random access) READ requests
             * @mapping:                    page cache
             * @filp:                       file object
             * @offset:                     page offset in page cache
             * @nr_to_read:                 number of pages have to read
             * return:                      number of readahead pages OR error code
             * # this routine chunk the readahead into 2MB units
             */
            int force_page_cache_readahead(struct address_space *mapping, struct file *filp,
                                           pgoff_t offset, unsigned long nr_to_read);

            brief description for force_page_cache_readahead() :
              first test @a_ops->readpage AND @a_ops->readpages,if these routine are not implemented,
              return -EINVAL to caller. /* we need a way to read page */
              adjust @nr_to_read,this is done by call to max_sane_readahead().the function get the
              minimum number between
                [@nr_to_read, (node_page_state(numa_node_id(), NR_INACTIVE_FILE) +
                               node_page_state(numa_node_id(), NR_FREE_PAGES)) / 2]
              next,make use of a while-cycle,it would stopped when @nr_to_read is FALSE {
                      once iteration,we do page cache readahead with size is 2MB,if @nr_to_read is
                      less than 2MB,then the size is @nr_to_read * PAGE_CACHE_SIZE.
            
                      the routine handle readahead is named __do_page_cache_readahead(),we pass it
                      @mapping,@filp,@offset,this_chunk == 2MB / PAGE_CACHE_SIZE or @nr_to_read,0.

                      if error happened,break cycle;otherwise,update @offset += this_chunk,and
                      @nr_to_read -= this_chunk,then get into next iteration.
              }
              finally,return the number of readahead pages.

            /**
             * __do_page_cache_readahead - actually read a chunk of disk
             * @mapping:                   page cache
             * @filp:                      file object
             * @offset:                    page offset
             * @nr_to_read:                number of pages to readahead
             * @lookahead_size:            lookahead size
             * return:                     number of pages requested,OR the maximum amount of I/O allowed
             * # this routine allocates all the page first in the page cache,then submit them all
             *   for I/O,this avoid the very bad behaviour which would occur if page allocations are
             *   causing VM writeback(prevent intermingle)
             */
            static int __do_page_cache_readahead(struct address_space *mapping, struct file *filp,
                                                 pgoff_t offset, unsigned long nr_to_read,
                                                 unsigned long lookahead_size);

            brief description for __do_page_cache_readahead() :
              get @isize from i_size_read() on the inode @mapping->host,for zero size inode,return 0 to caller.
              get end index of page of the inode (@isize - 1) >> PAGE_CACHE_SHIFT.
              make use of a for-cycle to allocates all pages in page cache,the condition is
              page index < @nr_to_read.
              for-cycle {
                      page index started from _zero_,by add @offset,we can get the real page index.
                      if current page index is greater than end index,break cycle.(exceeded)
                      
                      invoke rcu_read_lock() for radix tree lookup,release it after lookup finished.
                      try to find current page in page cache through radix_tree_lookup(),if it been in
                      page cache,we must skip it.

                      current page is not in page cache,invoke page_cache_alloc_cold() to allocate it,
                      break cycle if failed on allocation.     /* just alloc,do not add into page cache */
                      set the new page's index,link it into local list_head @page_pool through its @lru member.
                
                      check,if current page index is equal to @nr_to_read - @lookahead_size,then set PG_readahead
                      for it.

                      update pages number recorder.
              }
              now,allocation have accomplished,if pages number recorder is not _zero_,then we must submit the pages
              newly allocated for I/O.so,invoke read_pages() on @page_pool,number is the pages number recorder.
              ! this is a BUG,if @page_pool is not empty after read_pages() finished.

              finally,return the number of pages.(pages number recorder)

              /**
               * read_pages() is defined in the same file.
               *
               * if @mapping->a_ops->readpages have implemented,then invoke it with pages in the list,and empty the list.
               * # empty the list by invoke put_pages_list(),this function is defined in <mm/swap.c>,for each page in the
               *   list,remove it from list,invoke page_cache_release() for it.
               * return the value returned by readpages().
               *
               * if no such method have implemented,then we must traverse each page in the list,retrieve it from the list,
               * invoke add_to_page_cache_lru() to add it into page cache,call to readpage() method of the @a_ops if and
               * only if add_to_page_cache_lru() succeed.page_cache_release() current page and handle the next page.
               * finally,return 0 to caller.
               * # the page would be removed from the list after we have retrieved it.but if add to page cache failed,
               *   the page's ref count must be 1(because it is not in page cache),thus page_cache_release() will destroy
               *   it.                          !! add_to_page_cache_locked() invoke page_cache_get() -- get_page() on it.
               * # pages traversing is started from the tail.
               */

            /**
             * ondemand_readahead - readahead ondemand
             * @mapping:            page cache
             * @ra:                 readahead state
             * @filp:               file object
             * @hit_readahead_marker:
             *                      whether merge windows
             * @offset:             page index offset into page cache
             * @req_size:           number of request pages
             * return:              number of requested pages,OR maximum amount of I/O allowed
             * # ondemand - do not wait until the application consumed all readahead pages and stalled on
             *              the missing page at readahead index;instead,submit an asynchronous readahead
             *              I/O as soon as there are only @async_size pages left in the readahead window
             */
            static unsigned long ondemand_readahead(struct address_space *mapping, struct file_ra_state *ra,
                                                    struct file *filp, bool hit_readahead_marker, pgoff_t offset,
                                                    unsigned long req_size);

            description for ondemand_readahead() :
              first,get the maximum readahead size through max_sane_readahead() on @ra->ra_pages.
              if @offset is _zero_,that means we are at the starting of file,then goto label "initial_readahead".
              otherwise,check @offset whether at the PG_readahead marked page,then ramp up size,and push forward
              the readahead window.
              that is,@offset == @ra->start + @ra->size - @ra->async_size OR @offset == @ra->start + @ra->size
              /**
               * |-- cur window --|<------ async_size -----|
               * |----------------- size ----------------->|
               * |================#========================|
               * ^start           ^PG_readahead page
               */
              /**
               * get_next_ra_size(@ra, @max) -
               *   get the previous window size,ramp it up,and return it as the new window size.
               *   # never exceed @max
               *   # current size in @ra->size
               */

              if it is the PG_readahead marked page,then
                      let @ra->start += @ra->size /* go to the end */
                      set @ra->size to get_next_ra_size(@ra, maximum ra size)
                      set @ra->async_size to @ra->size

                                                   |<-------------- async size ------------|
                      |--------- old size -------->|---------------- size ---------------->|==> next ra size
                      |==========#=================|=======================================|
                                 ^PG_readahead     ^start

                      goto label "readit"

              in another case,we are neither at PG_readahead marked page,nor the initial case,then
                      check @hit_readahead_marker
                      if it is TRUE {
                              lock RCU read
                              invoke radix_tree_next_hole() from @offset + 1 to maximum ra pages(max scanning),
                              this routine will attempt to find the next hole which is no entry present.
                              its return value is page offset in page cache.
                              release RCU read                 /*  [@offset + 1, @offset + 1 + max scanning) */

                              if radix_tree_next_hole() returned _zero_,that means wrap-around happened,
                              so we have to return 0,because no hole is found.
                              if the returned index ihole - @offset is greater than maximum ra size,
                              we also have to return 0,because it is outside of the set specified.

                              a valid index is returned
                                      set @ra->start to the ihole
                                      let @ra->size = ihole - @offset
                                      let @ra->size += @req_size
                                      set @ra->size to get_next_ra_size(@ra, maximum ra size)
                                      set @ra->async_size to @ra->size
                                      goto label "readit"
                                                                
                                                                         |<---------- async size ------|
                                                                         |------------> size --------->|
                                                                         |ih - off | @req_size|........|
                                      |=====#==============@============= =============================|
                                            ^PG_readahead  ^@offset      ^next hole(start)
                                                                          # ihole
                      }

                      @hit_readahead_marker is FALSE
                              check if @req_size is greater than max ra size,if it is the case,that means
                              oversize read,goto label "initial_readahead"
                            
                                |------------ size -------------->|
                                |============#====================|.......@......
                                ^start       ^PG_readahead                ^@offset

                              if @offset - last read operation index <= 1UL --> @offset <= @ra->prev_pos + 1 
                              @offset < last read operation index,that means sequential cache miss,goto
                              "initial_readahead".
                              /* @offset in range (..., last read index + 1],the minimum offset is 0(beginning) */
                                                 
                                |======@=========L@==============#=====================================|
                                ^start ^@offset   ^next seq pos  ^PG_readahead
                                                   (@offset)
                                                 
                              query the page cache and look for the traces(cached history pages) that a
                              sequential stream would leave behind,invoke try_context_readahead(),if returned
                              TRUE,goto label "readit".
                              /**
                               * static int try_context_readahead(@mapping, @ra, @offset, @req_size, @max)
                               * - invoke count_history_pages(),if no history pages,return 0 to caller.
                               *   if size of history pages is greater than or equal to @offset,let size *= 2.
                               *   # starts from begining of file
                               *
                               *   set @ra->start to @offset
                               *   set @ra->size to get_init_ra_size(size + @req_size, @max)
                               *   set @ra->async_size to @ra->size
                               *   return 1
                               *
                               * static pgoff_t count_history_pages(@mapping, @ra, @offset, @max)
                               * - lock RCU and invoke radix_tree_prev_hole() from @offset - 1 to @max
                               *   return @offset - 1 - the returned index
                               *   # may occur undeflow,in this case,we are starting from begining of file
                               *   ! do traverse to radix_tree_next_hole().
                               *              |-- history size -->|        # minimum index is _zero_
                               *     |======== ===================@      # (@offset - 1 - max scanning, @offset - 1]
                               *              ^hole               ^offset
                               *
                               * # init ra size : initial window size,round to next power of 2 and square for small size,
                               *   x4 for medium,and x2 for large for 128k(32 pages) max ra
                               *   1--8 page = 32k initial, > 8 page = 128k initial
                               */

                              the last,standalone,small random read,invoke __do_page_cache_readahead() and
                              return the value what it would returns.      /* lookahead size is 0
                                                                            * offset is @offset
                                                                            * @nr_to_read is @req_size
                                                                            */

              "initial_readahead":
                set @ra->start to @offset /* offset is 0,starts from beginning of file */
                set @ra->size to get_init_ra_size(@req_size, max ra size)
                set @async_size to @ra->size - @req_size if @ra->size > @req_size,or @ra->size if @ra->size <= @req_size
                /**
                 * reset readahead window.
                 */
                                            +--> size - @req_size
                                            |
                                  |<------- async size ----------|
                 |--------------- size(initial ra size) -------->|
                 |================[==============================|
                 ^start(@offset)  ^async starting position

              "readit":
                if @offset is equal to @ra->start AND @ra->size is equal to @ra->async_size
                then
                        /* makeup async window */
                        set @ra->async_size to get_next_ra_size(@ra, max ra size)
                        let @ra->size += @ra->async_size    

                                                 +--> next ra size
                                                 |
                                        |<------ async size --------|
                        |------------------> size ----------------->|
                        |===============[===========================|
                        ^start(@offset) ^async starting position
                                         (readahead indicator)


                return ra_sumit(@ra, @mapping, @filp)
                       /**
                        * invoke __do_page_cache_readahead() with lookahead size is @ra->async_size.
                        * # this will mark PG_readahead of the first page in async zone,
                        *   and in this case the parameter @nr_to_read of that function is @ra->size,
                        *   the parameter @offset of that function is @ra->start.
                        * 
                        *   |<---------- read pages ---------->| => expected case
                        *   |<---------- allocate pages ------>| => expected case
                        *   |=============#====================|
                        *   ^start        ^PG_readahead
                        */

            ! obviously,by walk-through ondemand_readahead(),we have known that,the page marked PG_readahead
              is as an indicator to indicates where ahead window is starting,and the member @start of
              file_ra_state is used to indicate where current window is starting.

            /**
             * page_cache_async_readahead - file readahead for marked pages
             * @mapping:                    page cache
             * @ra:                         file readahead state
             * @filp:                       file object
             * @page:                       the page at @offset which has the PG_readahead flag set
             * @offset:                     start offset into @mapping,in page cache page-sized units
             * @req_size:                   total size of the read which the caller is performing in
             *                              page cache pages
             * # EXPORT_SYMBOL_GPL
             * # page_cache_async_ondemand() should be called when a page is used which has the
             *   PG_readahead flag,this is a marker to suggest that the application has used up
             *   enough of the readahead window that we should start pulling in more pages
             */
            void page_cache_async_readahead(struct address_space *mapping, struct file_ra_state *ra,
                                            struct file *filp, struct page *page, pgoff_t offset,
                                            unsigned long req_size);

            brief description for page_cache_async_readahead() :
              check @ra->ra_pages at first,if it is _zero_,that means ReadAhead is disabled,return to caller.
              if the @page is in PG_writeback,then we must return to caller.
              clear PG_readahead of @page.
              check,if BDI(@mapping->backing_dev_info) is READ congested,then return to caller,this will defer
              asynchronous read-ahead.
              invoke ondemand_readahead().
              #ifdef CONFIG_BLOCK
                normally the current page is not uptodate and lock_page() will be immediately called to
                implicitly unplug the device.however this is not always true for RAID configurations,
                where data arrives not strictly in their submission order.in this case we need to 
                explicitly kick off the I/O.
                thus,if @page has PG_uptodate set,invoke blk_run_backing_dev() with arguments @mapping's bdi
                and a NULL pointer.
              #endif

      Writing to a File :
        the write system call invoke vfs_write(),then it is the write method of that file object.for many
        disk-based filesystems,the method is set to do_sync_write(),and do_sync_write() is similar to
        do_sync_read(),which processes many aio_write() in an infinite for-cycle,break until write operation
        completed.
        function aio_write() of the file object usually is generic_file_aio_write().

        <fs/read_write.c>
          /**
           * do_sync_write - do synchronous write operation
           * @filp:          file object
           * @buf:           data buffer from userspace
           * @len:           data length in @buf
           * @ppos:          write starting position
           *                 # sys_write() retrieve file write position by invoke
           *                   file_pos_read(),and pass it to vfs_write(),then it is
           *                   this routine
           * return:         return the value returned by aio_write() method of @filp
           *                 OR
           *                 return kiocb.ki_user_data
           * # EXPORT_SYMBOL
           * # this routine is similar to do_sync_read(),it initialize a local kiocb object and
           *   initialize a local iovec object from @buf and @len
           *   use @ppos,@len to set @ki_pos,@ki_left,@ki_nbytes
           *   invoke aio_write() of @filp in an infinite for-cycle,break cycle if return value of
           *   aio_write() is not -EIOCBRETRY,otherwise,invoke wait_on_retry_sync_kiocb() and get
           *   into next iteration
           *   when cycle stopped,check return value of aio_write(),if it is -EIOCBQUEUED,then
           *   invoke wait_on_sync_kiocb() to wait the kiocb,reset return value to the result from
           *   wait_on_sync_kiocb() # @ki_user_data
           *   use @ki_pos to update @ppos,return the return value to caller
           */
          ssize_t do_sync_write(struct file *filp, const char __user *buf, size_t len, loff_t *ppos);

        <mm/filemap.c>
          /**
           * generic_file_aio_write - write data to a file
           * @iocb:                   kiocb
           * @iov:                    io vector(s)
           * @nr_segs:                number of io vector(s)
           * @pos:                    operation starting position
           * return:                  return the value returned by __generic_file_aio_write()
           *                          return error code if error happened
           * # EXPORT_SYMBOL
           * # if this routine is called from do_sync_write(),then @nr_segs is 1
           */
          ssize_t generic_file_aio_write(struct kiocb *iocb, const struct iovec *iov,
                                         unsigned long nr_segs, loff_t pos);

          brief description for generic_file_aio_write() :
            this routine is a wrapper around __generic_file_aio_write() to be used by most filesystems.
            at the beginning,get @ki_filp,and get page cache host from the file's @f_mapping member.
            if @ki_pos is not equal to @pos,then it is a BUG.
            acquire the inode's @i_mutex,because this routine have to take care of O_SYNC mode.
            /* only one process can write into this file at the same time */
            invoke __generic_file_aio_write(),and then release @i_mutex.

            if result from the function is greater than _zero_ OR equal to -EIOCBQUEUED(pending)
            then,invoke generic_write_sync() to sync writing on the file at position @pos,the data length
            is returned by __generic_file_aio_write();if generic_write_sync() returned an error code AND
            the length of data been written is greater than _zero_,that means we have encountered an error,
            return the error code to caller.             /**
                                                          * generic_write_sync() is defined in <fs/sync.c>,
                                                          * it invoke vfs_fsync_range() if no O_DSYNC of the file
                                                          * AND no MS_SYNCHRONOUS of the inode.
                                                          * if __O_SYNC is set in the file's flags,ask
                                                          * vfs_fsync_range() sync everything,
                                                          * otherwise,perform only datasync.
                                                          */

            else,just return the length of data been written to caller.

          /**
           * __generic_file_aio_write - do generic asynchronous I/O for write data to a file
           * @iocb:                     kiocb
           * @iov:                      io vector(s)
           * @nr_segs:                  number of io vector(s)
           * @ppos:                     starting position,usually is @iocb.ki_pos
           * return:                    number of data have written OR error code
           * # EXPORT_SYMBOL
           * # this routine handle O_DIRECT mode,but do not take care of O_SYNC write,caller
           *   have to handle this
           */
          ssize_t __generic_file_aio_write(struct kiocb *iocb, const struct iovec *iov,
                                           unsigned long nr_segs, loff_t *ppos);

          description for __generic_file_aio_write() :
            first,invoke generic_segment_checks() with permission VERIFY_READ,this routine check
            each segment in iovec array;the routine also adjust the length of data will be written,
            caller pass it a pointer as the third parameter,and the adjusted length is stored in the
            object. /* this routine might return -EINVAL or -EFAULT,no problem,_zero_ will be returned.
                     * even _zero_ is returned,the length might stay adjusted because there is some
                     * segmens are no good
                     * it also update number of segments for give up to writing no good segments.
                     * # the remain segments are give up even they are good segments
                     *
                     * the local variable @ocount is passed as the third parameter to this function,
                     * __generic_file_aio_write() use @ocount to reset @count a local vairable used
                     * to record data length.
                     */
            if error is detected,return error to caller;otherwise,make use of the adjusted length.
            invoke vfs_check_frozen() on the inode's super block,the event is @s_wait_unfrozen,level
            is SB_FREEZE_WRITE. /* if vfs have frozen write,the we must wait until it unfrozen */
                                /* the macro is defined in <linux/fs.h>,which invoke wait_event()
                                 * wait until @s_frozen < SB_FREEZE_WRITE
                                 */
                                                     /* allow the current process to write back dirty pages
                                                      * owned by the mapping.
                                                      */
            reset @current's backing_dev_info to the mapping's backing_dev_info of that inode.
            invoke generic_write_checks(). /* this routine can adjust writing position or amount of
                                            * bytes to write.return 0 if passed checkings,or error code
                                            * if failed
                                            * arguments :
                                            *   @ki_filp, &@pos, &@write_length, S_ISBLK(@inode->i_mode)
                                            *                    ^adjusted by generic_segment_checks()
                                            *                     stored in @count
                                            * checkings include -
                                            *   rlimit file size, O_APPEND position, filesystem LFS rule,
                                            *   filesystem size limit, block device inode checking
                                            */
            goto label "out" if error happened OR length is set to _zero_(by generic_write_checks()).
            call to file_remove_suid() on @ki_filp/* file permission SUID */,goto "out" if returned error.
            invoke file_update_time() to update mtime and ctime.

            check O_DIRECT,for direct IO and buffered IO,this routine make use of different ways.
            O_DIRECT mode :
              first,try generic_file_direct_write(),result is stored in @written.
              /* this routine will invoke to direct_IO() method of the page cache with op WRITE */
              if happened error(@written < 0) or have written everything,then goto "out".
              if only a part have written,then must make use of buffered write for the rest.
              update @pos += @written.
              update @count(the length of adjusted data length) -= @written.
              invoke generic_file_buffered_write() with the arguments :
                @iocb, @iov, @nr_segs, @pos, @ppos, @count, @written
                                       ^/* local variable used to record write position
                                         * generic_write_checks() may adjust the position
                                         */
              if an error is returned,then goto "out".
              otherwise,call to filemap_write_and_wait_range() to ensure that the page cache pages
              are written to disk. /* range : [@pos, @pos + @written_buffered - @written - 1]
                                    * the range about buffered writing,not the direct I/O
                                    */          
              if it exactly is the case,reset @written to @written_buffered,and invoke function
              invalidate_mapping_pages() on the mapping of that inode to invalidate all the unlocked
              pages of the inode. /* same range as filemap_write_and_wait_range() */
              else,filemap_write_and_wait_range() returned a non-zero value,then goto "out".

            not O_DIRECT :
              just invoke generic_file_buffered_write() to process buffered write on all segments.

            "out":
              reset @current's backing_dev_info to NULL
              return @written to caller if @written is TRUE,otherwise,return error code to caller.

          /**
           * generic_file_direct_write - do generic direct I/O write on a file
           * @iocb:                      kiocb
           * @iov:                       io vector(s)
           * @nr_segs:                   number of io vector(s)
           * @pos:                       writing start position - adjusted
           * @ppos:                      original writing start position - file I/O position
           * @count:                     total length of data will to be written
           *                             # might be adjusted by generic_write_checks()
           * @ocount:                    total length of data in segments
           *                             # might be adjusted by generic_segment_checks()
           * return:                     number of bytes of data been written
           * # EXPORT_SYMBOL
           */
          ssize_t generic_file_direct_write(struct kiocb *iocb, const struct iovec *iov, unsigned long *nr_segs,
                                            loff_t pos, loff_t **ppos, size_t count, size_t ocount);

          brief description for generic_file_direct_write() :
            if @count is not equal to @ocount,invoke iov_shorten() to short io vector(s),modify @nr_segs.
            the @write_len is get through function iov_length(),@nr_segs might be have adjusted.
            and the @end boundary is calculated from "(@pos + @write_len - 1) >> PAGE_CACHE_SHIFT".
                    /* page index */
            invoke filemap_write_and_wait_range(),this routine as result,would invoke do_writepages() to
            write data in range [@pos, @pos + @write_len - 1] to disk,the corresponding pages should been cached.
                                /* file position */
            /* if succeed,filemap_write_and_wait_range() should return 0 */
            if an error code is returned,then return it to caller.
            next,check the mapping's @nrpages,if there is some page been cached,then
                    invoke invalidate_inode_pages2_range() to invalidate pages of the inode's mapping 
                    in range [@pos >> PAGE_CACHE_SHIFT, @end] /* page index */
                    /* for each page,wait for writeback completed,if page_mapped() is TRUE,unmap the page */
                                                                     /* @page->_mapcount >= 0 ? */
                    this function should return 0,when no error encountered.
                    if an error code has returned,it is not -EBUSY,return the error code to caller;otherwise,
                    return 0 to caller because it is busy.
            invoke direct_IO() method of the mapping,the number of bytes have written should be returned by
            the method.
            try again invalidate_inode_pages2_range() on the same range if @nrpages of the mapping is not _zero_.
            /* clean pages which migth have been cached by non-direct readahead,or faulted in by get_user_pages()
             * if the source of the write was an mmap'ed region of the file we are writing.
             */
            if direct_IO() method returned a positive value,calculate the next write position,check whether current
            inode's size have not updated AND the inode is not block device file inode,set the new size for this inode,
            call mark_inode_dirty() to mark it is a dirty inode,set *@ppos to @pos + @written(returned by direct_IO()).
                                                                /* this pointer usually points to @iocb.ki_pos,
                                                                 * do_sync_write() will use this member to update
                                                                 * the pointer passed by sys_write(),and finally,
                                                                 * the file position would be updated through file_pos_write()
                                                                 */
            finally,return the bytes of data been written.

          /**
           * generic_file_buffered_write - do generic buffered file write
           * @iocb:                        kiocb
           * @iov:                         io vector(s)
           * @nr_segs:                     number of io vector(s)
           * @pos:                         write position
           * @ppos:                        original write position
           * @count:                       data length in bytes
           * @written:                     length in bytes of data been written through other ways
           * return:                       number of data in bytes have written
           *                               error code if failed
           * # EXPORT_SYMBOL
           * # this routine is very simple,it initialize an iterator of io vector(s),the position is
           *   determined by @nr_segs,@count,and @written
           *   invoke generic_perform_write() with arguments @iocb->ki_filp,@the_iterator,@pos
           *   if no error happened,let @written += return_value_of_genrric_perform_write(),update
           *   *@ppos by let *@ppos += return_value_of_generic_perform_write()
           *   if @written is TRUE,return it,otherwise,return the value returned from generic_perform_write()
           *   ! because @written might be _zero_ is set when call to this routine
           */
          ssize_t generic_file_buffered_write(struct kiocb *iocb, const struct iovec *iov, unsigned long nr_segs,
                                              loff_t pos, loff_t *ppos, size_t count, ssize_t written);

          /**
           * <linux/fs.h>
           *   struct iov_iter {
           *           const struct iovec *iov; /* io vector(s),used to point current io vector */
           *           unsigned long nr_segs;   /* number of io vector(s) */
           *           size_t iov_offset;       /* offset into current io vector */
           *           size_t count;            /* total length of data remained in the io vector(s) */
           *   };
           * static inline void iov_iter_init(struct iov_iter @i, const struct iovec *iov, unsigned nr_segs,
           *                                  size_t count, size_t written);
           * - *@i = struct iov_iter{
           *           @iov,
           *           @nr_segs,
           *           0,
           *           @count + @written
           *   };
           *   iov_iter_advance(@i, @written)
           *   - step to the io vector,the io vectors before it have spanned the data with length @written.
           *   ! everytime,when advance iov iterator,@count would decrease,and @iov increase,@offset reset.
           *     but,@nr_segs never changes.
           *     when process copying,if @count is 1,then only current io vector would be copied;otherwise,
           *     the copying might spanned several io vectors.
           *     kernel must establish KM_USER0 kernel address mapping for the page in which the data from
           *     userspace will copied into,and unmap later.
           */

          /**
           * generic_perform_write - buffered write,copy data from io vector(s) to pages in page cache
           * @file:                  file object
           * @i:                     io vector iterator
           * @pos:                   write position
           * return:                 number of bytes been written
           *                         error code if failed
           */
          static ssize_t generic_perform_write(struct file *file, struct iov_iter *i, loff_t pos);

          brief description for generic_perform_write() :
            the main concept is copy the data from userspace,the addresses are recorded in io vector(s).
            first,do segment check,if get_fs() is equal to KERNEL_DS,then let local variable
            @flags |= AOP_FLAG_UNINTERRUPTIBLE (@flags default value is _zero_)
                      /* address space operation flag */
            make use of a do-while cycle,would stopped until @i points to the end /* @i->count is _zero_ */ {
                    calculate offset into a page for @pos the file position /* @pos & (PAGE_CACHE_SIZE - 1) */
                    calculate page index in page cache for the @pos /* @pos >> PAGE_CACHE_SHIFT */
                    calculate bytes of data have to be copied through min(PAGE_CACHE_SIZE - @offset, iov_iter_count(@i))
                                                                      /* value type is unsigned long
                                                                       * iov_iter_count() get the @count member of
                                                                       * this iov_iter
                                                                       */
                    again:
                    check whether iov_iter_fault_in_readable() is TRUE,in this case,we can not read the region
                    of data,return -EFAULT to caller.
                    /* fault_in_pages_readable(userspace address, current io vector length - current io vector offset) */
                    
                    invoke @a_ops->write_begin(),pass it @file,@file->f_mapping,@pos,@bytes,@flags,&@page,&@fsdata
                    /* @page is a local pointer,write_begin() method might allocate new cache page
                     * @fsdata is used to store filesystem-dependent data
                     */
                    the return value is stored in local variable @status,if no error happened,_zero_ should be returned;
                    otherwise,break cycle.

                    check,if @file->f_mapping is mapping writably mapped - @i_mmap_writable != 0,then invoke
                    flush_dcache_page() on @page to flush dcache for this page.

                    disable Page Fault.
                    call iov_iter_copy_from_user_atomic() to copy the data in userspace to the @page start at specified
                    @offset with specified @bytes.
                    enable Page Fault.
                    invoke flush_dcache_page() on @page.
                    mark_page_accessed() to @page.

                    invoke @a_ops->write_end(),pass it @file,@file->f_mapping,@pos,@bytes,@length_of_copied_data,@page,@fsdata
                    the return value still stored in @status.
                    if @status is less than _zero_,break cycle,that means we have encountered an error.

                    use @status to reset local variable @copied(used to store the value returned by iov_iter_copy_from_user_atomic()).
                    check kernel preempt through cond_resched().

                    advance iov iterator @i,@copied record how many data in bytes been copied into the page,so we can use it
                    to update iterator @i.

                    check,if @copied is equal to 0,that means we copied nothing into the @page,thus,we must adjust @bytes
                    to min(PAGE_CACHE_SIZE - @offset, iov_iter_single_seg_count(@i)),and try again.
                                                                                         /* restart from iov_iter_fault_in_readable() */
                       /**
                        * iov_iter_single_seg_count() - 
                        *   if @i->nr_segs == 1,return @i->count
                        *   else,return min(@i->count, @i->iov->iov_len - @i->iov_offset)
                        */

                    if @copied is not equal to 0,we let @pos += @copied,@written += @copied.
                                                        /* file pos */  /* local vairable used to record
                                                                         * length in bytes of data have
                                                                         * written(default value is 0)
                                                                         */
                    invoke balance_dirty_pages_ratelimited() on @file->f_mapping,this routine look at the number of dirty pages
                    in the machine and will force the caller to perform writeback if the system is over "vm_dirty_ratio".
            }

            finally,if @written is TRUE,return it;otherwise,return @status.

            /**
             * for ext2 filesystem,method write_begin() of address_space_operations is implemented by ext2_write_begin(),which
             * invoke __ext2_write_begin(),and is the common interface block_write_begin();the method write_end() is implemented
             * by generic_write_end().
             * # block_write_begin() -> __block_prepare_write()
             *   generic_write_end() -> block_write_end() -> __block_commit_write()
             */
            
        The write_begin() and write_end() method of address_space_operations :
          the these two methods are set by filesystem,but the common interfaces are block_write_begin() and generic_write_end(),
          respectively;filesystem let its <fs>_write_begin() call to block_write_begin(),and let its <fs>_write_end() call to
          generic_write_end(),such ext2 filesystem.
          both the functions are invoked once for every page of the file that is affected by the write operation.

          <fs/buffer.c>
            /**
             * block_write_begin - write begin for address_space_operations,bh version
             * @file:              file object
             * @mapping:           page cache
             * @pos:               file position
             * @len:               data length
             * @flags:             behaviour control
             * @pagep:             page pointer's pointer
             *                     # if *@pagep is NULL,this function will allocate a new page
             * @fsdata:            filesystem private data pointer's pointer
             * @get_block:         get_block method
             * return:             0 OR error code
             * # EXPORT_SYMBOL
             */
            int block_write_begin(struct file *file, struct address_space *mapping, loff_t pos, unsigned len,
                                  unsigned flags, struct page **pagep, void **fsdata, get_block_t *get_block);

            brief description for block_write_begin() :
              this routine just take care of the basic task of block allocation and bringing partial write
              blocks uptodate first.
              it get page index,offset into page, end boundary through @pos and @len.
              if *@pagep is NULL,invoke grab_cache_page_write_begin() at the specified page index.
              /**
               * grab_cache_page_write_begin() is defined in <mm/filemap.c>.
               * it attempts to find out a cached page in page cache at specified index,if it is exist,
               * return it;otherwise,invoke __page_cache_alloc() to allocate a new page.if AOP_FLAG_NOFS
               * is enabled in @flags,then the GFP flag used for allocation will disable __GFP_FS.
               * after succeed to allocate a new page,invoke add_to_page_cache_lru(),add the page to
               * page cache,GFP flag is "GFP_KERNEL & ~__GFP_FS".
               * finally,return the page;in other cases,return NULL to caller.
               */
              if get page failed,return -ENOMEM to caller.
              if *@pagep is not NULL,*@pagep must been locked,otherwise,it is a BUG.
              invoke __block_prepare_write(),this helper do the main works.
              if __block_prepare_write() returned an error code,clear PG_uptodate on *@pagep or on the
              allocated page;if the page is allocated by this function,page_cache_release() it,then
              check whether "@pos + @len" is greater than the inode's @i_size,it is,then vmtruncate()
              the inode with @i_size. /* prepare_write() may have instantiated a few blocks outside @i_size */
              finally,return to caller the value returned by __block_prepare_write().

            /**
             * __block_prepare_write - do the main work of prepare block write
             * @inode:                 the file's inode
             * @page:                  cached page to write
             * @from:                  starting offset into the page
             * @end:                   ending offset into the page
             * @get_block:             filesystem-dependent method
             * return:                 0 OR error code
             * # create buffers,get blocks,zero segments or zero all buffers
             */
            static int __block_prepare_write(struct inode *inode, struct page *page, unsigned from, unsigned to,
                                             get_block_t *get_block);

            description for __block_prepare_write() :
              get block-size through "1 << @inode->i_blkbits".
              if @page has no buffers,invoke create_empty_buffers() to create buffers on it,each buffer correspond
              to a block has size block-size. /* PG_private is not set */
              calculate block number through "@page->index << (PAGE_CACHE_SHIFT - @inode->i_blkbits)".
              make use of a for-cycle to traverse each buffers on @page {
                      let the logical block-start pos to 0,let the logical block-end pos to start + block-size.
                      /* buffer corresponds to a block
                       * |<-- blk size -->| => 512 bytes
                       * |================| => a buffer in a page
                       * ^block-start     ^block-end
                       * |================|================|================|================|...| => a page,4096 bytes
                       */
                       /* block-start is update to block-end before next iteration */

                      if block-end <= @from OR block-start >= @to,that means the block is not our target,skip it.
                      of course,if the @page has PG_uptodate but current buffer has not BH_Uptodate,set BH_Uptodate
                      for it.

                      clear current buffer's BH_New if BH_New is set,if @page has no buffers at the beginning,the
                      buffers on it should are newly created.

                                              /* no a disk block it associated */
                      check current buffer if it is not mapping a disk logical block,in this case,we must get_block for it.
                              get_block() for it,if no such block,let filesystem create it,the block number has
                              calculated previously.
                              @b_size should equal to block-size,if the buffer is newly created,otherwise,print
                              warning.
                              break cycle if get_block() returned an error code.
                              if current buffer has BH_New set by get_block()
                                      invoke unmap_underlying_metadata() for current buffer
                                      /* find out the old bh,clear dirty state,wait until BH_Lock is cleared
                                       * clear BH_Req,__brelse() the old buffer
                                       * finding rely on __find_get_block_slow(),block number is current buffer's
                                       * @b_blocknr.
                                       */
                                      if @page has PG_uptodate,clear BH_New of current buffer,set BH_Uptodate for it,
                                      mark it dirty,and continue to next iteration.

                                      otherwise,check if block-end > @to OR block-start < @from,the buffer contains
                                      our target,then call zero_user_segments() to zero regions with ranges
                                      [@to, block-end) and [block-start, @from] in current buffer.
                                      /* range [@from, @to) is included in this block */

                                      continue to next iteration.

                      the current buffer is mapped
                              check PG_uptodate for @page,if it is the case,check BH_Uptodate for current buffer,
                              if no BH_Uptodate,set the flag for it.continue to next iteration.

                                                           /* no uptodate */      /* been allocated on disk */
                              the last case,current buffer has no BH_Uptodate AND has no BH_Delay AND has no BH_Unwritten
                              AND (block-start < @from OR block-end > @to)                            /* unwritten */
                                      invoke ll_rw_block() the low-level r/w block routine for a READ operation on
                                      current buffer,record current buffer in a local array is type of struct buffer_head *,
                                      the maximum size of this array is 2.      /* named @wait_bh */
              }

              traverse @wait_bh,the buffers we have submit it to Generic Block Layer previously.
              wait_on_buffer() for each buffer in the collection,if it stay has no BH_Uptodate,return -EIO to caller.

                                                                                       /* might span several buffers */
              finally,check whether get_block() returned an error code,if it is,invoke page_zero_new_buffers() to zero
              out the buffers on @page related to range [@from, @to],return the error code to caller;otherwise,return
              0 to caller.                                                            ! only zero out buffer has BH_New.

            /**
             * generic_write_end - generic write end for address space operations
             * @file:              file object
             * @mapping:           page cache
             * @pos:               file position
             * @len:               data length
             * @copied:            data length copied from somewhere
             * @page:              cached page contains the copied data
             * @fsdata:            filesystem private data
             * return:             number of bytes been written
             * # EXPORT_SYMBOL
             * # the main work is hand over to block_write_end()
             * # this routine invoke block_write_end(),pass it all its parameters as arguments to that function,
             *   store return value in @copied
             *   if @pos + @copied is greater than current inode's @i_size,write new size @pos + @copied for it
             *   unlock @page,page_cache_release() @page
             *   if inode's size is changed,mark the inode dirty
             *   return @copied to caller
             */
            int generic_write_end(struct file *file, struct address_space *mapping, loff_t pos, unsigned len,
                                  unsigned copied, struct page *page, void *fsdata);

            /**
             * block_write_end - the routine wrapping around __block_commit_write()
             * @file:            file object
             * @mapping:         page cache
             * @pos:             file position
             * @len:             data length
             * @copied:          data length copied from somewhere
             * @page:            the page contains copied data
             * @fsdata:          filesystem private data
             * return:           number of bytes been written
             * # EXPORT_SYMBOL
             * # this routine hand over the main work to __block_commit_write()
             * # at first,calculate the start-offset into @page from @pos
             *   if @copied is less than @len,page_zero_new_buffers() to zero out the rest region of the page,
             *   if @page has no PG_uptodate,set @copied to _zero_,that means zero out the region from start-offset
             *   to start-offset + @len
             *   flush_dcache_page() for @page
             *   invoke __block_commit_write() to commit writing transaction,range [start-offset, start-offset + @copied)
             *   return @copied to caller
             */
            int block_write_end(struct file *file, struct address_space *mapping, loff_t pos, unsigned len,
                                unsigned copied, struct page *page, void *fsdata);

            /**
             * __block_commit_write - routine used to mark some special buffers dirty,and them would be write back later
             * @inode:                file's inode
             * @page:                 cached page contains data
             * @from:                 start-offset into @page
             * @to:                   end-offset into @page
             * return:                always 0
             * # traverse each buffer on @page,block-start initialize to 0 (block-start is update to block-end before next iteration)
             *           let block-end = block-start + block-size
             *           check,if block-end <= @from OR block-start >= @to,check if it has no BH_Uptodate for current buffer,
             *           set local recorder @partial to 1  --  current buffer is not our target
             *                                          | buffer |
             *                            |from       to|
             *                   | buffer |
             *           if current buffer is our target,set BH_Uptodate for it,mark it dirty through mark_buffer_dirty(),
             *           clear BH_New for current buffer,get into next iteration
             *   if all buffers are uptodate -- @partial is FALSE,set PG_uptodate for @page
             *   return 0
             *   ! mark_buffer_dirty() is defined in <fs/buffer.c>
             *     if the buffer been dirty,then we do not touch it
             *     otherwise,set BH_Dirty for it,and set PG_dirty for its @b_page if and only if the page is cached
             */
            static int __block_commit_write(struct inode *inode, struct page *page, unsigned from, unsigned to);

        
          !! by walking-through write_begin() and write_end(),we can know that the data copying is accomplished
             in generic_perform_write().write_begin() is used to allocate new page if necessary,get blocks,setup buffers;
             write_end() is used to change buffers' state and the page's state,the data inside buffers of the page
             would be written back by BDI later.

          !! for block device file -
               the aops of block device usually is @def_blk_aops,method write_begin() is implemented by blkdev_write_begin(),
               and method write_end() is implemented by blkdev_write_end().

               routine blkdev_write_begin() is defined in <fs/block_dev.c>
                 static int blkdev_write_begin(struct file *file, struct address_space *mapping, loff_t pos,
                                               unsigned len, unsigned flags, struct page **pagep, void **fsdata);
                 - set *@pagep to NULL
                   invoke block_write_begin(),the get_block method is blkdev_get_block()
                   return to caller the value returned by block_write_begin()
                
                   ! by set *@pagep to NULL,block_write_begin() will create a new page in the mapping of the
                     master block inode file.

               routine blkdev_write_end() is defined in <fs/block_dev.c>
                 static int blkdev_write_end(struct file *file, struct address_space *mapping, loff_t pos,
                                             unsigned len, unsigned copied, struct page *page, void *fsdata);
                 - invoke block_write_end()
                   unlock @page
                   page_cache_release() @page
                   return to caller the value returned by block_write_end().

      Writing Dirty Pages to Disk :
        when kernel need to write dirty pages into disk,it have to invoke the filesystem-dependent interface that
        is included in the address_space_operations object of that inode.for ext2 filesystem,writepage() method
        is ends up call to __block_write_full_page(),and which submit_bh() buffers on the page to Generic Block Layer.
        if the address_space_operations did not implement writepages() method,function generic_writepages() will be
        invoked,then it is write_cache_pages().
        if the method is implemented,just invoke the method as well.for ext2 filesystem,it usually is ext2_writepages(),
        which is a wrapper of mpage_writepages().
        /* ! __block_write_full_page() will set BH_Async_Write for the dirty buffers,that is submit it for asynchronous
         *   write,end routine is end_buffer_async_write().
         *   the vector of is the internal vector @bi_inline_vecs of that bio.
         */

        <fs/mpage.c> /* the same header that included mpage_readpage() routine */
          /**
           * mpage_writepages - walk the list of dirty pages of the given address space AND writepage() all of them
           * @mapping:          page cache
           * @wbc:              writeback control
           * @get_block:        get_block method
           * return:            0 OR error code
           * # EXPORT_SYMBOL
           * # this routine just process one of two branches
           *   if @get_block is not provided,invoke generic_writepages(),in this case,the writepage() method is
           *   __writepage() which is defined in <mm/page-writeback.c>,it call to writepage() method of the page cache
           *   else
           *           construct a local variable named @mpd is type of struct mpage_data
           *           initialize it { .bio = NULL, .last_block_in_bio = 0, .get_block = @get_block, .use_writepage = 1 }
           *           invoke write_cache_pages(),the writepage() method would be __mpage_writepage(),parameter @data
           *           of write_cache_pages() is @mpd
           *           after write_cache_pages() returned,if @mpd.bio is not NULL,invoke mpage_bio_submit() to submit
           *           the bio for WRITE
           */
          int mpage_writepages(struct address_space *mapping, struct writeback_control *wbc, get_block_t get_block);

          /**
           * if the page has no buffers,then the page is mapped here
           * if all blocks are found to be contiguous,then the page can go into the BIO
           * otherwise,fall back to the mapping's writepage()
           * ! only support pages which are fully mapped-and-dirty,with a special case for
           *   pages which are unmapped at the end: EOF
           */
          struct mpage_data {
                  struct bio *bio;              /* the bio */
                  sector_t last_block_in_bio;   /* last block number in bio */
                  get_block_t *get_block;       /* get_block() method */
                  unsigned use_writepage;       /* whether use writepage() method */
          };

          /**
           * __mpage_write - writepage() method for generic case
           * @page:          the page have to be written into disk
           * @wbc:           writeback control
           * @data:          usually is data object type of struct mpage_data
           * return:         0 OR error code
           */
          static int __mpage_write(struct page *page, struct writeback_control *wbc, void *data);

          what __mpage_writepage() does :
            1> get page cache,and get the host of that page cache.
               calculate blocks-per-page through "PAGE_CACHE_SIZE >> block-bits".
                                                 /* inode's @i_blkbits */
               initialize local variable @first_unmapped to blocks-per-page,@i_size the size of inode.
               an array of sector_t elements named @blocks,size is MAX_BUF_PER_PAGE.
            2> if @page has buffers,that PG_private is set
               then
                       traverse each buffer on @page,use a variable @page_block = 0

                         if current buffer is locked,that is a BUG

                         if current buffer is not mapped a block,but it is dirty,then goto label "confused".
                         if it is not mapped a block,and it is not dirty,set @first_unmapped to this block
                         if and only if @first_unmapped == blocks-per-page.
                         continue to next buffer.

                         if @first_unmapped is not equal to blocks-per-page,that means we encountered a
                         unmapped buffer,goto label "confused",because it is not fully mapped.

                         if current buffer is not dirty OR it is not BH_Uptodate,also goto "confused".
                         /* the buffer is not dirty,OR it is not containing valid data */

                         if @page_block is not _zero_,that means we have stored the mapped and dirty buffer
                         into @blocks,so check whether current buffer's @b_blocknr is not equal to @blocks[@page_block - 1] + 1,
                         if it is the case,that means the mapping is not contiguous,goto label "confused".

                         store current buffer's @b_blocknr to @blocks[@page_block++]

                         get current buffer's BH_Boundary flag,if it is TRUE,that means a discontiguity is followed
                         this buffer,we record the boundary current buffer's @b_blocknr and @b_bdev into local variables
                         named @boundary_block and @boundary_bdev,respectively.
                         use local variable @boundary to record the state of BH_Boundary flag of current buffer.

                         set local variable @bdev to current buffer's @b_bdev
                         /* we will work on this BDI */


                       traversing ended,check @first_unmapped,if it is TRUE,goto label "page_is_mapped",otherwise,goto
                       confused.        /* because @first_unmapped is set to @page_block if it is the time have to reset it,
                                         * then the control path may jump to "confused" at next iteration if the next buffer
                                         * is mapped.
                                         * if traversing is ended,that means no jumping,so @first_unmapped should equal to
                                         * blocks-per-page
                                         */
                       /* goto confused - page has buffers,but they are all unmapped,the page was created by pagein or
                        * read over a hole which was handled by block_read_full_page().if this @mapping is also using
                        * mpage_readpages(),then this can rarely happen.
                        */
            3> the @page has not PG_private,in this case,we have to "map it to disk".
               it is a BUG if PG_uptodate is set /* no buffers but contains valid data in page cache */
               set local variable @block_in_file to "@page->index << (PAGE_CACHE_SHIFT - block-bits)".
               /* we have seen this in do_mpage_readpage(),type is sector_t */
               set local variable @last_block to "inode size - 1 >> block-bits".
               set local variable @map_bh's @b_page to current @page /* type of @map_bh is struct buffer_head */
            4> make use of a for-cycle,iterator is @page_block,initialized to 0,stopped when @page_block >= blocks-paer_page.
               {
                       /* we have to map the page to disk,@map_bh is used to map block in iteration */
                       set members of @map_bh,@b_state to 0,set @b_size to block-size /* 1 << block-bits */
                       invoke get_block() method in @mpd to ask filesystem return the block this buffer will mapped.
                       if failed,goto label "confused".

                       if BH_New of @map_bh is set by filesystem,invoke unmap_underlying_metadata() to unmap the older
                       block in page cache if it is exist.

                       if @map_bh is set BH_Boundary,then record its @b_blocknr in @boundary_block,its @b_bdev in @boundary_bdev.

                       next,check the contiguous,if current block is discontiguous to the previous block,goto "confused".

                       store its @b_blocknr in @blocks,index is @page_block,@page_block increase.
                       store the state of BH_Boundary flag of current buffer in a local variable @boundary.
                       store its @b_bdev in @bdev.
                
                       if @block_in_file is equal to @last_block,that means we reached EOF,break cycle.
                       otherwise,increase @block_in_file.
               }
               /* even after mapping,the page stay no PG_private,if we will switch to writepage() method later,
                * create_empty_buffers() is called for create buffers,state is BH_Dirty | BH_Uptodate.
                * because @page is not mapped on disk,thus we must write the full page into disk for
                * keep data consistency.
                */
            5> it is a BUG if @page_block is equal to 0.if the cycle is ended normally,this variable must not be _zero_.
               set @first_unmapped to @page_block -- the next block that is not in current page,or EOF.
            page_is_mapped:
            6> calculate end-index from "inode size >> PAGE_CACHE_SHIFT".
               if @page->index is greater than or equal to end-index,that means this page straddles the inode's size,
               thus it must be zeroed out on each and every writepage() invocation because it may be mapped.
               /* a file is mapped in multiples of the page size.
                * for a file that is not a multiple of the page size,the remaining memory is zeroed when mapped,
                * and writes to that region are not written out to the file.
                */
               calculate the end offset into a page from inode'size. /* inode size & (PAGE_CACHE_SIZE - 1) */
               if @page->index greater than end-index OR @offset is _zero_,goto confused.
               otherwise,zero_user_segment() from @offset to then end of this @page.
            7> /* this page will go to BIO */
               check local variable @bio,its default value is @mpd->bio.
               if @bio is not NULL AND @mpd->last_block_info is not equal to "@blocks[0] - 1",
               then we must invoke mpage_bio_submit() on it for submit it to a WRITE operation.
               /* @blocks[0] contains the logical block number that first buffer of @page mapped to */
            alloc_new:
            8> @bio is NULL,invoke mpage_alloc() to allocate a new one,arguments are
                 @bdev, @blocks[0] << (block-bits - 9), bio_get_nr_vecs(@bdev), GFP_NOFS | __GFP_HIGH
                 /* bdev, first_sector, number of vectors, gfp flags
                  * @bdev is the bdev of the buffers on @page
                  * we construct a new bio through buffers,then we will no longer need these buffer heads.
                  */
               if allocation is failed,goto "confused".
               
               calculate @length of first unmapped block,at there,the variable should equal to blocks-per-page,
               thus we can know the total length of buffers on @page. /* because if there is some block discontiguous,
                                                                       * the control path should been jump to
                                                                       * label "confused".
                                                                       */
               invoke bio_add_page() to add @page to @bio,offset is _zero_.
               if the returned value from bio_add_page() is less than @length,mpage_bio_submit() @bio,and goto
               label "alloc_new" retry again,because the length is less than we have expected length.
            9> if @page has PG_private,then we clear BH_Dirty for each buffer on it,and,if the total number of
               buffer heads have overed system limit AND PG_uptodate is set,invoke try_to_free_buffers() on @page.
               /* we might need to free buffer heads,but this @page should been added into BIO at step 8 */
           10> if @page is in PG_writeback,that means it is a BUG,because we have not submit it for writeback.
               set PG_writeback for @page,because we are going to submit it for writeback.
               unlock @page.
               if @boundary is TRUE /* a buffer is set BH_Boundary */
               OR @first_unmapped is not equal to blocks-per-page /* page is not fully mapped */
               then
                       invoke mpage_bio_submit() to submit this @bio for WRITE.
                       check,if @boundary_block is TRUE /* the @b_blocknr of that boundary buffer */
                               invoke write_boundary_block(),bdev is @boundary_bdev,size is block size,
                               logical block number is @boundary_block;this function __find_get_block()
                               to get the buffer head that corresponding to @boundary_block + 1,
                               if succeed to get it and it is dirty,invoke ll_rw_block() process WRITE
                               operation on it,then put it.
                               /**
                                * for addressing the next block - @boundary_block + 1,disk must move
                                * its head.for reduce IO cost,we can submit the block for WRITE together
                                * with the bio,this will let disk move its head at this time.after this
                                * block I/O accomplished,its head been at the right position and ready to
                                * get the following blocks.
                                */
               otherwise,set @mpd->last_block_in_bio to @blocks[blocks-per-page - 1],the @b_blocknr of
               the last buffer on @page.

               goto out. /* no boundary,and fully mapped,in this case,submit bio is hand over to the caller */
            confused:
           11: if @bio is not NULL,invoke mpage_bio_submit() to submit it for WRITE.
                                                                /* not all blocks contiguous on the disk */
               next,check if @mpd->use_writepage is TRUE,invoke @a_ops->writepage() on @page with @wbc,
               return value is stored in ret;otherwise,set @ret to -EAGAIN,goto "out".

               invoke mapping_set_error(),if @ret is not an error code,then no error would be set on the
               mapping.
            out:
           12: set @mpd->bio = @bio
               return @ret

           /* mpage_end_io_write() : set by mpage_submi_io() if data direction is WRITE,it complete
            *                        each vector in the bio,and then destroy the bio.
            *                        for each vector,if block I/O on it is completed,invoke end_page_writeback()
            *                        on the page,this will clear PG_writeback,and wake up processes that waiting
            *                        PG_writeback cleared on this page.
            */

      Memory Mapping :
        a memory region can be associated with some portion of either a regular file or a block device file,
        access to a byte within the memory region is translated by kernel into an operation on the corresponding
        byte of the file,this is called "memory mapping".

        memory mapping types :
          1> shared
             each write operation on the pages of the memory region changes the file on disk,that means the
             changing is visible to all other processes that map the same file.
          2> private
             meant to be used when the process creates the mapping just to read the file,not to write it.
             thus,private mapping is more efficient than shared mapping,but the changing on a privately
             mapped page will cause it to stop mapping the page in the file,rather than create a copying
             that is owned by current process. /* Copy-on-Write,page table entry will be reset to the copied page */
             any modification on the mapping do not change the content of file on disk,nor is the change
             visible to any other processes that access the same file.
             ! if there is some process write into file through general methods such sys_write(),the change
               is visible to all processes they mapped the memory region even it is private mapping.

        system call mmap() is used to establish memory mapping for call process,and system call munmap()
        is used to eliminate the mapping. /* MAP_SHARED => VM_SHARED, MAP_PRIVATE => ~VM_SHARED */

        Memory Mapping Data Structures :
          a memory mapping is represented by a combination of the following data structures -
            > the inode object associated with the mapped file
            > the address_space object of the mapped file
            > a file object for each different mapping performed on the file by different processes
            > a vm_area_struct descriptor for each different mapping on the file
            > a page descriptor for each page frame assigned to a memory region that maps the file

          vm_area_struct.vm_file  => the file object that this VMA mapped
          vm_area_struct.vm_pgoff => the file portion offset in PAGE_SIZE
          vm_area_struct.vm_start -- vm_area_struct.vm_end => the length of file portion

          file.f_mapping          => address_space of the file's inode
          
          address_space.host      => the file's inode
          address_space.i_mmap    => radix priority search tree(PST),it collects the
                                     memory regions belonging to the address space

          ! pages of shared memory mapping are always included in the page cache,
            pages of private memory mapping are included in the page cache as long as
            they are unmodified.
            the duplicate page replace the original page in page table entry,and it is
            not included in page cache because it no longer contains valid data representing
            the file on disk.
          ! the newly established memory mapping is a memory region that does not include
            any page;as the process references an address inside the region,a Page Fault occurs
            and the handler checks whether @fault method of the memory region is defined,if
            undefined,the memory region does not map a file on disk;otherwise,it does,and the
            method takes care of reading the page by accessing the block device.
            /* almost all disk-based filesystems and block device files implemente the
             * @fault method by means of the filemap_fault() function,which is defined in
             * <mm/filemap.c>                ^called by __do_fault() from do_linear_fault()
             */

        Creating a Memory Mapping :
          system call mmap() can be issued by User Mode process to create a memory mapping.
          for compatibility reasons,in the 80x86 architecture,kernel reserves two entries in
          systemm call table for mmap(),one is 90,another is 192.system call number 90 corresponds
          to the old_mmap(),it is used by older C libraries;system call number 192 corresponds to
          the sys_mmap2(),it is used by recent C libraries.
          /* for 64 bit system,only one entry is exist,the number is 9 */
          sys_old_mmap() is defined in <mm/mmap.c>,it ends up call to sys_mmap_pgoff().
          the difference between them is :
            sys_old_mmap() requires one parameter is named @arg type of struct mmap_arg_struct *.
            but sys_mmap_pgoff() requires six parameters,and the routine invoke do_mmap_pgoff().
            /**
             * mmap type : MAP_ANONYMOUS - the mapping is not associated with any disk-based file
             *                             this type can combined with MAP_SHARED,in this case,
             *                             the mapping can be used to map the file in tmpfs.
             */

          function do_mmap_pgof() will invoke mmap_region(),if we are mapping a file,the function
          would invoke the mmap() method in the @f_op of that file.
          it usually implemented by generic_file_mmap().

          <mm/filemap.c>
            /**
             * generic_file_mmap - mmap() method for general disk-based filesystem
             * @file:              the file object
             * @vma;               VMA @file will map to
             * return:             0 OR -ENOEXEC
             * # this routine is very simple,because it just take care of the fault()
             *   method of the VMA
             *   first,check @a_ops->readpage(),it can not be un-implemented,because
             *   we need to readpage() from file on disk into memory,if it is the case,
             *   return -ENOEXEC
             *   second,invoke file_accessed() on @file,this function update atime of it
             *   next,use @generic_file_vm_ops to replace the @vm_ops of @vma,and
             *   enable VM_CAN_NONLINEAR flag in @vm_flags of the @vma
             * # @generic_file_vm_ops is an object type of const struct vm_operations_struct,
             *   which is defined in <mm/filemap.c>,the only valid member in which is
             *   fault() method,it is set to filemap_fault()
             * # page for mapping is not allocated immediately,just defer this until
             *   process need to access the linear address in the mapping,so Page Fault
             *   exception will occurs,and then filemap_fault() is called
             */
            int generic_file_mmap(struct file *file, struct vm_area_struct *vma);

            /**
             * <linux/mm.h>
             *   # vm_fault - filled by the Page Fault handler and passed to the vma's
             *   #            fault() method
             *   struct vm_fault {
             *           unsigned int flags;            # FAULT_FLAG_xxx flags
             *                                          # give details about how the fault was handled
             *
             *           pgoff_t pgoff;                 # logical page offset based on vma
             *                                          # should be used in favour of @virtual_address,
             *                                          # if possible
             *                                          # if @pgoff is used,one may set VM_CAN_NONLINEAR
             *                                          # in @vm_flags of the vma to get nonlinear
             *                                          # mapping support
             *
             *           void __user *virtual_address;  # faulting virtual address
             *
             *           struct page *page;             # fault() method should return
             *                                          # a page here,unless VM_FAULT_NOPAGE
             *                                          # is set --> VM_FAULT_ERROR implied NOPAGE
             *   };
             *   FAULT_FLAG_WRITE 0x01          =>    fault was a write access
             *   FAULT_FLAG_NONLINEAR 0x02      =>    fault was via a nonlinear mapping
             *   FAULT_FLAG_MKWRITE 0x04        =>    fault was mkwrite of existing pte
             */

            /**
             * filemap_fault - read in file dat for page fault handling
             * @vma:           vma in which the fault was taken
             * @vmf:           containing details of the fault
             * return:         VM_FAULT_LOCKED OR VM_FAULT_MAJOR | VM_FAULT_LOCKED
             *                 OR error code
             * # EXPORT_SYMBOL
             */
            int filemap_fault(struct vm_area_struct *vma, struct vm_fault *vmf);

            description for filemap_fault() :
              the main task should be handled by this routine is :
                find out the page that corresponds to the fault linear address.
                if it has been cached,then commit readahead on it.
                if it has not been cached,invoke page_cache_read() for allocarte
                and add it into page cache,then readpage() on it.
                if it has no PG_uptodate,then we have to update it.
                /**
                 * the linear address relative @vma,and @pgoff also is relative @vma.
                 * we can easily to know that the linear address resided on the page
                 * in @vma has offset @pgoff.
                 */

              first,retrieve page offset from @vmf->pgoff,then retrieve size of inode
              of the file @vma->vm_file.
              we must convert the size to page offset,then compare it with @vmf->pgoff,
              if the size is less than or equal to @pgoff,return VM_FAULT_SIGBUS --
              because the file portion is not existed.
              next,invoke find_get_page() on the page cache,index is @pgoff.
                                                            /* VMA use @vm_pgoff to associate
                                                             * mapping page with the page cache
                                                             */
              the page is found {
                      do_async_mmap_readahead() on it,pass it arguments
                        @vma, @vma->file->f_ra, @vma->file, the page, @pgoff
                        /* page_cache_async_readahead() if the page is PG_reclaim,
                         * macro function is PageReadahead()
                         * but,if @vma set VM_RandomReadHint,then do not readahead.
                         * @ra's @mmap_miss will decrease if its value greater than 0.
                         */
                      lock the page through lock_page() /* might sleep */
                                            /* trylock_page() => test_and_set_bit_lock()
                                             * if PG_locked have been setup,invoke __lock_page()
                                             * wait on the bit cleared
                                             */
                      check whether the page get truncated -- the page's @mapping != @file's @f_mapping
                      in this case,we must unlock it,and put it,then goto label "no_cached_page".
                      /* it is no longer cached by the page cache */
              }
              the page is not found {
                      invoke do_sync_mmap_readahead(),pass it arguments
                        @vma, @vma->vm_file->f_ra, @vma->vm_file, @pgoff
                      /**
                       * do_sync_mmap_readahead() is similar to do_async_mmap_readahead().
                       * if VM_RandomReadHint is set,then we do not readahead.
                       * if VM_SequentialReadHint is set OR the page we want is next to @ra->prev_pos,
                       * then invoke page_cache_sync_readahead() and return.
                       * if @ra->mmap_miss less than INT_MAX,increase it --> mmap miss.
                       * if @ra->mmap_miss greater than MMAP_LOTSAMISS,return without readahead.
                       *                   # we have been lost enough
                       * the last case,we need to update readahead window
                       *   calculate new @ra_pages through max_sane_readahead()
                       *   if it is not zero,then set
                       *     @ra->start := max_t(long, 0, @pgoff - @ra_pages / 2)
                       *     @ra->size := @ra_pages
                       *     @ra->async_size := 0
                       *     invoke ra_submit() to process readahead.
                       *            # __do_page_cache_readahead()
                       *   otherwise,return to caller without process readahead.
                       */
                      coun_vm_event() PGMAJFAULT
                      set local variable @ret to VM_FAULT_MAJOR,it is used to store return value.

              retry_find:
                      try find_lock_page() at @pgoff,if the page still un-cached,goto label "no_cached_page".
              }
              at there,we must been got the page descriptor,so check PG_uptodate for it.if it is not uptodated,
              goto label "page_not_uptodate",we must issue readpage() on it.
                         # if readahead is succeed,then PG_uptodate should been setup -- by mpage_readpage()
              we have to recheck again the size of inode,maybe a kernel control path have truncated the file.
              /* readpage() would release PG_locked when accomplished reading,but kernel preempt can take
               * place before we invoke lock_page() at previous steps.
               */
              if it is the case,we have to unlock the page,page_cache_release() it,return VM_FAULT_SIGBUS.
              /* the file is truncated,thus the data in that page is not consist with the file on disk */
              it is not the case,we have succeed to get page and read it.
              update @ra's @prev_pos to @pgoff,of course,in file offset.
              set @vmf->page to the page we got.
              return @ret | VM_FAULT_LOCKED. /* @ret's default value is 0
                                              * if we did async readahead,the return value is VM_FAULT_LOCKED,
                                              * if it is sync readahead,the value is VM_FAULT_MAJOR | VM_FAULT_LOCKED.
                                              */ ==> normal path
                                                     the page still locked

              no_cached_page: /* no readahead,just read a specified page */
                the page is not in page cache,invoke page_cache_read() to cache it.this function allocate new
                page,add it into pagecache,if succeed,call readpage() on it.
                if page_cache_read() succeed,we jump to "retry_find",then we can get into the normal path.
                otherwise,return VM_FAULT_OOM if the function returned -ENOMEM,return VM_FAULT_SIGBUS for others.

              page_not_uptodate:
                we got the page,but it has no PG_uptodate.
                ClearPageError() on the page.
                invoke readpage() of the @a_ops to read it.
                if succeed,then wait_on_page_locked,check PG_uptodate,it still no PG_uptodate,record the error -EIO.
                next,page_cache_release() the page.
                check error,if it is 0 -- readpage() succeed and has PG_uptodate OR it is AOP_TRUNCATED_PAGE,
                then we jump to "retry_find" attempt find out it again.
                otherwise,shrink_readahead_size_eio() for the file's @f_ra,return VM_FAULT_SIGBUS.
                /* shrink_readahead_size_eio() - @ra->ra_pages /= 4 */

            !! function do_linear_fault() invoke to __do_fault(),and __do_fault() invoke to fault()
               method of that VMA.
               if everything is OK,__do_fault() will makeup a new Page Table Entry that corresponds
               the fault address,the page in Page Table Entry is the page got from fault() method,its address 
               is recorded in @vmf.page.
               for a non-linear memory mapping that maps a disk file,function do_nonlinear_fault() will be called.    
            !! system call sys_munmap() is used to unmap memory mapping.even the memory mapping is removed
               by do_munmap(),the page that VMA mapped still alive in page cache of the file.function
               remove_vma() called by remove_vma_list() just fput() @vm_file member,page cache stay reference
               on it.the writeback is handled later by the BDI.
            !! User Mode process can issues system call madvise() for the linear address that tell kernel the
               future read operations on VMA mapped disk file are sequential reads -- MADV_SEQUENTIAL,that
               set VM_SEQ_READ for the VMA;and in other hand,MADV_RANDOM let flag VM_RAND_READ is set in that
               VMA.advice MADV_NORMAL means unspecified order,this cause both VM_RAND_READ and VM_SEQ_READ are
               set in VMA.

        Flushing Dirty Memory Mapping Pages to Disk :
          system call sys_msync() is used to sync a memory mapping.

          <mm/msync.c>
            /**
             * sys_msync - system call msync for sync memory mapping
             * @start:     starting linear address
             * @len:       length of memory region
             * @flags:     indicates what have to be done
             * return:     0 OR error code
             * # there are three kinds of flags :
             *     MS_SYNC  - syncs the entire file - including mappings
             *     MS_ASYNC - do not start I/O
             *     MS_INVALIDATE - asks to invalidate other mappings of the same file
             */
            SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags)
            => asmlinkage long sys_msync(unsigned long start, size_t len, int flags);

            description for sys_msync() :
              first,it have to check @flags,there must be at least one flag is enabled,
              and cant be MS_ASYNC AND MS_SYNC at the same time.
              @start must be a valid address.
              any checking is failed,return -EINVAL.

              use @len with @start to calculate @end of the region,@end < @start,return -ENOMEM;
              @end == @start,return 0 with do nothing. /* range [@start, @end) */

              acquire @mmap_sem for read,it come from @current->mm.
              try to find the VMA which spanned the interval,from linear address @start.

              next,make use of an infinite for-cycle {
                      if the VMA is NULL,release @mmap_sem and return -ENOMEM.

                      if @start is less than VMA->vm_start,that means there is some unmapped
                      address ranges,in this case,we reset @start to VMA->vm_start,ignore the
                      unmapped region;check again @start and @end,if it is greater than or equal to @end,
                      we release @mmap_sem and return -ENOMEM.
                      /* we ignored it,but still return -ENOMEM at the end */

                      check @flags,MS_INVALIDATE AND VM_LOCKED,then release semaphore and return -EBUSY.
                      /* the memory region is locked,we can not invalidate it */

                      /* ready to sync */
                      reset @start to VMA->vm_end.
                      if MS_SYNC AND VMA->vm_file is not NULL /* file mapping */ AND VM_SHARED
                              get file on @vm_file.
                              release semaphore.
                              invoke vfs_fsync() on the file,not only datasync,we need to sync everything.
                              put the file.                        /* ^only inode */        
                              if vfs_fsync() returned an error code OR @start is greater than or equal to @end,
                              return the error code.                   /* interval have been consumed */
                              file synchronization succeed,acquire semaphore again,and find_vma() from @start.
                              /* return value of vfs_fsync() is stored in local variable,it would be
                               * returned to userspace.
                               */
                      other cases
                              return 0 and release semaphore if interval have been consumed.
                              update VMA to VMA->vm_next.
                              /* so we can know that MS_ASYNC do no thing except check VMAs,
                               * and MS_SYNC without VM_SHARED as the same.
                               * this ensure there are some VMAs have spanned the interval [@start, @end).
                               * maybe some unmapped region,in this case,we need return -ENOMEM.
                               */
              }
              release semaphore,return error code or 0.

              !! maybe the last recent vfs_fsync() succeed,but if we have detected some unmapped address ranges
                 during iterating,-ENOMEM still have to be returned.

      Non-Linear Memory Mappings :
        Linux 2.6 kernel offers another kind of access method for regular files -
          the non-linear memory mappings
          non-linear memory mappings is similar to linear memory mapping,but its
          memory pages are not mapped to sequential pages on the file,rather,each
          memory page maps a random(arbitrary) page of file's data.
          /* User Mode process can call to mmap() several times to achieve this,but that is not very efficient,
           * each mapping page requires its own memory region(VMA).
           */

        VM_NONLINEAR => the VMA contains a non-linear mapping.
        all non-linear mappings of a file are collected in address_space.i_mmap_nonlinear.

        !! "Understanding the Linux Kernel" HAVE MENTIONED SYSTEM CALL remap_file_pages(),BUT
           THIS SYSTEM CALL IS "deprecated".AND SINCE Linux 2.6.23,THE SYSTEM CALL CREATES
           NON-LINEAR MAPPING ONLY ON IN-MEMORY FILESYSTEMS.
           FOR FILESYSTEMS WITH A BACKING STORE,SHOULD USE mmap() TO ADJUST WHICH PARTS OF THE FILE ARE
           MAPPED TO WHICH ADDRESSES.

        if User Mode process want to establish non-linear memory mapping,it must call to mmap() for create
        a normal shared memory mapping associated with a file,then issue remap_file_pages() on the pages in
        the shared memory region.

        sys_remap_file_pages() :
          <mm/fremap.c>
            /**
             * sys_remap_file_pages - establish non-linear memory mapping for the file's pages
             * @start:                starting linear address
             * @size:                 size of new mapping
             * @prot:                 (memory protection)unused,but must be 0
             * @pgoff:                initial page index of the file pages
             * @flags:                0 OR MAP_NONBLOCKED,the later will cause no I/O
             * return:                0 OR error code
             * # deprecated
             */
            SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,
                            unsigned long, prot, unsigned long, pgoff, unsigned long, flags)
            => asmlinkage long sys_remap_file_pages(unsigned long start, unsigned long size,
                                                    unsigned long prot, unsigned long pgoff,
                                                    unsigned long flags);

            description for sys_remap_file_pages() :
              calculate @end from @start and @size.
              do @prot checking,if condition(must be 0) is not satisfied,return -EINVAL.
              sanitize Offset field of linear address @start,convert @size to the same type.
              /* @start &= PAGE_MASK => Directory AND Table
               * @size  &= PAGE_MASK => this request the size of remap region at least one page,
               *                        if the remap region is less than 4096,the result would be 0
               */
              if address range wrap,or span zero-size,return -EINVAL.
              /* @start + @size <= @start */

              #if PTE_FILE_MAX_BITS < BITS_PER_LONG
                               /* number of pages about @size */
                      @pgoff + (@size >> PAGE_SHIFT) >= 1UL << PTE_FILE_MAX_BITS
                              return -EINVAL
                      /* exceeded maximum PTE(s) limit */
              #endif
                                         /* retry */
              acquire @mmap_sem for read,invoke find_vma() with the @current's mm and @start to find out
              VMA which overlap the interval.

              /* we must work on a exist VMA and which has VM_SHARED set */
              VMA unfound OR VMA is not VM_SHARED goto "out".
              /* we can not touch the VMA has private data and without VM_NONLINEAR */
              VMA has private data(@vm_private_data) AND VMA has no VM_NONLINEAR set,goto "out".
              /* we can not establish non-linear mapping on this VMA */
              VMA has no VM_CAN_NONLINEAR set,goto "out".

              /* bad interval */
              @end <= @start OR @start < VMA->vm_start OR @end > VMA->vm_end,goto "out".
              
              now,we have checked all conditions,so we can try to establish non-linear mapping on VMA.
              if VMA has no VM_NONLINEAR
              then
                      /* the file portion been exist */
                      if @pgoff is equal to linear_page_index(VMA, @start)
                      then set return value to 0,goto "out".
                      /* the index got by
                       *   (((@start - VMA->vm_start) >> PAGE_SHIFT) + VMA->vm_pgoff) >> (PAGE_CACHE_SHIFT - PAGE_SHIFT)
                       *   we have known @vm_start is started from 0,for get the real memory linear address for
                       *   the memory unit,must combined it with memory descriptor
                       */

                      /* we are going to establish new mapping,thus WRITE lock is needed */
                      if we have not acquired write lock,then we must release @mmap_sem,and acquire it again for write,
                      then goto "retry" find VAM.

                                                    /* no BDI_CAP_NO_ACCT_DIRTY in mapping's @backing_dev_info */
                      if current file's mapping has capicity for account dirty pages,then
                              let @flags &= MAP_NONBLOCK,it MAP_NONBLOCK is set,we would not start I/O.
                              get_file() on the file.
                              invoke mmap_region(),pass it the file, @start, @size, @flags, VMA->vm_flags, @pgoff
                              as the parameters.
                              fput() the file.
                              if a valid address is returned by mmap_region(),we let the return value record it,
                              else BUG_ON() if the address != @start,return value is 0.
                              goto "out".
                              /* we establish new memory region at there,dirty pages should contribute to accounting */

                      acquire mapping's @i_mmap_lock.
                      flush_dcache_mmap_lock() on the mapping.

                      set VM_NONLINEAR for the VMA.
                      remove VAM from mapping's @i_mmap,the priority tree. /* through vma_prio_tree_remove() */
                      insert VAM into mapping's @i_mmap_nonlinear. /* through vma_nonlinear_insert() */

                      flush_dcache_mmap_unlock() on the mapping.
                      release @i_mmap_lock.

              /* maybe it has non-linear mapping before,or we just set VM_NONLINEAR for it */
              VMA has VM_NONLINEAR,that means this VMA contains a non-linear mapping.
              check VM_LOCKED,if VMA is locked,we must drop PG_Mlocked flag for over-mapped range.
              /* backup @vm_flags,invoke munlock_vma_pages_range() on [@start, @start + @size),restore flags */

              call to mmu_notifier_invalidate_range_start() on the interval,this ask MMU invalid the specified memory range.
              /* invalidate_range_start() is called when all pages in the range are still mapped and have at least one refcount */
              call to populate_range() on the interval with @pgoff.
              /* for each page in the interval,it invoke install_file_pte() on it.
               * install_file_pte() retrieve the PTE corresonding to the page's address,then
               * makeup a new PTE through pgoff_to_pte(current_pgoff),and use the new one instead the PTE. ! pgoff auto increased.
               * macro pgoff_to_pte() is defined in <arch/x86/asm/pgtable-3level.h>,@pte_low is _PAGE_FILE,@pte_high is the pgoff.
               *                                    #^suppose 3-level paging.
               * if encountered error,install_file_pte() returns -ENOMEM.
               */
              call to mmu_notifier_invalidate_range_end().
              /* invalidate_range_end() is called when all pages in the range have been unmapped and the pages have been freed by
               * VM.
               * we need establish non-linear mapping,thus the older mappings should be removed.
               */

              if no error occurred AND no MAP_NONBLOCK is set,then we must try to start I/O.
                      if VM_LOCKED,then invoke mlock_vm_pages_range() on the interval to keep consistency.
                      else,invoke make_pages_present() on the interval,it we are holding @mmap_sem for write,must release it
                      before the calling.
            
              "out":
                      /* if we acquired WRITE,then we must have released READ */
                      if we have not holding @mmap_sem for write,then release @mmap_sem READ.
                      otherwise,releae @mmap_sem WRITE.

                      return error code or normal return value.

      Direct I/O Transfers :
        program that want to have full control of the whole I/O data transfer mechanism,can make use of Direct I/O.
        such program usually is self-caching application,for example,high-performance database servers.
        most of database servers implement their own caching mechanisms that exploit the peculiar nature of the queries
        to the database.
          reasons about kernel page cache is detrimental :
            > lots of page frames are wasted to duplicate disk data already in RAM(in user-level disk cache).
            > the read() and write() system calls are slowed down by the redundant instructions that handle.
              the page cache and the read-ahead,ditto for the paging operations related to the file memory mappings
            > rather than transferring the data directly between the disk and the user memory,the read() and write()
              system calls make two transfers: between the disk and a kernel buffer and between the kernel buffer and
              the user memory.

        Direct I/O : kernel programs the disk controller to transfer the data directly from/to pages belonging to the
                     User Mode address space of a self-caching application. -- bypass kernel page cache
        for ordinary I/O,while the data transfer is in progress,process switch may occurs,because the data transfer is
        asynchronous.thus,the pages belong to User Mode process can be swapped out,but kernel Disk cache cant be swapped
        out and visible to all processes in Kernel Mode,so the page contains data copied from User Mode to Kernel Mode
        is valid even process switch has occurred. -- Disk cache
        for Direct I/O,kernel must take care that these pages in User Mode are accessible by every process in Kernel Mode
        and they are not swapped out while the data transfer is in progress.

        when process open a file with O_DIRECT enabled,function __dentry_open() will check the address_space of the file by
        test @a_ops,and @a_ops->direct_IO(),@a_ops->get_xip_mem().   /* ^called by dentry_open(),in <fs/open.c> */
        if NULL @a_ops OR no either direct_IO() or get_xip_mem() is defined,-EINVAL would be returned.
        /* system call fcntl() can be used set O_DIRECT for a been opened file through command F_SETFL */

        for READ,function generic_file_aio_read() invoke direct_IO() method of address_space;for WRITE,function
        __generic_file_aio_write() invoke generic_file_direct_write(),and in which,method direct_IO() is called.
        implementation of direct I/O depends on filesystem,but for most disk-based filesystem,its direct I/O
        implementation just a wrapper of blockdev_direct_IO().

        <linux/fs.h>
          enum {
                  DIO_LOCKING = 0x01,       /* need locking between buffered and direct access */
                  DIO_SKIP_HOLES = 0x02,    /* filesystem does not support filling holes */
          };

          /**
           * blockdev_direct_IO - a wrapper of __blockdev_direct_IO(),but pass DIO_LOCKING | DIO_SKIP_HOLES
           *                      to the function as the last parameter
           * @rw:                 data direction
           * @iocb:               kernel io control
           * @inode:              file's inode
           * @bdev:               block device
           * @iov:                io vector(s)
           * @offset:             file offset
           * @nr_segs:            number of io vectors
           * @get_block:          get_block() method
           * @end_io:             end I/O method
           *                      # typedef void (dio_iodone_t)(struct kiocb *iocb, loff_t offset, ssize_t bytes, void *private);
           * return:              return value from __blockdev_direct_IO()
           * # there is another version for no locking,blockdev_direct_IO_no_locking(),it pass 0 as the last parameter
           *   to __blockdev_direct_IO()
           */
          static inline ssize_t blockdev_direct_IO(int rw, struct kiocb *iocb, struct inode *inode,
                                                   struct block_device *bdev, const struct iovec *iov,
                                                   loff_t offset, unsigned long nr_segs, get_block_t get_block,
                                                   dio_iodone_t end_io);

        <fs/direct-io.c>
          /**
           * how many user pages to map in one call to get_user_pages(),this determines
           * the size of a structure on the stack.
           */
          #define DIO_PAGES 64

          /**
           * dio - dio block,a dio block is somewhere between the hard sector size and the
           *       filesystem block size.it is determined on a per-invocation basis.when
           *       talking to the filesystem,we need to convert dio blocks to fs blocks by
           *       scaling the dio block quantity down by @blkfactor;similarly,fs block size
           *       quantities are converted to bio block quantities by shifting left by @blkfactor
           * @bio:        bio under assembly,the BIO of current submission
           * @inode:      file's inode
           * @rw:         data direction
           * @i_size:     inode size when submitted
           * @flags:      flags, DIO_LOCKING | DIO_SKIP_HOLES
           * @blkbits:    block size bits
           * @blkfactor:  when we are using an alignment which is finer than the filesystem's soft
           *              block size,this specifies how much finer.value 2 means 1/4 block alignment
           * @start_zero_done:
           *              flag,sub-blocksize zeroing has been performed at the start of a WRITE
           * @pages_in_io:
           *              approximate total IO pages
           * @size:       total request size
           * @block_in_file:
           *              logical block number of current offset in the file
           * @blocks_available:
           *              current number of available blocks
           * @final_block_in_request:
           *              number of last block in the request
           *              it usually is @block_in_file + (current segment length >> @blkbits)
           * @first_block_in_page:
           *              the logical block number corresponding to the segment address
           *              of this request
           *              # suppose offset is 4000,block size is 512,then the result
           *                is 4000 >> 9 => 7
           * @boundary:   previous block is at a boundary
           *              if @map_bh is BH_Boundary,its value is TRUE,otherwise is FALSE
           * @reap_counter:
           *              rate limit reaping
           * @get_block:  get_block() method
           * @end_io:     end_io() method
           * @final_block_in_bio:
           *              current final block in @bio + 1
           * @next_block_for_io:
           *              next block to be put under I/O in dio_blocks units
           *              for the time that no I/O task now,it should be the block number
           *              of @cur_page the new comming page
           * @map_bh:     last get_block() result
           * @cur_page:   current deferred I/O page
           * @cur_page_offset:
           *              offset into @cur_page
           * @cur_page_len:
           *              remaining number of bytes at @cur_page_offset
           * @cur_page_block:
           *              the block number corresponding to @cur_page to starts
           * @bio_lock:   protect BIO fields
           * @refcount:   refcount of this dio and BIOs
           * @bio_list:   singly linked via @bi_private
           * @waiter:     waiting task
           * @iocb:       kernel io control block
           * @is_async:   IO async indicator
           * @io_error:   IO error in completion path
           * @result:     IO result,usually is the data length
           * @curr_page:  number of pages been get via get_user_pages()
           * @total_pages:
           *              number of pages involve this request
           * @curr_user_address:
           *              current segment's address
           * @head:       next page to process,this field usually as subscript
           *              to @pages[]
           * @tail:       last valid page + 1
           * @page_errors:
           *              errno from get_user_pages()
           * @pages:      array of page descriptors,used to stores page descriptors
           *              get from get_user_pages(),these pages will be pinned during
           *              data transferring
           */
          struct dio {
                  /* BIO submission state */
                  struct bio *bio;
                  struct inode *inode;
                  int rw;
                  loff_t i_size;
                  int flags;
                  unsigned blkbits;
                  unsigned blkfactor;
                  unsigned start_zero_done;
                  int pages_in_io;
                  size_t size;
                  sector_t block_in_file;
                  unsigned blocks_available;
                  sector_t final_block_in_request;
                  unsigned first_block_in_page;
                  int boundary;
                  int reap_counter;
                  get_block_t *get_block;
                  dio_iodone_t *end_io;
                  sector_t final_block_in_bio;
                  sector_t next_block_for_io;
                  struct buffer_head map_bh;
                  
                  /* deferred addtion of a page to the bio */
                  struct page *cur_page;
                  unsigned cur_page_offset;
                  unsigned cur_page_len;
                  sector_t cur_page_block;

                  /* BIO completion state */
                  spinlock_t bio_lock;
                  unsigned long refcount;
                  struct bio *bio_list;
                  struct task_struct *waiter;

                  /* AIO related stuff */
                  struct kiocb *iocb;
                  int is_async;
                  int io_error;
                  ssize_t result;

                  /* Page fetching state */
                  int curr_page;
                  int total_pages;
                  unsigned long curr_user_address;
                  
                  /* Page queue */
                  unsigned head;
                  unsigned tail;
                  int page_errors;

                  /* pages[] are not zeroed out at allocation time */
                  struct page *pages[DIO_PAGES];
          };


          /**
           * __blockdev_direct_IO - routine execute Direct I/O on a block device
           * @rw:                   data direction
           * @iocb:                 kernel io control
           * @inode:                file's inode
           * @bdev:                 block device
           * @iov:                  io vector(s)
           * @offset:               file offset
           * @nr_segs:              number of io vectors
           * @get_block:            get_block() method
           * @end_io:               method called when I/O completed
           * @flags:                behavior control
           * return:                number of data length OR error code
           * # EXPORT_SYMBOL
           * # this routine is a library function for use by filesystem drivers
           * # locking rules -
           *     > DIO_LOCKING specified
           *       for WRITE,this routine is called under @i_mutex and returns with @i_mutex held
           *       for READ,@i_mutex is not held on entry,but it is taken and dropped again before returning
           *       for R/W,@i_alloc_sem is taken in shard mode,and released on I/O completion
           *     > DIO_LOCKING is not specified
           *       no any internal locking,rely on the filesystem to synchronize direct I/O R/W versus each
           *       other and truncate
           *       for R/W,both @i_mutex and @i_alloc_sem are not held on entry and are never taken
           */
          ssize_t __blockdev_direct_IO(int rw, struct kiocb *iocb, struct inode *inode, struct block_device *bdev,
                                       const struct iovec *iov, loff_t offset, unsigned long nr_segs, get_block_t get_block,
                                       dio_iodone_t end_io, int flags);

          what __blockdev_direct_IO() does :
            1> get blk-bits from @inode->i_blkbits.
               get blk-size-mask through "(1 << blk-bits) - 1". /* for 512 block size,blk-bits is 9,thus mask is 0x01ff */
               set @end to @offset.
               declare a struct dio pointer @dio.
            2> if @rw is WRITE,then set @rw := WRITE_ODIRECT_PLUG.
               if @bdev is not NULL,set bdev-blk-bits to blksize_bits(bdev_logical_block_size(@bdev)).
                                                         /* calculate blk size bits according to
                                                          * @bdev->bd_disk->queue->limits.logical_block_size,
                                                          * if NULL queue,the block size default is 512
                                                          */
            3> if @offset & blk-size-mask is not 0,that means @offset is not a multiple of block-size.
               in this case,we must re-caculate blk-size-mask depend on bdev-blk-bits.
               reset blk-bits to bdev-blk-bits if @bdev is not NULL.
               let blk-size-mask := (1 << blk-bits) - 1.
               and check again,if the result still is TRUE,goto label "out",the error code is -EINVAL.
               /**
                * this is because Direct I/O bypass page cache.in page cache,kernel read a full block,
                * and only fill the buffer with data it wants.
                * if we bypass page cache,then no one will help us align to disk block(the size is 512).
                * if @offset is not block size aligned,the disk controller can not perform "zero-copy"
                * DMA transfers starting at the @offset.
                * the minimum size of data read from disk is one sector.
                * for example,@offset is 303,if we want to read data starting from 303,then we must read
                * the file block has number 0;but we are in direct I/O,no one help us to adjust the file block
                * number that neeed to be readed,thus,@offset & blk-size-mask can not get correct block number.
                * the hard disk only knows the sector0, sector1, ... , sector_i, secotr_k, sector_n .
                * if we ask disk to read file start from 303,it do not know what is the sector stored data
                * we want;but if @offset is 1024,it will know the sector2 contains the data we want.
                * (suppose file start from sector0, sector0 [0, 511], sector1 [512, 1023], sector2[1024, 1535],
                *  and so on.)
                */
            4> make use of a for-cycle,for each segments,do {
                       store current segment's buffer address in local variable @addr.
                       store current segment's buffer size in local variable @size.
                       update @end to @end + @size.(the initial value of @end is @offset)
                       check,if @addr is not a multiple of blk-size OR @size is not a multiple of blk-size,
                       then   
                               /* we require address of segment and length of segment both align to block size.
                                * because @offset is aligned to block size,and the minimum size of data for I/O
                                * on a disk usually is 512(block size),thus the segment must is large enough to
                                * store the dat read from/write into the disk.
                                */

                               re-calculate blk-size-mask as same as the above.
                               and check again,if either conditions is not satisfied,goto label "out",the
                               error code is -EINVAL.
               }
            5> kmalloc() an object is type of struct dio,store the pointer in @dio,gfp flag is GFP_KERNEL.
               return -ENOMEM if failed on allocating.
               set all fields except @pages array in the object to zero.
               set @dio->flags to @flags.
            6> if DIO_LOCKING is set,then
                       rw is READ AND @end is greater than @offset
                               acquire mutex @inode->i_mutex.
                               invoke filemap_write_and_wait_range() on the file's @f_mapping,range is
                               [@offset, @end - 1].
                               if any error occurred,release @i_mutex,free @dio,goto "out",the error code
                               is returned by filemap_write_and_wait_range().
                       otherwise,invoke down_read_non_owner() on @inode->i_alloc_sem,acquire semaphore for
                       READ in shared mode.
            7> set dio->is_async to
                 not KIOCB_SYNC_KEY AND not (WRITE AND @end > inode's size)
                                            #^valid range
               call to direct_io_worker(),the arguments are :
                 @rw, @iocb, @inode, @iov, @offset, @nr_segs, blk-bits, @get_block, @end_io, @dio
            8> if DIO_LOCKING is set,then
                       @rw is WRITE AND return value from direct_io_worker() less than 0
                               check whether @end is greater than current inode's size,if it is the case,
                               call to vmtruncate() on @inode with its current size.
            
            "out":
              just return the length of data or error code.if direct_io_worker() encountered error,the
              error code will be returned to the caller who called blockdev_direct_IO().

          /**
           * direct_io_worker - routine execute direct IO
           * @rw:               data direction
           * @iocb:             kernel io control
           * @inode:            file's inode
           * @iov:              io vector(s)
           * @offset:           file offset
           * @nr_segs:          number of io vectors
           * @blkbits:          block size bits
           * @get_block:        get_block() method
           * @end_io:           called when I/O completed
           * @dio:              dio object
           * return:            data length OR error code
           * # this routine will release both @i_mutex and @i_alloc_sem
           */
          static ssize_t
          direct_io_worker(int rw, struct kiocb *iocb, struct inode *inode, const struct iovec *iov,
                           loff_t offset, unsigned long nr_segs, unsigned blkbits, get_block_t get_block,
                           dio_iodone_t end_io, struct dio *dio);

          what direct_io_worker() does :
            1> set up @dio :
                 .inode              = @inode
                 .rw                 = @rw
                 .blkbits            = @blkbits
                 .blkfactor          = @inode->i_blkbits - @blkbits
                                       /* differ between inode's blkbits and caller passed blkbits,
                                        * inode's i_blkbits is set by filesystem.
                                        * if we are called from __blockdev_direct_IO(),@blkbits either
                                        * is filesystem's blkbits or the block device's blkbits.
                                        * # suppose filesystem block size is 1024,but block device block size
                                        *   is 512,and @blkbits is the block device's blkbits.
                                        *   @i_blkbits of inode is 10,@blkbits is 9,result is 1.
                                        *   dio.blkfactor is 1,the bytes gap is 512B.
                                        *   for blkfactor is 2,disk block size is 512,the filesystem's
                                        *   soft block size is 2048,512 is 1/4 of 2048.
                                        */
                 .block_in_file      = @offset >> @blkbits
                 .get_block          = @get_block
                 .end_io             = @end_io
                 .final_block_in_bio = -1
                 .next_block_for_io  = -1
                 .iocb               = @iocb
                 .i_size             = @inode's size
                 init .bio_lock through spin_lock_init()
                 .refcount           = 1
               if @dio->blkfactor is TRUE,set @dio->pages_in_io to 2.
            2> for each segments,do {
                       let @user_addr store current vector's @iov_base.
                       let @dio->pages_in_io +=
                               ((@user_addr + current vector's @len + PAGE_SIZE - 1) / PAGE_SIZE
                                - @user_addr / PAGE_SIZE)             #^4095
               } /* calculates how many pages in this direct I/O.
                  *                            (ceiling)
                  *                      (linear address arithmetic)
                  *                |<-- iov_len -->|<-- PAGE_SIZE - 1 -->|
                  * |##############|===============@=====================|===============@=====================|...|
                  *                ^iov_base                             ^iov_base
                  *                 iov[0]
                  * |####################################################|===============@=====================|...|
                  *                                                      ^iov_base     
                  *                                                       iov[1]
                  *                                                      |<-- iov_len -->|<-- PAGE_SIZE - 1 -->|...|
                  *
                  * if this routine is called from __blockdev_direct_IO(),then segment's address and segment's length
                  * must been aligned to block size.
                  */
            3> for each segments,do {
                       /* we handle one vector in once iteration,place it in current request,and submit it */

                       let @user_addr store current vector's @iov_base.
                       set @bytes to current vector's @iov_len.
                       let @dio->size += @bytes.
                       calculate block number of @user_addr,store it in @dio->first_block_in_page.
                       calculate the last block number relative to current segment,depends on @dio->block_in_file,
                       store it in @dio->final_block_in_request. /* current segment's length >> @blkbits + 
                                                                  * @dio->block_in_file
                                                                  */
                       set @dio->head to 0.
                       set @dio->tail to 0.
                       set @dio->curr_page to 0.
                       set @dio->total_pages to 0.

                       if @user_addr & PAGE_SIZE - 1 is TRUE,then
                       /* suppose PAGE_SIZE is 4096 => 0001000000000000 (0x1000)
                        * PAGE_SHIFT is 12
                        * PAGE_SIZE - 1 is 4095 => 0000111111111111 (0x0fff)
                        * @iov_base is in the middle of a page,thus,during I/O,
                        * the page from User Mode containing the address @iov_base also
                        * required to be pinned.this is why @total_pages have to be increased.
                        */
                               increase @dio->total_pages.
                               let @bytes -= PAGE_SIZE - (@user_addr & (PAGE_SIZE - 1)).
                               /**
                                * for @bytes,if it is less than the remaining space of current page,
                                * this expression will get a negative value,then @bytes will
                                * wrapped.
                                */

                                /* Kernel "C" optimization,use defined behavior of unsigned integer wrapping
                                 * to eliminate conditional branches.
                                 */

                                                /**
                                                 * if the buffer @iov_base spanned two pages,
                                                 * then will have to pin the next page,too.
                                                 * if @bytes is wrapped,that means it is less than
                                                 * the remaining space of current page.
                                                 * and expression
                                                 *   "@bytes + PAGE_SIZE - 1"
                                                 * would get a result is greater than MAX(typeof(@bytes),
                                                 * thus it will be cut,so the @result / PAGE_SIZE will
                                                 * get a correct value.it is 0 if @bytes did not wrapped
                                                 * 1 page,or greater than 0 if it wrapped several pages.
                                                 * # e.g.
                                                 *     iov_base is 4000, iov_len is 32
                                                 *     @bytes get value -64,and wrapped to 65472
                                                 *     65472 + 4095 = 69567
                                                 *     cut to 4031
                                                 *     4031 / 4096 = 0
                                                 */
                       let @dio->total_pages += (@bytes + PAGE_SIZE - 1) / PAGE_SIZE.
                       set @dio->curr_user_address to @user_addr.

                       call to do_direct_IO() on @dio to handle this request.

                       let @dio->result += current vector's @iov_len -
                                           ((@dio->final_block_in_request - @dio->block_in_file) << @blkbits).
                       the @result store the sum of data length of each request.
                    
                       invoke dio_cleanup() on @dio and break cycle,if do_direct_IO() returned an error.
               }

            4> if ret value of do_direct_IO() returned -ENOTBLK AND WRITE is set in @rw
               then,reset @ret to 0.
               call to dio_zero_block(),pass it @dio and 1 as second parameter.
               /* there may be some unwritten disk at the end of a part-written fs-block-sized block,
                * go zero that now.
                */
            5> if @dio->cur_page is TRUE,then /* remain a I/O task */
                       call to dio_send_cur_page() on @dio.
                       if @ret is 0,set @ret to the result of this calling.
                       page_cache_release() the @cur_page,and set this field to NULL.
            6> if @dio->bio is TRUE,then call to dio_bio_submit() on @dio to submit the BIO. /* remain a BIO */
               call to dio_cleanup() on @dio.
               /**
                * this routine make use a while-cycle,stop condition is dio_pages_present() returned 0.
                * for each page get through dio_get_page(),it invoke page_cache_release() on it.
                * # dio_get_page() would let @dio->head increase,if @dio->tail - @dio->head is 0,dio_pages_present()
                *   will return 0,then cycle stopped.
                *   these pages are get through dio_refill_pages().even we put page at there,if the BIO this page
                *   associated with is not completed,the refcount stay a positive number.
                */
            7> if READ is set in @rw AND DIO_LOCKING is set in @dio->flags,release @i_mutex of @dio->inode.
               BUG_CHECK,it is a BUG if @ret == -EIOCBQUEUED.
            8> if @dio->is_async is TRUE AND @ret is equal to 0 AND @dio->result is TRUE AND
                  (READ is set in @rw OR @dio->result is equal to @dio->size)
                       set @ret to -EIOCBQUEUED
               if @ret is not equal to -EIOCBQUEUED
                       kick off the inode's address_space by invoke blk_run_address_space(@dio->inode->i_mapping).
                       invoke dio_await_completion() on @dio.
                              /* wait on and process all in-flight BIOs.
                               * this must only be called once all bios have been issued
                               * so that the refcount can only decrease.
                               * this just waits for all bios to make it through dio_bio_complete().
                               * io error information is stored in @dio->io_error,and should be propagated via
                               * dio_complete().
                               */
            9> acquire @dio->bio_lock,and disable local interrupt.
               retrieve --@dio->refcount.
               release the lock,and restore local interrupt.
               if we found the refount is equal to 0,we must call to dio_complete(),pass it @dio,@offset,@ret as
               paramters,and kfree() the @dio.                       /* this routine will release @i_alloc_sem
                                                                      * it also call to @end_io if @dio->result
                                                                      * is not FALSE
                                                                      */
               otherwise,do BUG CHECK,it is a BUG if @ret is not equal to -EIOCBQUEUED.
           10> return @ret to caller.

          /**
           * do_direct_IO - do direct I/O
           *                walk the user pages,and the file,mapping blocks to disk and generating
           *                a sequence of (page,offset,len,block) mappings.these mappings are injected
           *                into submit_page_section(),which take care of the next stage of submission
           * @dio:          dio block
           * return:        0 OR error code
           * # direct IO against a blockdev is different from a file.because we can happily perform page-sized
           *   but 512B aligned IOs,this is important that blockdev IO be able to have fine alignment and large sizes
           *   so what we do is to permit the @get_block() function to populate bh.b_size with the size of IO which
           *   is permitted at this @offset and this @i_blkbits
           *   for best results,the blockdev should be set up with 512B @i_blkbits and it should set @b_size to
           *   PAGE_SIZE or more inside get_block().this gives fine alignment but still allows thisfunction
           *   to work in PAGE_SIZE units
           */
          static int do_direct_IO(struct dio *dio);

          what do_direct_IO() does :
            1> get blk-bits through @dio->blkbits.
               get blocks-per-page by calculate "PAGE_SIZE >> blk-bits".
               make use the buffer @dio->map_bh.
               get block-in-page through @dio->first_block_in_page. /* the block corresponding to the segment address */
            2> enter a while-cycle,stop condition is @dio->block_in_file >= @dio->final_block_in_request {
                       call to dio_get_page() on @dio to get a page descriptor in @page.
                       /* if @dio->tail - @dio->head is 0,that means no pages present,we must refill it.
                        * this willd be done by dio_refill_pages() on @dio.
                        * when there is some pages present,we retrieve the page descriptor through @dio->pages[@dio->head++]
                        *
                        * dio_refill_pages() :
                        *         first,calculate number of pages have to be refilled - min(total_pages - curr_page, DIO_PAGES).
                        *         then call to get_user_pages_fast() to get pages,and stores page descriptors in @dio->pages[].
                        *         !! because direct_io_worker() will clear page fetching state,thus @total_pages is a relative
                        *            value about current segment.if current segment spanned more than one pages,then it is the
                        *            number of pages than segment have spanned.
                        *            because @curr_user_address is set to the address of current segment,and @curr_page is set
                        *            to 0,dio_refill_pages() is called again from dio_get_page().
                        *            in this case,get_user_pages_fast() have to get user pages from current segment's linear
                        *            address,number of pages is @total_pages or DIO_PAGES
                        *                                       #^because @curr_page is set to 0
                        *
                        *         if failed but @blocks_available is not 0 AND @rw is WRITE,then use a page - ZERO_PAGE(0).
                        *         # macro ZERO_PAGE() is defined in <arch/x86/include/asm/pgtable.h>,ZERO_PAGE is a global
                        *           shared page that is always zero.
                        *           array named @empty_zero_page is type of unsigned long,size of elements is 128 = 4096 / 32
                        *           for 32 bit architecture.(unsigned long is 32 bit,32 * 128 == 4096B)
                        *           ZERO_PAGE(vaddr) => (virt_to_page(empty_zero_page))
                        *         if we are using ZERO_PAGE,then we must store the error code from get_user_pages_fast() in
                        *         @dio->page_errors,if the field is 0.then page_cache_get() the ZERO_PAGE,let @dio->pages[0]
                        *         stores the page descriptor of ZERO_PAGE,set @head to 0,set tail to 1,return 0 to caller.
                        *         if get_user_pages_fast() was succeed,then we have to
                        *                 update @curr_user_address to @curr_user_address + number of pinned pages 
                        *                 # this is because we get pages starting from @curr_user_address
                        *                 update @curr_page to @curr_page + number of pinned pages
                        *                 set @head to 0,set @tail to number of pinned pages
                        *                 return 0 to caller
                        *                 # now @curr_user_address is the address of last page frame.
                        *                   but dio_get_page() still return page at @pages[0],current segment's
                        *                   address.
                        *                   when we need to refill pages again,the linear address is started from @curr_user_address,
                        *                   but before we consumed all pages in @pages[],@curr_user_address is useless.
                        *                   direct_io_worker() will reset page fetching state,in this case,dio_refill_pages()
                        *                   will be called for each segment,and @curr_user_address is the current segment's
                        *                   linear address.
                        *         !! when do_direct_IO() is called from direct_io_worker(),@curr_user_address must be the
                        *            current segment's iov_base.
                        */

                       if it is bad page frame,goto label "out",return value is PTR_ERR(@page).

                       enter a nested while-cycel,stop condition is block-in-page >= blocks-per-page {
                               /* we handle one page in this cycle */
                               get offset into this page through "block-in-page << blk-bits".
                                   (offset-in-page)

                               if @dio->blocks_available is _zero_,we must get more blocks for it.
                               so,we have to
                                       invoke get_more_blocks() on it.
                                       /**
                                        * this routine invoke @dio->get_block() if and only if @dio->page_errors is 0,that
                                        * means no error was detected at previous stage.
                                        *
                                        * ! it is a BUG if @dio->block_in_file >= @dio->final_block_in_request,that means
                                        *   the work about file data handling is not belongs to current request.
                                        *
                                        * if @rw is WRITE,then we need to @create new block,but if DIO_SKIP_HOLES is set AND
                                        * @dio->block_in_file is less than (@dio->inode's size >> @dio->blkbits),then we can
                                        * not @create new block.
                                        * ! for writes inside @i_size on a DIO_SKIP_HOLES filesystem we forbid block
                                        *   creations: only overwrites are permitted.
                                        *   we will return early to the caller once we see an unmapped buffer head returned,
                                        *   and the caller will fall back to buffered I/O.
                                        *   otherwise,the decision is left to the get_blocks() method,which may decide to
                                        *   handle it or also return an unmapped buffer head.
                                        * but argument for @iblock of get_block() method depends on filesystem,routine
                                        * get it by calculate "@dio->block_in_file >> @dio->blkfactor".
                                        * this routine make use of @dio->map_bh as buffer head,@b_state is set to 0;
                                        * @b_size is set to "@fs_count << @dio->inode->i_blkbits".
                                        * about @fs_count:
                                        *                     |<---------------- dio count --------------------->|
                                        *         fs_count := (@dio->final_block_in_request - @dio->block_in_file) >> @dio->blkfactor
                                        *         fs_count++ if and only if "dio count" & "(1 << @dio->blkfactor) -1" is not zero.
                                        *         /* alignment */
                                        *         /* suppose fblkinreq is 8,and blkinfile is 2,@blkfactor is 1,
                                        *          * fs_count is 6 >> 1 => 0b110 >> 1 => 0b11 => 3
                                        *          * 0b110 & (1 << 1) - 1 => 0b110 & 0b001 => 0,no increasement
                                        *          * @b_size => 3 << 10(1024B) => 1100000000000 => 6144 == 6 * 1024
                                        *          * thus,@blocks_available would be 6144 >> 9 => 12
                                        *          */
                                        * we get a whole chunk at once,instead to invoke get_block() for each piece which is block-sized.
                                        * function return the result of get_block() to caller or @dio->page_errors if it is not _zero_.
                                        */
                                       if we failed to get more blocks,we have to page_cache_release() and goto label "out".
                                       in this case,the error code either is @dio->page_errors or error code from get_block().
        
                                       if @map_bh has no BH_Mapped,goto label "do_holes".
        
                                       update @dio->blocks_available,it is the result of "@map_bh->b_size >> @dio->blkbits",because we
                                       get the whole chunk at once.
                                       update @dio->next_block_for_io,it is the result of "@map_bh->b_blocknr << @dio->blkfactor".
                                                                                          /* because we get a chunk of blocks,
                                                                                           * this will get the initial block number
                                                                                           * of blocks in this chunk
                                                                                           */
        
                                       if @map_bh has BH_New,we must call to clean_blockdev_aliases() on @dio,this function will
                                       unmap_underlying_metadata() for each block in the chunk.
        
                                       /* @blkfactor is _zero_,means no remainder gap,we can just get into next stage */
                                       if @dio->blkfactor is _zero_,we have goto label "do_holes".
        
                                       /* if we are at the start of IO and that IO starts partway into a fs-block,
                                        * dio remainder will be non-zero.
                                        * if IO is READ then we can simply advance the IO cursor to the first block
                                        * which is to be read.
                                        * if IO is WRITE AND the block was newly allocated,we can not do that,the start of
                                        * fs block must be zeroed out on-disk.
                                        */
                                       if @map_bh has no BH_New,we update @dio->next_block_for_io to
                                               @dio->next_block_for_io + (@dio->block_in_file & ((1 << @dio->blkfactor) - 1))
                                                                         |<---------------- dio remainder ----------------->|o
                                       update @dio->blocks_available to
                                              @dio->blocks_available - (@dio->block_in_file & ((1 << @dio->blkfactor) - 1))
                                              /* follow the suppose at above,@block_availabe is 12,
                                               * dio remainder is "2 & ((1 << 1) - 1)" => 0b10 & 0b01 => 0
                                               * @blocks_availabe -= 0 => 12
                                               */

                               do_holes: /* handle holes */

                               if @map_bh has no BH_Mapped,that is no block in disk corresponds to the buffer head,it is a hole.
                               then we have to
                                       for WRITE operation,we just page_cache_release() @page and return -ENOTBLK.
                                       
                                       get size alignment for inode's size,it is ALIGN(@dio->inode's @i_size, 1 << blk-bits).
                                       if @dio->block_in_file is greater than or equal to (i_size_aligned >> blk-bits),that means
                                       we encountered EOF,just page_cache_release() @page,goto label "out".

                                       otherwise,zero_user() on @page from block-in-page << blk-bits to maximum block size,that is,
                                       we zero out this block.
                                       let @dio->block_in_file increase,block-in-page increase,goto label "next_block".

                               @map_bh is BH_Mapped.

                               if @dio->blkfactor is not _zero_ AND not @dio->start_zero_done,then dio_zero_block() on @dio with
                               parameter @end = 0,this routine zero out unused portion of the block with zeros.
                               /* this is happens only if user-buffer,file offset or io length is not filesystem block-size multiple.
                                * end is zero if we are doing the start of the IO,1 at the end of the IO.
                                * dio_zero_block(dio, end) :
                                *         set @dio->start_zero_done = 1. # we only zero out at I/O starting
                                *         if @dio->blkfactor is 0 OR @dio->map_bh has no BH_New,just return to caller.
                                *         if @dio->block_in_file & ((1 << @dio->blkfactor) - 1) is zero,just return to caller.
                                *            |<----------- this_chunk_blocks ---------------->|
                                *         if @end is 1,we set @this_chunk_blocks to "(1 << @dio->blkfactor) - @this_chunk_blocks".
                                *         calculate bytes of this chunk,it is @this_chunk_blocks << @dio->blkbits.
                                *         make use of ZERO_PAGE(0).
                                *         try submit_page_section(),pass it arguments :
                                *                 @dio, ZERO_PAGE(0), bytes of this chunk, @dio->next_block_for_io
                                *         if submit_page_section() returned an error code,just return to caller without zero out.
                                *         otherwise,we update @dio->next_block_for_io += @this_chunk_blocks.
                                */

                               let local variable @this_chunk_blocks = @dio->blocks_available.
                               store the result of "(PAGE_SIZE - offset-in-page) >> blk-bits" in local variable @u,it is
                               the number of blocks that corresponding to the reamining space.
                               
                               if @this_chunk_blocks is greater than @u,let @this_chunk_blocks = @u,that means se use the
                               remaining space.
                               reset @u to @dio->final_block_in_request - @dio->block_in_file,it is the number of blocks
                               that corresponding to the length of data in this request.
                               check again,if @this_chunk_blocks is greater than @u,let @this_chunk_blocks = @u,we only
                               need space that is right large enough to store the data we want.
                               
                               calculate @this_chunk_bytes,it is @this_chunk_blocks << blk-bits.
                               /* it is a BUG if @this_chunk_bytes is 0 */

                               if @map_bh has BH_Boundary,we set @dio->boundary to TRUE,otherwise,it must be FALSE.

                               invoke submit_page_section(),pass it the arguments :
                                       @dio, @page, offset-in-page, @this_chunk_bytes, @dio->next_block_for_io
                                             #^dio_get_page()

                               if submit_page_section() returned an error code,page_cache_release() @page,goto label
                               "out",in this case,the return value is the error code.


                               submit_page_section() succeed.
                               update @dio->next_block_for_io to @dio->next_block_for_io + @this_chunk_blocks.
                               update @dio->block_in_file to @dio->block_in_file + @this_chunk_blocks.
                               advance block-in-page,it is block-in-page + @this_chunk_blocks.
                               /* cycle will end when block-in-page >= blocks-per-page */
                               update @dio->blocks_available to @dio->blocks_available -= @this_chunk_blocks.
                                                                                          #^blocks spanned segment length

                               next_blocks:
                                
                               it is a BUG if @dio->block_in_file is greater than @dio->final_block_in_request.
                               /* because we call do_direct_IO() in direct_io_worker() for each segment in a for-cycle,
                                * thus,for current request,the block in file should never greater than final block in request
                                */

                               if @dio->block_in_file is equal to @dio->final_block_in_request,that means we have handled this
                               request,so break cycle.
                       }

                       page_cache_release() @page.
                       set block-in-page to 0.
                       /* this will let the next nested while-cycle start from block 0 of the next page.
                        * if @dio->block_in_file == @dio->final_block_in_request,this cycle should be stopped before next
                        * iteration.
                        */
               }            
            3> return 0 or error code to caller.

          /**
           * submit_page_section - an autonomous function to put a chunk of a page under deferred I/O
           * @dio:                 dio block
           * @page:                the page
           * @offset:              offset into page
           * @len:                 chunk length
           * @blocknr:             logical block number for next I/O start
           * return:               0 OR error code
           */
          static int submit_page_section(struct dio *dio, struct page *page, unsigned offset,
                                         unsigned len, sector_t blocknr);

          description for submit_page_section() :
            this function do not immediately submit I/O,but prefer to wait and see if the next chunck
            can be merged into this I/O.
            if @dio->rw set WRITE,have to invoke task_io_account_write() on @len to update write accounting.
            next,check if this section is able to be merged with the previous one :
              @dio->cur_page == @page AND @dio->cur_page_offset + @dio->cur_page_len == @offset
              AND @dio->cur_page_block + (@dio->cur_page_len >> @dio->blkbits) == @blocknr
              /**
               *
               *                |<------ previous len ------>|<------ this len ------>|
               * |..............|============================@========================|..........| => same page
               *                #^previous offset            #^this offset
               *                |                            |
               *                +--> previous blocknr        +--> this blocknr
               */
              then
                      let @dio->cur_page_len += @len /* we merge them */
                      if @dio->boundary is TRUE,that means a mapped block in this chunk has BH_Boundary,thus
                      we have to dispatch I/O.
                              call to dio_send_cur_page() on @dio,return value stored in local variable @ret.
                              page_cache_release() on @dio->cur_page,we no longer need this page.
                              set @dio->cur_page to NULL,because the old one been dispatched.
                      return @ret to caller /* @ret's default value is 0 */

            can not merge them :
                    if @dio->cur_page is not NULL,that means there been a submission wait for dispatching.
                    so,dispatch it as the same.
                    if @ret is not 0,that means dio_send_cur_page() returned an error,we have to return the
                    error code to caller.

            the case @dio->cur_page is NULL(we have dispatched the old one OR no submission is waiting) :
              page_cache_get() on @page.
              set @dio->cur_page to @page.
              set @dio->cur_page_offset to @offset.
              set @dio->cur_page_len to @len.
              set @dio->cur_page_block to @blocknr.

              return @ret(value is 0) to caller.

          /**
           * dio_send_cur_page - dispatch I/O task of dio block
           * @dio:               dio block
           * return:             0 OR error code
           */
          static int dio_send_cur_page(struct dio *dio);

          description for dio_send_cur_page() :
            this routine is used to dispatch the I/O that involve @dio->cur_page.

            if @dio->bio is not NULL,that means @dio been holding a BIO,we have to check
            if we can merge them to instead create a new BIO.
                    @dio->final_block_in_bio != @dio->cur_page_block => they are not contiguous
                            we just dio_bio_submit() the current BIO @dio->bio.
                    if @dio->boundary is TRUE => we can not merge them
                            just dio_bio_submit() current BIO @dio->bio.
                    
                    /* dio_bio_submit() :
                     *         set @bio->bi_private to the @dio
                     *         acquire lock and disable local interrupt
                     *         increase @dio->recount # because we dispatched a BIO
                     *         release lock and restore interrupt
                     *         call to bio_set_pages_dirty(@dio->bio) if @dio->is_async is TRUE and @dio->rw is READ
                     *                                                   # asynchronous READ operation
                     *         dispatch the BIO through submit_bio()
                     *         reset @dio->bio to NULL,reset @dio->boundary to 0
                     */

            if @dio->bio is NULL,no one we can merge into
                    so,have to invoke dio_new_bio() on @dio with @dio->cur_page_block as the starting blocknr.
                    if allocating failed,just return error code to caller.

            try dio_bio_add_page() on @dio,just attempt merge I/O,do not submit.
            if dio_bio_add_page() returned 1,that means these I/O can not be merged
                    so we have to submit current BIO at first
                    dio_new_bio() to create a new BIO for this I/O
                    if create new bio succeed,call to dio_bio_add_page() on @dio to place @dio->cur_page to the new BIO
                    return the result to caller
                    if creating was failed,we have to return the error code to caller
            dio_bio_add_page() returned 0,that means these I/O have been merged,return 0 to caller

            /**
             * static int dio_bio_add_page(dio) :
             *   first,try bio_add_page() on @dio->bio with page @dio->cur_page and its relative informations
             *   bio_add_page() returned a value is equal to @dio->cur_page_len,that means merging succeed
             *           if @dio->cur_page_len + @dio->cur_page_offset == PAGE_SIZE,that means we used a whole page,
             *           so have to decrease @dio->pages_in_io # we are done this this page
             *           page_cache_get() @dio->cur_page
             *           update @dio->final_block_in_bio,the result is @dio->cur_page_block + @dio->cur_page_len >> @dio->blkbits
             *           return 0 to caller,tell it merging succeed
             *   merging failed,return 1 to caller.
             *
             * ! we increased page refcount in this function,the refcount would be decreased in dio_bio_complelte() if that
             *   BIO has BIO_UPTODATE.
             *   for synchronous I/O,this routine is called by dio_await_completion() in direct_io_worker();method @end_io()
             *   is called by dio_complete() in direct_io_worker(),of course,@end_io can be NULL.
             */


        /**
         * general flow
         *
         * User Mode Process =>
         *   open(O_DIRECT)
         *   READ/WRITE the file =>
         *     filp->f_ops->direct_IO() =>
         *       filesystem direct_IO() =>
         *         blockdev_direct_IO =>
         *           __blockdev_direct_IO() =>
         *             direct_io_worker() =>
         *               do_direct_IO() =>
         *                 dio_get_page() =>
         *                   dio_refill_pages() =>
         *                     get_user_pages_fast() =>
         *                       handle_mm_fault()
         *                 submit_page_section() =>
         *                   dio_send_cur_page() =>
         *                     /* submit old at first,if can not merge them */
         *                     dio_bio_submit() =>
         *                       submit_bio() =>
         *                         __make_request() /* Generic Block Layer*/
         *                     dio_bio_add_page() =>
         *                       bio_add_page() /* can be merged,merge them */
         *                                      /* just merge,not submitting */
         *               dio_cleanup()
         *               dio_await_completion()
         *               dio_complete() =>
         *                 @end_io()
         *
         */

      Asynchrnous I/O :
        User Mode process issues an asynchrnous I/O task via POSIX library functions,and the control will be returned
        to the process after I/O task have enqueued.process can continues its execution,when I/O completed,it will
        received a signal from Kernel.structure aiocb is defined in header <aio.h>,it is the User Mode AIO control block,
        used to tell POSIX library how to handle the Asynchrnous I/O request.
          /*
           *  struct aiocb {
           *          int aio_fildes;                   /* fd */
           *          off_t aio_offset;                 /* file offset */
           *          volatile void *aio_buf;           /* buffer */
           *          size_t aio_nbytes;                /* length of transfer */
           *          int aio_reqprio;                  /* request priority */
           *          struct sigevent aio_sigevent;     /* notification method */
           *          int aio_lio_opcode;               /* operation to be performed,lio_listio() only */
           *  };
           *
           *  primitives :
           *    aio_read        -    enqueue a read request
           *    aio_write       -    enqueue a write request
           *    aio_fsync       -    enqueue a sync request
           *    aio_error       -    obtain the error status of an enqueued I/O request
           *    aio_return      -    obtain the return status of a completed I/O request
           *    aio_suspend     -    suspend the caller until one or more of a specified set of I/O requests completes
           *    aio_cancel      -    attempt to cancle outstanding I/O requests
           *    lio_listio      -    enqueue multiple I/O requests using a single function call
           *
           *  library : POSIX Real-Time - rt
           */

        Asynchrnous I/O in Kernel :
          asynchrnous I/O can be implemented by a system library without any kernel support at all.the method is
          start a lightweight process,let it to handle asynchronous I/O,and I/O request have completed,it inform the
          main thread.just make use of synchrnous I/O interfaces as well,but queue I/O requests in User Mode.
          /* but kernel-level implementation of asynchrnous I/O more faster */

          kernel support asynchrinous I/O primitives :
            io_setup
            io_submit
            io_getevents
            io_cancel
            io_destroy

            !! because glibc does not provide wrapper functions for these system call,user have to issue them via
               syscall() interface.
               libaio provides some wrappers,but it use different io_context_t instance,and it will return the
               -error_code when error is encountered.
               for issue them via syscall(),the error is -1,and @errno is set.

          Asynchrnous I/O Context :
            an asynchrnous I/O context is a set of data structures that keep track of the on-going progresses of
            the asynchrnous I/O operations requested by the process.
            each AIO context is associated with a "kioctx" object,it contains all information relevant for the
            context.
            the kioctx objects created for a process are collected in member @ioctx_list of the memory descriptor.
                                                                             /* hash list */
            /* <linux/aio_abi.h> - typedef __kernel_ulong_t aio_context_t;
             * before call to io_setup(),*@ctxp must be initialized to 0.if io_setup() is succeed,
             * then @ctxp is filled in with the resulting handle.
             */

            <linux/aio_abi.h>
              /**
               * io_event - structure used to represent aio event
               * @data:     data field from the iocb
               * @obj:      what iocb this event came from
               * @res:      result code for this event
               * @res2:     secondary result
               * # read() from /dev/aio returns these structures
               */
              struct io_event {
                      __u64 data;
                      __u64 obj;
                      __s64 res;
                      __s64 res2;
              };

            <linux/aio.h>
              /* current number of aio requests in system wide */
              unsigned long aio_nr;

              /* maximum number of aio requests at the same time in system wide */
              unsigned long aio_max_nr = 0x10000;

              #define AIO_RING_MAGIC 0xa10a10a1
              #define AIO_RING_COMPAT_FEATURES 1
              #define AIO_RING_INCOMPAT_FEATURES 0

              /**
               * aio_ring - AIO ring header
               * @id:       @user_id of the kioctx object which holds this ring
               * @nr:       capacity of io events in the ring buffer
               *            # user copy,it might greater than kioctx.max_reqs + 2,
               *              because kernel allocate memory at least one page
               *            # 2 - include buffer's head/tail overlap entry
               *              @max_reqs usually is @nr_events required by user
               * @head:     head of the requests,default is 0(initial no events)
               * @tail:     tail of the requests,default is 0(initial no events)
               * @magic:    AIO RING MAGIC
               * @compat_features:
               *            AIO RING COMPAT FEATURES
               * @incompat_features:
               *            AIO RING INCOMPAT FEATURES
               * @header_length:
               *            sizeof(struct aio_ring)
               * @io_events:
               *            array of io_event instances,"io_events[0]" used as placeholder,
               *            the real array is allocated in aio_setup_ring()
               * # this structure is initialized in routine aio_setup_ring(),
               *   and the instance is stored in the first page of aio_ring_info.ring_pages           
               */
              struct aio_ring {
                      unsigned id;
                      unsigned nr;
                      unsigned head;
                      unsigned tail;

                      unsigned magic;
                      unsigned compat_features;
                      unsigned incompat_features;
                      unsigned header_length;

                      struct io_event io_events[0];
              };

              #define AIO_RING_PAGES 8

              /**
               * aio_ring_info - memory buffer in the address space of the User Mode process,and is also accessible
               *                 by all processes in Kernel Mode
               * @mmap_base:     User Mode starting address,get through do_mmap()
               *                                                        # PROT_READ | PROT_WRITE,MAP_ANONYMOUS | MAP_PRIVATE
               * @mmap_size:     length of the AIO ring
               * @ring_pages:    array of page descriptors used when the @internal_pages is not sufficient for request
               * @ring_lock:     protection
               * @nr_pages:      number of pages in this AIO ring
               *                 # this field is set in aio_setup_ring() by invoke get_user_pages(),
               *                   because do_mmap() will invoke mmap_region(),new VMAs that cover the memory buffer
               *                   are created,get_user_pages() then attempt get page for them
               *                   page descriptors are stored in @ring_pages
               * @nr:            trust copy
               * @tail:          tail of the ring
               * @internal_pages:
               *                 pre-defined buffer,maximum size is 8 * PAGE_SIZE bytes
               */
              struct aio_ring_info {
                      unsigned long mmap_base;
                      unsigned long mmap_size;
    
                      struct page **ring_pages;
                      spinlock_t ring_lock;
                      long nr_pages;

                      unsigned nr, tail;

                      struct page *internal_pages[AIO_RING_PAGES];
              };

              !! about @nr in struct aio_ring_info and in struct aio_ring -
                   the memory space have to be allocated in aio_setup_ring() is
                     nr_events += 2
                     size = sizeof(struct aio_ring)
                     size += sizeof(struct io_event) * nr_events
                     nr_pages = (size + (PAGE_SIZE - 1)) >> PAGE_SHIFT /* ceiling */
                     nr_events = (PAGE_SIZE * nr_pages - sizeof(struct aio_ring)) / sizeof(struct io_event)

                     aio_ring_info.nr := nr_events
                     aio_ring.nr := aio_ring_info.nr

                   +--> aio_ring_info.ring_pages
                   |
                   |          +--> aio_ring.io_events[]                          +--> compensate buffer's head/tail overlap entry
                   |          |                                                  |
                   |          |                                                 |<---- additional ---->|
                   +----------+----------+----------+----------+-----+----------+----------+-----------+- - - - - -+
                   |          |          |          |          |     |          |          |           |           |
                   | aio_ring | io_event | io_event | io_event | ... | io_event | io_event | io_event  |   unused  |
                   |          |          |          |          |     |          |          |           |           |
                   +----------+----------+----------+----------+-----+----------+----------+-----------+- - - - - -+
                   |          |<------------ nr_events user required ---------->|                                  |
                   |                                                                                               |
                   |<------------------------------------- aio_ring_info.mmap_size ------------------------------->|
                   |                                       (PAGE_SIZE * nr_pages)
                   +--> starting linear address : aio_ring_info.mmap_base

                   suppose @nr_events is 8
                   suppose unsigned int is 8B
                   suppose PAGE_SIZE is 4096
                   sizeof(io_event) is 8 + 8 + 8 + 8 => 32B
                   sizeof(aio_ring) is 8 + 8 + 8 + 8 + 8 + 8 + 8 + 8 => 64B
                   @nr_events += 2 => 10
                   size of memory space is 10 * 64 + 32 => 672B
                   ceiling : (672 + 4095) >> PAGE_SHIFT => 4767 / 4096 = 1 page
                   @nr_events = (1 * 4096 - 64) / 32 => 126 (aio_ring.nr = aio_ring_info.nr = @nr_events)
                   but we only use 8 io_event instances in the memory space
                   except the additional two io_event instances,there is remaining 126 - (8 + 2) = 116 unused instances
                   /* unused instances -> holes,aio use their positions,but not themselves */

              /**
               * kioctx - Kernel Mode asynchrnous I/O context corresponing to a User Mode AIO context
               * @users:  number of users of this kioctx,get_ioctx() will increase this member
               * @dead:   is this kioctx dead?this member is set to 1 when kioctx is destroyed
               * @user_id:
               *          user identifier,usually is @ring_info.mmap_base
               * @list:   connector,used to link this kioctx to memory descriptor
               * @wait:   wait queue,used to wait events completion on this kioctx
               * @ctx_lock:
               *          protection
               * @reqs_active:
               *          number of active requests
               * @active_reqs:
               *          list collects active requests,and eacn request is a kiocb instance,
               *          linked by use @ki_list
               * @run_list:
               *          list collects requests in run queue,linked by use @ki_run_list
               *                                /* in progress */
               * @max_reqs:
               *          number of events that required by user specified in io_setup()
               * @ring_info:
               *          AIO ring info
               * @wq:     kioctx delayed work,the handler is aio_kick_handler()
               * @rcu_head:
               *          RCU mechanism,used to put kioctx(free it)
               * # slab cache : @kioctx_cachep
               */
              struct kioctx {
                      atomic_t users;
                      int dead;
                      struct mm_struct *mm;
                      
                      /* needs improving */
                      unsigned long user_id;
                      struct hlist_node list;

                      wait_queue_head_t wait;
                
                      spinlock_t ctx_lock;
                    
                      int reqs_active;
                      struct list_head active_reqs;
                      struct list_head run_list;
                    
                      /* sys_io_setup() currently limits this to an unsigned int */
                      unsigned max_reqs;

                      struct aio_ring_info ring_info;

                      struct delayed_work wq;

                      struct rcu_head rcu_head;
              };

          The Asynchronous I/O Infrastructures :
            Create and Destroy :
              <fs/aio.c>
                /**
                 * sys_io_setup - setup a new AIO context
                 * @nr_events:    the number of aio events
                 * @ctxp:         User Mode pointer used to store context handle
                 *                # because it is User Mode pointer,kernel read/write
                 *                  it must use User Mode access routines such get_user()/put_user()
                 * return:        0 OR error code
                 * # this routine first get *@ctxp,it must be initialized to 0,otherwise,return -EINVAL
                 *   if @nr_events is 0,also return -EINVAL
                 *   call to ioctx_alloc() with @nr_events,this function create a new aio context
                 *   succeed,then put the handle into @ctxp and return 0,put user failed,get_ioctx()
                 *   on the recently allocated kioctx and io_destroy() it,return error code to caller,
                 *   failed on allocate new kioctx,just return error to caller as well
                 */
                SYSCALL_DEFINE2(io_setup, unsigned, nr_events, aio_context_t __user *, ctxp)
                => asmlinkage long sys_io_setup(unsigned nr_events, aio_context_t __user *ctxp);

                /**
                 * sys_io_destroy - destroy a specified aio context
                 * @ctx:            aio context handle
                 * return:          0 OR error code
                 * # first,this function must to lookup the kioctx which is associated with @ctx,
                 *   so it invoke lookup_ioctx(),if found,io_destroy() it and return 0
                 *   no such kioctx,print debug info and return -EINVAL
                 * # name of this function is sys_io_destroy(),it is not io_destroy()
                 */
                SYSCALL_DEFINE1(io_destroy, aio_context_t, ctx)
                => asmlinkage long sys_io_destroy(aio_context_t ctx);

                /**
                 * ioctx_alloc - attempt to allocate a new kioctx object and initialize it
                 * @nr_events:   number of aio events that user required
                 * return:       new kioctx's pointer OR bad pointer -- ERR_PTR
                 * # @nr_events greater than 0x10000000U / sizeof(struct io_event) OR @nr_events
                 *   greater than 0x10000000U / sizeof(struct kiocb) => too high,return -EINVAL,
                 *   and @nr_events can not exceeds @aio_max_nr,in this case,return -EAGAIN
                 * # this routine allocate new kioctx from slab cache @kioctx_cachep with GFP_KERNEL flag,
                 *   return -ENOMEM if failed
                 * # new kioctx initialization :
                 *     .max_reqs = @nr_events
                 *     .mm = @current->mm,increase @current->mm->mm_count(because this kioctx is referring it)
                 *     .users = 1
                 *     spinlocks(.ctx_lock and .ring_info.ring_lock) are initialized through spin_lock_init()
                 *     .wait initialized through init_wait_queue_head()
                 *     lists are initialized through INIT_LIST_HEAD()
                 *     .wq initialized through INIT_DELAYED_WORK(),handler is aio_kick_handler()
                 *     the rest is setup in aio_setup_ring()
                 * # after we have setup the kioctx,we have to check systemwide limits
                 *   if @aio_nr + @nr_events > @aio_max_nr(if overflow happened,the result will less than @aio_nr),
                 *   then we have to wait a while to see if someone destroyed some aio requests.so,invoke synchronize_rcu()
                 *   and try again,if it is still exceeded limits,we must give up to it,clean up the newly allocated
                 *   kioctx object through __put_ioctx() and return -EAGAIN
                 *   if everything is OK,link this kioctx into memory descriptor's @ioctx_list,debug printk related info,
                 *   return address of this kioctx object to caller                            (dprintk => prink,if DEBUG > 1
                 *                                                                              otherwise,nop)
                 * # in the case,we succeed to allocate new kioctx,but aio_setup_ring() failed,we have to decrease @mm_count
                 *   of @current's memory descriptor,free the kioctx object,debug printk info and return -ENOMEM to caller 
                 */
                static struct kioctx *ioctx_alloc(unsigned nr_events);

                /**
                 * aio_setup_ring - setup AIO ring buffer for a kioctx object
                 * @ctx:            pointer to kioctx
                 * return:          0 OR error code
                 */
                static int aio_setup_ring(struct kioctx *ctx);

                brief description for aio_setup_ring() :
                  about the calculation for number of aio events and the number of pages have to
                  be allocated been described as above.but,if the number of pages is less than
                  AIO_RING_PAGES,then make use of the internal pages @internal_pages of aio_ring_info
                  object.
                  this function setup the rest of @ring_info of @ctx :
                    .mmap_size is @nr_pages * PAGE_SIZE

                    .mmap_base is got by invoke do_mmap(),size is @mmap_size,protection is PROT_READ | PROT_WRITE,
                    type is MAP_ANONYMOUS | MAP_PRIVATE (@file,@address and @offset are zeros)

                    .nr_pages is set to the return value from get_user_pages(),the page descriptors are stored
                    in @ring_pages member of @ring_info,starting linear address is @mmap_base,@nr_pages represents
                    the number of pages have to pinning,@write of the routine is 1,@force is 0
                                                        /* get_user_pages() returns the number of pages have pinned,
                                                         * which maybe not reached our required number
                                                         */
                    .nr is set to @nr_events as above

                  this function must acquire @ctx->mm->mmap_sem for write before call to do_mmap(),and release
                  it after get_user_pages() returned.
                  set @ctx->user_id to @mmap_base of the aio_ring_info object.
                  next,it setup the ring header,it is stored in the head of buffer :
                    .nr is @nr_events
                    .id is @ctx->user_id
                    .head is 0
                    .tail is 0
                    .magic is AIO_RING_MAGIC
                    .compat_features is AIO_RING_COMPAT_FEATURES
                    .incompat_features is AIO_RING_INCOMPAT_FEATURES
                    .header_length is sizeof(struct aio_ring)
                    /* for setup header,we have to kmap_atomic() the first page @ring_pages[0] of that aio_ring_info
                     * object with type KM_USER0,kunmap_atomic() it later before we return to caller.
                     * handle_mm_fault() have setup Page Table Entry,but that is for User Mode,for access the page
                     * in Kernel Mode,we have to kmap it.
                     */

                /**
                 * lookup_ioctx - lookup a kioctx object in current taks's memory descriptor
                 * @ctx_id:       user id of that kioctx object
                 * return:        address of that kioctx OR NULL
                 * # the matching requires @user_id is equal to @ctx_id AND which kioctx can not be dead(@dead cant be 1),
                 *   if found,get_ioctx() on it and return it
                 * # for traverse @ioctx_list(hash list) of current task's memory descriptor,must call to rcu_read_lock()
                 *   to avoid incoming updating,and release the lock later before we return to caller
                 */
                static struct kioctx *lookup_ioctx(unsigned long ctx_id);

            Submitting the Asynchrnous I/O Operations :
              <fs/aio.c>
                /* batch hash entry */
                struct aio_batch_entry {
                        struct hlist_node list; /* connector */
                        struct address_space *mapping; /* file's mapping */
                };

                mempool_t *abe_pool; /* used to allocate struct aio_batch_entry object */

                /**
                 * sys_io_submit - system call issued by User Mode process to submit AIO operations
                 * @ctx_id:        aio context
                 * @nr:            number of struct *iocb objects in @iocbpp
                 * @iocbpp:        array of pointers point to struct iocb objects
                 * return:         0 OR error code
                 * # this routine is a wrapper for do_io_submit(),@compat of do_io_submit() is 0
                 */
                SYSCALL_DEFINE3(io_submit, aio_context_t, ctx_id, long, nr, struct iocb __user * __user *, iocbpp)
                => asmlinkage long sys_io_submit(aio_context_t ctx_id, long nr, struct iocb __user * __user *iocbpp);

                /**
                 * do_io_submit - do aio submitting
                 * @ctx_id:       the target AIO context
                 * @nr:           number of iocb pointers
                 * @iocbpp:       pointer array of struct iocb *
                 * return:        0 OR error code
                 */
                long do_io_submit(aio_context_t ctx_id, long nr, struct iocb __user * __user *iocbpp, bool compat);

                brief description for do_io_submit() :
                  @nr cant be 0,if it is 0,return -EINVAL to caller.
                  all pointers in @iocbpp must be VERIFY_READ is succeed,otherwise,return -EFAULT to caller.
                  if @ctx_id is not found out in @current's memory descriptor,return -EINVAL to caller.
                  for each struct iocb pointer objects in @iocbpp,invoke __get_user() and copy_from_user() to
                  copy the struct iocb object from User Mode into kernel buffer.
                  /* __get_user(): copy pointer. copy_from_user(): copy data from User Mode that pointed by pointer */
                  invoke io_submit_cone() with arguments :
                    @ctx, local variable user_iocb, local variable tmp's address, local variable batch_hash, @compat
                    /* @user_iocb  => __user pointer points to current iocb objcet(used by __get_user())
                     * @tmp        => Kernel Mode buffer used to store current iocb object(used by copy_from_user())
                     * @batch_hash => struct hlist_head array,size is AIO_BATCH_HASH_SIZE
                     */
                  if __get_user() failed or copy_from_user() failed,set error code to -EFAULT and break iteration.
                  if io_submit_one() returned an error code,use the return value set error code,and break iteration.

                  normal return path :
                  
                  call to aio_batch_free() on @batch_hash.for hash list in the array(the hash list linked some
                  struct aio_batch_entry pointer objects),traverse the hash list.
                  for each aio_batch_entry object,call to blk_run_address_space() on @abe(temporary variable)->mapping,
                  iput() the host of this mapping,delete current entry from hash list,invoke mempool_free() to recycle
                  current entry.

                  invoke put_ioctx() on @ctx.
                  return number of dispatched tasks or error code.

                /**
                 * io_submit_one - submit one AIO task
                 * @ctx:           target kioctx object
                 * @user_iocb:     User Mode pointer points to an object is type of struct iocb
                 * @iocb:          Kernel Mode pointer points to kernel buffer where stroes a struct iocb object
                 * @batch_hash:    array of hash lists
                 * @compat:        compat feature
                 * return:         0 OR error code
                 */
                static int io_submit_one(struct kioctx *ctx, struct iocb __user *user_iocb, struct iocb *iocb,
                                         struct hlist_head *batch_hash, bool compat);

                what io_submit_one() does :
                  1> check @iocb->aio_reserved1 and @iocb->aio_reserved2,if either is TRUE,return -EINAL and
                     print debug message. /* enforce forwards compatibility on users */
                     check overflows,if overflow happened,return -EINVAL to caller.
                     /* @iocb->aio_buf != (unsigned long)@iocb->aio_buf
                      * OR
                      * @iocb->aio_nbytes != (size_t)iocb->aio_nbytes
                      * OR
                      * (ssize_t)@iocb->aio_nbytes < 0)
                      * TRUE => overflow happened
                      */
                  2> get the target file corresponds to file descriptor in @iocb through fget(),if no file,return
                     -EBADF.
                  3> invoke aio_get_req(),this function will allocate a new kiocb object from the slab cache @kiocb_cachep.
                     almost all fields are set to zero,but @req->ki_users is set to 2.
                     next,it checks the aio ring header of @ctx,if @ctx->reqs_active is less than
                     aio_ring_avail(&@ctx->ring_info,@ring(pointer points to the ring header)),link @req->ki_list into
                     @ctx->active_reqs,increase @ctx->reqs_active,return the @req.
                     if no more free reqs,then free @req,and return NULL.
                     /**
                      * aio_ring_avail(@info, @ring) is defined in <linux/aio.h>
                      * it expanded to "(@ring->head + @info->nr - 1 - @ring->tail) % @info->nr"
                      */
                     ! if no free req is available,this function will call to aio_fput_routine() with NULL,attempts to
                       free some reqs on list @fput_head,then try again get req.
                                              (defined in <fs/aio.c>,in function __aio_put_req(),if the req's file's
                                               refcount is not 0 after decreased,it will add the req into @fput_head list,
                                               rather than really put the req)
                                               # after added,call queue_work(),enqueue @fput_work in work queue @aio_wq.
                  4> if we failed to get req,then fput() the file,and return -EAGAIN.
                     otherwise,set @req->ki_filp to the file.
                     if IOCB_FLAG_RESFD is set in @iocb->aio_flags,then
                             invoke eventfd_ctx_fdget() on @iocb->aio_resfd,store the eventfd in @req->ki_eventfd.
                             if @ki_eventfd is ERR PTR,then set return value to the ERR PTR,goto "out_put_req" label.
                  5> invoke put_user(),send @req->ki_key into User Mode object @user_iocb->aio_key.if we put user failed,
                     dprintk() error message,goto label "out_put_req",in this case,error code is the result of put_user().
                  6> setup @req {
                             .ki_obj.user is @user_iocb
                             .ki_user_data is @iocb->aio_data
                             .ki_pos is @iocb->aio_offset
                             .ki_buf is @iocb->aio_buf,converted to char _user * type
                             .ki_nbytes is @iocb->aio_nbytes                   
                             .ki_left is @iocb->aio_nbytes
                             .ki_opcode is @iocb->aio_lio_opcode
                     }
                     invoke aio_setup_iocb() for this @req with @compat.
                     this routine peforms the initial checks,and aio retry method setup for the kiocb(the @req).
                     the behavior is depends on current aio cmd.
                             IOCB_CMD_PREAD 
                             IOCB_CMD_PWRITE
                               -> check file mode,check file permisson,test access_ok() for buffer,
                                  set up single vector for @req,set @ki_retry to aio_rw_vect_retry()
                                  (through aio_setup_single_vector(),make use the inline vector of kiocb)
                             IOCB_CMD_PREADV => "scatter input"
                             IOCB_CMD_PWRITEV => "gather output"
                               -> check file mode,check file permisson,test access_ok() for buffer,
                                  set up several vectors for @req,set @ki_retry to aio_rw_vect_retry()
                                  (through aio_setup_vectored_rw(),if the inline vector of kiocb is not large
                                   enough,kmalloc() io vectors,make use the allocated io vectors;otherwise,
                                   make use of the inline io vector of kiocb)
                             IOCB_CMD_FDSYNC
                             IOCB_CMD_FSYNC
                               -> only set @ki_retry to aio_fdsync() and aio_fsync(),respectively;but if the method
                                  is not implemented,return -EINVAL.
                             !! aio_rw_vect_retry() will call to file's @aio_read() method or @aio_write() method,
                                depends on the real operation.
                                aio_fdsync() and aio_fsync() both are call to the file's @aio_fsync() method,but
                                aio_fdsync() pass 1 as the second parameter,aio_fsync() pass 0 as the second parameter.
                  7> if aio_setup_iocb() failed,goto "out_put_req",in this case,the error code is the return value of
                     this calling.
                     otherwise,acquire @ctx->ctx_lock,invoke aio_run_iocb() on this @req.
                     next,check,if @ctx->run_list is not empty,drain the run list of @ctx,that is invoke __aio_run_iocbs()
                     on @ctx in a while-cycle,stop condition is __aio_run_iocbs() returned FALSE.
                                                                /* process all pending retries queued on the ioctx run list.
                                                                 * for each object on run list,retrieve it,unlink it,
                                                                 * pin it,and invoke aio_run_iocb() for it,invoke __aio_put_req()
                                                                 * for it,get into next iteration.
                                                                 * the traversing operates on a copying of @ctx's @run_list,
                                                                 * after traversing accomplished,if @ctx's @run_list have some
                                                                 * new pending retries,return 1 to caller,otherwise,return 0 to caller
                                                                 */
                  8> unlock @ctx_lock.
                     if current aio cmd is R/W operation,invoke aio_batch_add(),add the file's @f_mapping into the hash 
                     list,it is stored in @batch_hash,index is calculated through hash_ptr(@f_mapping, AIO_BATCH_HASH_BITS).
                     if the file been existed in batch hash,return with do nothing.
                     /* for add the @f_mapping into batch hash,this function have to allocate new aio_batch_entry object
                      * from memory pool @abe_pool,gfp flag is GFP_KERNEL.
                      */
                  9> aio_put_req() on @req to drop extra ref to this @req,return 0 to caller.

                  "out_put_req":
                     invoke aio_put_req() twice on @req(because its default refcount is 2),and return error code to caller.

                /**
                 * aio_rw_vect_retry - AIO Read/Write method operates on vector(s)
                 * @iocb:              kiocb object
                 * return:             number of bytes that transferred OR error code
                 */
                static ssize_t aio_rw_vect_retry(struct kiocb *iocb);

                brief description for aio_rw_vect_retry() :
                  first,check @ki_opcode of @iocb.for Read operation,we make use of the file's @aio_read() method;for
                  Write operation,we make use of the file's @aio_write() method.
                  record current opcode in a local variable.
                  check @ki_pos,it cant less than 0,otherwise,return -EINVAL.
                  process the operation in a do-while cycle,the stop condition is
                    nothing transferred
                    OR
                    @ki_left less than or equal to 0 /* no left data */
                    OR
                    current operations is not Write AND (current inode is a FIFO OR it is a Socket)
                  we invoke the real method in the cycle,the buffer's address  is &@iocb->ki_iovec[@iocb->ki_cur_seg],
                  the number of segments is "@iocb->ki_nr_segs - @iocb->ki_cur_seg".
                  if we Read/Write some thing(return value is not 0 AND it is not a error code),invoke aio_advance_iovec(),
                  this routine will updates fields of @iocb according to the length of data transferred.
                  after do-while cycle end,prepare return value -
                    if we have transferred all that we could,the return value is the @ki_nbytes of @iocb
                    if we write partial data,return the length of data we have written rather than error code
                  return the value to caller.

                /**
                 * aio_run_iocb - the core AIO execution routine,it is invoked both for initial I/O submission and
                 *                subsequent retries via the aio_kick_handler()
                 * @iocb:         the AIO task
                 * return:        0 OR error code
                 * # this routine is called with @ctx_lock is held,the lock is released and reacquired as needed
                 *   during processing
                 * # the retry method needs to be non-blocking as far as possible to avoid holding up other iocbs
                 *   waiting to be serviced by the retry kernel thread
                 * # only one retry instance is in progress for a given iocb at any time,providing that guarantee
                 *   simplifies the coding of individual aio operations as it avoids various potential races
                 */
                static ssize_t aio_run_iocb(struct kiocb *iocb);

                what aio_run_iocb() does :
                  1> retrieve @iocb->ki_retry,if it is NULL,return 0 and print kernel message to report this.
                  2> invoke kiocbClearKicked() of @iocb,this will avoid start the next retry iteration(serialization).
                     because wait queue functions can trigger a kick iocb from interrupt context in the meantime,
                     indicating that data is available for the next iteration.by clear kick flag,can make the
                     kick code "think" that the iocb is still on the run list until we are actually done this one.
                     when we are done with this iteration,we check the iocb whether it was kicked in the
                     meantime,and if so,queue it up afresh.
                  3> reset next and prev of @ki_run_list of @iocb to NULL,it will no longer on run list.
                     relese @ctx_lock.                                    /* but "kick" code "think" it still on run list */
                  4> check kiocbISCancelled() for @iocb,if it is cancelled,we would not invoke retry method for it.
                     instead,we set return value to -EINTR,call to aio_complete() with arguments @iocb, -EINTR, 0.
                     and goto "out" label.
                  5> @iocb is not cancelled,so,we invoke retry method for it.
                     if the return value neither is not -EIOCBRETRY,nor is not -EIOCBQUEUED,invoke aio_complete()
                     on it,the second parameter is the return value of retry method.
                  "out":
                  6> acquire @ctx_lock again.
                     if retry returned -EIOCBRETRY,then
                             INIT_LIST_HEAD() on @ki_run_list of this @iocb.
                             check if this @iocb is kicked via kiocbIsKicked(),then
                                     /* we must queue the next iteration ourselves,if it has already been kicked,
                                      * triggered by wait queue functions from interrupt context.
                                      */
                                     call to __queue_kicked_iocb() for it.
                                             /**
                                              * static inline int __queue_kicked_iocb(struct kiocb *iocb);
                                              * - this routine have to be called when @ctx_lock is held.
                                              *   if @iocb's @ki_run_list is empty,then add this @iocb to
                                              *   tail of its kioctx's run_list,and return 1;otherwise,return 0.
                                              */
                                     invoke aio_queue_work() for the kioctx that retrieved from @iocb->ki_ctx.
                                            /**
                                             * static void aio_queue_work(struct kioctx *ctx);
                                             * - this routine process a SMP memory barrier,then configure time out
                                             *   value.if @ctx's @wait the wait queue is active(not empty),timeout is 1;
                                             *   otherwise,timeout is "HZ / 10".
                                             *   finally,invoke queue_delayed_work() to queue @ctx->wq on work queue
                                             *   @aio_wq with the configured timeout.
                                             */
                  7> return the result to caller.

                  /**
                   * kiocbClearKicked(iocb) is defined in <linux/aio.h>
                   * -  clear bit KIF_KICKED in @iocb->ki_flags.
                   * kiocbIsKicked(iocb) is defined in <linux/aio.h>
                   * - test KIF_KICKED in @iocb->ki_flags.
                   * kiocbIsCancelled(iocb) is defined in <linux/aio.h>
                   * - test KIF_CANCELLED in @iocb->ki_flags.
                   */

                /**
                 * aio_complete - routine called when the I/O request on the given iocb is complete
                 * @iocb:         the completed iocb
                 * @res:          @res of io event
                 * @res2:         @res2 of io event
                 * return:        TRUE if this is the last user of the request
                 * # EXPORT_SYMBOL
                 */
                int aio_complete(struct kiocb *iocb, long res, long res2);

                description for aio_complete() :
                  if @iocb is a sync kiocb(@ki_key == KIOCB_SYNC_KEY),then
                          do BUG checking,it is a BUG if @iocb->ki_users is not 1.
                          set @ki_user_data of @iocb to @res.
                          set @ki_users of @iocb to 0.
                          wake up processes waiting on this @iocb(@iocb->ki_obj.tsk).
                          return 1 to caller.
                  otherwise,we have to get ring info of the kioctx,because we have to update
                  io event related with this @iocb.
                  acquire @ctx_lock,disable local interrupt.
                  if @iocb is not the last kiocb of run list,AND its @ki_run_list is not empty,
                  we unlink it from run list.
                  next,check,if @iocb is cancelled,we goto label "put_rq".

                  /* update io event */
                  kmap_atomic(),kmap ring header.invoke aio_ring_event() on the ring info with
                  @tail field of the ring info,the third parameter is km_type value KM_IRQ0.
                  this routine calculate the position of io event,kmap it,and return the linear
                  address.
                  increase local variable @tail,its previous value is the ring info's @tail field.
                  if @tail is greater than or equal to ring info's @nr,reset @tail to 0.
                  update the io event {
                          obj := @iocb->ki_obj.user(converted to u64 unsigned long)
                          data := @iocb->ki_user_data
                          res := @res
                          res2 := @res2
                  }
                  process a SMP memory barrier,set ring info's @tail to the local @tail;set ring header's
                  @tail to the local @tail.
                  invoke put_aio_ring_event() with km_type KM_IRQ0 on the io event,this routine do the
                  reverse to aio_ring_event(),kunmap_atomic() unmap the ring header.

                  if @iocb->ki_eventfd is not NULL => user specified the destination to deliver the result,
                  invoke eventfd_signal() on the fd with second parameter 1.(this routine is safe to be called
                                                                             in interrupt context)

                  "put_rq": /* normal return path */
                    invoke __aio_put_req() on this @iocb.
                    process SMP memory barrier.
                    if the kioctx's @wait is active => it is not empty,then wake up the processes on the waitqueue.
                    release @ctx_lock,restore local interrupt.
                    return 0(not the last user) or 1(the last user) to caller.

                /**
                 * aio_kick_handler - the handler for AIO delayed work,triggered to process pending retries on
                 *                    an kioctx
                 * @work:             the work
                 * # the main work is invoke __aio_run_iocbs() on the kioctx this @work belongs to,
                 *   for call to the function,have to use USER_DS,use the kioctx's @mm to process drain iocbs,
                 *   acquire @ctx_lock and disable local interrupt;these will be UNDO after
                 *   __aio_run_iocbs() returned.if __aio_run_iocbs() returned TRUE,then must call to
                 *   queue_delayed_work() requeue @wq of the kioctx with timeout 0 on @aio_wq
                 * # this routine invoke use_mm() to switch @current's memory to the kioctx's @mm.function use_mm()
                 *   is defined in <mm/mmu_context.c>,it reset @current's @mm to the kioctx's @mm,then invoke
                 *   switch_mm() to operates @current's @active_mm and @current's @mm.function switch_mm() usually
                 *   is a architecture-dependent function.if @active_mm is different from current memory,then reload
                 *   cr3 register from current memory,and reload LDT if LDTs also different.
                 *   function unuse_mm() also defined in <mm/mmu_context.c>,it reverse the effect of use_mm()
                 */
                static void aio_kick_handler(struct work_struct *work);

            !! THE KERNEL-LEVEL AIO FINALLY RETURNS WHEN I/O REQUEST HAVE BEEN EQUEUED INTO DRIVER'S REQUEST QUEUE.
               FOR ASYNCHRONOUS I/O,PROCESS HAVE TO WAIT FOR DATA TRANSFER ACCOMPLISHED.
               /* FOR BUFFERED I/O,DATA IS COPIED INTO PAGE CACHE,DO NOT WASTE TIME TO WAIT WRITEBACK. */


/* END OF CHAPTER16 */


Chapter 17 : Page Frame Reclaiming
    The Page Frame Reclaiming Algorithm :
      when the system load is low,the RAM is filled mostly by the disk caches and few running processes can benefit
      from the information stored in them.
      when the system load increases,the RAM is filled mostly by pages of the processes and the caches are shrunken
      to make room for additional processes.

      the page frame reclaiming algorithm of the Linux kernel refills the lists of free blocks of the buddy system
      by "stealing" page frames from both User Mode processes and kernel caches.and actually,page frame reclaiming
      must be performed before all the free memory has been used up;essentially,to free a page frame the kernel
      must write its data to disk.(need buffer head data structures)

      one of the goals of page frame reclaiming is conserve a minimal pool of free page frames so that the kernel
      may safely recover from "low on memory" conditions.

      Selecting a Target Page :
        the objective of the page frame reclaiming algorithm(PFRA) is to pick up page frames and make them free.
        these page frames must not been included in one of @free_area arrays of buddy system.

        PFRA page frame type : /* distinguish according to their contents */
          unreclaimable pages -> no reclaiming allowed or needed
          - free pages(included in buddy system lists)
            reserved pages(PG_reserved set)
            pages dynamically allocated by the kernel
            pages in the Kernel Mode stacks of the processes
            temporarily locked pages(PG_locked set)
            memory locked pages(VM_LOCKED set)

          swappable pages -> save the page contents in a swap area
          - anonymous pages in User Mode address spaces(e.g. pages in heap or stack)
            mapped pages of "tmpfs" filesystem(e.g. IPC shared memory)
            /* usually,the pages of special filesystem are not reclaimable */

            /**
             * a shared page frame belongs to multiple User Mode address spaces,while a
             * non-shared page frame belongs to just one.
             * a non-shared page frame might belong to several lightweight processes referring
             * to the same memory descriptor.
             * shared page frames are typically created when a process spawns a child,or when
             * two or more processes access the same file by means of a shared memory mapping.
             *
             * a single process accesses to a file through a shared memory mapping,the corresponding
             * pages are non-shared as far as the PFRA is concerned;a page belonging to a private
             * memory mapping may be treated as shared by the PFRA if none of them modified the data
             * in the page(ReadOnly)
             */

          syncable pages -> synchronize the page with its image to disk,if necessary
          - mapped pages in User Mode address spaces
            pages included in the page cache and containing data of disk files
            block device buffer pages
            pages of some disk caches(e.g. inode cache)

          discardable pages -> nothing to be done
          - unused pages included in memory caches(e.g. slab allocator caches)
            unused pages of the dentry cache

        Design of the PFRA :
          in roughly speadking,any page belonging to a disk or memory cache,or to the User Mode address space
          of a process,selecting the proper target pages is perhaps the most sensitive issue in kernel design.
          the hardest job of a developer working on the virtual memory subsystem consists of finding an algorithm
          that ensures acceptable performance both for desktop machines and for high-level machines such as
          large database servers.
          but,finding a good page frame reclaiming algorithm is a rather empirical job,with very little support
          from theory.
          /* thus,kernel code about PFRA might changed very quickly */

          the general rules adopted by the PFRA :
            1> Free the "harmless" page first
                 pages included in disk and memory caches not referenced by any process should be reclaimed before
                 pages belonging to the User Mode address spaces of the processes;in the case that pages included
                 in disk caches,the page frame reclaiming can be done without modifying any Page Table Entry.

            2> Make all pages of a User Mode process reclaimable
                 with the exception of locked pages,the PFRA must be able to steal any page of a User Mode process,
                 including the anonymous pages.in this way,processes that have been sleeping for a long period of time
                 will progressively lose all their page frames.

            3> Reclaim a shared page frame by unmapping at once all page table entries that reference it
                 when the PFRA wants to free a page frame shared by several processes,it clears all page table
                 entries that refer to the shared page frame,and then reclaims the page frame.

            4> Reclaim "unused" pages only
                 PFRA uses a simplified LRU replacement algorithm to classify pages as in-use and unused.if a page
                 has not been accessed for a long time,the probability that it will be accessed in the near future
                 is low and it can be considered "unused",on the other hand,if a page has been accessed recently,the
                 probability that it will continue to be accessed is high and it must be considered as "in-use".
                 /* the main idea behind the LRU algorithm is to associate a counter storing the age of the page
                  * with each page in RAM - that is,the interval of time elapsed since the last access to the page.
                  * PFRA reclaim only the oldest page of any process.
                  * some platform support such LRU counter,but 80x86 do not support this,thus kernel cant rely on
                  * the counter.
                  * but "Accessed" flag of Page Table Entry is automatically set by the hardware when the page is
                  * accessed,thus kernel can keeps track of the age of every page by combine this with LRU lists.
                  */

            !! careful selection of the order in which caches are examined.
               ordering of pages based on aging.
               distinction of pages based on the page state.

        Reverse Mapping :
          locate quickly all the Page Table Entries that point to the same page frame - reverse mapping.
          a technique named object-based reverse mapping,it is a sophisticated solution used in Linux 2.6.
          essentially,for any reclaimable User Mode page,the kernel stores the backward links to all memory
          regions in the system(the objects) that include the page itself,and each memory region descriptor
          stores a pointer to a memory descriptor,which in turn includes a pointer to a Page Global Directory.

          related data and routines :
            PFRA must have a way to determine whether the page to be reclaimed is shared or non-shared,and
            whether it is mapped or anonymous.

            struct page.@_mapcount => -1 -> no Page Table entry references the page frame
                                       0 -> the page is non-shared
                                       > 0 -> the page is shared
                                       /**
                                        * routine page_mapcount() is defined in <linux/mm.h>,it returns
                                        * page descriptor's @_mapcount + 1
                                        */
            struct page.@mapping   =>  NULL -> the page belongs to the swap cache
                                       not NULL,the least significant bit is 1
                                       -> the page is anonymous,and the @mapping field encodes the pointer
                                          to an anon_vma descriptor
                                       not NULL,the least significant bit is 0
                                       -> the page is mapped,@mapping points to the address_space object
                                          /* Linux aligns address of address_space object to multiple of four,
                                           * thus the least significant bit can be used as a flag denoting
                                           * whether the field contains a pointer to an address_space object
                                           * or to an anon_vma descriptor.
                                           */
                                       /**
                                        * routine PageAnon() is defined in <linux/mm.h>,it returns 1 if
                                        * the least significant bit of @mapping is 1;otherwise,return 0.
                                        */
            
            <mm/rmap.c>
              /**
               * try_to_unmap - try to remove all page table mappings to a page
               * @page:         the page descriptor
               * @flags:        action and flags
               *                # enum ttu_flags = > try to unmap flags,defined in <linux/rmap.h>
               *                  TTU_UNMAP = 0                   ! unmap mode
               *                  TTU_MIGRATION = 1               ! migration mode
               *                  TTU_MUNLOCK = 2                 ! munlock mode
               *                  TTU_ACTION_MASK = 0xff          ! used to extract the modes
               *                  TTU_IGNORE_MLOCK = (1 << 8)     ! ignore mlock
               *                  TTU_IGNORE_ACCESS = (1 << 9)    ! do not age
               *                  TTU_IGNORE_HWPOISON = (1 << 10) ! corrupted page is recoverable
               * return:        SWAP_SUCCESS => succeed in removing all mappings   0
               *                SWAP_AGAIN   => missed a mapping,try again later   1
               *                SWAP_FAIL    => the page is unswappable            2
               *                SWAP_MLOCK   => page is mlocked                    3
               * # this routine if very short,at the first,it checks whether @page is locked,if it
               *   is not PG_locked,that means we have encountered a BUG.
               *   for KSM(Kernel Samepage Merging) page,this routine invoke try_to_unmap_ksm();
               *   for anonymous page,this routine invoke try_to_unmap_anon();
               *   for file mapped page,this routine invoke try_to_unmap_file();
               *   if the result of that called function is not SWAP_MLOCK AND @page is not mapped,
               *   we reset return value to SWAP_SUCCESS,otherwise,return the result to caller
               */
              int try_to_unmap(struct page *page, enum ttu_flags flags);

          Reverse Mapping for Anonymous Pages :
            anonymous pages are often shared among several processes.
            the common case : process fork a child,the child will share the memory with its parent
                              until it has made changing to the memory area.
            the quite unusual case : process create a memory region with MAP_ANONYMOUS and MAP_SHARED
                                     flags,the pages of such a region will be shared among the future
                                     descendants of the process.

            strategy to link together all the anonymous pages that refer to the same page frame :
              anonymous memory regions that include the page frame are collected in a doubly linked
              circular list.
            
            <linux/rmap.h>
              /**
               * anon_vma_chain - structure used to support find the anon_vma(s) associated
               *                  with a VMA,or the VMAs associated with an anon_vma
               * @vma:            VMA that anon_vmas in the chain associated with 
               * @anon_vma:       anon_vma that VMAs associated with
               * @same_vma:       list of anon_vma_chain(s) that linking all the anon_vma(s)
               *                  associated with this VMA
               * @same_anon_vma:  list of anon_vma_chains that link all the VMAs associated
               *                  with @anon_vma
               * # slab cache :   @anon_vma_chain_cachep in <mm/rmap.c>
               * # in function anon_vma_prepare(),this structure is linked into VMA's @anon_vma_chain
               *   list through @same_vma;and it also be linked into anon_vma's @head list through
               *   @same_anon_vma.
               */
              struct anon_vma_chain {
                      struct vm_area_struct *vma;
                      struct anon_vma *anon_vma;
                      struct list_head same_vma;
                      struct list_head same_anon_vma;
              };

               +->anon_vma.head ------+        +----> VMA <-----------------+
               |  ^                   |        |                            |
               |  |    (@anon_vma)    |        |(@vma)                      |
               |  +-------+-+         |        |                            |
               |          | |         V        |                            |
               |          | | (@same_anon_vma) |                            |
               |  ... <-->|anon_vma_chain <--> anon_vma_chain <--> ...      |
               |          ||                   |                            |
               |          ||                   |                            |
               |          ++-------------------+                            |
               |(@anon_vma)|                                                |
               |  +--------+                                                |
               |  |(@vma)                                                   |
               |  | +-------------------------------------------------------+ (share the same anon_vma)
               |  | |
               |  V V
               +--VMA.anon_vma_chain ----+        +--> anon_vma (this anon_vma is associated with the same VMA)
                  ^                      |        |
                  |     (@vma)           |        |
                  +---------+-+          |        |(@anon_vma)
                            | |          V        |
                            | |       (@same_vma) |
                    ... <-->|anon_vma_chain <--> anon_vma_chain <--> ...
                            |                    |
                            |                    |
                            |                    |
                            +--------------------+


              /**
               * anon_vma - structure anon_vma is used to collect memory regions that include the
               *            same anonymous page frame
               * @lock:     protection
               * @ksm_refcount:
               *            Kernel Samepage Merging support
               * @head:     doubly linked list used to collects anon_vma_chain objects,connector
               *            is @same_anon_vma member of anon_vma_chain structure
               * # struct vm_area_struct.anon_vma_chain is used to collects anon_vma_chain objects,
               *   connector is @same_vma member of anon_vma_chain structure.
               * # if a page frame is an anonymous page frame,its @mapping stores the pointer to
               *   the anon_vma descriptor.
               * # for each anonymous page frame there must be an anon_vma associated with it.
               */
              struct anon_vma {
                      spinlock_t lock;

              #ifdef CONFIG_KSM
                      atomic_t ksm_refcount;
              #endif

                      struct list_head head;
              };

            ! when a page frame already referenced by process A is inserted into a Page Table entry of
              process B(by fork(), clone(), vfork()),kernel just inserts the VMA of B that include the page
              frame into the anon_vma list in which process A is linked,and let B.@anon_vma := A.@anon_vma .
              struct vm_area_struct.vm_mm stores the memory descriptor,in which PGD is stored in @pgd member.
              the Page Table entry of that anonymous page frame can then be determined by considering the
              starting linear address of the anonymous page,which is easily obtained from the memory region
              descriptor and the @index member of the page descriptor.

            The try_to_unmap_anon() function :
              <mm/rmap.c>
                /**
                 * try_to_unmap_anon - routine is used to try unmap anonymous page
                 * @page:              page descriptor
                 * @flags:             try to unmap flags
                 * return:             same as try_to_unmap(),except SWAP_ACCESS
                 * # this routine usually is called from try_to_unmap().
                 */
                static int try_to_unmap_anon(struct page *page, enum ttu_flags flags);

                brief description for try_to_unmap_anon() :
                  at first,this routine get the anon_vma through page_lock_anon_vma() function from @page.
                  /* get descriptor from @page->mapping,and acquire @lock of the anon_vma.
                   * when the function is in progress, RCU read would be locked,even after it returned a
                   * valid anon_vma descriptor.(return NULL if failed,and without held any lock)
                   */
                  if NULL is returned,this routine return SWAP_AGAIN to caller.
                  next.traverse VMAs that sharing "this" anon_vma {
                          /* retrieve the next element in @anon_vma->head,the list
                           * collecting anon_vma_chain object.
                           * then,traverse each anon_vma_chain objects in the list @same_anon_vma
                           */
                          for current @avc(anon_vma_chain),get its @vma member.
                          call to vma_address(),pass it @page and @vma as the parameters,this function
                          returns the linear address corresponding to @page about to @vma;it return
                          -EFAULT if the address is not fall inside to this VMA.
                          in the case -EFAULT is returned,we just skip this @vma.
                          otherwise,invoke try_to_unmap_one(),pass it @page, @vma, @the-address, @flags
                          as the parameters.
                          break cycle,if result is not SWAP_AGAIN or @page is not mapped.
                                                                     /* @page->_mapcount < 0 */
                  }
                  finally,invoke page_unlock_anon_vma() to release the spin lock and the RCU read lock,
                  return the last recent result of try_to_unmap_one() to caller.
                  /* we skipped all VMAs,the default return value is SWAP_AGAIN */

            The try_to_unmap_one() function :
              <mm/rmap.c>
                /**
                 * try_to_unmap_one - subfunction of try_to_unmap(),used to unmap the exact PTE mapping
                 * @page:             the page
                 * @vma:              the VMA in which @page is resided
                 * @address:          linear address of the page
                 * @flags:            behavior control
                 * return:            same as try_to_unmap(),except SWAP_SUCCESS
                 * # this routine is called repeatedly from either try_to_unmap_anon() or
                 *   try_to_unmap_file()
                 */
                int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
                                     unsigned long address, enum ttu_flags flags);

                what try_to_unmap_one() does :
                  1> get the PTE through page_check_address() routine,this routine use the @page,
                     memory descriptor(@mm) of @vma,@address to retrieve pointer to the PTE,and acquire
                     the spin lock @page_table_lock of memory descriptor.
                     /* if defined USE_SPLIT_PTLOCKS,use the spin lock @ptl of PMD page to instead */
                  2> if NULL PTE,goto label "out".

                     if TTU_IGNORE_MLOCK is not set AND VM_LOCKED is set,goto label "out_mlock".
                     /* require ignore mlock but VMA is locked */
                     if TTU_IGNORE_MLOCK is not set AND TTU_MUNLOCK is set,goto label "out_unmap".
                     /* require ignore mlock but set TTU_MUNLOCK */

                     if TTU_IGNORE_ACCESS is not set,then invoke ptep_clear_flush_young_notify(),pass it
                     @vma,@address,PTE as the parameters.if the result is not 0,set return value to SWAP_FAIL,
                     goto label "out_unmap".                           /* page is considered "in-use" */
                     /**
                      * ptep_clear_flush_young_notify(vma, addr, ptep) is a macro function defined in
                      * <linux/mmu_notifier.h>.
                      * this routine invoke ptep_clear_flush_young(),then bitwise OR the result with the result
                      * returned by mmu_notifier_clear_flush_yong() on @vma->mm,@addr,finally,return the result
                      * of bitwise operation to caller.
                      *
                      * x86-platform ptep_clear_flush_young() function is defined in <arch/x86/mm/pgtable.c>,
                      * it invoke ptep_test_and_clear_young() which is in the same source file.
                      *
                      * about ptep_test_and_clear_young(),if *@ptep is _PAGE_ACCESSED,then it call to routine
                      * test_and_clear_bit() on @ptep->pte,bit is _PAGE_BIT_ACCESSED -> _PAGE_ACCESSED.
                      * if we have clear _PAGE_BIT_ACCESSED,then invoke pte_update() to ask MMU update the PTE
                      * at @ptep,return 1 if we have clear the bit,return 0 if *@ptep is not young.
                      *
                      * after ptep_test_and_clear_young() returned,if the result is 1,ptep_clear_flush_young()
                      * next invoke flush_tbl_page() on @vma,@addr,this will flush TLB cache that associated
                      * with the page in @vma starting at @addr.
                      * finally,ptep_clear_flush_young() return the result of ptep_test_and_clear_young(),it is
                      * 1 => if we have clear _PAGE_BIT_ACCESSED OR 0 => we did nothing.
                      */
                  3> invoke flush_cache_page(),on x86 platform,this function is a NOP,but on ARM platform,
                     it process flush icache,instruction-cache.
                     invoke ptep_clear_flush_notify() with @vma,@address,PTE,it will notify MMU to invalidate
                     the page starting from @address in the @vma,the older value of PTE is returned by this routine,
                     and it is stored in local variable named @pteval.
                     next,check whether the PTE is dirty,if it is the case,set @page dirty.
                     call to update_hiwater_rss() on the memory descriptor,this routine reset @hiwater_rss of
                     @mm(the memory descriptor) to the result of get_mm_rss() on @mm,if hig watermark of rss is
                     less than the result.
                  4> /* bad page */
                     if page is hardware poisoned, AND no TTU_IGNORE_HWPOISON is set in @flags,then
                             /* in linux kernel,Hardware Poison is a feature used to detects and marks
                              * physically failing memory as _unusable_.
                              *                 (RAM or Cache)
                              */
                             @page is anonymous page => decrease MM_ANONPAGES counter of @mm
                                           otherwise => decrease MM_FILEPAGES counter of @mm
                             invoke set_pte_at() to reset PTE to the new entry
                               "swp_entry_to_pte(make_hwpoison_entry(@page))"

                     /* anonymous page */
                     else if @page is anonymous page,then
                             initialize a local variable named @entry is type of swp_entry_t,the type is defined
                             in <linux/swap.h>.it is a structure contains only one member named @val is type of
                             unsigned long.@entry is initialized from @page's private data.
                
                             if @page is swap cache(PG_swapcache is set),then
                                     /* swap page */
                                     if @entry is a duplicated swap entry,restore PTE from @pteval,set return value
                                     to SWAP_FAIL,and goto label "out_unmap". /* it is flushed previously */

                                     if @mmlist of @mm is empty,acquire @mmlist_lock and check again,if it still is
                                     empty,add @mm->@mmlist to the @init_mm's @mmlist.release the lock.

                                     decrease MM_ANONPAGES of @mm,increase MM_SWAPENTS of @mm.

                             else if macro PAGE_MIGRATION is TRUE,then /* the macro is 1 if defined CONFIG_MIGRATION */
                                     /* store the pfn of the page in a special migration pte,do_swap_page() will wait
                                      * until the migration pte is removed and then restart fault handling.
                                      */
                                     do BUG test at first,it is a BUG if TUU_MIGRATION is not set in @flags.
                                     reset @entry to the result of make_migration_entry(),pass it @page, pte_write(@pteval)
                                     as the parameters.            /* makeup a new migration PTE,and enable Write flag */

                             invoke set_pte_at() to use @entry set PTE,of course,have to convert swap entry to Page Table Entry.
                             do BUG test again,if PTE is a file PTE,we can not swap it out.

                     /* file page */
                     else if PAGE_MIGRATION is TRUE AND TTU_MIGRATION is set in @flags,then
                             use a swap_entry_t object @entry,set it to the result of make_migration_entry(@page, pte_write(@pteval)),
                             use @entry to set PTE,also have to convert swap entry to Page Table Entry. /* page migration PTE */
                     
                     /* other case */
                     else
                             just decrease MM_FILEPAGES counter of @mm as well.
                            
                  5> call to page_remove_rmap() on @page.this routine atomic let @page->_mapcount -= 1,if @page is still mapped
                     by someone,routine will return to caller without remove rmap.
                     otherwise,for non-anonymous page OR (swap page AND dirty ),it clear PG_dirty for @page,and set PG_dirty again.
                     next,if @page is anonymous page,it uncharge memory cgroup for @page,decrease NR_ANON_PAGES counter of the
                     memory zone where this @page is come from;for file page,it decrease NR_FILE_MAPPED counter of the memory zone
                     where this file page come from,then update memory cgroup stat "file mapped" about @page to -1.

                     call to page_cache_release() on @page to decrease refcount. /* may trigger free page */

                  "out_unmap":
                  6> invoke pte_unmap_unlock(),it will kunmap_atomic() the PTE,release the spinlock acquired in step 1>.
                            /**
                             * because page_check_address() called pte_offset_map().
                             */
                  "out":
                  7> return the result to caller.default is SWAP_AGAIN,and maybe changed to SWAP_FAIL.

                  "out_mlock":
                  8> invoke pte_unmap_unlock().
                     try to lock @vma->vm_mm->mmap_sem for READ.
                     if succeed to acqurie semaphore,then check @vma->vm_flags,if VM_LOCKED is set,invoke mlock_vma_page()
                     on @page,reset return value to SWAP_MLOCK,release semaphore.                         /* will set PG_locked */
                     return the result(default is SWAP_AGAIN) to caller.
                     
          Reverse Mapping for Mapped Pages :
            reverse mapping for mapped pages based on a simple idea :
              it's always possible to retrieve the Page Table entries that refer to a given page frame by
              accessing the descriptors of the memory regions that include the corresponding mapped pages.

            ! files usually are shared frequently,thus linear scanning is not a good idea for find out VMAs that sharing
              the file page.as instead,use a "priority search tree" to handles the scanning for reduce time cost.
            
            The priority search tree(PST) :
              introduced by Edward McCreight in 1985.PST is a hybrid of a heap and a balanced search tree,it is
              used to perform queries on the set of intervals.
              each interval in a PST corresponds to a node of the tree,and it is characterized by two indices :
                radix index => corresponds to the starting pointer of the interval
                heap index  => corresponds to the final point
              PST essentially is a search tree on the radix tree,with the additional heap-like property that the
              heap index of a node is never smaller than the heap indices of its children.

              PST in Linux,the differences :
                1> it is not always kept balanced.
                2> it is adapted so as to store memory regions
                   instead of linear intervals.
                ! each memory region can be considered as an interval of file pages identified by the
                  initial position in the file(radix inde) and the final position(heap index).
                  VMA tend to start from the same pages,but McCreight tree cant store intervals have the
                  very same starting point,thus,each node of a PST carries an additional size index.
                    size index => corresponding to the size of the memory region in pages minus one
                                  (number of pages this VMA covered - 1)
                  size index allows the search program to distinguish different memory regions that 
                  start at the same file position.

                  disadvantage : increase significantly the number of different nodes that may end up
                                 in a PST.PST could not contain all the nodes has same radix index but
                                 different heap index.
            
                  solution : PST may include "overflow subtrees" rooted at the leaves of the PST and
                             containing nodes having a common radix tree.
                    
                  for the case different processes mapped the same portion of the same file,kernel must
                  insert the memory region descriptor in a doubly linked circular list rooted at the
                  older PST node.  /* the node been existing */    

                  Order : no child node has a heap index greater than the heap index of the parent.
                          radix index of the left child of any node is never greater than the radix
                          index of the right child.
                          in case of tie between the radix indices,the ordering is given by the size
                          index.(children's size index must less than parent's size index if they have
                          the same radix index)

              Index of PST in Linux about to VMA :
                vm_pgoff => member @vm_pgoff
                vm_size_in_pages => number of pages this VMA convered - 1
                start_vm_pgoff => pgoff of file start
                end_vm_pgoff => pgoff of file end


                heap_index = vm_pgoff + vm_size_in_pages : end_vm_pgoff
                radix_index = vm_pgoff : start_vm_pgoff
                size_index = vm_size_in_pages

                ! A regular radix-priority-search-tree indexes vmas using only heap_index and radix_index.
                  Order :
                    cur->heap_index >= cur->left->heap_index AND cur->heap_index >= cur->right->heap_index
                    if cur->heap_index == cur->left->heap_index,then cur->radix_index < cur->left->radix_index
                    if cur->heap_index == cur->right->heap_index,then cur->radix_index < cur->right->radix_index

                    nodes are hashed to left or right subtree using radix_index similar to a pure binary radix tree.

              Extended regular radix-priority-search-tree in Linux :
                a regular radix-priority-search-tree is not suitable for vmas with same radix indices.
                handling for the vmas with the same vm_pgoff :
                  1> all vmas with the same radix _and_ heap indices are linked using VMA.shared.vm_set.list
                  2> if there are many vmas with the same radix index,but different heap indices and if
                     the regular radix-priority-search-tree cannot index them all,build an overflow-sub-tree
                     that indexes such vmas using heap and size indices instead of heap and radix indices.
                     (overflow-tree resides on leave,the above is regular radix-priority-search-tree)

              e.g. (radix index, size index, heap index)

                                           (root)                                   -
                                           0, 5, 5                                   |
                                (left child)             (right child)               |
                                0, 4, 4                  2, 3, 5                     | => regular 
                      (leave)                                                        |
                      0, 3, 3          1, 2, 4       1, 2, 3           2, 0, 2       |
                                                                                    -
                                                                                    -
                    0, 2, 2           1, 1, 3                                        |
                                                                                     | => overflow-sub-tree
                  0, 1, 1            1, 0, 2                                         |
                                                                                    -
                    

                PFRA needs the VMAs mapped file page has index 5(heap index) >
                  root(0, 5, 5) => OK           (retrieve VMA)
                  left child(0, 4, 4) => BAD
                  right child(2, 3, 5) => OK    (retrieve VMA)
                    left child(1, 2, 3) => BAD
                    right child(2, 0, 2) => BAD

              <linux/prio_tree.h> /* general implementations of priority tree is in <lib/prio_tree.c> */
                /**
                 * prio_tree_node - node of PST
                 * @left:           left child
                 * @right:          right child
                 * @parent:         parent of this node
                 * @start:          radix index
                 * @last:           heap index
                 */
                struct prio_tree_node {
                        struct prio_tree_node *left;
                        struct prio_tree_node *right;
                        struct prio_tree_node *parent;
                        unsigned long start;
                        unsigned long last;
                };

                /* raw PST node */
                struct raw_prio_tree_node {
                        struct prio_tree_node *left;
                        struct prio_tree_node *right;
                        struct prio_tree_node *parent;
                };

                /**
                 * prio_tree_root - root of PST
                 * @prio_tree_node: root node
                 * @index_bits:     level of leave,for the
                 *                  initial PST,this member is
                 *                  initialized to 1
                 *                  # current height of PST is "current level + 1"
                 *                  # maximum number of leaves is 2^@index_bits
                 *                  # in source file <lib/prio_tree.c>,an array named
                 *                    @index_bits_to_maxindex is defined,size is
                 *                    BITS_PER_LONG.the array stores [1, 2, 4, 8, 16, ..., ~0UL],
                 *                    by use @index_bits as subscript of @index_bits_to_maxindex,
                 *                    we can quickly get current maxindex of PST.
                 *                      @index_bits_to_maxindex[@index_bits - 1] => 1 - 1 == 0 => 1
                 * @raw:            0 => node is type of struct prio_tree_node
                 *                  1 => node is type of struct raw_prio_tree_node
                 */
                struct prio_tree_root {
                        struct prio_tree_node *prio_tree_node;
                        unsigned short index_bits;
                        unsigned short raw;
                };

                /**
                 * prio_tree_iter - iterator for PST
                 * @cur:            current position
                 * @mask:           mask for index
                 * @value:          current value
                 * @size_level:     current size level
                 * @root:           tree root
                 * @r_index:        target radix index
                 * @h_index:        target heap index
                 * # init routine : prio_tree_iter_init(*iter, *root, r_index, h_index)
                 *   default @cur is NULL
                 * # routine prio_tree_next() is defined in <lib/prio_tree.c>,it acts on
                 *   a given iterator,get the next prio_tree_node that overlaps with the
                 *   input interval in iterator.
                 */
                struct prio_tree_iter {
                        struct prio_tree_node *cur;
                        unsigned long mask;
                        unsigned long value;
                        int size_level;

                        struct prio_tree_root *root;
                        pgoff_t r_index;
                        pgoff_t h_index;
                };

              <linux/mm.h>
                #define vma_prio_tree_foreach(vma, iter, root, begin, end) \
                        for (prio_tree_iter_init(iter, root, begin, end), vma = NULL; \
                             (vma = vma_prio_tree_next(vma, iter)); )

                /**
                 * vma_nonlinear_insert - insert nonlinear file mapping priority tree node
                 * @vma:                  the VMA contains nonlinear file mapping
                 * @list:                 list used to collects nonlinear file mapping
                 * # this routine set @vma->shared.vm_set.parent to NULL,then list_add_tail()
                 *   @vm_set.list into @list
                 */
                static inline void vma_nonlinear_insert(struct vm_area_struct *vma, struct list_head *list);

              <mm/prio_tree.c> /* PST for mapping->i_mmap,special implementations */
                #define RADIX_INDEX(vma) ((vma)->vm_pgoff)
                /* vm_size_in_pages */
                #define VMA_SIZE(vma)    (((vma)->vm_end - (vma)->vm_start) >> PAGE_SHIFT)
                /* avoid overflow */
                #define HEAP_INDEX(vma)  ((vma)->vm_pgoff + (VMA_SIZE(vma) - 1))

                /* Radix priority search tree for address_space->i_mmap */
                                prio_tree_root
                                      |
                                      A       vm_set.head
                                     / \      /
                                    L   R -> H-I-J-K-M-N-O-P-Q-S /* same radix_index and heap_index */
                                    ^   ^    <-- vm_set.list -->
                                  tree nodes

                /* there need some way to identify whether a VMA is a tree node,head of a vm_set list,
                 * or just a member of a vm_set list.
                 * ! cant use vm_flags,when replace head to tree node,require to held two mmap_sem,but
                 *   actually we can not acquire the mmap_sem of head.
                 * 
                 * VMA radix priority search tree node rules :
                 *   vma->shared.vm_set.parent != NULL ==> a tree node
                 *     vma->shared.vm_set.head != NULL ==> list of others mapping same range
                 *     vma->shared.vm_set.head == NULL ==> no others map the same range
                 *
                 *   vma->shared.vm_set.parent == NULL
                 *     vma->shared.vm_set.head != NULL ==> list head of vmas mapping same range
                 *     vma->shared.vm_set.head == NULL ==> a list node
                 */

                /**
                 * vma_prio_tree_add - add a new vma known to map the same set of pages as the
                 *                     old vma
                 * @vma:               new vma
                 * @old:               old vma
                 * # @old is not tree node -> add @vma into @old->shared.vm_set.list
                 *   @old is tree node AND others mapping same range -> add @vma into @old->shared.vm_set.head->shared.vm_set.list
                 *   @old is tree node AND no others mapping same range -> @old become @vma->shared.vm_set.head,@vma become
                 *                                                         @old->shared.vm_set.head
                 *                    H-@old-...K => not tree node
                 *                      \
                 *                       add @vma
                 *                  
                 *                    \
                 *                     @old -> H-I-J-...Q -- add @vma
                 *
                 *                    \
                 *                     @old -> NULL
                 *                      \       \
                 *                       \       @vma as head
                 *                        \      /
                 *                         +--> head
                 */
                void vma_prio_tree_add(struct vm_area_struct *vma, struct vm_area_struct *old);

                /**
                 * vma_prio_tree_insert - insert new vma to the tree represented by root node
                 * @vma:                  the new vma
                 * @root:                 priority search tree's root
                 * # this routine invoke raw_prio_tree_insert() attempts to find out a proper position
                 *   for place @vma.if found,invoke vma_prio_tree_add() to add the @vma into the old
                 *   vma retrieved through prio_tree_entry() on the pointer
                 */
                void vma_prio_tree_insert(struct vm_area_struct *vma, struct prio_tree_root *root);

                /**
                 * vma_prio_tree_remove - remove a given node from PST
                 * @vma:                  the node is going to be removed
                 * @root:                 PST root
                 * # this routine relies on raw_prio_tree_remove() and raw_prio_tree_replace().
                 *   if @vma is not a head,then list delete it if it is not a tree node,or raw_prio_tree_remove()
                 *   it if it is a tree node.
                 *   for the case @vma is a head,things get more complex.
                 *   it is a head and also a tree node,we must adjust the head list and replace current tree node
                 *   with a node from the others list,this rely on raw_prio_tree_replace(),then adjust the head of
                 *   the old VMA which is the head of @vma,set its head to the new head.
                 *     ! new head => next element in vm_set.list of @vma->shared.vm_set.head->shared.
                 *                   new head is NULL if the list is empty.
                 *   it is a head but is not a tree node,we replace it with another node in the list,and adjust
                 *   head information.
                 */
                void vma_prio_tree_remove(struct vm_area_struct *vma, struct prio_tree_root *root);

                /**
                 * vma_prio_tree_next - return VMAs that at least map a single page in the given range of
                 *                      contiguous file pages.
                 * @vma:                the VMA at starting point
                 * @iter:               PST iterator
                 * return:              NULL OR next VMA
                 * # if first call is with NULL vma,the next is retrieved by prio_tree_next(@iter) or NULL.
                 *   if @vma is a tree node AND there is others mapping same range,next is head of @vma.
                 *   if @vma is not a tree node,next is @vma's @vm_set.list.next if it is a list node.
                 *   for other cases,invoke prio_tree_next() on @iter,if returned pointer is not NULL,next
                 *   is the VMA corresponding to the pointer,or NULL if no next.
                 */
                struct vm_area_struct *vma_prio_tree_next(struct vm_area_struct *vma, struct prio_tree_iter *iter);    

            The try_to_unmap_file() function :
              <mm/rmap.c>
                /**
                 * try_to_unmap_file - unmap/unlock file page using the object-based rmap method
                 * @page:              the page to unmap/unlock
                 * @flags:             action and flags
                 * return:             same as try_to_unmap(),except SWAP_SUCCESS
                 */
                static int try_to_unmap_file(struct page *page, enum ttu_flags flags);

                what try_to_unmap_file() does :
                  1> retrieve mapping of the file page through @page->mapping.
                     calculate @pages pgoff through "@page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT)".
                     /* if PAGE_CACHE_SHIFT == PAGE_SHIFT == 12 (4096kB),@page's index is its pgoff */
                  2> acquire @i_mmap_lock of the address_space.
                     invoke vma_prio_tree_foreach() pass it 
                       @vma(local pointer), &@iter(local PST iter), @mapping->i_mmap, @pgoff(@page's pgoff),
                       @pgoff                                                 #^the PST of mapping   #^begin
                       #^end
                     for each VMA mapped this file page { /* deal with linear file page mapping */
                             invoke vma_address() on current VMA and the @page,continue to next if -EFAULT is
                             returned by this routine.
                             for the good VMA,invoke try_to_unmap_one() on it.
                             if result of try_to_unmap_one() is not SWAP_AGAIN OR @page is not mapped,goto label
                             "out".                             /* error */       /* page is no longer mapped,
                                                                                   * @_mapcount is -1.
                                                                                   */
                     }
                  3> check @i_mmap_nonlinear of @mapping,if the list is empty,goto label "out".
                                                                        /* no nonlinear file mapping */
                  4> /* the case @page is still mapped AND last result is SWAP_AGAIN AND @i_mmap_nonlinear is not empty,
                      * we have to deal with nonlinear mapping.
                      */
                     if TTU_MUNLOCK is set,goto label "out",because we do not bother to try to find the munlocked page
                     in nonlinears. /* TTU_MUNLOCK may called by try_to_unmap() and recover PG_mlocked lazily */
                     traverse list @i_mmap_nonlinear of @mapping,the connector is @shared.vm_set.list {
                             /* the goal of this traversing is to find out the maximum nonlinear mapping size,
                              * and record the maximum nonlinear mapping cursor for further virtual scanning.
                              */

                             for current VMA,retrieve its @vm_private_data store in local variable @cursor,it is type of
                             unsigned long.
                             reset local variable @max_nl_cursor to @cursor,if @cursor is greater than @max_nl_cursor.
                             /* default value of @max_nl_cursor is 0 */
                             /* @max_nl_cursor => maxinum of nonlinear mapping cursor */
                             reset @cursor to the result of VMA->vm_end - VMA->vm_start,the length of interval.
                             reset @max_nl_size to @cursor,if @cursor is greater than @max_nl_size.
                             /* default value of @max_nl_size is 0 */
                             /* @max_nl_size => maximum of nonlinear mapping size */
                     }
                  5> if @max_nl_size is _zero_ => all nonlinears locked or reserved,set return value to SWAP_FAIL and
                     goto "out".
                     get map count of @page through page_mapcout(),store in @mapcount the local variable.
                     if the value is 0 => no one is mapped it,then goto "out".
                  6> check kernel preempt,but acquire @i_mmap_lock of the @mapping.
                  7> reset @max_nl_size to the result of "@max_nl_size + CLUSTER_SIZE - 1) & CLUSTER_MASK".
                     if @max_nl_cursor is _zero_,reset it to CLUSTER_SIZE. /* overflow */
                     /**
                      * CLUSTER_SIZE => min(32 * PAGE_SIZE, PMD_SIZE)
                      * CLUSTER_MASK => (~(CLUSTER_SIZE - 1))
                      */
                  8> make use of a do-while cycle,stop condition is @max_nl_cursor > @max_nl_size {
                             traverse list @mapping->i_mmap_nonlinear,connector is @shared.vm_set.list {
                                     /* the next cluster cursor */
                                     retrieve current VMA' @vm_private_data store in @cursor.
                                     make use of a while-cycle,stop condition is "@cursor >= @max_nl_cursor OR
                                     @cursor >= current VMA's @vm_end - @vm_start" {
                                                              /* memory region length */

                                             invoke try_to_unmap_cluster(),pass it current @cursor, &@mapcount,
                                             current VMA, @page as the parameters.
                                             if the result is SWAP_MLOCK,reset return value to SWAP_MLOCK.
                                             update @cursor,let it add CLUSTER_SIZE.
                                             /* once try_to_unmap_cluster() we only handle CLUSTER_SIZE
                                              * chunk of memory in the memory region
                                              * usually is 32 * 4096 = 131072kB(128MB)
                                              */
                                             modify current VMA's @vm_private_data to the updated @cursor.
                                             /* record the cursor we have reached.
                                              * if we restart virtual scanning on this VMA,we have to start
                                              * at this cursor.
                                              */
                                             check @mapcount,if it is less than or equal to 0,goto label "out".
                                                   /* page un-mapped */
                                     }
                                     modify current VMA's @vm_private_data to @max_nl_cursor.
                                             /**
                                              * if the while-cycle() is end,that means we have reached the
                                              * maximum cursor,so we will reset the cursor of this VMA
                                              * to the @max_nl_cursor.
                                              */
                             }
                             check kernel preempt,but acquire @i_mmap_lock of @mapping.
                             update @max_nl_cursor,let it add CLUSTER_SIZE. /* the maximum cursor we have reached */
                               /* note,this updating is happens after we traversed all elements on the list,
                                * so the next traversing(if have),virtual scanning will scan larger chunk of memory.
                                * ! but never exceed the size of VMA.
                                */
                     }
                  9> traverse @mapping->i_mmap_nonlinear,for each VMA,set its @vm_private_data to NULL.
                  out:
                 10> release @i_mmap_lock and return the return value to caller.

            The try_to_unmap_cluster() function :

                                                                 +--> min(32 * PAGE_SIZE, PMD_SIZE)
                                                                 |
                                          +--> floor((@address + CLUSTER_SIZE) / PAGE_SIZE)
                                          |
                    |<------------------- @ ----------------->|
              +-----------------------------------------------------+
              |     |        |        |        |     |        |     |
              | ... | chunk0 | chunk1 | chunk2 | ... | chunkN | ... | => VMA
              |     |        |        |        |     |        |     |
              +-----------------------------------------------------+
                    | |        |        |              |      |
                    | p        p        p              p      |
                    | a        a        a              a      |
                    | g        g        g              g      |
                    | e        e        e              e      |
                    | 0        1        2              N      |
                    |                                         |
                    | ^        ^        ^              ^      |
                    | |        |        |              |      +--> end <= @address + CLUSTER_SIZE
                    | PTE0     PTE1     PTE2           PTE_N
                    |
                    +--> addres <= (current cursor + @vm_start) & CLUSTER_MASK

              object-based rmap does not work for nonlinear VMAs because the assumption that offset-into-file
              correlates with offset-into-virtual-addresses does not hold.
              consequently,given a particular page and its @index,we can not locate the PTEs which are mapping
              that page without an exhaustive linear search.
              let @vm_private_data to holds the current cursor into the mini "virtual scan" of each nonliner
              VMA which maps the file to which the target page belongs,successive searches will circulate
              around the VMA's virtual address space.

              mlocked pages : check VM_LOCKED under @mmap_sem held for read if we can acquire it without blocking.
                              for VM_LOCKED VMAs,mlock the pages in the cluster ranther than unmapping them.

              <mm/rmap.c>
                /**
                 * try_to_unmap_cluster - virtual scan and try to unmap file page in the memory region
                 * @cursor:               current cursor
                 * @mapcount:             file page's mapcount
                 * @vma:                  the VMA
                 * @check_page:           the page have to be checked
                 * return:                same as try_to_unmap(),except SWAP_SUCCESS
                 * # try_to_unmap_cluster() may missing the page to be unmapped,in this case,@_mapcount of the page
                 *   must be not -1,thus try_to_unmap() will detects this and return SWAP_AGAIN.
                 */
                static int try_to_unmap_cluster(unsigned long cursor, unsigned int *mapcout, struct vm_area_struct *vma,
                                                struct page *check_page);

                what try_to_unmap_cluster() does :
                  1> calculate address corresponding to current @cursor in @vma through "(@vm_start + @cursor) & CLUSTER_MASK".
                     the end address of cluster is @address + CLUSTER_SIZE.
                     if @address less than @vm_start,make use of @vm_start,and same as end address(@vm_end).
                  2> get PGD,PUD,PMD corresponding to @address.either of them is not present,return SWAP_AGAIN.
                     try to lock @mmap_sem of @vm_mm of @vma for READ.
                     if we succeed to acquire the semaphore,then check VM_LOCKED for @vma,if it is set,let
                     local variable @locked_vma to 1,and we would not release semaphore for now;otherwise,release semaphore,
                     in this case,@locked_vma == 0.
                  3> pte_offset_map_lock() on PMD and @address to map the PTE that corresponds to @address and lock the pt lock.
                     invoke update_hiwater_rss() for @mm.
                  4> for each chunk in PAGE_SIZE from @address to @end {
                             /* before the next iteration starting,++@pte => next PTE
                              *            (next,not the first)
                              */

                             check current *@pte if present,if it is not present,we skip current chunk.
                             invoke vm_normal_page() on current @pte to get the page descriptor that associated with this
                             @pte in @vma,store it in @page the local variable.
                             do BUG test,it is a BUG if @page is NULL(but PTE is present) OR @page is anonymous page(we dealing with
                             file page).

                             if @locked_vma is TRUE,then
                                     lock @page through mlock_vma_page().
                                     if @page is equal to @check_page,our target,then set return value to SWAP_MLOCK.
                                     continue to next chunk.

                             /* @locked_vm is FALSE */
                             invoke ptep_clear_flush_young_notify() on @pte,if result is 1 => we cleared _PAGE_ACCESSED,
                             then continue to next chunk.

                             /* we failed to clear _PAGE_ACCESSED because it is not a young page */
                             invoke flush_cache_page() on @pte => on x86,it is NOP.
                             invoke ptep_clear_flush_notify() on @pte,the old value is stored in @pteval the local variable.

                             check @page's @index,if it is not equal to linear_page_index() on @address,that means it is not
                             an linear mapping,so call to set_pte_at() to use @page's @index to set current @pte.
                                                                              /* pgoff_to_pte() */
                                                                              /* @pte_high member of PTE structure */
                             
                             check if the old PTE is dirty(@pteval),then set PG_dirty for @page.
                             
                             /* last work */
                             invoke page_remove_rmap() on @page.
                             put page refcount of @page.
                             decrease MM_FILEPAGES counter of @mm.
                             decrease *@mapcount.
                     }
                  5> invoke pte_unmap_unlock() on "@pte - 1" because the cycle in step 4> will let PTE + 1 before stop condition
                     checking is processed,and release pt lock.
                     if @locked_vma is TRUE,that means we are holding @mmap_sem for READ,so release it.
                     return the return value to caller.

      Implementing the PFRA :
        PFRA is composed of a large number of functions,because it must take care of many kinds of pages owned by User Mode
        processes,disk cache,and memory caches;moreover,it has to obey several heuristic rules.

        figure : /* FROM THE BOOK,Figure 17-3 */

                LOW ON MEMORY RECLAIMING                                HIBERNATION RECLAIMING                PERIODIC RECLAIMING
               
                low memory on           low memory on                   suspend to disk(hibernation)          kswapd   reap_work
                buffer allocation       page allocation                                                                work queue
                __getblk()
                alloc_page_buffers()    __alloc_pages()                 pm_suspend_disk()
                |                       |                               |
                |                       |i                              |                                     kswapd() cache_reap()
                |                       |n                              |                                     |        |
                |                       |v                              |                                     |        |
                V                       |o                              |                                     |        |
                free_more_memory()      |k                              |      +------------------------------+        |
                     |                  |e                              |      |                                       |
                     |                  V                               V      V                                       |
                     +----------> try_to_free_pages()                   balance_pgdat()                                |
                                  |     |     |                            |   |                                       |
                                  |     |     |                            |   |                                       |
                                  |     |     |                            |   |                                       V
                        +---------+     |     |                            |   |                                      slab_destroy()
                        |               |     +---------> shrink_slab() <--+   |
                        |               V                                      |
                        V             shrink_caches()                          |
                     out_of_memory()    |                                      |
                                        |                                      |
                                        |                                      V
                                        +--------------------------------> shrink_zone()
                                                                            |      |
                                                                            |      |
                                                                            |      +----> shrink_cache()
                                                                            |                |
                                                                            |                |
                                                                +-----------+                |
                                                                |                            V
                                                                |                         shrink_list() ----+
                                                                V                         |                 |
                                                           refill_inactive_zone()         |                 |
                                                                    |                     |                 |
                                                                    |                     V                 V
                                                                    +----> page_referenced()              pageout()

                !! LINUX 2.6.34.1 HAS NO FUNCTION pm_suspend_disk().INSTEAD,WHEN SUSPEND TO DISK,FUNCTION hibernate() IS
                   CALLED,WHICH IS DEFINED IN <kernel/power/hibernate.c>.THE FUNCTION CALL TO create_basic_memory_bitmaps(),
                   AND WHICH IS DEFINED IN <kernel/power/snapshot.c>.THE GOAL OF THIS FUNCTION IS TO CREATE BITMAPS NEEDED FOR
                   MARKING PAGE FRAMES THAT SHOULD NOT BE SAVED AND FREE PAGE FRAMES.
                   WHEN KERNEL RESUME FROM HIBERNATION,FUNCTION free_basic_memory_bitmaps() IS INVOKED,THEN THE PAGE FRAMES MARKED
                   NEED TO BE FREED IS RECLAIMED BY memory_bm_free() THROUGH __free_page().


        Page frame reclaiming is performed on essentially three occasions :
          1> Low on memory reclaiming
               kernel detects a "low on memory" condition.
          2> Hibernation reclaiming
               kernel must free memory because it is entering in the suspend-to-disk state.
          3> Periodic reclaiming
               a kernel thread is activated periodically to perform memory reclaiming,if necessary.

        Low on Memory reclaiming cases :
          1> grow_buffers() fails to allocate a new buffer page.
          2> alloc_page_buffers() fails to allocate the temporary buffer heads for a page.
          3> __alloc_pages() fails in allocating a group of contiguous page frames in a given list of
             memory zones.

        Periodic reclaiming is activatd by two different kthreads :
          1> the "kswapd" kernel threads,which check whether the number of free page frames in some
             memory zone has fallen below the "pages_high" watermark.
          2> the "events" kernel threads,which are the worker threads of the predefined work queue.
             PFRA periodically schedules the execution of a task in the predefined work queue to
             reclaim all free slabs included in the memory caches handled by the slab allocator.